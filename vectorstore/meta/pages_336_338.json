{
  "doc_id": "pages_336_338",
  "text": "314\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 6.3\nA simple panograph consisting of three images automatically aligned with a\ntranslational model and then averaged together.\nσ2\nn (8.44). Weighting each squared residual by its inverse covariance Σ−1\ni\n= σ−2\nn Ai (which\nis called the information matrix), we obtain\nECWLS =\nX\ni\n∥ri∥2\nΣ−1\ni\n=\nX\ni\nrT\ni Σ−1\ni ri =\nX\ni\nσ−2\nn rT\ni Airi.\n(6.11)\n6.1.2 Application: Panography\nOne of the simplest (and most fun) applications of image alignment is a special form of image\nstitching called panography. In a panograph, images are translated and optionally rotated and\nscaled before being blended with simple averaging (Figure 6.3). This process mimics the\nphotographic collages created by artist David Hockney, although his compositions use an\nopaque overlay model, being created out of regular photographs.\nIn most of the examples seen on the Web, the images are aligned by hand for best artistic\neffect.4 However, it is also possible to use feature matching and alignment techniques to\nperform the registration automatically (Nomura, Zhang, and Nayar 2007; Zelnik-Manor and\nPerona 2007).\nConsider a simple translational model. We want all the corresponding features in different\nimages to line up as best as possible. Let tj be the location of the jth image coordinate frame\nin the global composite frame and xij be the location of the ith matched feature in the jth\nimage. In order to align the images, we wish to minimize the least squares error\nEPLS =\nX\nij\n∥(tj + xij) −xi∥2,\n(6.12)\n4 http://www.ﬂickr.com/groups/panography/.\n6.1 2D and 3D feature-based alignment\n315\nwhere xi is the consensus (average) position of feature i in the global coordinate frame.\n(An alternative approach is to register each pair of overlapping images separately and then\ncompute a consensus location for each frame—see Exercise 6.2.)\nThe above least squares problem is indeterminate (you can add a constant offset to all the\nframe and point locations tj and xi). To ﬁx this, either pick one frame as being at the origin\nor add a constraint to make the average frame offsets be 0.\nThe formulas for adding rotation and scale transformations are straightforward and are\nleft as an exercise (Exercise 6.2). See if you can create some collages that you would be\nhappy to share with others on the Web.\n6.1.3 Iterative algorithms\nWhile linear least squares is the simplest method for estimating parameters, most problems in\ncomputer vision do not have a simple linear relationship between the measurements and the\nunknowns. In this case, the resulting problem is called non-linear least squares or non-linear\nregression.\nConsider, for example, the problem of estimating a rigid Euclidean 2D transformation\n(translation plus rotation) between two sets of points. If we parameterize this transformation\nby the translation amount (tx, ty) and the rotation angle θ, as in Table 2.1, the Jacobian of\nthis transformation, given in Table 6.1, depends on the current value of θ. Notice how in\nTable 6.1, we have re-parameterized the motion matrices so that they are always the identity\nat the origin p = 0, which makes it easier to initialize the motion parameters.\nTo minimize the non-linear least squares problem, we iteratively ﬁnd an update ∆p to the\ncurrent parameter estimate p by minimizing\nENLS(∆p)\n=\nX\ni\n∥f(xi; p + ∆p) −x′\ni∥2\n(6.13)\n≈\nX\ni\n∥J(xi; p)∆p −ri∥2\n(6.14)\n=\n∆pT\n\"X\ni\nJT J\n#\n∆p −2∆pT\n\"X\ni\nJT ri\n#\n+\nX\ni\n∥ri∥2\n(6.15)\n=\n∆pT A∆p −2∆pT b + c,\n(6.16)\nwhere the “Hessian”5 A is the same as Equation (6.9) and the right hand side vector\nb =\nX\ni\nJT (xi)ri\n(6.17)\n5 The “Hessian” A is not the true Hessian (second derivative) of the non-linear least squares problem (6.13).\nInstead, it is the approximate Hessian, which neglects second (and higher) order derivatives of f(xi; p + ∆p).\n316\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nis now a Jacobian-weighted sum of residual vectors. This makes intuitive sense, as the pa-\nrameters are pulled in the direction of the prediction error with a strength proportional to the\nJacobian.\nOnce A and b have been computed, we solve for ∆p using\n(A + λdiag(A))∆p = b,\n(6.18)\nand update the parameter vector p ←p + ∆p accordingly. The parameter λ is an addi-\ntional damping parameter used to ensure that the system takes a “downhill” step in energy\n(squared error) and is an essential component of the Levenberg–Marquardt algorithm (de-\nscribed in more detail in Appendix A.3). In many applications, it can be set to 0 if the system\nis successfully converging.\nFor the case of our 2D translation+rotation, we end up with a 3×3 set of normal equations\nin the unknowns (δtx, δty, δθ). An initial guess for (tx, ty, θ) can be obtained by ﬁtting a\nfour-parameter similarity transform in (tx, ty, c, s) and then setting θ = tan−1(s/c). An\nalternative approach is to estimate the translation parameters using the centroids of the 2D\npoints and to then estimate the rotation angle using polar coordinates (Exercise 6.3).\nFor the other 2D motion models, the derivatives in Table 6.1 are all fairly straightforward,\nexcept for the projective 2D motion (homography), which arises in image-stitching applica-\ntions (Chapter 9). These equations can be re-written from (2.21) in their new parametric form\nas\nx′ = (1 + h00)x + h01y + h02\nh20x + h21y + 1\nand y′ = h10x + (1 + h11)y + h12\nh20x + h21y + 1\n.\n(6.19)\nThe Jacobian is therefore\nJ = ∂f\n∂p = 1\nD\n\"\nx\ny\n1\n0\n0\n0\n−x′x\n−x′y\n0\n0\n0\nx\ny\n1\n−y′x\n−y′y\n#\n,\n(6.20)\nwhere D = h20x + h21y + 1 is the denominator in (6.19), which depends on the current\nparameter settings (as do x′ and y′).\nAn initial guess for the eight unknowns {h00, h01, . . . , h21} can be obtained by multiply-\ning both sides of the equations in (6.19) through by the denominator, which yields the linear\nset of equations,\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\nx\ny\n1\n0\n0\n0\n−ˆx′x\n−ˆx′y\n0\n0\n0\nx\ny\n1\n−ˆy′x\n−ˆy′y\n#\n\n\nh00\n...\nh21\n\n.\n(6.21)\nHowever, this is not optimal from a statistical point of view, since the denominator D, which\nwas used to multiply each equation, can vary quite a bit from point to point.6\n6 Hartley and Zisserman (2004) call this strategy of forming linear equations from rational equations the direct",
  "image_path": "page_337.jpg",
  "pages": [
    336,
    337,
    338
  ]
}