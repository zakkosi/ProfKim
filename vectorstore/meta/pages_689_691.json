{
  "doc_id": "pages_689_691",
  "text": "14.1 Object detection\n667\nFigure 14.8d shows a sample input image, while Figure 14.8e shows the associated HOG\ndescriptors.\nOnce the descriptors have been computed, a support vector machine (SVM) is trained\non the resulting high-dimensional continuous descriptor vectors. Figures 14.8b–c show a\ndiagram of the (most) positive and negative SVM weights in each block, while Figures 14.8f–\ng show the corresponding weighted HOG responses for the central input image. As you can\nsee, there are a fair number of positive responses around the head, torso, and feet of the\nperson, and relatively few negative responses (mainly around the middle and the neck of the\nsweater).\nThe ﬁelds of pedestrian and general object detection have continued to evolve rapidly\nover the last decade (Belongie, Malik, and Puzicha 2002; Mikolajczyk, Schmid, and Zis-\nserman 2004; Leibe, Seemann, and Schiele 2005; Opelt, Pinz, and Zisserman 2006; Tor-\nralba 2007; Andriluka, Roth, and Schiele 2009, 2010; Doll`ar, Belongie, and Perona 2010).\nMunder and Gavrila (2006) compare a number of pedestrian detectors and conclude that\nthose based on local receptive ﬁelds and SVMs perform the best, with a boosting-based ap-\nproach coming close. Maji, Berg, and Malik (2008) improve on the best of these results using\nnon-overlapping multi-resolution HOG descriptors and a histogram intersection kernel SVM\nbased on a spatial pyramid match kernel from Lazebnik, Schmid, and Ponce (2006).\nWhen detectors for several different classes are being constructed simultaneously, Tor-\nralba, Murphy, and Freeman (2007) show that sharing features and weak learners between\ndetectors yields better performance, both in terms of faster computation times and fewer\ntraining examples. To ﬁnd the features and decision stumps that work best in a shared man-\nner, they introduce a novel joint boosting algorithm that optimizes, at each stage, a summed\nexpected exponential loss function using the “gentleboost” algorithm of Friedman, Hastie,\nand Tibshirani (2000).\nIn more recent work, Felzenszwalb, McAllester, and Ramanan (2008) extend the his-\ntogram of oriented gradients person detector to incorporate ﬂexible parts models (Section 14.4.2).\nEach part is trained and detected on HOGs evaluated at two pyramid levels below the overall\nobject model and the locations of the parts relative to the parent node (the overall bounding\nbox) are also learned and used during recognition (Figure 14.9b). To compensate for inac-\ncuracies or inconsistencies in the training example bounding boxes (dashed white lines in\nFigure 14.9c), the “true” location of the parent (blue) bounding box is considered a latent\n(hidden) variable and is inferred during both training and recognition. Since the locations\nof the parts are also latent, the system can be trained in a semi-supervised fashion, without\nneeding part labels in the training data. An extension to this system (Felzenszwalb, Girshick,\nMcAllester et al. 2010), which includes among its improvements a simple contextual model,\nwas among the two best object detection systems in the 2008 Visual Object Classes detection\nchallenge. Other recent improvements to part-based person detection and pose estimation in-\n668\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.9\nPart-based object detection (Felzenszwalb, McAllester, and Ramanan 2008)\nc⃝2008 IEEE: (a) An input photograph and its associated person (blue) and part (yellow)\ndetection results. (b) The detection model is deﬁned by a coarse template, several higher\nresolution part templates, and a spatial model for the location of each part. (c) True positive\ndetection of a skier and (d) false positive detection of a cow (labeled as a person).\nclude the work by Andriluka, Roth, and Schiele (2009) and Kumar, Zisserman, and H.S.Torr\n(2009).\nAn even more accurate estimate of a person’s pose and location is presented by Rogez,\nRihan, Ramalingam et al. (2008), who compute both the phase of a person in a walk cycle and\nthe locations of individual joints, using random forests built on top of HOGs (Figure 14.11).\nSince their system produces full 3D pose information, it is closer in its application domain to\n3D person trackers (Sidenbladh, Black, and Fleet 2000; Andriluka, Roth, and Schiele 2010),\nwhich we discussed in Section 12.6.4.\nOne ﬁnal note on person and object detection. When video sequences are available, the\nadditional information present in the optic ﬂow and motion discontinuities can greatly aid in\nthe detection task, as discussed by Efros, Berg, Mori et al. (2003), Viola, Jones, and Snow\n(2003), and Dalal, Triggs, and Schmid (2006).\n14.2 Face recognition\nAmong the various recognition tasks that computers might be asked to perform, face recog-\nnition is the one where they have arguably had the most success.5 While computers cannot\npick out suspects from thousands of people streaming in front of video cameras (even people\ncannot readily distinguish between similar people with whom they are not familiar (O’Toole,\nJiang, Roark et al. 2006; O’Toole, Phillips, Jiang et al. 2009)), their ability to distinguish\n5Instance recognition, i.e., the re-recognition of known objects such as locations or planar objects, is the other\nmost successful application of general image recognition. In the general domain of biometrics, i.e., identity recogni-\ntion, specialized images such as irises and ﬁngerprints perform even better (Jain, Bolle, and Pankanti 1999; Pankanti,\nBolle, and Jain 2000; Daugman 2004).\n14.2 Face recognition\n669\nFigure 14.10\nPart-based object detection results for people, bicycles, and horses (Felzen-\nszwalb, McAllester, and Ramanan 2008) c⃝2008 IEEE. The ﬁrst three columns show correct\ndetections, while the rightmost column shows false positives.\nFigure 14.11 Pose detection using random forests (Rogez, Rihan, Ramalingam et al. 2008)\nc⃝2008 IEEE. The estimated pose (state of the kinematic model) is drawn over each input\nframe.",
  "image_path": "page_690.jpg",
  "pages": [
    689,
    690,
    691
  ]
}