{
  "doc_id": "pages_152_154",
  "text": "130\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n.\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n1\n2\n0\n0\n0\n0\n1\n1\n2\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1\n2\n2\n3\n1\n0\n0\n1\n2\n2\n3\n1\n0\n0\n1\n2\n2\n2\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1\n2\n3\n0\n1\n2\n2\n1\n1\n0\n0\n1\n2\n2\n1\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n2\n1\n0\n0\n0\n0\n1\n2\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n(a)\n(b)\n(c)\n(d)\nFigure 3.22\nCity block distance transform: (a) original binary image; (b) top to bottom\n(forward) raster sweep: green values are used to compute the orange value; (c) bottom to top\n(backward) raster sweep: green values are merged with old orange value; (d) ﬁnal distance\ntransform.\nEfﬁciently computing the Euclidean distance transform is more complicated. Here, just\nkeeping the minimum scalar distance to the boundary during the two passes is not sufﬁcient.\nInstead, a vector-valued distance consisting of both the x and y coordinates of the distance\nto the boundary must be kept and compared using the squared distance (hypotenuse) rule. As\nwell, larger search regions need to be used to obtain reasonable results. Rather than explaining\nthe algorithm (Danielsson 1980; Borgefors 1986) in more detail, we leave it as an exercise\nfor the motivated reader (Exercise 3.13).\nFigure 3.11g shows a distance transform computed from a binary image. Notice how\nthe values grow away from the black (ink) regions and form ridges in the white area of the\noriginal image. Because of this linear growth from the starting boundary pixels, the distance\ntransform is also sometimes known as the grassﬁre transform, since it describes the time at\nwhich a ﬁre starting inside the black region would consume any given pixel, or a chamfer,\nbecause it resembles similar shapes used in woodworking and industrial design. The ridges\nin the distance transform become the skeleton (or medial axis transform (MAT)) of the region\nwhere the transform is computed, and consist of pixels that are of equal distance to two (or\nmore) boundaries (Tek and Kimia 2003; Sebastian and Kimia 2005).\nA useful extension of the basic distance transform is the signed distance transform, which\ncomputes distances to boundary pixels for all the pixels (Lavall´ee and Szeliski 1995). The\nsimplest way to create this is to compute the distance transforms for both the original bi-\nnary image and its complement and to negate one of them before combining. Because such\ndistance ﬁelds tend to be smooth, it is possible to store them more compactly (with mini-\nmal loss in relative accuracy) using a spline deﬁned over a quadtree or octree data structure\n(Lavall´ee and Szeliski 1995; Szeliski and Lavall´ee 1996; Frisken, Perry, Rockwood et al.\n2000). Such precomputed signed distance transforms can be extremely useful in efﬁciently\naligning and merging 2D curves and 3D surfaces (Huttenlocher, Klanderman, and Rucklidge\n3.3 More neighborhood operators\n131\n(a)\n(b)\n(c)\nFigure 3.23 Connected component computation: (a) original grayscale image; (b) horizontal\nruns (nodes) connected by vertical (graph) edges (dashed blue)—runs are pseudocolored with\nunique colors inherited from parent nodes; (c) re-coloring after merging adjacent segments.\n1993; Szeliski and Lavall´ee 1996; Curless and Levoy 1996), especially if the vectorial version\nof the distance transform, i.e., a pointer from each pixel or voxel to the nearest boundary or\nsurface element, is stored and interpolated. Signed distance ﬁelds are also an essential com-\nponent of level set evolution (Section 5.1.4), where they are called characteristic functions.\n3.3.4 Connected components\nAnother useful semi-global image operation is ﬁnding connected components, which are de-\nﬁned as regions of adjacent pixels that have the same input value (or label). (In the remainder\nof this section, consider pixels to be adjacent if they are immediate N4 neighbors and they\nhave the same input value.) Connected components can be used in a variety of applications,\nsuch as ﬁnding individual letters in a scanned document or ﬁnding objects (say, cells) in a\nthresholded image and computing their area statistics.\nConsider the grayscale image in Figure 3.23a. There are four connected components in\nthis ﬁgure: the outermost set of white pixels, the large ring of gray pixels, the white enclosed\nregion, and the single gray pixel. These are shown pseudocolored in Figure 3.23c as pink,\ngreen, blue, and brown.\nTo compute the connected components of an image, we ﬁrst (conceptually) split the image\ninto horizontal runs of adjacent pixels, and then color the runs with unique labels, re-using\nthe labels of vertically adjacent runs whenever possible. In a second phase, adjacent runs of\ndifferent colors are then merged.\nWhile this description is a little sketchy, it should be enough to enable a motivated stu-\ndent to implement this algorithm (Exercise 3.14). Haralick and Shapiro (1992, Section 2.3)\ngive a much longer description of various connected component algorithms, including ones\n132\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nthat avoid the creation of a potentially large re-coloring (equivalence) table. Well-debugged\nconnected component algorithms are also available in most image processing libraries.\nOnce a binary or multi-valued image has been segmented into its connected components,\nit is often useful to compute the area statistics for each individual region R. Such statistics\ninclude:\n• the area (number of pixels);\n• the perimeter (number of boundary pixels);\n• the centroid (average x and y values);\n• the second moments,\nM =\nX\n(x,y)∈R\n\"\nx −x\ny −y\n# h\nx −x\ny −y\ni\n,\n(3.46)\nfrom which the major and minor axis orientation and lengths can be computed using\neigenvalue analysis.7\nThese statistics can then be used for further processing, e.g., for sorting the regions by the area\nsize (to consider the largest regions ﬁrst) or for preliminary matching of regions in different\nimages.\n3.4 Fourier transforms\nIn Section 3.2, we mentioned that Fourier analysis could be used to analyze the frequency\ncharacteristics of various ﬁlters. In this section, we explain both how Fourier analysis lets us\ndetermine these characteristics (or equivalently, the frequency content of an image) and how\nusing the Fast Fourier Transform (FFT) lets us perform large-kernel convolutions in time that\nis independent of the kernel’s size. More comprehensive introductions to Fourier transforms\nare provided by Bracewell (1986); Glassner (1995); Oppenheim and Schafer (1996); Oppen-\nheim, Schafer, and Buck (1999).\nHow can we analyze what a given ﬁlter does to high, medium, and low frequencies? The\nanswer is to simply pass a sinusoid of known frequency through the ﬁlter and to observe by\nhow much it is attenuated. Let\ns(x) = sin(2πfx + φi) = sin(ωx + φi)\n(3.47)\n7 Moments can also be computed using Green’s theorem applied to the boundary pixels (Yang and Albregtsen\n1996).",
  "image_path": "page_153.jpg",
  "pages": [
    152,
    153,
    154
  ]
}