{
  "doc_id": "pages_582_584",
  "text": "560\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nA\nE\nD\nC\nB\nleft\nmiddle\nright\nF\nx\nt\n(a)\n(b)\nFigure 11.15 Epipolar plane image (EPI) (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996\nACM and a schematic EPI (Kang, Szeliski, and Chai 2001) c⃝2001 IEEE. (a) The Lumigraph\n(light ﬁeld) (Section 13.3) is the 4D space of all light rays passing through a volume of space.\nTaking a 2D slice results in all of the light rays embedded in a plane and is equivalent to a\nscanline taken from a stacked EPI volume. Objects at different depths move sideways with\nvelocities (slopes) proportional to their inverse depth. Occlusion (and translucency) effects\ncan easily be seen in this representation. (b) The EPI corresponding to Figure 11.16 showing\nthe three images (middle, left, and right) as slices through the EPI volume. The spatially and\ntemporally shifted window around the black pixel is indicated by the rectangle, showing the\nright image is not being used in matching.\nstrips in the EPI. If we are given a dense enough set of images, we can ﬁnd such strips and\nreason about their relationships in order to both reconstruct the 3D scene and make inferences\nabout translucent objects (Tsin, Kang, and Szeliski 2006) and specular reﬂections (Swami-\nnathan, Kang, Szeliski et al. 2002; Criminisi, Kang, Swaminathan et al. 2005). Alternatively,\nwe can treat the series of images as a set of sequential observations and merge them using\nKalman ﬁltering (Matthies, Kanade, and Szeliski 1989) or maximum likelihood inference\n(Cox 1994).\nWhen fewer images are available, it becomes necessary to fall back on aggregation tech-\nniques such as sliding windows or global optimization. With additional input images, how-\never, the likelihood of occlusions increases. It is therefore prudent to adjust not only the best\nwindow locations using a shiftable window approach, as shown in Figure 11.16a, but also to\noptionally select a subset of neighboring frames in order to discount those images where the\nregion of interest is occluded, as shown in Figure 11.16b (Kang, Szeliski, and Chai 2001).\nthere is enough information in a light ﬁeld to recover both the shape and the BRDF of objects (Soatto, Yezzi, and Jin\n2003), although relatively little progress has been made to date on this topic.\n11.6 Multi-view stereo\n561\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\n(a)\n(b)\nFigure 11.16 Spatio-temporally shiftable windows (Kang, Szeliski, and Chai 2001) c⃝2001\nIEEE: A simple three-image sequence (the middle image is the reference image), which has\na moving frontal gray square (marked F) and a stationary background. Regions B, C, D, and\nE are partially occluded. (a) A regular SSD algorithm will make mistakes when matching\npixels in these regions (e.g. the window centered on the black pixel in region B) and in\nwindows straddling depth discontinuities (the window centered on the white pixel in region\nF). (b) Shiftable windows help mitigate the problems in partially occluded regions and near\ndepth discontinuities. The shifted window centered on the white pixel in region F matches\ncorrectly in all frames. The shifted window centered on the black pixel in region B matches\ncorrectly in the left image, but requires temporal selection to disable matching the right image.\nFigure 11.15b shows an EPI corresponding to this sequence and describes in more detail how\ntemporal selection works.\nFigure11.15b shows how such spatio-temporal selection or shifting of windows corresponds\nto selecting the most likely un-occluded volumetric region in the epipolar plane image vol-\nume.\nThe results of applying these techniques to the multi-frame ﬂower garden image sequence\nare shown in Figure 11.17, which compares the results of using regular (non-shifted) SSSD\nwith spatially shifted windows and full spatio-temporal window selection.\n(The task of\napplying stereo to a rigid scene ﬁlmed with a moving camera is sometimes called motion\nstereo). Similar improvements from using spatio-temporal selection are reported by (Kang\nand Szeliski 2004) and are evident even when local measurements are combined with global\noptimization.\nWhile computing a depth map from multiple inputs outperforms pairwise stereo match-\ning, even more dramatic improvements can be obtained by estimating multiple depth maps\nsimultaneously (Szeliski 1999; Kang and Szeliski 2004). The existence of multiple depth\nmaps enables more accurate reasoning about occlusions, as regions which are occluded in\none image may be visible (and matchable) in others. The multi-view reconstruction problem\ncan be formulated as the simultaneous estimation of depth maps at key frames (Figure 8.13c)\nwhile maximizing not only photoconsistency and piecewise disparity smoothness but also the\nconsistency between disparity estimates at different frames. While Szeliski (1999) and Kang\n562\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 11.17 Local (5 × 5 window-based) matching results (Kang, Szeliski, and Chai 2001)\nc⃝2001 IEEE: (a) window that is not spatially perturbed (centered); (b) spatially perturbed\nwindow; (c) using the best ﬁve of 10 neighboring frames; (d) using the better half sequence.\nNotice how the results near the tree trunk are improved using temporal selection.\nand Szeliski (2004) use soft (penalty-based) constraints to encourage multiple disparity maps\nto be consistent, Kolmogorov and Zabih (2002) show how such consistency measures can\nbe encoded as hard constraints, which guarantee that the multiple depth maps are not only\nsimilar but actually identical in overlapping regions. Newer algorithms that simultaneously\nestimate multiple disparity maps include papers by Maitre, Shinagawa, and Do (2008) and\nZhang, Jia, Wong et al. (2008).\nA closely related topic to multi-frame stereo estimation is scene ﬂow, in which multiple\ncameras are used to capture a dynamic scene. The task is then to simultaneously recover the\n3D shape of the object at every instant in time and to estimate the full 3D motion of every\nsurface point between frames. Representative papers in this area include those by Vedula,\nBaker, Rander et al. (2005), Zhang and Kambhamettu (2003), Pons, Keriven, and Faugeras\n(2007), Huguet and Devernay (2007), and Wedel, Rabe, Vaudrey et al. (2008). Figure 11.18a\nshows an image of the 3D scene ﬂow for the tango dancer shown in Figure 11.2h–j, while\nFigure 11.18b shows 3D scene ﬂows captured from a moving vehicle for the purpose of\nobstacle avoidance. In addition to supporting mensuration and safety applications, scene\nﬂow can be used to support both spatial and temporal view interpolation (Section 13.5.4), as\ndemonstrated by Vedula, Baker, and Kanade (2005).\n11.6.1 Volumetric and 3D surface reconstruction\nAccording to Seitz, Curless, Diebel et al. (2006):\nThe goal of multi-view stereo is to reconstruct a complete 3D object model from\na collection of images taken from known camera viewpoints.\nThe most challenging but potentially most useful variant of multi-view stereo reconstruc-\ntion is to create globally consistent 3D models. This topic has a long history in computer\nvision, starting with surface mesh reconstruction techniques such as the one developed by",
  "image_path": "page_583.jpg",
  "pages": [
    582,
    583,
    584
  ]
}