{
  "doc_id": "pages_642_644",
  "text": "620\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 13.1 Image-based and video-based rendering: (a) a 3D view of a Photo Tourism re-\nconstruction (Snavely, Seitz, and Szeliski 2006) c⃝2006 ACM; (b) a slice through a 4D light\nﬁeld (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996 ACM; (c) sprites with depth (Shade,\nGortler, He et al. 1998) c⃝1998 ACM; (d) surface light ﬁeld (Wood, Azuma, Aldinger et al.\n2000) c⃝2000 ACM; (e) environment matte in front of a novel background (Zongker, Werner,\nCurless et al. 1999) c⃝1999 ACM; (f) real-time video environment matte (Chuang, Zongker,\nHindorff et al. 2000) c⃝2000 ACM; (g) Video Rewrite used to re-animate old video (Bregler,\nCovell, and Slaney 1997) c⃝1997 ACM; (h) video texture of a candle ﬂame (Sch¨odl, Szeliski,\nSalesin et al. 2000) c⃝2000 ACM; (i) video view interpolation (Zitnick, Kang, Uyttendaele\net al. 2004) c⃝2004 ACM.\n13.1 View interpolation\n621\nOver the last two decades, image-based rendering has emerged as one of the most exciting\napplications of computer vision (Kang, Li, Tong et al. 2006; Shum, Chan, and Kang 2007).\nIn image-based rendering, 3D reconstruction techniques from computer vision are combined\nwith computer graphics rendering techniques that use multiple views of a scene to create inter-\nactive photo-realistic experiences, such as the Photo Tourism system shown in Figure 13.1a.\nCommercial versions of such systems include immersive street-level navigation in on-line\nmapping systems1 and the creation of 3D Photosynths2 from large collections of casually\nacquired photographs.\nIn this chapter, we explore a variety of image-based rendering techniques, such as those\nillustrated in Figure 13.1. We begin with view interpolation (Section 13.1), which creates a\nseamless transition between a pair of reference images using one or more pre-computed depth\nmaps. Closely related to this idea are view-dependent texture maps (Section 13.1.1), which\nblend multiple texture maps on a 3D model’s surface. The representations used for both the\ncolor imagery and the 3D geometry in view interpolation include a number of clever variants\nsuch as layered depth images (Section 13.2) and sprites with depth (Section 13.2.1).\nWe continue our exploration of image-based rendering with the light ﬁeld and Lumigraph\nfour-dimensional representations of a scene’s appearance (Section 13.3), which can be used\nto render the scene from any arbitrary viewpoint. Variants on these representations include\nthe unstructured Lumigraph (Section 13.3.1), surface light ﬁelds (Section 13.3.2), concentric\nmosaics (Section 13.3.3), and environment mattes (Section 13.4).\nThe last part of this chapter explores the topic of video-based rendering, which uses one\nor more videos in order to create novel video-based experiences (Section 13.5). The topics\nwe cover include video-based facial animation (Section 13.5.1), as well as video textures\n(Section 13.5.2), in which short video clips can be seamlessly looped to create dynamic real-\ntime video-based renderings of a scene. We close with a discussion of 3D videos created from\nmultiple video streams (Section 13.5.4), as well as video-based walkthroughs of environments\n(Section 13.5.5), which have found widespread application in immersive outdoor mapping\nand driving direction systems.\n13.1 View interpolation\nWhile the term image-based rendering ﬁrst appeared in the papers by Chen (1995) and\nMcMillan and Bishop (1995), the work on view interpolation by Chen and Williams (1993)\nis considered as the seminal paper in the ﬁeld. In view interpolation, pairs of rendered color\nimages are combined with their pre-computed depth maps to generate interpolated views that\n1 http://maps.bing.com and http://maps.google.com.\n2 http://photosynth.net.\n622\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 13.2\nView interpolation (Chen and Williams 1993) c⃝1993 ACM: (a) holes from\none source image (shown in blue); (b) holes after combining two widely spaced images; (c)\nholes after combining two closely spaced images; (d) after interpolation (hole ﬁlling).\nmimic what a virtual camera would see in between the two reference views.\nView interpolation combines two ideas that were previously used in computer vision and\ncomputer graphics. The ﬁrst is the idea of pairing a recovered depth map with the refer-\nence image used in its computation and then using the resulting texture-mapped 3D model\nto generate novel views (Figure 11.1). The second is the idea of morphing (Section 3.6.3)\n(Figure 3.53), where correspondences between pairs of images are used to warp each refer-\nence image to an in-between location while simultaneously cross-dissolving between the two\nwarped images.\nFigure 13.2 illustrates this process in more detail. First, both source images are warped\nto the novel view, using both the knowledge of the reference and virtual 3D camera pose\nalong with each image’s depth map (2.68–2.70). In the paper by Chen and Williams (1993),\na forward warping algorithm (Algorithm 3.1 and Figure 3.46) is used. The depth maps are\nrepresented as quadtrees for both space and rendering time efﬁciency (Samet 1989).\nDuring the forward warping process, multiple pixels (which occlude one another) may\nland on the same destination pixel. To resolve this conﬂict, either a z-buffer depth value can\nbe associated with each destination pixel or the images can be warped in back-to-front order,\nwhich can be computed based on the knowledge of epipolar geometry (Chen and Williams\n1993; Laveau and Faugeras 1994; McMillan and Bishop 1995).\nOnce the two reference images have been warped to the novel view (Figure 13.2a–b), they\ncan be merged to create a coherent composite (Figure 13.2c). Whenever one of the images\nhas a hole (illustrated as a cyan pixel), the other image is used as the ﬁnal value. When both\nimages have pixels to contribute, these can be blended as in usual morphing, i.e., according\nto the relative distances between the virtual and source cameras. Note that if the two images\nhave very different exposures, which can happen when performing view interpolation on real\nimages, the hole-ﬁlled regions and the blended regions will have different exposures, leading",
  "image_path": "page_643.jpg",
  "pages": [
    642,
    643,
    644
  ]
}