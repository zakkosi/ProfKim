{
  "doc_id": "pages_398_400",
  "text": "376\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nShum, Han, and Szeliski (1998) describe a 3D modeling system which ﬁrst constructs\ncalibrated panoramas from multiple images (Section 7.4) and then has the user draw vertical\nand horizontal lines in the image to demarcate the boundaries of planar regions. The lines\nare initially used to establish an absolute rotation for each panorama and are later used (along\nwith the inferred vertices and planes) to infer a 3D structure, which can be recovered up to\nscale from one or more images (Figure 12.15).\nA fully automated approach to line-based structure from motion is presented vy Werner\nand Zisserman (2002). In their system, they ﬁrst ﬁnd lines and group them by common van-\nishing points in each image (Section 4.3.3). The vanishing points are then used to calibrate the\ncamera, i.e., to performa a “metric upgrade” (Section 6.3.2). Lines corresponding to common\nvanishing points are then matched using both appearance (Schmid and Zisserman 1997) and\ntrifocal tensors. The resulting set of 3D lines, color coded by common vanishing directions\n(3D orientations) is shown in Figure 12.16a. These lines are then used to infer planes and a\nblock-structured model for the scene, as described in more detail in Section 12.6.1.\n7.5.2 Plane-based techniques\nIn scenes that are rich in planar structures, e.g., in architecture and certain kinds of manu-\nfactured objects such as furniture, it is possible to directly estimate homographies between\ndifferent planes, using either feature-based or intensity-based methods. In principle, this in-\nformation can be used to simultaneously infer the camera poses and the plane equations, i.e.,\nto compute plane-based structure from motion.\nLuong and Faugeras (1996) show how a fundamental matrix can be directly computed\nfrom two or more homographies using algebraic manipulations and least squares. Unfortu-\nnately, this approach often performs poorly, since the algebraic errors do not correspond to\nmeaningful reprojection errors (Szeliski and Torr 1998).\nA better approach is to hallucinate virtual point correspondences within the areas from\nwhich each homography was computed and to feed them into a standard structure from mo-\ntion algorithm (Szeliski and Torr 1998). An even better approach is to use full bundle adjust-\nment with explicit plane equations, as well as additional constraints to force reconstructed\nco-planar features to lie exactly on their corresponding planes. (A principled way to do this\nis to establish a coordinate frame for each plane, e.g., at one of the feature points, and to use\n2D in-plane parameterizations for the other points.) The system developed by Shum, Han,\nand Szeliski (1998) shows an example of such an approach, where the directions of lines and\nnormals for planes in the scene are pre-speciﬁed by the user.\n7.6 Additional reading\n377\n7.6 Additional reading\nThe topic of structure from motion is extensively covered in books and review articles on\nmulti-view geometry (Faugeras and Luong 2001; Hartley and Zisserman 2004; Moons, Van\nGool, and Vergauwen 2010). For two-frame reconstruction, Hartley (1997a) wrote a highly\ncited paper on the “eight-point algorithm” for computing an essential or fundamental ma-\ntrix with reasonable point normalization. When the cameras are calibrated, the ﬁve-point\nalgorithm of Nist´er (2004) can be used in conjunction with RANSAC to obtain initial recon-\nstructions from the minimum number of points. When the cameras are uncalibrated, various\nself-calibration techniques can be found in work by Hartley and Zisserman (2004); Moons,\nVan Gool, and Vergauwen (2010)—I only brieﬂy mention one of the simplest techniques, the\nKruppa equations (7.35).\nIn applications where points are being tracked from frame to frame, factorization tech-\nniques, based on either orthographic camera models (Tomasi and Kanade 1992; Poelman\nand Kanade 1997; Costeira and Kanade 1995; Morita and Kanade 1997; Morris and Kanade\n1998; Anandan and Irani 2002) or projective extensions (Christy and Horaud 1996; Sturm\nand Triggs 1996; Triggs 1996; Oliensis and Hartley 2007), can be used.\nTriggs, McLauchlan, Hartley et al. (1999) provide a good tutorial and survey on bundle\nadjustment, while Lourakis and Argyros (2009) and Engels, Stew´enius, and Nist´er (2006)\nprovide tips on implementation and effective practices. Bundle adjustment is also covered\nin textbooks and surveys on multi-view geometry (Faugeras and Luong 2001; Hartley and\nZisserman 2004; Moons, Van Gool, and Vergauwen 2010). Techniques for handling larger\nproblems are described by Snavely, Seitz, and Szeliski (2008b); Agarwal, Snavely, Simon\net al. (2009); Jeong, Nist´er, Steedly et al. (2010); Agarwal, Snavely, Seitz et al. (2010).\nWhile bundle adjustment is often called as an inner loop inside incremental reconstruction\nalgorithms (Snavely, Seitz, and Szeliski 2006), hierarchical (Fitzgibbon and Zisserman 1998;\nFarenzena, Fusiello, and Gherardi 2009) and global (Rother and Carlsson 2002; Martinec and\nPajdla 2007) approaches for initialization are also possible and perhaps even preferable.\nAs structure from motion starts being applied to dynamic scenes, the topic of non-rigid\nstructure from motion (Torresani, Hertzmann, and Bregler 2008), which we do not cover in\nthis book, will become more important.\n7.7 Exercises\nEx 7.1: Triangulation\nUse the calibration pattern you built and tested in Exercise 6.7 to\ntest your triangulation accuracy. As an alternative, generate synthetic 3D points and cameras\nand add noise to the 2D point measurements.\n378\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Assume that you know the camera pose, i.e., the camera matrices. Use the 3D distance\nto rays (7.4) or linearized versions of Equations (7.5–7.6) to compute an initial set of\n3D locations. Compare these to your known ground truth locations.\n2. Use iterative non-linear minimization to improve your initial estimates and report on\nthe improvement in accuracy.\n3. (Optional) Use the technique described by Hartley and Sturm (1997) to perform two-\nframe triangulation.\n4. See if any of the failure modes reported by Hartley and Sturm (1997) or Hartley (1998)\noccur in practice.\nEx 7.2: Essential and fundamental matrix\nImplement the two-frame E and F matrix es-\ntimation techniques presented in Section 7.2, with suitable re-scaling for better noise immu-\nnity.\n1. Use the data from Exercise 7.1 to validate your algorithms and to report on their accu-\nracy.\n2. (Optional) Implement one of the improved F or E estimation algorithms, e.g., us-\ning renormalization (Zhang 1998b; Torr and Fitzgibbon 2004; Hartley and Zisserman\n2004), RANSAC (Torr and Murray 1997), least media squares (LMS), or the ﬁve-point\nalgorithm developed by Nist´er (2004).\nEx 7.3: View morphing and interpolation\nImplement automatic view morphing, i.e., com-\npute two-frame structure from motion and then use these results to generate a smooth anima-\ntion from one image to the next (Section 7.2.3).\n1. Decide how to represent your 3D scene, e.g., compute a Delaunay triangulation of the\nmatched point and decide what to do with the triangles near the border. (Hint: try ﬁtting\na plane to the scene, e.g., behind most of the points.)\n2. Compute your in-between camera positions and orientations.\n3. Warp each triangle to its new location, preferably using the correct perspective projec-\ntion (Szeliski and Shum 1997).\n4. (Optional) If you have a denser 3D model (e.g., from stereo), decide what to do at the\n“cracks”.\n5. (Optional) For a non-rigid scene, e.g., two pictures of a face with different expressions,\nnot all of your matched points will obey the epipolar geometry. Decide how to handle\nthem to achieve the best effect.",
  "image_path": "page_399.jpg",
  "pages": [
    398,
    399,
    400
  ]
}