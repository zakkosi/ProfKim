{
  "doc_id": "pages_392_394",
  "text": "370\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7.4.3 Uncertainty and ambiguities\nBecause structure from motion involves the estimation of so many highly coupled parameters,\noften with no known “ground truth” components, the estimates produced by structure from\nmotion algorithms can often exhibit large amounts of uncertainty (Szeliski and Kang 1997).\nAn example of this is the classic bas-relief ambiguity, which makes it hard to simultaneously\nestimate the 3D depth of a scene and the amount of camera motion (Oliensis 2005).16\nAs mentioned before, a unique coordinate frame and scale for a reconstructed scene can-\nnot be recovered from monocular visual measurements alone. (When a stereo rig is used,\nthe scale can be recovered if we know the distance (baseline) between the cameras.) This\nseven-degree-of-freedom gauge ambiguity makes it tricky to compute the covariance matrix\nassociated with a 3D reconstruction (Triggs, McLauchlan, Hartley et al. 1999; Kanatani and\nMorris 2001). A simple way to compute a covariance matrix that ignores the gauge freedom\n(indeterminacy) is to throw away the seven smallest eigenvalues of the information matrix (in-\nverse covariance), whose values are equivalent to the problem Hessian A up to noise scaling\n(see Section 6.1.4 and Appendix B.6). After we do this, the resulting matrix can be inverted\nto obtain an estimate of the parameter covariance.\nSzeliski and Kang (1997) use this approach to visualize the largest directions of variation\nin typical structure from motion problems. Not surprisingly, they ﬁnd that (ignoring the gauge\nfreedoms), the greatest uncertainties for problems such as observing an object from a small\nnumber of nearby viewpoints are in the depths of the 3D structure relative to the extent of the\ncamera motion.17\nIt is also possible to estimate local or marginal uncertainties for individual parameters,\nwhich corresponds simply to taking block sub-matrices from the full covariance matrix. Un-\nder certain conditions, such as when the camera poses are relatively certain compared to 3D\npoint locations, such uncertainty estimates can be meaningful. However, in many cases, indi-\nvidual uncertainty measures can mask the extent to which reconstruction errors are correlated,\nwhich is why looking at the ﬁrst few modes of greatest joint variation can be helpful.\nThe other way in which gauge ambiguities affect structure from motion and, in particular,\nbundle adjustment is that they make the system Hessian matrix A rank-deﬁcient and hence\nimpossible to invert. A number of techniques have been proposed to mitigate this problem\n(Triggs, McLauchlan, Hartley et al. 1999; Bartoli 2003). In practice, however, it appears that\nsimply adding a small amount of the Hessian diagonal λdiag(A) to the Hessian A itself, as is\ndone in the Levenberg–Marquardt non-linear least squares algorithm (Appendix A.3), usually\n16 Bas-relief refers to a kind of sculpture in which objects, often on ornamental friezes, are sculpted with less\ndepth than they actually occupy. When lit from above by sunlight, they appear to have true 3D depth because of the\nambiguity between relative depth and the angle of the illuminant (Section 12.1.1).\n17 A good way to minimize the amount of such ambiguities is to use wide ﬁeld of view cameras (Antone and\nTeller 2002; Levin and Szeliski 2006).\n7.4 Bundle adjustment\n371\nworks well.\n7.4.4 Application: Reconstruction from Internet photos\nThe most widely used application of structure from motion is in the reconstruction of 3D\nobjects and scenes from video sequences and collections of images (Pollefeys and Van Gool\n2002). The last decade has seen an explosion of techniques for performing this task auto-\nmatically without the need for any manual correspondence or pre-surveyed ground control\npoints. A lot of these techniques assume that the scene is taken with the same camera and\nhence the images all have the same intrinsics (Fitzgibbon and Zisserman 1998; Koch, Polle-\nfeys, and Van Gool 2000; Schaffalitzky and Zisserman 2002; Tuytelaars and Van Gool 2004;\nPollefeys, Nist´er, Frahm et al. 2008; Moons, Van Gool, and Vergauwen 2010). Many of\nthese techniques take the results of the sparse feature matching and structure from motion\ncomputation and then compute dense 3D surface models using multi-view stereo techniques\n(Section 11.6) (Koch, Pollefeys, and Van Gool 2000; Pollefeys and Van Gool 2002; Pollefeys,\nNist´er, Frahm et al. 2008; Moons, Van Gool, and Vergauwen 2010).\nThe latest innovation in this space has been the application of structure from motion and\nmulti-view stereo techniques to thousands of images taken from the Internet, where very little\nis known about the cameras taking the photographs (Snavely, Seitz, and Szeliski 2008a). Be-\nfore the structure from motion computation can begin, it is ﬁrst necessary to establish sparse\ncorrespondences between different pairs of images and to then link such correspondences into\nfeature tracks, which associate individual 2D image features with global 3D points. Because\nthe O(N 2) comparison of all pairs of images can be very slow, a number of techniques have\nbeen developed in the recognition community to make this process faster (Section 14.3.2)\n(Nist´er and Stew´enius 2006; Philbin, Chum, Sivic et al. 2008; Li, Wu, Zach et al. 2008;\nChum, Philbin, and Zisserman 2008; Chum and Matas 2010).\nTo begin the reconstruction process, it is important to to select a good pair of images,\nwhere there are both a large number of consistent matches (to lower the likelihood of in-\ncorrect correspondences) and a signiﬁcant amount of out-of-plane parallax,18 to ensure that\na stable reconstruction can be obtained (Snavely, Seitz, and Szeliski 2006). The EXIF tags\nassociated with the photographs can be used to get good initial estimates for camera focal\nlengths, although this is not always strictly necessary, since these parameters are re-adjusted\nas part of the bundle adjustment process.\nOnce an initial pair has been reconstructed, the pose of cameras that see a sufﬁcient num-\nber of the resulting 3D points can be estimated (Section 6.2) and the complete set of cameras\nand feature correspondences can be used to perform another round of bundle adjustment. Fig-\n18 A simple way to compute this is to robustly ﬁt a homography to the correspondences and measure reprojection\nerrors.\n372\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 7.11 Incremental structure from motion (Snavely, Seitz, and Szeliski 2006) c⃝2006\nACM: Starting with an initial two-frame reconstruction of Trevi Fountain, batches of images\nare added using pose estimation, and their positions (along with the 3D model) are reﬁned\nusing bundle adjustment.\nure 7.11 shows the progression of the incremental bundle adjustment algorithm, where sets of\ncameras are added after each successive round of bundle adjustment, while Figure 7.12 shows\nsome additional results. An alternative to this kind of seed and grow approach is to ﬁrst re-\nconstruct triplets of images and then hierarchically merge triplets into larger collections, as\ndescribed by Fitzgibbon and Zisserman (1998).\nUnfortunately, as the incremental structure from motion algorithm continues to add more\ncameras and points, it can become extremely slow. The direct solution of a dense system\nof O(N) equations for the camera pose updates can take O(N 3) time; while structure from\nmotion problems are rarely dense, scenes such as city squares have a high percentage of\ncameras that see points in common. Re-running the bundle adjustment algorithm after every\nfew camera additions results in a quartic scaling of the run time with the number of images\nin the dataset. One approach to solving this problem is to select a smaller number of images\nfor the original scene reconstruction and to fold in the remaining images at the very end.\nSnavely, Seitz, and Szeliski (2008b) develop an algorithm for computing such a skele-\ntal set of images, which is guaranteed to produce a reconstruction whose error is within a\nbounded factor of the optimal reconstruction accuracy. Their algorithm ﬁrst evaluates all\npairwise uncertainties (position covariances) between overlapping images and then chains\nthem together to estimate a lower bound for the relative uncertainty of any distant pair. The\nskeletal set is constructed so that the maximal uncertainty between any pair grows by no\nmore than a constant factor. Figure 7.13 shows an example of the skeletal set computed for\n784 images of the Pantheon in Rome. As you can see, even though the skeletal set contains\njust a fraction of the original images, the shapes of the skeletal set and full bundle adjusted\nreconstructions are virtually indistinguishable.\nThe ability to automatically reconstruct 3D models from large, unstructured image col-\nlections has opened a wide variety of additional applications, including the ability to automat-",
  "image_path": "page_393.jpg",
  "pages": [
    392,
    393,
    394
  ]
}