{
  "doc_id": "pages_340_342",
  "text": "318\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n6.1.4 Robust least squares and RANSAC\nWhile regular least squares is the method of choice for measurements where the noise follows\na normal (Gaussian) distribution, more robust versions of least squares are required when\nthere are outliers among the correspondences (as there almost always are). In this case, it is\npreferable to use an M-estimator (Huber 1981; Hampel, Ronchetti, Rousseeuw et al. 1986;\nBlack and Rangarajan 1996; Stewart 1999), which involves applying a robust penalty function\nρ(r) to the residuals\nERLS(∆p) =\nX\ni\nρ(∥ri∥)\n(6.25)\ninstead of squaring them.\nWe can take the derivative of this function with respect to p and set it to 0,\nX\ni\nψ(∥ri∥)∂∥ri∥\n∂p\n=\nX\ni\nψ(∥ri∥)\n∥ri∥\nrT\ni\n∂ri\n∂p = 0,\n(6.26)\nwhere ψ(r) = ρ′(r) is the derivative of ρ and is called the inﬂuence function. If we introduce\na weight function, w(r) = Ψ(r)/r, we observe that ﬁnding the stationary point of (6.25) using\n(6.26) is equivalent to minimizing the iteratively reweighted least squares (IRLS) problem\nEIRLS =\nX\ni\nw(∥ri∥)∥ri∥2,\n(6.27)\nwhere the w(∥ri∥) play the same local weighting role as σ−2\ni\nin (6.10). The IRLS algo-\nrithm alternates between computing the inﬂuence functions w(∥ri∥) and solving the result-\ning weighted least squares problem (with ﬁxed w values).\nOther incremental robust least\nsquares algorithms can be found in the work of Sawhney and Ayer (1996); Black and Anan-\ndan (1996); Black and Rangarajan (1996); Baker, Gross, Ishikawa et al. (2003) and textbooks\nand tutorials on robust statistics (Huber 1981; Hampel, Ronchetti, Rousseeuw et al. 1986;\nRousseeuw and Leroy 1987; Stewart 1999).\nWhile M-estimators can deﬁnitely help reduce the inﬂuence of outliers, in some cases,\nstarting with too many outliers will prevent IRLS (or other gradient descent algorithms) from\nconverging to the global optimum. A better approach is often to ﬁnd a starting set of inlier\ncorrespondences, i.e., points that are consistent with a dominant motion estimate.7\nTwo widely used approaches to this problem are called RANdom SAmple Consensus, or\nRANSAC for short (Fischler and Bolles 1981), and least median of squares (LMS) (Rousseeuw\n1984). Both techniques start by selecting (at random) a subset of k correspondences, which is\n7 For pixel-based alignment methods (Section 8.1.1), hierarchical (coarse-to-ﬁne) techniques are often used to\nlock onto the dominant motion in a scene.\n6.1 2D and 3D feature-based alignment\n319\nthen used to compute an initial estimate for p. The residuals of the full set of correspondences\nare then computed as\nri = ˜x′\ni(xi; p) −ˆx′\ni,\n(6.28)\nwhere ˜x′\ni are the estimated (mapped) locations and ˆx′\ni are the sensed (detected) feature point\nlocations.\nThe RANSAC technique then counts the number of inliers that are within ϵ of their pre-\ndicted location, i.e., whose ∥ri∥≤ϵ. (The ϵ value is application dependent but is often\naround 1–3 pixels.) Least median of squares ﬁnds the median value of the ∥ri∥2 values. The\nrandom selection process is repeated S times and the sample set with the largest number of\ninliers (or with the smallest median residual) is kept as the ﬁnal solution. Either the initial\nparameter guess p or the full set of computed inliers is then passed on to the next data ﬁtting\nstage.\nWhen the number of measurements is quite large, it may be preferable to only score a\nsubset of the measurements in an initial round that selects the most plausible hypotheses for\nadditional scoring and selection. This modiﬁcation of RANSAC, which can signiﬁcantly\nspeed up its performance, is called Preemptive RANSAC (Nist´er 2003). In another variant\non RANSAC called PROSAC (PROgressive SAmple Consensus), random samples are ini-\ntially added from the most “conﬁdent” matches, thereby speeding up the process of ﬁnding a\n(statistically) likely good set of inliers (Chum and Matas 2005).\nTo ensure that the random sampling has a good chance of ﬁnding a true set of inliers, a\nsufﬁcient number of trials S must be tried. Let p be the probability that any given correspon-\ndence is valid and P be the total probability of success after S trials. The likelihood in one\ntrial that all k random samples are inliers is pk. Therefore, the likelihood that S such trials\nwill all fail is\n1 −P = (1 −pk)S\n(6.29)\nand the required minimum number of trials is\nS = log(1 −P)\nlog(1 −pk).\n(6.30)\nStewart (1999) gives examples of the required number of trials S to attain a 99% proba-\nbility of success. As you can see from Table 6.2, the number of trials grows quickly with the\nnumber of sample points used. This provides a strong incentive to use the minimum number\nof sample points k possible for any given trial, which is how RANSAC is normally used in\npractice.\nUncertainty modeling\nIn addition to robustly computing a good alignment, some applications require the compu-\ntation of uncertainty (see Appendix B.6). For linear problems, this estimate can be obtained\n320\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nk\np\nS\n3\n0.5\n35\n6\n0.6\n97\n6\n0.5\n293\nTable 6.2 Number of trials S to attain a 99% probability of success (Stewart 1999).\nby inverting the Hessian matrix (6.9) and multiplying it by the feature position noise (if these\nhave not already been used to weight the individual measurements, as in Equations (6.10)\nand 6.11)). In statistics, the Hessian, which is the inverse covariance, is sometimes called the\n(Fisher) information matrix (Appendix B.1.1).\nWhen the problem involves non-linear least squares, the inverse of the Hessian matrix\nprovides the Cramer–Rao lower bound on the covariance matrix, i.e., it provides the minimum\namount of covariance in a given solution, which can actually have a wider spread (“longer\ntails”) if the energy ﬂattens out away from the local minimum where the optimal solution is\nfound.\n6.1.5 3D alignment\nInstead of aligning 2D sets of image features, many computer vision applications require the\nalignment of 3D points. In the case where the 3D transformations are linear in the motion\nparameters, e.g., for translation, similarity, and afﬁne, regular least squares (6.5) can be used.\nThe case of rigid (Euclidean) motion,\nER3D =\nX\ni\n∥x′\ni −Rxi −t∥2,\n(6.31)\nwhich arises more frequently and is often called the absolute orientation problem (Horn\n1987), requires slightly different techniques. If only scalar weightings are being used (as\nopposed to full 3D per-point anisotropic covariance estimates), the weighted centroids of the\ntwo point clouds c and c′ can be used to estimate the translation t = c′ −Rc.8 We are then\nleft with the problem of estimating the rotation between two sets of points {ˆxi = xi −c}\nand {ˆx′\ni = x′\ni −c′} that are both centered at the origin.\nOne commonly used technique is called the orthogonal Procrustes algorithm (Golub and\nVan Loan 1996, p. 601) and involves computing the singular value decomposition (SVD) of\n8 When full covariances are used, they are transformed by the rotation and so a closed-form solution for transla-\ntion is not possible.",
  "image_path": "page_341.jpg",
  "pages": [
    340,
    341,
    342
  ]
}