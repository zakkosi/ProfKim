{
  "doc_id": "pages_345_347",
  "text": "6.2 Pose estimation\n323\npi = (Xi,Yi,Zi,Wi)\nxi\npj\ndij\ndi\ndj\nxj\nθij\nc\nFigure 6.4\nPose estimation by the direct linear transform and by measuring visual angles\nand distances between pairs of points.\npair of 2D points ˆxi and ˆxj must be the same as the angle between their corresponding 3D\npoints pi and pj (Figure 6.4).\nGiven a set of corresponding 2D and 3D points {(ˆxi, pi)}, where the ˆxi are unit directions\nobtained by transforming 2D pixel measurements xi to unit norm 3D directions ˆxi through\nthe inverse calibration matrix K,\nˆxi = N(K−1xi) = K−1xi/∥K−1xi∥,\n(6.36)\nthe unknowns are the distances di from the camera origin c to the 3D points pi, where\npi = diˆxi + c\n(6.37)\n(Figure 6.4). The cosine law for triangle ∆(c, pi, pj) gives us\nfij(di, dj) = d2\ni + d2\nj −2didjcij −d2\nij = 0,\n(6.38)\nwhere\ncij = cos θij = ˆxi · ˆxj\n(6.39)\nand\nd2\nij = ∥pi −pj∥2.\n(6.40)\nWe can take any triplet of constraints (fij, fik, fjk) and eliminate the dj and dk using\nSylvester resultants (Cox, Little, and O’Shea 2007) to obtain a quartic equation in d2\ni ,\ngijk(d2\ni ) = a4d8\ni + a3d6\ni + a2d4\ni + a1d2\ni + a0 = 0.\n(6.41)\nGiven ﬁve or more correspondences, we can generate (n−1)(n−2)\n2\ntriplets to obtain a linear\nestimate (using SVD) for the values of (d8\ni , d6\ni , d4\ni , d2\ni ) (Quan and Lan 1999). Estimates for\n324\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nd2\ni can computed as ratios of successive d2n+2\ni\n/d2n\ni\nestimates and these can be averaged to\nobtain a ﬁnal estimate of d2\ni (and hence di).\nOnce the individual estimates of the di distances have been computed, we can generate\na 3D structure consisting of the scaled point directions diˆxi, which can then be aligned with\nthe 3D point cloud {pi} using absolute orientation (Section 6.1.5) to obtained the desired\npose estimate. Quan and Lan (1999) give accuracy results for this and other techniques,\nwhich use fewer points but require more complicated algebraic manipulations. The paper by\nMoreno-Noguer, Lepetit, and Fua (2007) reviews more recent alternatives and also gives a\nlower complexity algorithm that typically produces more accurate results.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 7.4.3), it is often preferable to use\nthe linear six-point algorithm to guess an initial pose and then optimize this estimate using\nthe iterative technique described in Section 6.2.2.\nAn alternative pose estimation algorithm involves starting with a scaled orthographic pro-\njection model and then iteratively reﬁning this initial estimate using a more accurate perspec-\ntive projection model (DeMenthon and Davis 1995). The attraction of this model, as stated\nin the paper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\n6.2.2 Iterative algorithms\nThe most accurate (and ﬂexible) way to estimate pose is to directly minimize the squared (or\nrobust) reprojection error for the 2D points as a function of the unknown pose parameters in\n(R, t) and optionally K using non-linear least squares (Tsai 1987; Bogart 1991; Gleicher\nand Witkin 1992). We can write the projection equations as\nxi = f(pi; R, t, K)\n(6.42)\nand iteratively minimize the robustiﬁed linearized reprojection errors\nENLP =\nX\ni\nρ\n\u0012 ∂f\n∂R∆R + ∂f\n∂t ∆t + ∂f\n∂K ∆K −ri\n\u0013\n,\n(6.43)\nwhere ri = ˜xi −ˆxi is the current residual vector (2D error in predicted position) and the\npartial derivatives are with respect to the unknown pose parameters (rotation, translation, and\noptionally calibration). Note that if full 2D covariance estimates are available for the 2D\nfeature locations, the above squared norm can be weighted by the inverse point covariance\nmatrix, as in Equation (6.11).\nAn easier to understand (and implement) version of the above non-linear regression prob-\nlem can be constructed by re-writing the projection equations as a concatenation of simpler\nsteps, each of which transforms a 4D homogeneous coordinate pi by a simple transformation\n6.2 Pose estimation\n325\nfC(x) = Kx\nk\nfP(x) = p/z\nfR(x) = Rx\nqj\nfT(x) = x-c\ncj\npi\nxi\ny(1)\ny(2)\ny(3)\nFigure 6.5 A set of chained transforms for projecting a 3D point pi to a 2D measurement xi\nthrough a series of transformations f (k), each of which is controlled by its own set of param-\neters. The dashed lines indicate the ﬂow of information as partial derivatives are computed\nduring a backward pass.\nsuch as translation, rotation, or perspective division (Figure 6.5). The resulting projection\nequations can be written as\ny(1)\n=\nf T(pi; cj) = pi −cj,\n(6.44)\ny(2)\n=\nf R(y(1); qj) = R(qj) y(1),\n(6.45)\ny(3)\n=\nf P(y(2)) = y(2)\nz(2) ,\n(6.46)\nxi\n=\nf C(y(3); k) = K(k) y(3).\n(6.47)\nNote that in these equations, we have indexed the camera centers cj and camera rotation\nquaternions qj by an index j, in case more than one pose of the calibration object is being\nused (see also Section 7.4.) We are also using the camera center cj instead of the world\ntranslation tj, since this is a more natural parameter to estimate.\nThe advantage of this chained set of transformations is that each one has a simple partial\nderivative with respect both to its parameters and to its input. Thus, once the predicted value\nof ˜xi has been computed based on the 3D point location pi and the current values of the pose\nparameters (cj, qj, k), we can obtain all of the required partial derivatives using the chain\nrule\n∂ri\n∂p(k) =\n∂ri\n∂y(k)\n∂y(k)\n∂p(k) ,\n(6.48)\nwhere p(k) indicates one of the parameter vectors that is being optimized. (This same “trick”\nis used in neural networks as part of the backpropagation algorithm (Bishop 2006).)\nThe one special case in this formulation that can be considerably simpliﬁed is the compu-\ntation of the rotation update. Instead of directly computing the derivatives of the 3×3 rotation\nmatrix R(q) as a function of the unit quaternion entries, you can prepend the incremental ro-\ntation matrix ∆R(ω) given in Equation (2.35) to the current rotation matrix and compute the",
  "image_path": "page_346.jpg",
  "pages": [
    345,
    346,
    347
  ]
}