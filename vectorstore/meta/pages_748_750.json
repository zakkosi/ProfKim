{
  "doc_id": "pages_748_750",
  "text": "726\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nas explained in Algorithm 14.1, devise an efﬁcient algorithm to ﬁnd values of θ and s = ±1\nthat maximize\nX\ni\nwiyih(sfi, θ),\n(14.43)\nwhere h(x, θ) = sign(x −θ).\nEx 14.3: Face recognition using eigenfaces\nCollect a set of facial photographs and then\nbuild a recognition system to re-recognize the same people.\n1. Take several photos of each of your classmates and store them.\n2. Align the images by automatically or manually detecting the corners of the eyes and\nusing a similarity transform to stretch and rotate each image to a canonical position.\n3. Compute the average image and a PCA subspace for the face images\n4. Take a new set of photographs a week later and use them as your test set.\n5. Compare each new image to each database image and select the nearest one as the\nrecognized identity. Verify that the distance in PCA space is close to the distance\ncomputed with a full SSD (sum of squared difference) measure.\n6. (Optional) Compute different principal components for identity and expression, and\nuse them to improve your recognition results.\nEx 14.4: Bayesian face recognition\nMoghaddam, Jebara, and Pentland (2000) compute\nseparate covariance matrices ΣI and ΣE by looking at differences between all pairs of im-\nages. At run time, they select the nearest image to determine the facial identity. Does it make\nsense to estimate statistics for all pairs of images and use them for testing the distance to the\nnearest exemplar? Discuss whether this is statistically correct.\nHow is the all-pair intrapersonal covariance matrix ΣI related to the within-class scatter\nmatrix SW? Does a similar relationship hold between ΣE and SB?\nEx 14.5: Modular eigenfaces\nExtend your face recognition system to separately match the\neye, nose, and mouth regions, as shown in Figure 14.18.\n1. After normalizing face images to a canonical scale and location, manually segment out\nsome of the eye, nose, and face regions.\n2. Build separate detectors for these three (or four) kinds of region, either using a subspace\n(PCA) approach or one of the techniques presented in Section 14.1.1.\n3. For each new image to be recognized, ﬁrst detect the locations of the facial features.\n14.8 Exercises\n727\n4. Then, match the individual features against your database and note the locations of\nthese features.\n5. Train and test a classiﬁer that uses the individual feature matching IDs as well as (op-\ntionally) the feature locations to perform face recognition.\nEx 14.6: Recognition-based color balancing\nBuild a system that recognizes the most im-\nportant color areas in common photographs (sky, grass, skin) and color balances the image\naccordingly. Some references and ideas for skin detection are given in Exercise 2.8 and\nby Forsyth and Fleck (1999), Jones and Rehg (2001), Vezhnevets, Sazonov, and Andreeva\n(2003), and Kakumanu, Makrogiannis, and Bourbakis (2007). These may give you ideas\nfor how to detect other regions or you can try more sophisticated MRF-based approaches\n(Shotton, Winn, Rother et al. 2009).\nEx 14.7: Pedestrian detection\nBuild and test one of the pedestrian detectors presented in\nSection 14.1.2.\nEx 14.8: Simple instance recognition\nUse the feature detection, matching, and alignment\nalgorithms you developed in Exercises 4.1–4.4 and 9.2 to ﬁnd matching images given a query\nimage or region (Figure 14.26).\nEvaluate several feature detectors, descriptors, and robust geometric veriﬁcation strate-\ngies, either on your own or by comparing your results with those of classmates.\nEx 14.9: Large databases and location recognition\nExtend the previous exercise to larger\ndatabases using quantized visual words and information retrieval techniques, as described in\nAlgorithm 14.2.\nTest your algorithm on a large database, such as the one used by Nist´er and Stew´enius\n(2006) or Philbin, Chum, Sivic et al. (2008), which are listed in Table 14.1. Alternatively,\nuse keyword search on the Web or in a photo sharing site (e.g., for a city) to create your own\ndatabase.\nEx 14.10: Bag of words\nAdapt the feature extraction and matching pipeline developed in\nExercise 14.8 to category (class) recognition, using some of the techniques described in Sec-\ntion 14.4.1.\n1. Download the training and test images from one or more of the databases listed in\nTables 14.1 and 14.2, e.g., Caltech 101, Caltech 256, or PASCAL VOC.\n2. Extract features from each of the training images, quantize them, and compute the tf-idf\nvectors (bag of words histograms).\n728\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. As an option, consider not quantizing the features and using pyramid matching (14.40–\n14.41) (Grauman and Darrell 2007b) or using a spatial pyramid for greater selectivity\n(Lazebnik, Schmid, and Ponce 2006).\n4. Choose a classiﬁcation algorithm (e.g., nearest neighbor classiﬁcation or support vector\nmachine) and “train” your recognizer, i.e., build up the appropriate data structures (e.g.,\nk-d trees) or set the appropriate classiﬁer parameters.\n5. Test your algorithm on the test data set using the same pipeline you developed in steps\n2–4 and compare your results to the best reported results.\n6. Explain why your results differ from the previously reported ones and give some ideas\nfor how you could improve your system.\nYou can ﬁnd a good synopsis of the best-performing classiﬁcation algorithms and their ap-\nproaches in the report of the PASCAL Visual Object Classes Challenge found on their Web\nsite (http://pascallin.ecs.soton.ac.uk/challenges/VOC/).\nEx 14.11: Object detection and localization\nExtend the classiﬁcation algorithm developed\nin the previous exercise to localize the objects in an image by reporting a bounding box around\neach detected object. The easiest way to do this is to use a sliding window approach. Some\npointers to recent techniques in this area can be found in the workshop associated with the\nPASCAL VOC 2008 Challenge.\nEx 14.12: Part-based recognition\nChoose one or more of the techniques described in Sec-\ntion 14.4.2 and implement a part-based recognition system. Since these techniques are fairly\ninvolved, you will need to read several of the research papers in this area, select which gen-\neral approach you want to follow, and then implement your algorithm. A good starting point\ncould be the paper by Felzenszwalb, McAllester, and Ramanan (2008), since it performed\nwell in the PASCAL VOC 2008 detection challenge.\nEx 14.13: Recognition and segmentation\nChoose one or more of the techniques described\nin Section 14.4.3 and implement a simultaneous recognition and segmentation system. Since\nthese techniques are fairly involved, you will need to read several of the research papers in this\narea, select which general approach you want to follow, and then implement your algorithm.\nTest your algorithm on one or more of the segmentation databases in Table 14.2.\nEx 14.14: Context\nImplement one or more of the context and scene understanding sys-\ntems described in Section 14.5 and report on your experience. Does context or whole scene\nunderstanding perform better at naming objects than stand-alone systems?",
  "image_path": "page_749.jpg",
  "pages": [
    748,
    749,
    750
  ]
}