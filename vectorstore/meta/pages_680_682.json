{
  "doc_id": "pages_680_682",
  "text": "658\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Section 14.3.2). We also present applications of location recognition (Section 14.3.3).\nIn the second half of the chapter, we address the most challenging variant of recognition,\nnamely the problem of category recognition (Section 14.4). This includes approaches that use\nbags of features (Section 14.4.1), parts (Section 14.4.2), and segmentation (Section 14.4.3).\nWe show how such techniques can be used to automate photo editing tasks, such as 3D mod-\neling, scene completion, and creating collages (Section 14.4.4). Next, we discuss the role\nthat context can play in both individual object recognition and more holistic scene under-\nstanding (Section 14.5). We close this chapter with a discussion of databases and test sets for\nconstructing and evaluating recognition systems (Section 14.6).\nWhile there is no comprehensive reference on object recognition, an excellent set of notes\ncan be found in the ICCV 2009 short course (Fei-Fei, Fergus, and Torralba 2009), Antonio\nTorralba’s more comprehensive MIT course (Torralba 2008), and two recent collections of\npapers (Ponce, Hebert, Schmid et al. 2006; Dickinson, Leonardis, Schiele et al. 2007) and a\nsurvey on object categorization (Pinz 2005). An evaluation of some of the best performing\nrecognition algorithms can be found on the PASCAL Visual Object Classes (VOC) Challenge\nWeb site at http://pascallin.ecs.soton.ac.uk/challenges/VOC/.\n14.1 Object detection\nIf we are given an image to analyze, such as the group portrait in Figure 14.2, we could try to\napply a recognition algorithm to every possible sub-window in this image. Such algorithms\nare likely to be both slow and error-prone. Instead, it is more effective to construct special-\npurpose detectors, whose job it is to rapidly ﬁnd likely regions where particular objects might\noccur.\nWe begin this section with face detectors, which are some of the more successful examples\nof recognition. For example, such algorithms are built into most of today’s digital cameras to\nenhance auto-focus and into video conferencing systems to control pan-tilt heads. We then\nlook at pedestrian detectors, as an example of more general methods for object detection.\nSuch detectors can be used in automotive safety applications, e.g., detecting pedestrians and\nother cars from moving vehicles (Leibe, Cornelis, Cornelis et al. 2007).\n14.1.1 Face detection\nBefore face recognition can be applied to a general image, the locations and sizes of any faces\nmust ﬁrst be found (Figures 14.1c and 14.2). In principle, we could apply a face recognition\nalgorithm at every pixel and scale (Moghaddam and Pentland 1997) but such a process would\nbe too slow in practice.\n14.1 Object detection\n659\nFigure 14.2\nFace detection results produced by Rowley, Baluja, and Kanade (1998a) c⃝\n1998 IEEE. Can you ﬁnd the one false positive (a box around a non-face) among the 57 true\npositive results?\nOver the years, a wide variety of fast face detection algorithms have been developed.\nYang, Kriegman, and Ahuja (2002) provide a comprehensive survey of earlier work in this\nﬁeld; Yang’s ICPR 2004 tutorial2 and the Torralba (2007) short course provide more recent\nreviews.3\nAccording to the taxonomy of Yang, Kriegman, and Ahuja (2002), face detection tech-\nniques can be classiﬁed as feature-based, template-based, or appearance-based. Feature-\nbased techniques attempt to ﬁnd the locations of distinctive image features such as the eyes,\nnose, and mouth, and then verify whether these features are in a plausible geometrical ar-\nrangement. These techniques include some of the early approaches to face recognition (Fis-\nchler and Elschlager 1973; Kanade 1977; Yuille 1991), as well as more recent approaches\nbased on modular eigenspaces (Moghaddam and Pentland 1997), local ﬁlter jets (Leung,\nBurl, and Perona 1995; Penev and Atick 1996; Wiskott, Fellous, Kr¨uger et al. 1997), support\nvector machines (Heisele, Ho, Wu et al. 2003; Heisele, Serre, and Poggio 2007), and boosting\n(Schneiderman and Kanade 2004).\nTemplate-based approaches, such as active appearance models (AAMs) (Section 14.2.2),\ncan deal with a wide range of pose and expression variability. Typically, they require good\ninitialization near a real face and are therefore not suitable as fast face detectors.\n2 http://vision.ai.uiuc.edu/mhyang/face-detection-survey.html.\n3 An alternative approach to detecting faces is to look for regions of skin color in the image (Forsyth and Fleck\n1999; Jones and Rehg 2001). See Exercise 2.8 for some additional discussion and references.\n660\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 14.3\nPre-processing stages for face detector training (Rowley, Baluja, and Kanade\n1998a) c⃝1998 IEEE: (a) artiﬁcially mirroring, rotating, scaling, and translating training\nimages for greater variability; (b) using images without faces (looking up at a tree) to generate\nnon-face examples; (c) pre-processing the patches by subtracting a best ﬁt linear function\n(constant gradient) and histogram equalizing.\nAppearance-based approaches scan over small overlapping rectangular patches of the im-\nage searching for likely face candidates, which can then be reﬁned using a cascade of more\nexpensive but selective detection algorithms (Sung and Poggio 1998; Rowley, Baluja, and\nKanade 1998a; Romdhani, Torr, Sch¨olkopf et al. 2001; Fleuret and Geman 2001; Viola and\nJones 2004). In order to deal with scale variation, the image is usually converted into a\nsub-octave pyramid and a separate scan is performed on each level. Most appearance-based\napproaches today rely heavily on training classiﬁers using sets of labeled face and non-face\npatches.\nSung and Poggio (1998) and Rowley, Baluja, and Kanade (1998a) present two of the ear-\nliest appearance-based face detectors and introduce a number of innovations that are widely\nused in later work by others.\nTo start with, both systems collect a set of labeled face patches (Figure 14.2) as well as a\nset of patches taken from images that are known not to contain faces, such as aerial images or\nvegetation (Figure 14.3b). The collected face images are augmented by artiﬁcially mirroring,\nrotating, scaling, and translating the images by small amounts to make the face detectors less\nsensitive to such effects (Figure 14.3a).\nAfter an initial set of training images has been collected, some optional pre-processing\ncan be performed, such as subtracting an average gradient (linear function) from the image\nto compensate for global shading effects and using histogram equalization to compensate for\nvarying camera contrast (Figure 14.3c).",
  "image_path": "page_681.jpg",
  "pages": [
    680,
    681,
    682
  ]
}