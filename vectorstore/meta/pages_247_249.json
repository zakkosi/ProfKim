{
  "doc_id": "pages_247_249",
  "text": "4.1 Points and patches\n225\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.19 The gradient location-orientation histogram (GLOH) descriptor uses log-polar\nbins instead of square bins to compute orientation histograms (Mikolajczyk and Schmid\n2005).\ndescriptors. Hua, Brown, and Winder (2007) extend this work by learning lower-dimensional\nprojections of higher-dimensional descriptors that have the best discriminative power. Both\nof these papers use a database of real-world image patches (Figure 4.20b) obtained by sam-\npling images at locations that were reliably matched using a robust structure-from-motion\nalgorithm applied to Internet photo collections (Snavely, Seitz, and Szeliski 2006; Goesele,\nSnavely, Curless et al. 2007). In concurrent work, Tola, Lepetit, and Fua (2010) developed a\nsimilar DAISY descriptor for dense stereo matching and optimized its parameters based on\nground truth stereo data.\nWhile these techniques construct feature detectors that optimize for repeatability across\nall object classes, it is also possible to develop class- or instance-speciﬁc feature detectors that\nmaximize discriminability from other classes (Ferencz, Learned-Miller, and Malik 2008).\n4.1.3 Feature matching\nOnce we have extracted features and their descriptors from two or more images, the next step\nis to establish some preliminary feature matches between these images. In this section, we\ndivide this problem into two separate components. The ﬁrst is to select a matching strategy,\nwhich determines which correspondences are passed on to the next stage for further process-\ning. The second is to devise efﬁcient data structures and algorithms to perform this matching\nas quickly as possible. (See the discussion of related techniques in Section 14.3.2.)\n226\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 4.20 Spatial summation blocks for SIFT, GLOH, and some newly developed feature\ndescriptors (Winder and Brown 2007) c⃝2007 IEEE: (a) The parameters for the new features,\ne.g., their Gaussian weights, are learned from a training database of (b) matched real-world\nimage patches obtained from robust structure from motion applied to Internet photo collec-\ntions (Hua, Brown, and Winder 2007).\nMatching strategy and error rates\nDetermining which feature matches are reasonable to process further depends on the context\nin which the matching is being performed. Say we are given two images that overlap to a fair\namount (e.g., for image stitching, as in Figure 4.16, or for tracking objects in a video). We\nknow that most features in one image are likely to match the other image, although some may\nnot match because they are occluded or their appearance has changed too much.\nOn the other hand, if we are trying to recognize how many known objects appear in a clut-\ntered scene (Figure 4.21), most of the features may not match. Furthermore, a large number\nof potentially matching objects must be searched, which requires more efﬁcient strategies, as\ndescribed below.\nTo begin with, we assume that the feature descriptors have been designed so that Eu-\nclidean (vector magnitude) distances in feature space can be directly used for ranking poten-\ntial matches. If it turns out that certain parameters (axes) in a descriptor are more reliable\nthan others, it is usually preferable to re-scale these axes ahead of time, e.g., by determin-\ning how much they vary when compared against other known good matches (Hua, Brown,\nand Winder 2007). A more general process, which involves transforming feature vectors\ninto a new scaled basis, is called whitening and is discussed in more detail in the context of\neigenface-based face recognition (Section 14.2.1).\nGiven a Euclidean distance metric, the simplest matching strategy is to set a threshold\n(maximum distance) and to return all matches from other images within this threshold. Set-\nting the threshold too high results in too many false positives, i.e., incorrect matches being\nreturned. Setting the threshold too low results in too many false negatives, i.e., too many\ncorrect matches being missed (Figure 4.22).\nWe can quantify the performance of a matching algorithm at a particular threshold by\n4.1 Points and patches\n227\nFigure 4.21 Recognizing objects in a cluttered scene (Lowe 2004) c⃝2004 Springer. Two of\nthe training images in the database are shown on the left. These are matched to the cluttered\nscene in the middle using SIFT features, shown as small squares in the right image. The afﬁne\nwarp of each recognized database image onto the scene is shown as a larger parallelogram in\nthe right image.\n1\n1\n2\n1\n3\n4\nFigure 4.22\nFalse positives and negatives: The black digits 1 and 2 are features being\nmatched against a database of features in other images. At the current threshold setting (the\nsolid circles), the green 1 is a true positive (good match), the blue 1 is a false negative (failure\nto match), and the red 3 is a false positive (incorrect match). If we set the threshold higher\n(the dashed circles), the blue 1 becomes a true positive but the brown 4 becomes an additional\nfalse positive.",
  "image_path": "page_248.jpg",
  "pages": [
    247,
    248,
    249
  ]
}