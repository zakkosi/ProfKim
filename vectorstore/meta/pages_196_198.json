{
  "doc_id": "pages_196_198",
  "text": "174\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmorph, using different blends (and amounts of deformation) at each interval. Let t ∈[0, 1] be\nthe time parameter that describes the sequence of interpolated frames. The weighting func-\ntions for the two warped images in the blend go as (1 −t) and t. Conversely, the amount of\nmotion that image 0 undergoes at time t is t of the total amount of motion that is speciﬁed\nby the correspondences. However, some care must be taken in deﬁning what it means to par-\ntially warp an image towards a destination, especially if the desired motion is far from linear\n(Sederberg, Gao, Wang et al. 1993). Exercise 3.25 has you implement a morphing algorithm\nand test it out under such challenging conditions.\n3.7 Global optimization\nSo far in this chapter, we have covered a large number of image processing operators that\ntake as input one or more images and produce some ﬁltered or transformed version of these\nimages. In many applications, it is more useful to ﬁrst formulate the goals of the desired\ntransformation using some optimization criterion and then ﬁnd or infer the solution that best\nmeets this criterion.\nIn this ﬁnal section, we present two different (but closely related) variants on this idea.\nThe ﬁrst, which is often called regularization or variational methods (Section 3.7.1), con-\nstructs a continuous global energy function that describes the desired characteristics of the\nsolution and then ﬁnds a minimum energy solution using sparse linear systems or related\niterative techniques. The second formulates the problem using Bayesian statistics, model-\ning both the noisy measurement process that produced the input images as well as prior\nassumptions about the solution space, which are often encoded using a Markov random ﬁeld\n(Section 3.7.2).\nExamples of such problems include surface interpolation from scattered data (Figure 3.54),\nimage denoising and the restoration of missing regions (Figure 3.57), and the segmentation\nof images into foreground and background regions (Figure 3.61).\n3.7.1 Regularization\nThe theory of regularization was ﬁrst developed by statisticians trying to ﬁt models to data\nthat severely underconstrained the solution space (Tikhonov and Arsenin 1977; Engl, Hanke,\nand Neubauer 1996). Consider, for example, ﬁnding a smooth surface that passes through\n(or near) a set of measured data points (Figure 3.54). Such a problem is described as ill-\nposed because many possible surfaces can ﬁt this data. Since small changes in the input can\nsometimes lead to large changes in the ﬁt (e.g., if we use polynomial interpolation), such\nproblems are also often ill-conditioned. Since we are trying to recover the unknown function\nf(x, y) from which the data point d(xi, yi) were sampled, such problems are also often called\n3.7 Global optimization\n175\n(a)\n(b)\nFigure 3.54 A simple surface interpolation problem: (a) nine data points of various height\nscattered on a grid; (b) second-order, controlled-continuity, thin-plate spline interpolator, with\na tear along its left edge and a crease along its right (Szeliski 1989) c⃝1989 Springer.\ninverse problems. Many computer vision tasks can be viewed as inverse problems, since we\nare trying to recover a full description of the 3D world from a limited set of images.\nIn order to quantify what it means to ﬁnd a smooth solution, we can deﬁne a norm on\nthe solution space. For one-dimensional functions f(x), we can integrate the squared ﬁrst\nderivative of the function,\nE1 =\nZ\nf 2\nx(x) dx\n(3.92)\nor perhaps integrate the squared second derivative,\nE2 =\nZ\nf 2\nxx(x) dx.\n(3.93)\n(Here, we use subscripts to denote differentiation.) Such energy measures are examples of\nfunctionals, which are operators that map functions to scalar values. They are also often called\nvariational methods, because they measure the variation (non-smoothness) in a function.\nIn two dimensions (e.g., for images, ﬂow ﬁelds, or surfaces), the corresponding smooth-\nness functionals are\nE1 =\nZ\nf 2\nx(x, y) + f 2\ny (x, y) dx dy =\nZ\n∥∇f(x, y)∥2 dx dy\n(3.94)\nand\nE2 =\nZ\nf 2\nxx(x, y) + 2f 2\nxy(x, y) + f 2\nyy(x, y) dx dy,\n(3.95)\nwhere the mixed 2f 2\nxy term is needed to make the measure rotationally invariant (Grimson\n1983).\nThe ﬁrst derivative norm is often called the membrane, since interpolating a set of data\npoints using this measure results in a tent-like structure. (In fact, this formula is a small-\ndeﬂection approximation to the surface area, which is what soap bubbles minimize.) The\n176\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nsecond-order norm is called the thin-plate spline, since it approximates the behavior of thin\nplates (e.g., ﬂexible steel) under small deformations. A blend of the two is called the thin-\nplate spline under tension; versions of these formulas where each derivative term is mul-\ntiplied by a local weighting function are called controlled-continuity splines (Terzopoulos\n1988). Figure 3.54 shows a simple example of a controlled-continuity interpolator ﬁt to nine\nscattered data points. In practice, it is more common to ﬁnd ﬁrst-order smoothness terms\nused with images and ﬂow ﬁelds (Section 8.4) and second-order smoothness associated with\nsurfaces (Section 12.3.1).\nIn addition to the smoothness term, regularization also requires a data term (or data\npenalty). For scattered data interpolation (Nielson 1993), the data term measures the dis-\ntance between the function f(x, y) and a set of data points di = d(xi, yi),\nEd =\nX\ni\n[f(xi, yi) −di]2.\n(3.96)\nFor a problem like noise removal, a continuous version of this measure can be used,\nEd =\nZ\n[f(x, y) −d(x, y)]2 dx dy.\n(3.97)\nTo obtain a global energy that can be minimized, the two energy terms are usually added\ntogether,\nE = Ed + λEs,\n(3.98)\nwhere Es is the smoothness penalty (E1, E2 or some weighted blend) and λ is the regulariza-\ntion parameter, which controls how smooth the solution should be.\nIn order to ﬁnd the minimum of this continuous problem, the function f(x, y) is usually\nﬁrst discretized on a regular grid.21 The most principled way to perform this discretization is\nto use ﬁnite element analysis, i.e., to approximate the function with a piecewise continuous\nspline, and then perform the analytic integration (Bathe 2007).\nFortunately, for both the ﬁrst-order and second-order smoothness functionals, the judi-\ncious selection of appropriate ﬁnite elements results in particularly simple discrete forms\n(Terzopoulos 1983). The corresponding discrete smoothness energy functions become\nE1\n=\nX\ni,j\nsx(i, j)[f(i + 1, j) −f(i, j) −gx(i, j)]2\n(3.99)\n+ sy(i, j)[f(i, j + 1) −f(i, j) −gy(i, j)]2\nand\nE2\n=\nh−2 X\ni,j\ncx(i, j)[f(i + 1, j) −2f(i, j) + f(i −1, j)]2\n(3.100)\n21 The alternative of using kernel basis functions centered on the data points (Boult and Kender 1986; Nielson\n1993) is discussed in more detail in Section 12.3.1.",
  "image_path": "page_197.jpg",
  "pages": [
    196,
    197,
    198
  ]
}