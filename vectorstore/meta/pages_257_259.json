{
  "doc_id": "pages_257_259",
  "text": "4.1 Points and patches\n235\n4.1.4 Feature tracking\nAn alternative to independently ﬁnding features in all candidate images and then matching\nthem is to ﬁnd a set of likely feature locations in a ﬁrst image and to then search for their\ncorresponding locations in subsequent images. This kind of detect then track approach is\nmore widely used for video tracking applications, where the expected amount of motion and\nappearance deformation between adjacent frames is expected to be small.\nThe process of selecting good features to track is closely related to selecting good features\nfor more general recognition applications. In practice, regions containing high gradients in\nboth directions, i.e., which have high eigenvalues in the auto-correlation matrix (4.8), provide\nstable locations at which to ﬁnd correspondences (Shi and Tomasi 1994).\nIn subsequent frames, searching for locations where the corresponding patch has low\nsquared difference (4.1) often works well enough. However, if the images are undergo-\ning brightness change, explicitly compensating for such variations (8.9) or using normalized\ncross-correlation (8.11) may be preferable. If the search range is large, it is also often more\nefﬁcient to use a hierarchical search strategy, which uses matches in lower-resolution images\nto provide better initial guesses and hence speed up the search (Section 8.1.1). Alternatives\nto this strategy involve learning what the appearance of the patch being tracked should be and\nthen searching for it in the vicinity of its predicted position (Avidan 2001; Jurie and Dhome\n2002; Williams, Blake, and Cipolla 2003). These topics are all covered in more detail in\nSection 8.1.3.\nIf features are being tracked over longer image sequences, their appearance can undergo\nlarger changes. You then have to decide whether to continue matching against the originally\ndetected patch (feature) or to re-sample each subsequent frame at the matching location. The\nformer strategy is prone to failure as the original patch can undergo appearance changes such\nas foreshortening. The latter runs the risk of the feature drifting from its original location\nto some other location in the image (Shi and Tomasi 1994). (Mathematically, small mis-\nregistration errors compound to create a Markov Random Walk, which leads to larger drift\nover time.)\nA preferable solution is to compare the original patch to later image locations using an\nafﬁne motion model (Section 8.2). Shi and Tomasi (1994) ﬁrst compare patches in neigh-\nboring frames using a translational model and then use the location estimates produced by\nthis step to initialize an afﬁne registration between the patch in the current frame and the\nbase frame where a feature was ﬁrst detected (Figure 4.28). In their system, features are only\ndetected infrequently, i.e., only in regions where tracking has failed. In the usual case, an\narea around the current predicted location of the feature is searched with an incremental reg-\nistration algorithm (Section 8.1.3). The resulting tracker is often called the Kanade–Lucas–\nTomasi (KLT) tracker.\n236\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.28 Feature tracking using an afﬁne motion model (Shi and Tomasi 1994) c⃝1994\nIEEE, Top row: image patch around the tracked feature location. Bottom row: image patch\nafter warping back toward the ﬁrst frame using an afﬁne deformation. Even though the speed\nsign gets larger from frame to frame, the afﬁne transformation maintains a good resemblance\nbetween the original and subsequent tracked frames.\nSince their original work on feature tracking, Shi and Tomasi’s approach has generated a\nstring of interesting follow-on papers and applications. Beardsley, Torr, and Zisserman (1996)\nuse extended feature tracking combined with structure from motion (Chapter 7) to incremen-\ntally build up sparse 3D models from video sequences. Kang, Szeliski, and Shum (1997)\ntie together the corners of adjacent (regularly gridded) patches to provide some additional\nstability to the tracking, at the cost of poorer handling of occlusions. Tommasini, Fusiello,\nTrucco et al. (1998) provide a better spurious match rejection criterion for the basic Shi and\nTomasi algorithm, Collins and Liu (2003) provide improved mechanisms for feature selec-\ntion and dealing with larger appearance changes over time, and Shaﬁque and Shah (2005)\ndevelop algorithms for feature matching (data association) for videos with large numbers of\nmoving objects or points. Yilmaz, Javed, and Shah (2006) and Lepetit and Fua (2005) survey\nthe larger ﬁeld of object tracking, which includes not only feature-based techniques but also\nalternative techniques based on contour and region (Section 5.1).\nOne of the newest developments in feature tracking is the use of learning algorithms to\nbuild special-purpose recognizers to rapidly search for matching features anywhere in an\nimage (Lepetit, Pilet, and Fua 2006; Hinterstoisser, Benhimane, Navab et al. 2008; Rogez,\nRihan, Ramalingam et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010).2 By taking the time\nto train classiﬁers on sample patches and their afﬁne deformations, extremely fast and reliable\nfeature detectors can be constructed, which enables much faster motions to be supported\n(Figure 4.29). Coupling such features to deformable models (Pilet, Lepetit, and Fua 2008) or\nstructure-from-motion algorithms (Klein and Murray 2008) can result in even higher stability.\n2 See also my previous comment on earlier work in learning-based tracking (Avidan 2001; Jurie and Dhome\n2002; Williams, Blake, and Cipolla 2003).\n4.1 Points and patches\n237\nFigure 4.29 Real-time head tracking using the fast trained classiﬁers of Lepetit, Pilet, and\nFua (2004) c⃝2004 IEEE.\n4.1.5 Application: Performance-driven animation\nOne of the most compelling applications of fast feature tracking is performance-driven an-\nimation, i.e., the interactive deformation of a 3D graphics model based on tracking a user’s\nmotions (Williams 1990; Litwinowicz and Williams 1994; Lepetit, Pilet, and Fua 2004).\nBuck, Finkelstein, Jacobs et al. (2000) present a system that tracks a user’s facial expres-\nsions and head motions and then uses them to morph among a series of hand-drawn sketches.\nAn animator ﬁrst extracts the eye and mouth regions of each sketch and draws control lines\nover each image (Figure 4.30a). At run time, a face-tracking system (Toyama 1998) deter-\nmines the current location of these features (Figure 4.30b). The animation system decides\nwhich input images to morph based on nearest neighbor feature appearance matching and\ntriangular barycentric interpolation. It also computes the global location and orientation of\nthe head from the tracked features. The resulting morphed eye and mouth regions are then\ncomposited back into the overall head model to yield a frame of hand-drawn animation (Fig-\nure 4.30d).\nIn more recent work, Barnes, Jacobs, Sanders et al. (2008) watch users animate paper\ncutouts on a desk and then turn the resulting motions and drawings into seamless 2D anima-\ntions.",
  "image_path": "page_258.jpg",
  "pages": [
    257,
    258,
    259
  ]
}