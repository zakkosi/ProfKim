{
  "doc_id": "pages_518_520",
  "text": "496\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntaken under low light conditions often suffer from excessive noise (because of the high ISO\ngains and low photon counts) and blur (due to longer exposures). Is there some way to\ncombine a non-ﬂash photo taken just before the ﬂash goes off with the ﬂash photo to produce\nan image with good color values, sharpness, and low noise?15\nPetschnigg, Agrawala, Hoppe et al. (2004) approach this problem by ﬁrst ﬁltering the no-\nﬂash (ambient) image A with a variant of the bilateral ﬁlter called the joint bilateral ﬁlter16\nin which the range kernel (3.36)\nr(i, j, k, l) = exp\n\u0012\n−∥f(i, j) −f(k, l)∥2\n2σ2r\n\u0013\n(10.23)\nis evaluated on the ﬂash image F instead of the ambient image A, since the ﬂash image is less\nnoisy and hence has more reliable edges (Figure 10.29b). Because the contents of the ﬂash\nimage can be unreliable inside and at the boundaries of shadows and specularities, these are\ndetected and a regular bilaterally ﬁltered image ABase is used instead (Figure 10.30).\nThe second stage of their algorithm computes a ﬂash detail image\nF Detail =\nF + ϵ\nF Base + ϵ,\n(10.24)\nwhere F Base is a bilaterally ﬁltered version of the ﬂash image F and ϵ = 0.02. This detail im-\nage (Figure 10.29c) encodes details that may have been ﬁltered away from the noise-reduced\nno-ﬂash image ANR, as well as additional details created by the ﬂash camera, which often\nadd crispness. The detail image is used to modulate the noise-reduced ambient image ANR\nto produce the ﬁnal results\nAFinal = (1 −M)ANRF Detail + MABase\n(10.25)\nshown in Figures 10.1b and 10.29d.\nEisemann and Durand (2004) present an alternative algorithm that shares some of the\nsame basic concepts. Both papers are well worth reading and contrasting (Exercise 10.6).\nFlash images can also be used for a variety of additional applications such as extracting\nmore reliable foreground mattes of objects (Raskar, Tan, Feris et al. 2004; Sun, Li, Kang et al.\n2006). Flash photography is just one instance of the more general topic of active illumination,\nwhich is discussed in more detail by Raskar and Tumblin (2010).\n15 In fact, the discontinued FujiFilm FinePix F40fd camera takes a pair of ﬂash and no ﬂash images in quick\nsuccession; however, it only lets you decide to keep one of them.\n16 Eisemann and Durand (2004) call this the cross bilateral ﬁlter.\n10.3 Super-resolution and blur removal\n497\nFigure 10.30\nFlash/no-ﬂash photography algorithm (Petschnigg, Agrawala, Hoppe et al.\n2004) c⃝2004 ACM. The ambient (no-ﬂash) image A is ﬁltered with a regular bilateral ﬁlter\nto produce ABase, which is used in shadow and specularity regions, and a joint bilaterally\nﬁltered noise reduced image ANR. The ﬂash image F is bilaterally ﬁltered to produce a\nbase image F Base and a detail (ratio) image F Detail, which is used to modulate the de-\nnoised ambient image. The shadow/specularity mask M is computed by comparing linearized\nversions of the ﬂash and no-ﬂash images.\n10.3 Super-resolution and blur removal\nWhile high dynamic range imaging enables us to obtain an image with a larger dynamic\nrange than a single regular image, super-resolution enables us to create images with higher\nspatial resolution and less noise than regular camera images (Chaudhuri 2001; Park, Park,\nand Kang 2003; Capel and Zisserman 2003; Capel 2004; van Ouwerkerk 2006). Most com-\nmonly, super-resolution refers to the process of aligning and combining several input images\nto produce such high-resolution composites (Irani and Peleg 1991; Cheeseman, Kanefsky,\nHanson et al. 1993; Pickup, Capel, Roberts et al. 2009). However, some newer techniques\ncan super-resolve a single image (Freeman, Jones, and Pasztor 2002; Baker and Kanade 2002;\nFattal 2007) and are hence closely related to techniques for removing blur (Sections 3.4.3 and\n3.4.4).\nThe most principled way to formulate the super-resolution problem is to write down the\nstochastic image formation equations and image priors and to then use Bayesian inference to\nrecover the super-resolved (original) sharp image. We can do this by generalizing the image\n498\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nformation equations (3.75) used for image deblurring (Section 3.4.3), which we also used\nin (10.2) for blur kernel (PSF) estimation (Section 10.1.4). In this case, we have several ob-\nserved images {ok(x)}, as well as an image warping function ˆhk(x) for each observed image\n(Figure 3.47). Combining all of these elements, we get the (noisy) observation equations17\nok(x) = D{b(x) ∗s(ˆhk(x))} + nk(x),\n(10.26)\nwhere D is the downsampling operator, which operates after the super-resolved (sharp)\nwarped image s(ˆhk(x)) has been convolved with the blur kernel b(x). The above image\nformation equations lead to the following least squares problem,\nX\nk\n∥ok(x) −D{bk(x) ∗s(ˆhk(x))}∥2.\n(10.27)\nIn most super-resolution algorithms, the alignment (warping) ˆhk is estimated using one of\nthe input frames as the reference frame; either feature-based (Section 6.1.3) or direct (image-\nbased) (Section 8.2) parametric alignment techniques can be used. (A few algorithms, such\nas those described by Schultz and Stevenson (1996) or Capel (2004) use dense (per-pixel\nﬂow) estimates.) A better approach is to re-compute the alignment by directly minimizing\n(10.27) once an initial estimate of s(x) has been computed (Hardie, Barnard, and Armstrong\n1997) or to marginalize out the motion parameters altogether (Pickup, Capel, Roberts et al.\n2007)—see also the work of Protter and Elad (2009) for some related video super-resolution\nwork.\nThe point spread function (blur kernel) bk is either inferred from knowledge of the image\nformation process (e.g., the amount of motion or defocus blur and the camera sensor optics)\nor calibrated from a test image or the observed images {ok} using one of the techniques\ndescribed in Section 10.1.4. The problem of simultaneously inferring the blur kernel and the\nsharp image is known as blind image deconvolution (Kundur and Hatzinakos 1996; Levin\n2006).18\nGiven an estimate of ˆhk and bk(x), (10.27) can be re-written using matrix/vector notation\nas a large sparse least squares problem in the unknown values of the super-resolved pixels s,\nX\nk\n∥ok −DBkW ks∥2.\n(10.28)\n17 It is also possible to add an unknown bias–gain term to each observation (Capel 2004), as was done for motion\nestimation in (8.8).\n18 Notice that there is a chicken-and-egg problem if both the blur kernel and the super-resolved image are un-\nknown. This can be “broken” either using structural assumptions about the sharp image, e.g., the presence of edges\n(Joshi, Szeliski, and Kriegman 2008) or prior models for the image, such as edge sparsity (Fergus, Singh, Hertzmann\net al. 2006).",
  "image_path": "page_519.jpg",
  "pages": [
    518,
    519,
    520
  ]
}