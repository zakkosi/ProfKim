{
  "doc_id": "pages_700_702",
  "text": "678\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.18\nModular eigenspace for face recognition (Moghaddam and Pentland 1997)\nc⃝1997 IEEE. (a) By detecting separate features in the faces (eyes, nose, mouth), separate\neigenspaces can be estimated for each one. (b) The relative positions of each feature can be\ndetected at recognition time, thus allowing for more ﬂexibility in viewpoint and expression.\nthe highest likelihood pI(∆i). In this case, nearest neighbor search techniques in the space\nspanned by the precomputed {aI\ni } vectors could be used to speed up ﬁnding the best match.13\nAnother way to improve the performance of eigenface-based approaches is to break up\nthe image into separate regions such as the eyes, nose, and mouth (Figure 14.18) and to match\neach of these modular eigenspaces independently (Moghaddam and Pentland 1997; Heisele,\nHo, Wu et al. 2003; Heisele, Serre, and Poggio 2007). The advantage of such a modular\napproach is that it can tolerate a wider range of viewpoints, because each part can move\nrelative to the others. It also supports a larger variety of combinations, e.g., we can model one\nperson as having a narrow nose and bushy eyebrows, without requiring the eigenfaces to span\nall possible combinations of nose, mouth, and eyebrows. (If you remember the cardboard\nchildren’s books where you can select different top and bottom faces, or Mr. Potato Head,\nyou get the idea.)\nAnother approach to dealing with large variability in appearance is to create view-based\n(view-speciﬁc) eigenspaces, as shown in Figure 14.19 (Moghaddam and Pentland 1997). We\ncan think of these view-based eigenspaces as local descriptors that select different axes de-\npending on which part of the face space you are in. Note that such approaches, however,\npotentially require large amounts of training data, i.e., pictures of every person in every pos-\nsible pose or expression. This is in contrast to the shape and appearance models we study in\n13 Note that while the covariance matrices ΣI and ΣE are computed by looking at differences between all pairs of\nimages, the run-time evaluation selects the nearest image to determine the facial identity. Whether this is statistically\ncorrect is explored in Exercise 14.4.\n14.2 Face recognition\n679\n(a)\n(b)\nFigure 14.19 View-based eigenspace (Moghaddam and Pentland 1997) c⃝1997 IEEE. (a)\nComparison between a regular (parametric) eigenspace reconstruction (middle column) and\na view-based eigenspace reconstruction (right column) corresponding to the input image (left\ncolumn). The top row is from a training image, the bottom row is from the test set. (b) A\nschematic representation of the two approaches, showing how each view computes its own\nlocal basis representation.\nSection 14.2.2, which can learn deformations across all individuals.\nIt is also possible to generalize the bilinear factorization implicit in PCA and SVD ap-\nproaches to multilinear (tensor) formulations that can model several interacting factors si-\nmultaneously (Vasilescu and Terzopoulos 2007). These ideas are related to currently active\ntopics in machine learning such as subspace learning (Cai, He, Hu et al. 2007), local distance\nfunctions (Frome, Singer, Sha et al. 2007), and metric learning (Ramanan and Baker 2009).\nLearning approaches play an increasingly important role in face recognition, e.g., in the work\nof Sivic, Everingham, and Zisserman (2009) and Guillaumin, Verbeek, and Schmid (2009).\n14.2.2 Active appearance and 3D shape models\nThe need to use modular or view-based eigenspaces for face recognition is symptomatic of\na more general observation, i.e., that facial appearance and identiﬁability depend as much\non shape as they do on color or texture (which is what eigenfaces capture). Furthermore,\nwhen dealing with 3D head rotations, the pose of a person’s head should be discounted when\nperforming recognition.\nIn fact, the earliest face recognition systems, such as those by Fischler and Elschlager\n(1973), Kanade (1977), and Yuille (1991), found distinctive feature points on facial images\nand performed recognition on the basis of their relative positions or distances. Newer tech-\nniques such as local feature analysis (Penev and Atick 1996) and elastic bunch graph match-\ning (Wiskott, Fellous, Kr¨uger et al. 1997) combine local ﬁlter responses (jets) at distinctive\n680\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.20 Manipulating facial appearance through shape and color (Rowland and Perrett\n1995) c⃝1995 IEEE. By adding or subtracting gender-speciﬁc shape and color characteristics\nto (b) an input image, different amounts of gender variation can be induced. The amounts\nadded (from the mean) are: (a) +50% (gender enhancement), (c) -50% (near “androgyny”),\n(d) -100% (gender switched), and (e) -150% (opposite gender attributes enhanced).\nfeature locations together with shape models to perform recognition.\nA visually compelling example of why both shape and texture are important is the work\nof Rowland and Perrett (1995), who manually traced the contours of facial features and then\nused these contours to normalize (warp) each image to a canonical shape. After analyzing\nboth the shape and color images for deviations from the mean, they were able to associate\ncertain shape and color deformations with personal characteristics such as age and gender\n(Figure 14.20). Their work demonstrates that both shape and color have an important inﬂu-\nence on the perception of such characteristics.\nAround the same time, researchers in computer vision were beginning to use simultane-\nous shape deformations and texture interpolation to model the variability in facial appearance\ncaused by identity or expression (Beymer 1996; Vetter and Poggio 1997), developing tech-\nniques such as Active Shape Models (Lanitis, Taylor, and Cootes 1997), 3D Morphable Mod-\nels (Blanz and Vetter 1999), and Elastic Bunch Graph Matching (Wiskott, Fellous, Kr¨uger et\nal. 1997).14\nOf all these techniques, the active appearance models (AAMs) of Cootes, Edwards, and\nTaylor (2001) are among the most widely used for face recognition and tracking. Like other\nshape and texture models, an AAM models both the variation in the shape of an image s,\nwhich is normally encoded by the location of key feature points on the image (Figure 14.21b),\n14 We have already seen the application of PCA to 3D head and face modeling and animation in Section 12.6.3.",
  "image_path": "page_701.jpg",
  "pages": [
    700,
    701,
    702
  ]
}