{
  "doc_id": "pages_645_647",
  "text": "13.1 View interpolation\n623\nto subtle artifacts.\nThe ﬁnal step in view interpolation (Figure 13.2d) is to ﬁll any remaining holes or cracks\ndue to the forward warping process or lack of source data (scene visibility). This can be done\nby copying pixels from the further pixels adjacent to the hole. (Otherwise, foreground objects\nare subject to a “fattening effect”.)\nThe above process works well for rigid scenes, although its visual quality (lack of alias-\ning) can be improved using a two-pass, forward–backward algorithm (Section 13.2.1) (Shade,\nGortler, He et al. 1998) or full 3D rendering (Zitnick, Kang, Uyttendaele et al. 2004). In the\ncase where the two reference images are views of a non-rigid scene, e.g., a person smiling\nin one image and frowning in the other, view morphing, which combines ideas from view\ninterpolation with regular morphing, can be used (Seitz and Dyer 1996).\nWhile the original view interpolation paper describes how to generate novel views based\non similar pre-computed (linear perspective) images, the plenoptic modeling paper of McMil-\nlan and Bishop (1995) argues that cylindrical images should be used to store the pre-computed\nrendering or real-world images. (Chen 1995) also propose using environment maps (cylin-\ndrical, cubic, or spherical) as source images for view interpolation.\n13.1.1 View-dependent texture maps\nView-dependent texture maps (Debevec, Taylor, and Malik 1996) are closely related to view\ninterpolation. Instead of associating a separate depth map with each input image, a single 3D\nmodel is created for the scene, but different images are used as texture map sources depending\non the virtual camera’s current position (Figure 13.3a).3\nIn more detail, given a new virtual camera position, the similarity of this camera’s view of\neach polygon (or pixel) is compared to that of potential source images. The images are then\nblended using a weighting that is inversely proportional to the angles αi between the virtual\nview and the source views (Figure 13.3a). Even though the geometric model can be fairly\ncoarse (Figure 13.3b), blending between different views gives a strong sense of more detailed\ngeometry because of the parallax (visual motion) between corresponding pixels. While the\noriginal paper performs the weighted blend computation separately at each pixel or coarsened\npolygon face, follow-on work by Debevec, Yu, and Borshukov (1998) presents a more efﬁ-\ncient implementation based on precomputing contributions for various portions of viewing\nspace and then using projective texture mapping (OpenGL-ARB 1997).\nThe idea of view-dependent texture mapping has been used in a large number of sub-\nsequent image-based rendering systems, including facial modeling and animation (Pighin,\n3 The term image-based modeling, which is now commonly used to describe the creation of texture-mapped 3D\nmodels from multiple images, appears to have ﬁrst been used by Debevec, Taylor, and Malik (1996), who also used\nthe term photogrammetric modeling to describe the same process.\n624\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 13.3 View-dependent texture mapping (Debevec, Taylor, and Malik 1996) c⃝1996\nACM. (a) The weighting given to each input view depends on the relative angles between the\nnovel (virtual) view and the original views; (b) simpliﬁed 3D model geometry; (c) with view-\ndependent texture mapping, the geometry appears to have more detail (recessed windows).\nHecker, Lischinski et al. 1998) and 3D scanning and visualization (Pulli, Abi-Rached, Duchamp\net al. 1998). Closely related to view-dependent texture mapping is the idea of blending be-\ntween light rays in 4D space, which forms the basis of the Lumigraph and unstructured Lu-\nmigraph systems (Section 13.3) (Gortler, Grzeszczuk, Szeliski et al. 1996; Buehler, Bosse,\nMcMillan et al. 2001).\nIn order to provide even more realism in their Fac¸ade system, Debevec, Taylor, and Malik\n(1996) also include a model-based stereo component, which optionally computes an offset\n(parallax) map for each coarse planar facet of their 3D model. They call the resulting analysis\nand rendering system a hybrid geometry- and image-based approach, since it uses traditional\n3D geometric modeling to create the global 3D model, but then uses local depth offsets, along\nwith view interpolation, to add visual realism.\n13.1.2 Application: Photo Tourism\nWhile view interpolation was originally developed to accelerate the rendering of 3D scenes\non low-powered processors and systems without graphics acceleration, it turns out that it\ncan be applied directly to large collections of casually acquired photographs. The Photo\nTourism system developed by Snavely, Seitz, and Szeliski (2006) uses structure from motion\nto compute the 3D locations and poses of all the cameras taking the images, along with a\nsparse 3D point-cloud model of the scene (Section 7.4.4, Figure 7.11).\nTo perform an image-based exploration of the resulting sea of images (Aliaga, Funkhouser,\nYanovsky et al. 2003), Photo Tourism ﬁrst associates a 3D proxy with each image. While a\ntriangulated mesh obtained from the point cloud can sometimes form a suitable proxy, e.g.,\nfor outdoor terrain models, a simple dominant plane ﬁt to the 3D points visible in each image\n13.1 View interpolation\n625\n(a)\n(b)\nFigure 13.4\nPhoto Tourism (Snavely, Seitz, and Szeliski 2006): c⃝2006 ACM: (a) a 3D\noverview of the scene, with translucent washes and lines painted onto the planar impostors;\n(b) once the user has selected a region of interest, a set of related thumbnails is displayed\nalong the bottom; (c) planar proxy selection for optimal stabilization (Snavely, Garg, Seitz et\nal. 2008) c⃝2008 ACM.\noften performs better, because it does not contain any erroneous segments or connections that\npop out as artifacts. As automated 3D modeling techniques continue to improve, however,\nthe pendulum may swing back to more detailed 3D geometry (Goesele, Snavely, Curless et\nal. 2007; Sinha, Steedly, and Szeliski 2009).\nThe resulting image-based navigation system lets users move from photo to photo, ei-\nther by selecting cameras from a top-down view of the scene (Figure 13.4a) or by selecting\nregions of interest in an image, navigating to nearby views, or selecting related thumbnails\n(Figure 13.4b). To create a background for the 3D scene, e.g., when being viewed from\nabove, non-photorealistic techniques (Section 10.5.2), such as translucent color washes or\nhighlighted 3D line segments, can be used (Figure 13.4a). The system can also be used to\nannotate regions of images and to automatically propagate such annotations to other pho-\ntographs.\nThe 3D planar proxies used in Photo Tourism and the related Photosynth system from\nMicrosoft result in non-photorealistic transitions reminiscent of visual effects such as “page\nﬂips”. Selecting a stable 3D axis for all the planes can reduce the amount of swimming and\nenhance the perception of 3D (Figure 13.4c) (Snavely, Garg, Seitz et al. 2008). It is also\npossible to automatically detect objects in the scene that are seen from multiple views and\ncreate “orbits” of viewpoints around such objects. Furthermore, nearby images in both 3D\nposition and viewing direction can be linked to create “virtual paths”, which can then be used\nto navigate between arbitrary pairs of images, such as those you might take yourself while\nwalking around a popular tourist site (Snavely, Garg, Seitz et al. 2008).\nThe spatial matching of image features and regions performed by Photo Tourism can\nalso be used to infer more information from large image collections. For example, Simon,\nSnavely, and Seitz (2007) show how the match graph between images of popular tourist sites",
  "image_path": "page_646.jpg",
  "pages": [
    645,
    646,
    647
  ]
}