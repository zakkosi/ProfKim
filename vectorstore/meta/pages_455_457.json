{
  "doc_id": "pages_455_457",
  "text": "9.1 Motion models\n433\nΠ∞:\n(0,0,0,1)·p= 0\nR10\nx1 = (x1,y1,f1)\n~\nx0 = (x0,y0,f0)\n~\nFigure 9.3 Pure 3D camera rotation. The form of the homography (mapping) is particularly\nsimple and depends only on the 3D rotation matrix and focal lengths.\n9.1.3 Rotational panoramas\nThe most typical case for panoramic image stitching is when the camera undergoes a pure ro-\ntation. Think of standing at the rim of the Grand Canyon. Relative to the distant geometry in\nthe scene, as you snap away, the camera is undergoing a pure rotation, which is equivalent to\nassuming that all points are very far from the camera, i.e., on the plane at inﬁnity (Figure 9.3).\nSetting t0 = t1 = 0, we get the simpliﬁed 3 × 3 homography\n˜\nH10 = K1R1R−1\n0 K−1\n0\n= K1R10K−1\n0 ,\n(9.5)\nwhere Kk = diag(fk, fk, 1) is the simpliﬁed camera intrinsic matrix (2.59), assuming that\ncx = cy = 0, i.e., we are indexing the pixels starting from the optical center (Szeliski 1996).\nThis can also be re-written as\n\n\nx1\ny1\n1\n\n∼\n\n\nf1\nf1\n1\n\nR10\n\n\nf −1\n0\nf −1\n0\n1\n\n\n\n\nx0\ny0\n1\n\n\n(9.6)\nor\n\n\nx1\ny1\nf1\n\n∼R10\n\n\nx0\ny0\nf0\n\n,\n(9.7)\nwhich reveals the simplicity of the mapping equations and makes all of the motion parameters\nexplicit. Thus, instead of the general eight-parameter homography relating a pair of images,\nwe get the three-, four-, or ﬁve-parameter 3D rotation motion models corresponding to the\ncases where the focal length f is known, ﬁxed, or variable (Szeliski and Shum 1997).3 Es-\ntimating the 3D rotation matrix (and, optionally, focal length) associated with each image is\n3 An initial estimate of the focal lengths can be obtained using the intrinsic calibration techniques described in\nSection 6.3.4 or from EXIF tags.\n434\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nintrinsically more stable than estimating a homography with a full eight degrees of freedom,\nwhich makes this the method of choice for large-scale image stitching algorithms (Szeliski\nand Shum 1997; Shum and Szeliski 2000; Brown and Lowe 2007).\nGiven this representation, how do we update the rotation matrices to best align two over-\nlapping images? Given a current estimate for the homography ˜\nH10 in (9.5), the best way to\nupdate R10 is to prepend an incremental rotation matrix R(ω) to the current estimate R10\n(Szeliski and Shum 1997; Shum and Szeliski 2000),\n˜\nH(ω) = K1R(ω)R10K−1\n0\n= [K1R(ω)K−1\n1 ][K1R10K−1\n0 ] = D ˜\nH10.\n(9.8)\nNote that here we have written the update rule in the compositional form, where the in-\ncremental update D is prepended to the current homography ˜\nH10. Using the small-angle\napproximation to R(ω) given in (2.35), we can write the incremental update matrix as\nD = K1R(ω)K−1\n1\n≈K1(I + [ω]×)K−1\n1\n=\n\n\n1\n−ωz\nf1ωy\nωz\n1\n−f1ωx\n−ωy/f1\nωx/f1\n1\n\n.\n(9.9)\nNotice how there is now a nice one-to-one correspondence between the entries in the D\nmatrix and the h00, . . . , h21 parameters used in Table 6.1 and Equation (6.19), i.e.,\n(h00, h01, h02, h00, h11, h12, h20, h21) = (0, −ωz, f1ωy, ωz, 0, −f1ωx, −ωy/f1, ωx/f1).\n(9.10)\nWe can therefore apply the chain rule to Equations (6.24 and 9.10) to obtain\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\n−xy/f1\nf1 + x2/f1\n−y\n−(f1 + y2/f1)\nxy/f1\nx\n# \n\nωx\nωy\nωz\n\n,\n(9.11)\nwhich give us the linearized update equations needed to estimate ω = (ωx, ωy, ωz).4 Notice\nthat this update rule depends on the focal length f1 of the target view and is independent\nof the focal length f0 of the template view. This is because the compositional algorithm\nessentially makes small perturbations to the target. Once the incremental rotation vector ω\nhas been computed, the R1 rotation matrix can be updated using R1 ←R(ω)R1.\nThe formulas for updating the focal length estimates are a little more involved and are\ngiven in (Shum and Szeliski 2000). We will not repeat them here, since an alternative up-\ndate rule, based on minimizing the difference between back-projected 3D rays, is given in\nSection 9.2.1. Figure 9.4 shows the alignment of four images under the 3D rotation motion\nmodel.\n4 This is the same as the rotational component of instantaneous rigid ﬂow (Bergen, Anandan, Hanna et al. 1992)\nand the update equations given by Szeliski and Shum (1997) and Shum and Szeliski (2000).\n9.1 Motion models\n435\nFigure 9.4\nFour images taken with a hand-held camera registered using a 3D rotation mo-\ntion model (Szeliski and Shum 1997) c⃝1997 ACM. Notice how the homographies, rather\nthan being arbitrary, have a well-deﬁned keystone shape whose width increases away from\nthe origin.\n9.1.4 Gap closing\nThe techniques presented in this section can be used to estimate a series of rotation matrices\nand focal lengths, which can be chained together to create large panoramas. Unfortunately,\nbecause of accumulated errors, this approach will rarely produce a closed 360◦panorama.\nInstead, there will invariably be either a gap or an overlap (Figure 9.5).\nWe can solve this problem by matching the ﬁrst image in the sequence with the last one.\nThe difference between the two rotation matrix estimates associated with the repeated ﬁrst\nindicates the amount of misregistration. This error can be distributed evenly across the whole\nsequence by taking the quotient of the two quaternions associated with these rotations and\ndividing this “error quaternion” by the number of images in the sequence (assuming relatively\nconstant inter-frame rotations). We can also update the estimated focal length based on the\namount of misregistration. To do this, we ﬁrst convert the error quaternion into a gap angle,\nθg and then update the focal length using the equation f ′ = f(1 −θg/360◦).\nFigure 9.5a shows the end of registered image sequence and the ﬁrst image. There is a\nbig gap between the last image and the ﬁrst which are in fact the same image. The gap is\n32◦because the wrong estimate of focal length (f = 510) was used. Figure 9.5b shows the\nregistration after closing the gap with the correct focal length (f = 468). Notice that both\nmosaics show very little visual misregistration (except at the gap), yet Figure 9.5a has been\ncomputed using a focal length that has 9% error. Related approaches have been developed by\nHartley (1994b), McMillan and Bishop (1995), Stein (1995), and Kang and Weiss (1997) to\nsolve the focal length estimation problem using pure panning motion and cylindrical images.",
  "image_path": "page_456.jpg",
  "pages": [
    455,
    456,
    457
  ]
}