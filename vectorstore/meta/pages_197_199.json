{
  "doc_id": "pages_197_199",
  "text": "3.7 Global optimization\n175\n(a)\n(b)\nFigure 3.54 A simple surface interpolation problem: (a) nine data points of various height\nscattered on a grid; (b) second-order, controlled-continuity, thin-plate spline interpolator, with\na tear along its left edge and a crease along its right (Szeliski 1989) c⃝1989 Springer.\ninverse problems. Many computer vision tasks can be viewed as inverse problems, since we\nare trying to recover a full description of the 3D world from a limited set of images.\nIn order to quantify what it means to ﬁnd a smooth solution, we can deﬁne a norm on\nthe solution space. For one-dimensional functions f(x), we can integrate the squared ﬁrst\nderivative of the function,\nE1 =\nZ\nf 2\nx(x) dx\n(3.92)\nor perhaps integrate the squared second derivative,\nE2 =\nZ\nf 2\nxx(x) dx.\n(3.93)\n(Here, we use subscripts to denote differentiation.) Such energy measures are examples of\nfunctionals, which are operators that map functions to scalar values. They are also often called\nvariational methods, because they measure the variation (non-smoothness) in a function.\nIn two dimensions (e.g., for images, ﬂow ﬁelds, or surfaces), the corresponding smooth-\nness functionals are\nE1 =\nZ\nf 2\nx(x, y) + f 2\ny (x, y) dx dy =\nZ\n∥∇f(x, y)∥2 dx dy\n(3.94)\nand\nE2 =\nZ\nf 2\nxx(x, y) + 2f 2\nxy(x, y) + f 2\nyy(x, y) dx dy,\n(3.95)\nwhere the mixed 2f 2\nxy term is needed to make the measure rotationally invariant (Grimson\n1983).\nThe ﬁrst derivative norm is often called the membrane, since interpolating a set of data\npoints using this measure results in a tent-like structure. (In fact, this formula is a small-\ndeﬂection approximation to the surface area, which is what soap bubbles minimize.) The\n176\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nsecond-order norm is called the thin-plate spline, since it approximates the behavior of thin\nplates (e.g., ﬂexible steel) under small deformations. A blend of the two is called the thin-\nplate spline under tension; versions of these formulas where each derivative term is mul-\ntiplied by a local weighting function are called controlled-continuity splines (Terzopoulos\n1988). Figure 3.54 shows a simple example of a controlled-continuity interpolator ﬁt to nine\nscattered data points. In practice, it is more common to ﬁnd ﬁrst-order smoothness terms\nused with images and ﬂow ﬁelds (Section 8.4) and second-order smoothness associated with\nsurfaces (Section 12.3.1).\nIn addition to the smoothness term, regularization also requires a data term (or data\npenalty). For scattered data interpolation (Nielson 1993), the data term measures the dis-\ntance between the function f(x, y) and a set of data points di = d(xi, yi),\nEd =\nX\ni\n[f(xi, yi) −di]2.\n(3.96)\nFor a problem like noise removal, a continuous version of this measure can be used,\nEd =\nZ\n[f(x, y) −d(x, y)]2 dx dy.\n(3.97)\nTo obtain a global energy that can be minimized, the two energy terms are usually added\ntogether,\nE = Ed + λEs,\n(3.98)\nwhere Es is the smoothness penalty (E1, E2 or some weighted blend) and λ is the regulariza-\ntion parameter, which controls how smooth the solution should be.\nIn order to ﬁnd the minimum of this continuous problem, the function f(x, y) is usually\nﬁrst discretized on a regular grid.21 The most principled way to perform this discretization is\nto use ﬁnite element analysis, i.e., to approximate the function with a piecewise continuous\nspline, and then perform the analytic integration (Bathe 2007).\nFortunately, for both the ﬁrst-order and second-order smoothness functionals, the judi-\ncious selection of appropriate ﬁnite elements results in particularly simple discrete forms\n(Terzopoulos 1983). The corresponding discrete smoothness energy functions become\nE1\n=\nX\ni,j\nsx(i, j)[f(i + 1, j) −f(i, j) −gx(i, j)]2\n(3.99)\n+ sy(i, j)[f(i, j + 1) −f(i, j) −gy(i, j)]2\nand\nE2\n=\nh−2 X\ni,j\ncx(i, j)[f(i + 1, j) −2f(i, j) + f(i −1, j)]2\n(3.100)\n21 The alternative of using kernel basis functions centered on the data points (Boult and Kender 1986; Nielson\n1993) is discussed in more detail in Section 12.3.1.\n3.7 Global optimization\n177\n+ 2cm(i, j)[f(i + 1, j + 1) −f(i + 1, j) −f(i, j + 1) + f(i, j)]2\n+ cy(i, j)[f(i, j + 1) −2f(i, j) + f(i, j −1)]2,\nwhere h is the size of the ﬁnite element grid. The h factor is only important if the energy is\nbeing discretized at a variety of resolutions, as in coarse-to-ﬁne or multigrid techniques.\nThe optional smoothness weights sx(i, j) and sy(i, j) control the location of horizon-\ntal and vertical tears (or weaknesses) in the surface. For other problems, such as coloriza-\ntion (Levin, Lischinski, and Weiss 2004) and interactive tone mapping (Lischinski, Farbman,\nUyttendaele et al. 2006a), they control the smoothness in the interpolated chroma or expo-\nsure ﬁeld and are often set inversely proportional to the local luminance gradient strength.\nFor second-order problems, the crease variables cx(i, j), cm(i, j), and cy(i, j) control the\nlocations of creases in the surface (Terzopoulos 1988; Szeliski 1990a).\nThe data values gx(i, j) and gy(i, j) are gradient data terms (constraints) used by al-\ngorithms, such as photometric stereo (Section 12.1.1), HDR tone mapping (Section 10.2.1)\n(Fattal, Lischinski, and Werman 2002), Poisson blending (Section 9.3.4) (P´erez, Gangnet,\nand Blake 2003), and gradient-domain blending (Section 9.3.4) (Levin, Zomet, Peleg et al.\n2004). They are set to zero when just discretizing the conventional ﬁrst-order smoothness\nfunctional (3.94).\nThe two-dimensional discrete data energy is written as\nEd =\nX\ni,j\nw(i, j)[f(i, j) −d(i, j)]2,\n(3.101)\nwhere the local weights w(i, j) control how strongly the data constraint is enforced. These\nvalues are set to zero where there is no data and can be set to the inverse variance of the data\nmeasurements when there is data (as discussed by Szeliski (1989) and in Section 3.7.2).\nThe total energy of the discretized problem can now be written as a quadratic form\nE = Ed + λEs = xT Ax −2xT b + c,\n(3.102)\nwhere x = [f(0, 0) . . . f(m −1, n −1)] is called the state vector.22\nThe sparse symmetric positive-deﬁnite matrix A is called the Hessian since it encodes the\nsecond derivative of the energy function.23 For the one-dimensional, ﬁrst-order problem, A\nis tridiagonal; for the two-dimensional, ﬁrst-order problem, it is multi-banded with ﬁve non-\nzero entries per row. We call b the weighted data vector. Minimizing the above quadratic\nform is equivalent to solving the sparse linear system\nAx = b,\n(3.103)\n22 We use x instead of f because this is the more common form in the numerical analysis literature (Golub and\nVan Loan 1996).\n23 In numerical analysis, A is called the coefﬁcient matrix (Saad 2003); in ﬁnite element analysis (Bathe 2007), it\nis called the stiffness matrix.",
  "image_path": "page_198.jpg",
  "pages": [
    197,
    198,
    199
  ]
}