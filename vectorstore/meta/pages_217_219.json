{
  "doc_id": "pages_217_219",
  "text": "3.9 Exercises\n195\nyour camera perform a simple linear mapping between RAW values and the color-\nbalanced values in a JPEG? Some high-end cameras have a RAW+JPEG mode, which\nmakes this comparison much easier.\n4. Can you think of any reason why you might want to perform a color twist (Sec-\ntion 3.1.2) on the images? See also Exercise 2.9 for some related ideas.\nEx 3.2: Compositing and reﬂections\nSection 3.1.3 describes the process of compositing\nan alpha-matted image on top of another. Answer the following questions and optionally\nvalidate them experimentally:\n1. Most captured images have gamma correction applied to them. Does this invalidate the\nbasic compositing equation (3.8); if so, how should it be ﬁxed?\n2. The additive (pure reﬂection) model may have limitations. What happens if the glass is\ntinted, especially to a non-gray hue? How about if the glass is dirty or smudged? How\ncould you model wavy glass or other kinds of refractive objects?\nEx 3.3: Blue screen matting\nSet up a blue or green background, e.g., by buying a large\npiece of colored posterboard. Take a picture of the empty background, and then of the back-\nground with a new object in front of it. Pull the matte using the difference between each\ncolored pixel and its assumed corresponding background pixel, using one of the techniques\ndescribed in Section 3.1.3) or by Smith and Blinn (1996).\nEx 3.4: Difference keying\nImplement a difference keying algorithm (see Section 3.1.3)\n(Toyama, Krumm, Brumitt et al. 1999), consisting of the following steps:\n1. Compute the mean and variance (or median and robust variance) at each pixel in an\n“empty” video sequence.\n2. For each new frame, classify each pixel as foreground or background (set the back-\nground pixels to RGBA=0).\n3. (Optional) Compute the alpha channel and composite over a new background.\n4. (Optional) Clean up the image using morphology (Section 3.3.1), label the connected\ncomponents (Section 3.3.4), compute their centroids, and track them from frame to\nframe. Use this to build a “people counter”.\nEx 3.5: Photo effects\nWrite a variety of photo enhancement or effects ﬁlters: contrast, so-\nlarization (quantization), etc. Which ones are useful (perform sensible corrections) and which\nones are more creative (create unusual images)?\n196\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 3.6: Histogram equalization\nCompute the gray level (luminance) histogram for an im-\nage and equalize it so that the tones look better (and the image is less sensitive to exposure\nsettings). You may want to use the following steps:\n1. Convert the color image to luminance (Section 3.1.2).\n2. Compute the histogram, the cumulative distribution, and the compensation transfer\nfunction (Section 3.1.4).\n3. (Optional) Try to increase the “punch” in the image by ensuring that a certain fraction\nof pixels (say, 5%) are mapped to pure black and white.\n4. (Optional) Limit the local gain f ′(I) in the transfer function. One way to do this is to\nlimit f(I) < γI or f ′(I) < γ while performing the accumulation (3.9), keeping any\nunaccumulated values “in reserve”. (I’ll let you ﬁgure out the exact details.)\n5. Compensate the luminance channel through the lookup table and re-generate the color\nimage using color ratios (2.116).\n6. (Optional) Color values that are clipped in the original image, i.e., have one or more\nsaturated color channels, may appear unnatural when remapped to a non-clipped value.\nExtend your algorithm to handle this case in some useful way.\nEx 3.7: Local histogram equalization\nCompute the gray level (luminance) histograms for\neach patch, but add to vertices based on distance (a spline).\n1. Build on Exercise 3.6 (luminance computation).\n2. Distribute values (counts) to adjacent vertices (bilinear).\n3. Convert to CDF (look-up functions).\n4. (Optional) Use low-pass ﬁltering of CDFs.\n5. Interpolate adjacent CDFs for ﬁnal lookup.\nEx 3.8: Padding for neighborhood operations\nWrite down the formulas for computing\nthe padded pixel values ˜f(i, j) as a function of the original pixel values f(k, l) and the image\nwidth and height (M, N) for each of the padding modes shown in Figure 3.13. For example,\nfor replication (clamping),\n˜f(i, j) = f(k, l),\nk = max(0, min(M −1, i)),\nl = max(0, min(N −1, j)),\n(Hint: you may want to use the min, max, mod, and absolute value operators in addition to\nthe regular arithmetic operators.)\n3.9 Exercises\n197\n• Describe in more detail the advantages and disadvantages of these various modes.\n• (Optional) Check what your graphics card does by drawing a texture-mapped rectangle\nwhere the texture coordinates lie beyond the [0.0, 1.0] range and using different texture\nclamping modes.\nEx 3.9: Separable ﬁlters\nImplement convolution with a separable kernel. The input should\nbe a grayscale or color image along with the horizontal and vertical kernels. Make sure\nyou support the padding mechanisms developed in the previous exercise. You will need this\nfunctionality for some of the later exercises. If you already have access to separable ﬁltering\nin an image processing package you are using (such as IPL), skip this exercise.\n• (Optional) Use Pietro Perona’s (1995) technique to approximate convolution as a sum\nof a number of separable kernels. Let the user specify the number of kernels and report\nback some sensible metric of the approximation ﬁdelity.\nEx 3.10: Discrete Gaussian ﬁlters\nDiscuss the following issues with implementing a dis-\ncrete Gaussian ﬁlter:\n• If you just sample the equation of a continuous Gaussian ﬁlter at discrete locations,\nwill you get the desired properties, e.g., will the coefﬁcients sum up to 0? Similarly, if\nyou sample a derivative of a Gaussian, do the samples sum up to 0 or have vanishing\nhigher-order moments?\n• Would it be preferable to take the original signal, interpolate it with a sinc, blur with a\ncontinuous Gaussian, then pre-ﬁlter with a sinc before re-sampling? Is there a simpler\nway to do this in the frequency domain?\n• Would it make more sense to produce a Gaussian frequency response in the Fourier\ndomain and to then take an inverse FFT to obtain a discrete ﬁlter?\n• How does truncation of the ﬁlter change its frequency response? Does it introduce any\nadditional artifacts?\n• Are the resulting two-dimensional ﬁlters as rotationally invariant as their continuous\nanalogs? Is there some way to improve this? In fact, can any 2D discrete (separable or\nnon-separable) ﬁlter be truly rotationally invariant?\nEx 3.11: Sharpening, blur, and noise removal\nImplement some softening, sharpening, and\nnon-linear diffusion (selective sharpening or noise removal) ﬁlters, such as Gaussian, median,\nand bilateral (Section 3.3.1), as discussed in Section 3.4.4.\nTake blurry or noisy images (shooting in low light is a good way to get both) and try to\nimprove their appearance and legibility.",
  "image_path": "page_218.jpg",
  "pages": [
    217,
    218,
    219
  ]
}