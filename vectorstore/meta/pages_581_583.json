{
  "doc_id": "pages_581_583",
  "text": "11.6 Multi-view stereo\n559\nFigure 11.14\nBackground replacement using z-keying with a bi-layer segmentation algo-\nrithm (Kolmogorov, Criminisi, Blake et al. 2006) c⃝2006 IEEE.\ncreating complete 3D object models, but also simpler techniques for improving the quality of\ndepth maps using multiple source images.\nAs we saw in our discussion of plane sweep (Section 11.1.2), it is possible to resample\nall neighboring k images at each disparity hypothesis d into a generalized disparity space\nvolume ˜I(x, y, d, k). The simplest way to take advantage of these additional images is to sum\nup their differences from the reference image Ir as in (11.4),\nC(x, y, d) =\nX\nk\nρ(˜I(x, y, d, k) −Ir(x, y)).\n(11.15)\nThis is the basis of the well-known sum of summed-squared-difference (SSSD) and SSAD\napproaches (Okutomi and Kanade 1993; Kang, Webb, Zitnick et al. 1995), which can be ex-\ntended to reason about likely patterns of occlusion (Nakamura, Matsuura, Satoh et al. 1996).\nMore recent work by Gallup, Frahm, Mordohai et al. (2008) show how to adapt the base-\nlines used to the expected depth in order to get the best tradeoff between geometric accuracy\n(wide baseline) and robustness to occlusion (narrow baseline). Alternative multi-view cost\nmetrics include measures such as synthetic focus sharpness and the entropy of the pixel color\ndistribution (Vaish, Szeliski, Zitnick et al. 2006).\nA useful way to visualize the multi-frame stereo estimation problem is to examine the\nepipolar plane image (EPI) formed by stacking corresponding scanlines from all the images,\nas shown in Figures 8.13c and 11.15 (Bolles, Baker, and Marimont 1987; Baker and Bolles\n1989; Baker 1989). As you can see in Figure 11.15, as a camera translates horizontally (in a\nstandard horizontally rectiﬁed geometry), objects at different depths move sideways at a rate\ninversely proportional to their depth (11.1).6 Foreground objects occlude background objects,\nwhich can be seen as EPI-strips (Criminisi, Kang, Swaminathan et al. 2005) occluding other\n6 The four-dimensional generalization of the EPI is the light ﬁeld, which we study in Section 13.3. In principle,\n560\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nA\nE\nD\nC\nB\nleft\nmiddle\nright\nF\nx\nt\n(a)\n(b)\nFigure 11.15 Epipolar plane image (EPI) (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996\nACM and a schematic EPI (Kang, Szeliski, and Chai 2001) c⃝2001 IEEE. (a) The Lumigraph\n(light ﬁeld) (Section 13.3) is the 4D space of all light rays passing through a volume of space.\nTaking a 2D slice results in all of the light rays embedded in a plane and is equivalent to a\nscanline taken from a stacked EPI volume. Objects at different depths move sideways with\nvelocities (slopes) proportional to their inverse depth. Occlusion (and translucency) effects\ncan easily be seen in this representation. (b) The EPI corresponding to Figure 11.16 showing\nthe three images (middle, left, and right) as slices through the EPI volume. The spatially and\ntemporally shifted window around the black pixel is indicated by the rectangle, showing the\nright image is not being used in matching.\nstrips in the EPI. If we are given a dense enough set of images, we can ﬁnd such strips and\nreason about their relationships in order to both reconstruct the 3D scene and make inferences\nabout translucent objects (Tsin, Kang, and Szeliski 2006) and specular reﬂections (Swami-\nnathan, Kang, Szeliski et al. 2002; Criminisi, Kang, Swaminathan et al. 2005). Alternatively,\nwe can treat the series of images as a set of sequential observations and merge them using\nKalman ﬁltering (Matthies, Kanade, and Szeliski 1989) or maximum likelihood inference\n(Cox 1994).\nWhen fewer images are available, it becomes necessary to fall back on aggregation tech-\nniques such as sliding windows or global optimization. With additional input images, how-\never, the likelihood of occlusions increases. It is therefore prudent to adjust not only the best\nwindow locations using a shiftable window approach, as shown in Figure 11.16a, but also to\noptionally select a subset of neighboring frames in order to discount those images where the\nregion of interest is occluded, as shown in Figure 11.16b (Kang, Szeliski, and Chai 2001).\nthere is enough information in a light ﬁeld to recover both the shape and the BRDF of objects (Soatto, Yezzi, and Jin\n2003), although relatively little progress has been made to date on this topic.\n11.6 Multi-view stereo\n561\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\n(a)\n(b)\nFigure 11.16 Spatio-temporally shiftable windows (Kang, Szeliski, and Chai 2001) c⃝2001\nIEEE: A simple three-image sequence (the middle image is the reference image), which has\na moving frontal gray square (marked F) and a stationary background. Regions B, C, D, and\nE are partially occluded. (a) A regular SSD algorithm will make mistakes when matching\npixels in these regions (e.g. the window centered on the black pixel in region B) and in\nwindows straddling depth discontinuities (the window centered on the white pixel in region\nF). (b) Shiftable windows help mitigate the problems in partially occluded regions and near\ndepth discontinuities. The shifted window centered on the white pixel in region F matches\ncorrectly in all frames. The shifted window centered on the black pixel in region B matches\ncorrectly in the left image, but requires temporal selection to disable matching the right image.\nFigure 11.15b shows an EPI corresponding to this sequence and describes in more detail how\ntemporal selection works.\nFigure11.15b shows how such spatio-temporal selection or shifting of windows corresponds\nto selecting the most likely un-occluded volumetric region in the epipolar plane image vol-\nume.\nThe results of applying these techniques to the multi-frame ﬂower garden image sequence\nare shown in Figure 11.17, which compares the results of using regular (non-shifted) SSSD\nwith spatially shifted windows and full spatio-temporal window selection.\n(The task of\napplying stereo to a rigid scene ﬁlmed with a moving camera is sometimes called motion\nstereo). Similar improvements from using spatio-temporal selection are reported by (Kang\nand Szeliski 2004) and are evident even when local measurements are combined with global\noptimization.\nWhile computing a depth map from multiple inputs outperforms pairwise stereo match-\ning, even more dramatic improvements can be obtained by estimating multiple depth maps\nsimultaneously (Szeliski 1999; Kang and Szeliski 2004). The existence of multiple depth\nmaps enables more accurate reasoning about occlusions, as regions which are occluded in\none image may be visible (and matchable) in others. The multi-view reconstruction problem\ncan be formulated as the simultaneous estimation of depth maps at key frames (Figure 8.13c)\nwhile maximizing not only photoconsistency and piecewise disparity smoothness but also the\nconsistency between disparity estimates at different frames. While Szeliski (1999) and Kang",
  "image_path": "page_582.jpg",
  "pages": [
    581,
    582,
    583
  ]
}