{
  "doc_id": "pages_483_485",
  "text": "9.3 Compositing\n461\nRather than solving the Poisson partial differential equations, Agarwala, Dontcheva, Agrawala\net al. (2004) directly minimize a variational problem,\nmin\nC(x) ∥∇C(x) −∇˜Il(x)(x)∥2.\n(9.44)\nThe discretized form of this equation is a set of gradient constraint equations\nC(x + ˆı) −C(x)\n=\n˜Il(x)(x + ˆı) −˜Il(x)(x) and\n(9.45)\nC(x + ˆ) −C(x)\n=\n˜Il(x)(x + ˆ) −˜Il(x)(x),\n(9.46)\nwhere ˆı = (1, 0) and ˆ= (0, 1) are unit vectors in the x and y directions.15 They then solve\nthe associated sparse least squares problem. Since this system of equations is only deﬁned\nup to an additive constraint, Agarwala, Dontcheva, Agrawala et al. (2004) ask the user to\nselect the value of one pixel. In practice, a better choice might be to weakly bias the solution\ntowards reproducing the original color values.\nIn order to accelerate the solution of this sparse linear system, Fattal, Lischinski, and\nWerman (2002) use multigrid, whereas Agarwala, Dontcheva, Agrawala et al. (2004) use\nhierarchical basis preconditioned conjugate gradient descent (Szeliski 1990b, 2006b) (Ap-\npendix A.5). In subsequent work, Agarwala (2007) shows how using a quadtree represen-\ntation for the solution can further accelerate the computation with minimal loss in accuracy,\nwhile Szeliski, Uyttendaele, and Steedly (2008) show how representing the per-image offset\nﬁelds using even coarser splines is even faster. This latter work also argues that blending\nin the log domain, i.e., using multiplicative rather than additive offsets, is preferable, as it\nmore closely matches texture contrasts across seam boundaries. The resulting seam blending\nworks very well in practice (Figure 9.14h), although care must be taken when copying large\ngradient values near seams so that a “double edge” is not introduced.\nCopying gradients directly from the source images after seam placement is just one ap-\nproach to gradient domain blending. The paper by Levin, Zomet, Peleg et al. (2004) examines\nseveral different variants of this approach, which they call Gradient-domain Image STitching\n(GIST). The techniques they examine include feathering (blending) the gradients from the\nsource images, as well as using an L1 norm in performing the reconstruction of the image\nfrom the gradient ﬁeld, rather than using an L2 norm as in Equation (9.44). Their preferred\ntechnique is the L1 optimization of a feathered (blended) cost function on the original image\ngradients (which they call GIST1-l1). Since L1 optimization using linear programming can\nbe slow, they develop a faster iterative median-based algorithm in a multigrid framework.\nVisual comparisons between their preferred approach and what they call optimal seam on\nthe gradients (which is equivalent to the approach of Agarwala, Dontcheva, Agrawala et al.\n(2004)) show similar results, while signiﬁcantly improving on pyramid blending and feather-\ning algorithms.\n15 At seam locations, the right hand side is replaced by the average of the gradients in the two source images.\n462\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nExposure compensation.\nPyramid and gradient domain blending can do a good job of\ncompensating for moderate amounts of exposure differences between images.\nHowever,\nwhen the exposure differences become large, alternative approaches may be necessary.\nUyttendaele, Eden, and Szeliski (2001) iteratively estimate a local correction between\neach source image and a blended composite. First, a block-based quadratic transfer function is\nﬁt between each source image and an initial feathered composite. Next, transfer functions are\naveraged with their neighbors to get a smoother mapping and per-pixel transfer functions are\ncomputed by splining (interpolating) between neighboring block values. Once each source\nimage has been smoothly adjusted, a new feathered composite is computed and the process is\nrepeated (typically three times). The results shown by Uyttendaele, Eden, and Szeliski (2001)\ndemonstrate that this does a better job of exposure compensation than simple feathering and\ncan handle local variations in exposure due to effects such as lens vignetting.\nUltimately, however, the most principled way to deal with exposure differences is to stitch\nimages in the radiance domain, i.e., to convert each image into a radiance image using its\nexposure value and then create a stitched, high dynamic range image, as discussed in Sec-\ntion 10.2 (Eden, Uyttendaele, and Szeliski 2006).\n9.4 Additional reading\nThe literature on image stitching dates back to work in the photogrammetry community in\nthe 1970s (Milgram 1975, 1977; Slama 1980). In computer vision, papers started appearing\nin the early 1980s (Peleg 1981), while the development of fully automated techniques came\nabout a decade later (Mann and Picard 1994; Chen 1995; Szeliski 1996; Szeliski and Shum\n1997; Sawhney and Kumar 1999; Shum and Szeliski 2000). Those techniques used direct\npixel-based alignment but feature-based approaches are now the norm (Zoghlami, Faugeras,\nand Deriche 1997; Capel and Zisserman 1998; Cham and Cipolla 1998; Badra, Qumsieh, and\nDudek 1998; McLauchlan and Jaenicke 2002; Brown and Lowe 2007). A collection of some\nof these papers can be found in the book by Benosman and Kang (2001). Szeliski (2006a)\nprovides a comprehensive survey of image stitching, on which the material in this chapter is\nbased.\nHigh-quality techniques for optimal seam ﬁnding and blending are another important\ncomponent of image stitching systems. Important developments in this ﬁeld include work by\nMilgram (1977), Burt and Adelson (1983b), Davis (1998), Uyttendaele, Eden, and Szeliski\n(2001),P´erez, Gangnet, and Blake (2003), Levin, Zomet, Peleg et al. (2004), Agarwala,\nDontcheva, Agrawala et al. (2004), Eden, Uyttendaele, and Szeliski (2006), and Kopf, Uyt-\ntendaele, Deussen et al. (2007).\nIn addition to the merging of multiple overlapping photographs taken for aerial or ter-\n9.5 Exercises\n463\nrestrial panoramic image creation, stitching techniques can be used for automated white-\nboard scanning (He and Zhang 2005; Zhang and He 2007), scanning with a mouse (Nakao,\nKashitani, and Kaneyoshi 1998), and retinal image mosaics (Can, Stewart, Roysam et al.\n2002). They can also be applied to video sequences (Teodosio and Bender 1993; Irani, Hsu,\nand Anandan 1995; Kumar, Anandan, Irani et al. 1995; Sawhney and Ayer 1996; Massey\nand Bender 1996; Irani and Anandan 1998; Sawhney, Arpa, Kumar et al. 2002; Agarwala,\nZheng, Pal et al. 2005; Rav-Acha, Pritch, Lischinski et al. 2005; Steedly, Pal, and Szeliski\n2005; Baudisch, Tan, Steedly et al. 2006) and can even be used for video compression (Lee,\nge Chen, lung Bruce Lin et al. 1997).\n9.5 Exercises\nEx 9.1: Direct pixel-based alignment\nTake a pair of images, compute a coarse-to-ﬁne afﬁne\nalignment (Exercise 8.2) and then blend them using either averaging (Exercise 6.2) or a Lapla-\ncian pyramid (Exercise 3.20). Extend your motion model from afﬁne to perspective (homog-\nraphy) to better deal with rotational mosaics and planar surfaces seen under arbitrary motion.\nEx 9.2: Featured-based stitching\nExtend your feature-based alignment technique from Ex-\nercise 6.2 to use a full perspective model and then blend the resulting mosaic using either\naveraging or more sophisticated distance-based feathering (Exercise 9.9).\nEx 9.3: Cylindrical strip panoramas\nTo generate cylindrical or spherical panoramas from\na horizontally panning (rotating) camera, it is best to use a tripod. Set your camera up to take\na series of 50% overlapped photos and then use the following steps to create your panorama:\n1. Estimate the amount of radial distortion by taking some pictures with lots of long\nstraight lines near the edges of the image and then using the plumb-line method from\nExercise 6.10.\n2. Compute the focal length either by using a ruler and paper, as in Figure 6.7 (Debevec,\nWenger, Tchou et al. 2002) or by rotating your camera on the tripod, overlapping the\nimages by exactly 0% and counting the number of images it takes to make a 360◦\npanorama.\n3. Convert each of your images to cylindrical coordinates using (9.12–9.16).\n4. Line up the images with a translational motion model using either a direct pixel-based\ntechnique, such as coarse-to-ﬁne incremental or an FFT, or a feature-based technique.\n5. (Optional) If doing a complete 360◦panorama, align the ﬁrst and last images. Compute\nthe amount of accumulated vertical mis-registration and re-distribute this among the\nimages.",
  "image_path": "page_484.jpg",
  "pages": [
    483,
    484,
    485
  ]
}