{
  "doc_id": "pages_373_375",
  "text": "7.2 Two-frame structure from motion\n351\nOnce ˆt has been recovered, how can we estimate the corresponding rotation matrix R?\nRecall that the cross-product operator [ˆt]× (2.32) projects a vector onto a set of orthogonal\nbasis vectors that include ˆt, zeros out the ˆt component, and rotates the other two by 90◦,\n[ˆt]× = SZR90◦ST =\nh\ns0\ns1\nˆt\ni\n\n\n1\n1\n0\n\n\n\n\n0\n−1\n1\n0\n1\n\n\n\n\nsT\n0\nsT\n1\nˆt\nT\n\n,\n(7.21)\nwhere ˆt = s0 × s1. From Equations (7.18 and 7.21), we get\nE = [ˆt]×R = SZR90◦ST R = UΣV T ,\n(7.22)\nfrom which we can conclude that S = U. Recall that for a noise-free essential matrix,\n(Σ = Z), and hence\nR90◦U T R = V T\n(7.23)\nand\nR = URT\n90◦V T .\n(7.24)\nUnfortunately, we only know both E and ˆt up to a sign. Furthermore, the matrices U and V\nare not guaranteed to be rotations (you can ﬂip both their signs and still get a valid SVD). For\nthis reason, we have to generate all four possible rotation matrices\nR = ±URT\n±90◦V T\n(7.25)\nand keep the two whose determinant |R| = 1. To disambiguate between the remaining pair\nof potential rotations, which form a twisted pair (Hartley and Zisserman 2004, p. 240), we\nneed to pair them with both possible signs of the translation direction ±ˆt and select the\ncombination for which the largest number of points is seen in front of both cameras.4\nThe property that points must lie in front of the camera, i.e., at a positive distance along\nthe viewing rays emanating from the camera, is known as chirality (Hartley 1998). In addition\nto determining the signs of the rotation and translation, as described above, the chirality (sign\nof the distances) of the points in a reconstruction can be used inside a RANSAC procedure\n(along with the reprojection errors) to distinguish between likely and unlikely conﬁgurations.5\nChirality can also be used to transform projective reconstructions (Sections 7.2.1 and 7.2.2)\ninto quasi-afﬁne reconstructions (Hartley 1998).\nThe normalized “eight-point algorithm” (Hartley 1997a) described above is not the only\nway to estimate the camera motion from correspondences. Variants include using seven points\n4 In the noise-free case, a single point sufﬁces. It is safer, however, to test all or a sufﬁcient subset of points,\ndownweighting the ones that lie close to the plane at inﬁnity, for which it is easy to get depth reversals.\n5 Note that as points get further away from a camera, i.e., closer toward the plane at inﬁnity, errors in chirality\nbecome more likely.\n352\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ne\nxi0\nxi1\nFigure 7.4\nPure translational camera motion results in visual motion where all the points\nmove towards (or away from) a common focus of expansion (FOE) e. They therefore satisfy\nthe triple product condition (x0, x1, e) = e · (x0 × x1) = 0.\nwhile enforcing the rank two constraint in E (7.19–7.20) and a ﬁve-point algorithm that\nrequires ﬁnding the roots of a 10th degree polynomial (Nist´er 2004). Since such algorithms\nuse fewer points to compute their estimates, they are less sensitive to outliers when used as\npart of a random sampling (RANSAC) strategy.\nPure translation (known rotation)\nIn the case where we know the rotation, we can pre-rotate the points in the second image to\nmatch the viewing direction of the ﬁrst. The resulting set of 3D points all move towards (or\naway from) the focus of expansion (FOE), as shown in Figure 7.4.6 The resulting essential\nmatrix E is (in the noise-free case) skew symmetric and so can be estimated more directly by\nsetting eij = −eji and eii = 0 in (7.13). Two points with non-zero parallax now sufﬁce to\nestimate the FOE.\nA more direct derivation of the FOE estimate can be obtained by minimizing the triple\nproduct\nX\ni\n(xi0, xi1, e)2 =\nX\ni\n((xi0 × xi1) · e)2,\n(7.26)\nwhich is equivalent to ﬁnding the null space for the set of equations\n(yi0 −yi1)e0 + (xi1 −xi0)e1 + (xi0yi1 −yi0xi1)e2 = 0.\n(7.27)\nNote that, as in the eight-point algorithm, it is advisable to normalize the 2D points to have\nunit variance before computing this estimate.\nIn situations where a large number of points at inﬁnity are available, e.g., when shooting\noutdoor scenes or when the camera motion is small compared to distant objects, this suggests\nan alternative RANSAC strategy for estimating the camera motion. First, pick a pair of\npoints to estimate a rotation, hoping that both of the points lie at inﬁnity (very far from the\n6 Fans of Star Trek and Star Wars will recognize this as the “jump to hyperdrive” visual effect.\n7.2 Two-frame structure from motion\n353\ncamera). Then, compute the FOE and check whether the residual error is small (indicating\nagreement with this rotation hypothesis) and whether the motions towards or away from the\nepipole (FOE) are all in the same direction (ignoring very small motions, which may be\nnoise-contaminated).\nPure rotation\nThe case of pure rotation results in a degenerate estimate of the essential matrix E and of\nthe translation direction ˆt.\nConsider ﬁrst the case of the rotation matrix being known. The\nestimates for the FOE will be degenerate, since xi0 ≈xi1, and hence (7.27), is degenerate.\nA similar argument shows that the equations for the essential matrix (7.13) are also rank-\ndeﬁcient.\nThis suggests that it might be prudent before computing a full essential matrix to ﬁrst\ncompute a rotation estimate R using (6.32), potentially with just a small number of points,\nand then compute the residuals after rotating the points before proceeding with a full E\ncomputation.\n7.2.1 Projective (uncalibrated) reconstruction\nIn many cases, such as when trying to build a 3D model from Internet or legacy photos taken\nby unknown cameras without any EXIF tags, we do not know ahead of time the intrinsic\ncalibration parameters associated with the input images. In such situations, we can still esti-\nmate a two-frame reconstruction, although the true metric structure may not be available, e.g.,\northogonal lines or planes in the world may not end up being reconstructed as orthogonal.\nConsider the derivations we used to estimate the essential matrix E (7.10–7.12). In the\nuncalibrated case, we do not know the calibration matrices Kj, so we cannot use the normal-\nized ray directions ˆxj = K−1\nj xj. Instead, we have access only to the image coordinates xj,\nand so the essential matrix (7.10) becomes\nˆxT\n1 Eˆx1 = xT\n1 K−T\n1\nEK−1\n0 x0 = xT\n1 F x0 = 0,\n(7.28)\nwhere\nF = K−T\n1\nEK−1\n0\n= [e]× ˜\nH\n(7.29)\nis called the fundamental matrix (Faugeras 1992; Hartley, Gupta, and Chang 1992; Hartley\nand Zisserman 2004).\nLike the essential matrix, the fundamental matrix is (in principle) rank two,\nF = [e]× ˜\nH = UΣV T =\nh\nu0\nu1\ne1\ni\n\n\nσ0\nσ1\n0\n\n\n\n\nvT\n0\nvT\n1\neT\n0\n\n.\n(7.30)",
  "image_path": "page_374.jpg",
  "pages": [
    373,
    374,
    375
  ]
}