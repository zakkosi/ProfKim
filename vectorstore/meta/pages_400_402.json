{
  "doc_id": "pages_400_402",
  "text": "378\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Assume that you know the camera pose, i.e., the camera matrices. Use the 3D distance\nto rays (7.4) or linearized versions of Equations (7.5–7.6) to compute an initial set of\n3D locations. Compare these to your known ground truth locations.\n2. Use iterative non-linear minimization to improve your initial estimates and report on\nthe improvement in accuracy.\n3. (Optional) Use the technique described by Hartley and Sturm (1997) to perform two-\nframe triangulation.\n4. See if any of the failure modes reported by Hartley and Sturm (1997) or Hartley (1998)\noccur in practice.\nEx 7.2: Essential and fundamental matrix\nImplement the two-frame E and F matrix es-\ntimation techniques presented in Section 7.2, with suitable re-scaling for better noise immu-\nnity.\n1. Use the data from Exercise 7.1 to validate your algorithms and to report on their accu-\nracy.\n2. (Optional) Implement one of the improved F or E estimation algorithms, e.g., us-\ning renormalization (Zhang 1998b; Torr and Fitzgibbon 2004; Hartley and Zisserman\n2004), RANSAC (Torr and Murray 1997), least media squares (LMS), or the ﬁve-point\nalgorithm developed by Nist´er (2004).\nEx 7.3: View morphing and interpolation\nImplement automatic view morphing, i.e., com-\npute two-frame structure from motion and then use these results to generate a smooth anima-\ntion from one image to the next (Section 7.2.3).\n1. Decide how to represent your 3D scene, e.g., compute a Delaunay triangulation of the\nmatched point and decide what to do with the triangles near the border. (Hint: try ﬁtting\na plane to the scene, e.g., behind most of the points.)\n2. Compute your in-between camera positions and orientations.\n3. Warp each triangle to its new location, preferably using the correct perspective projec-\ntion (Szeliski and Shum 1997).\n4. (Optional) If you have a denser 3D model (e.g., from stereo), decide what to do at the\n“cracks”.\n5. (Optional) For a non-rigid scene, e.g., two pictures of a face with different expressions,\nnot all of your matched points will obey the epipolar geometry. Decide how to handle\nthem to achieve the best effect.\n7.7 Exercises\n379\nEx 7.4: Factorization\nImplement the factorization algorithm described in Section 7.3 us-\ning point tracks you computed in Exercise 4.5.\n1. (Optional) Implement uncertainty rescaling (Anandan and Irani 2002) and comment on\nwhether this improves your results.\n2. (Optional) Implement one of the perspective improvements to factorization discussed\nin Section 7.3.1 (Christy and Horaud 1996; Sturm and Triggs 1996; Triggs 1996). Does\nthis produce signiﬁcantly lower reprojection errors? Can you upgrade this reconstruc-\ntion to a metric one?\nEx 7.5: Bundle adjuster\nImplement a full bundle adjuster. This may sound daunting, but\nit really is not.\n1. Devise the internal data structures and external ﬁle representations to hold your camera\nparameters (position, orientation, and focal length), 3D point locations (Euclidean or\nhomogeneous), and 2D point tracks (frame and point identiﬁer as well as 2D locations).\n2. Use some other technique, such as factorization, to initialize the 3D point and camera\nlocations from your 2D tracks (e.g., a subset of points that appears in all frames).\n3. Implement the code corresponding to the forward transformations in Figure 7.7, i.e.,\nfor each 2D point measurement, take the corresponding 3D point, map it through the\ncamera transformations (including perspective projection and focal length scaling), and\ncompare it to the 2D point measurement to get a residual error.\n4. Take the residual error and compute its derivatives with respect to all the unknown\nmotion and structure parameters, using backward chaining, as shown, e.g., in Figure 7.7\nand Equation (6.47). This gives you the sparse Jacobian J used in Equations (6.13–\n6.17) and Equation (6.43).\n5. Use a sparse least squares or linear system solver, e.g., MATLAB, SparseSuite, or\nSPARSKIT (see Appendix A.4 and A.5), to solve the corresponding linearized system,\nadding a small amount of diagonal preconditioning, as in Levenberg–Marquardt.\n6. Update your parameters, make sure your rotation matrices are still orthonormal (e.g.,\nby re-computing them from your quaternions), and continue iterating while monitoring\nyour residual error.\n7. (Optional) Use the “Schur complement trick” (7.56) to reduce the size of the system\nbeing solved (Triggs, McLauchlan, Hartley et al. 1999; Hartley and Zisserman 2004;\nLourakis and Argyros 2009; Engels, Stew´enius, and Nist´er 2006).\n380\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n8. (Optional) Implement your own iterative sparse solver, e.g., conjugate gradient, and\ncompare its performance to a direct method.\n9. (Optional) Make your bundle adjuster robust to outliers, or try adding some of the other\nimprovements discussed in (Engels, Stew´enius, and Nist´er 2006). Can you think of any\nother ways to make your algorithm even faster or more robust?\nEx 7.6: Match move and augmented reality\nUse the results of the previous exercise to\nsuperimpose a rendered 3D model on top of video. See Section 7.4.2 for more details and\nideas. Check for how “locked down” the objects are.\nEx 7.7: Line-based reconstruction\nAugment the previously developed bundle adjuster to\ninclude lines, possibly with known 3D orientations.\nOptionally, use co-planar sets of points and lines to hypothesize planes and to enforce\nco-planarity (Schaffalitzky and Zisserman 2002; Robertson and Cipolla 2002)\nEx 7.8: Flexible bundle adjuster\nDesign a bundle adjuster that allows for arbitrary chains\nof transformations and prior knowledge about the unknowns, as suggested in Figures 7.7–7.8.\nEx 7.9: Unordered image matching\nCompute the camera pose and 3D structure of a scene\nfrom an arbitrary collection of photographs (Brown and Lowe 2003; Snavely, Seitz, and\nSzeliski 2006).",
  "image_path": "page_401.jpg",
  "pages": [
    400,
    401,
    402
  ]
}