{
  "doc_id": "pages_391_393",
  "text": "7.4 Bundle adjustment\n369\n(a)\n(b)\nFigure 7.10\n3D augmented reality: (a) Darth Vader and a horde of Ewoks battle it out\non a table-top recovered using real-time, keyframe-based structure from motion (Klein and\nMurray 2007) c⃝2007 IEEE; (b) a virtual teapot is ﬁxed to the top of a real-world coffee cup,\nwhose pose is re-recognized at each time frame (Gordon and Lowe 2006) c⃝2007 Springer.\nsimultaneously build up a model of the 3D environment and then track it, so that graphics can\nbe superimposed.\nKlein and Murray (2007) describe a parallel tracking and mapping (PTAM) system,\nwhich simultaneously applies full bundle adjustment to keyframes selected from a video\nstream, while performing robust real-time pose estimation on intermediate frames.\nFig-\nure 7.10a shows an example of their system in use. Once an initial 3D scene has been\nreconstructed, a dominant plane is estimated (in this case, the table-top) and 3D animated\ncharacters are virtually inserted. Klein and Murray (2008) extend their previous system to\nhandle even faster camera motion by adding edge features, which can still be detected even\nwhen interest points become too blurred. They also use a direct (intensity-based) rotation\nestimation algorithm for even faster motions.\nInstead of modeling the whole scene as one rigid reference frame, Gordon and Lowe\n(2006) ﬁrst build a 3D model of an individual object using feature matching and structure\nfrom motion. Once the system has been initialized, for every new frame, they ﬁnd the object\nand its pose using a 3D instance recognition algorithm, and then superimpose a graphical\nobject onto that model, as shown in Figure 7.10b.\nWhile reliably tracking such objects and environments is now a well-solved problem,\ndetermining which pixels should be occluded by foreground scene elements still remains an\nopen problem (Chuang, Agarwala, Curless et al. 2002; Wang and Cohen 2007a).\n370\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7.4.3 Uncertainty and ambiguities\nBecause structure from motion involves the estimation of so many highly coupled parameters,\noften with no known “ground truth” components, the estimates produced by structure from\nmotion algorithms can often exhibit large amounts of uncertainty (Szeliski and Kang 1997).\nAn example of this is the classic bas-relief ambiguity, which makes it hard to simultaneously\nestimate the 3D depth of a scene and the amount of camera motion (Oliensis 2005).16\nAs mentioned before, a unique coordinate frame and scale for a reconstructed scene can-\nnot be recovered from monocular visual measurements alone. (When a stereo rig is used,\nthe scale can be recovered if we know the distance (baseline) between the cameras.) This\nseven-degree-of-freedom gauge ambiguity makes it tricky to compute the covariance matrix\nassociated with a 3D reconstruction (Triggs, McLauchlan, Hartley et al. 1999; Kanatani and\nMorris 2001). A simple way to compute a covariance matrix that ignores the gauge freedom\n(indeterminacy) is to throw away the seven smallest eigenvalues of the information matrix (in-\nverse covariance), whose values are equivalent to the problem Hessian A up to noise scaling\n(see Section 6.1.4 and Appendix B.6). After we do this, the resulting matrix can be inverted\nto obtain an estimate of the parameter covariance.\nSzeliski and Kang (1997) use this approach to visualize the largest directions of variation\nin typical structure from motion problems. Not surprisingly, they ﬁnd that (ignoring the gauge\nfreedoms), the greatest uncertainties for problems such as observing an object from a small\nnumber of nearby viewpoints are in the depths of the 3D structure relative to the extent of the\ncamera motion.17\nIt is also possible to estimate local or marginal uncertainties for individual parameters,\nwhich corresponds simply to taking block sub-matrices from the full covariance matrix. Un-\nder certain conditions, such as when the camera poses are relatively certain compared to 3D\npoint locations, such uncertainty estimates can be meaningful. However, in many cases, indi-\nvidual uncertainty measures can mask the extent to which reconstruction errors are correlated,\nwhich is why looking at the ﬁrst few modes of greatest joint variation can be helpful.\nThe other way in which gauge ambiguities affect structure from motion and, in particular,\nbundle adjustment is that they make the system Hessian matrix A rank-deﬁcient and hence\nimpossible to invert. A number of techniques have been proposed to mitigate this problem\n(Triggs, McLauchlan, Hartley et al. 1999; Bartoli 2003). In practice, however, it appears that\nsimply adding a small amount of the Hessian diagonal λdiag(A) to the Hessian A itself, as is\ndone in the Levenberg–Marquardt non-linear least squares algorithm (Appendix A.3), usually\n16 Bas-relief refers to a kind of sculpture in which objects, often on ornamental friezes, are sculpted with less\ndepth than they actually occupy. When lit from above by sunlight, they appear to have true 3D depth because of the\nambiguity between relative depth and the angle of the illuminant (Section 12.1.1).\n17 A good way to minimize the amount of such ambiguities is to use wide ﬁeld of view cameras (Antone and\nTeller 2002; Levin and Szeliski 2006).\n7.4 Bundle adjustment\n371\nworks well.\n7.4.4 Application: Reconstruction from Internet photos\nThe most widely used application of structure from motion is in the reconstruction of 3D\nobjects and scenes from video sequences and collections of images (Pollefeys and Van Gool\n2002). The last decade has seen an explosion of techniques for performing this task auto-\nmatically without the need for any manual correspondence or pre-surveyed ground control\npoints. A lot of these techniques assume that the scene is taken with the same camera and\nhence the images all have the same intrinsics (Fitzgibbon and Zisserman 1998; Koch, Polle-\nfeys, and Van Gool 2000; Schaffalitzky and Zisserman 2002; Tuytelaars and Van Gool 2004;\nPollefeys, Nist´er, Frahm et al. 2008; Moons, Van Gool, and Vergauwen 2010). Many of\nthese techniques take the results of the sparse feature matching and structure from motion\ncomputation and then compute dense 3D surface models using multi-view stereo techniques\n(Section 11.6) (Koch, Pollefeys, and Van Gool 2000; Pollefeys and Van Gool 2002; Pollefeys,\nNist´er, Frahm et al. 2008; Moons, Van Gool, and Vergauwen 2010).\nThe latest innovation in this space has been the application of structure from motion and\nmulti-view stereo techniques to thousands of images taken from the Internet, where very little\nis known about the cameras taking the photographs (Snavely, Seitz, and Szeliski 2008a). Be-\nfore the structure from motion computation can begin, it is ﬁrst necessary to establish sparse\ncorrespondences between different pairs of images and to then link such correspondences into\nfeature tracks, which associate individual 2D image features with global 3D points. Because\nthe O(N 2) comparison of all pairs of images can be very slow, a number of techniques have\nbeen developed in the recognition community to make this process faster (Section 14.3.2)\n(Nist´er and Stew´enius 2006; Philbin, Chum, Sivic et al. 2008; Li, Wu, Zach et al. 2008;\nChum, Philbin, and Zisserman 2008; Chum and Matas 2010).\nTo begin the reconstruction process, it is important to to select a good pair of images,\nwhere there are both a large number of consistent matches (to lower the likelihood of in-\ncorrect correspondences) and a signiﬁcant amount of out-of-plane parallax,18 to ensure that\na stable reconstruction can be obtained (Snavely, Seitz, and Szeliski 2006). The EXIF tags\nassociated with the photographs can be used to get good initial estimates for camera focal\nlengths, although this is not always strictly necessary, since these parameters are re-adjusted\nas part of the bundle adjustment process.\nOnce an initial pair has been reconstructed, the pose of cameras that see a sufﬁcient num-\nber of the resulting 3D points can be estimated (Section 6.2) and the complete set of cameras\nand feature correspondences can be used to perform another round of bundle adjustment. Fig-\n18 A simple way to compute this is to robustly ﬁt a homography to the correspondences and measure reprojection\nerrors.",
  "image_path": "page_392.jpg",
  "pages": [
    391,
    392,
    393
  ]
}