{
  "doc_id": "pages_347_349",
  "text": "6.2 Pose estimation\n325\nfC(x) = Kx\nk\nfP(x) = p/z\nfR(x) = Rx\nqj\nfT(x) = x-c\ncj\npi\nxi\ny(1)\ny(2)\ny(3)\nFigure 6.5 A set of chained transforms for projecting a 3D point pi to a 2D measurement xi\nthrough a series of transformations f (k), each of which is controlled by its own set of param-\neters. The dashed lines indicate the ﬂow of information as partial derivatives are computed\nduring a backward pass.\nsuch as translation, rotation, or perspective division (Figure 6.5). The resulting projection\nequations can be written as\ny(1)\n=\nf T(pi; cj) = pi −cj,\n(6.44)\ny(2)\n=\nf R(y(1); qj) = R(qj) y(1),\n(6.45)\ny(3)\n=\nf P(y(2)) = y(2)\nz(2) ,\n(6.46)\nxi\n=\nf C(y(3); k) = K(k) y(3).\n(6.47)\nNote that in these equations, we have indexed the camera centers cj and camera rotation\nquaternions qj by an index j, in case more than one pose of the calibration object is being\nused (see also Section 7.4.) We are also using the camera center cj instead of the world\ntranslation tj, since this is a more natural parameter to estimate.\nThe advantage of this chained set of transformations is that each one has a simple partial\nderivative with respect both to its parameters and to its input. Thus, once the predicted value\nof ˜xi has been computed based on the 3D point location pi and the current values of the pose\nparameters (cj, qj, k), we can obtain all of the required partial derivatives using the chain\nrule\n∂ri\n∂p(k) =\n∂ri\n∂y(k)\n∂y(k)\n∂p(k) ,\n(6.48)\nwhere p(k) indicates one of the parameter vectors that is being optimized. (This same “trick”\nis used in neural networks as part of the backpropagation algorithm (Bishop 2006).)\nThe one special case in this formulation that can be considerably simpliﬁed is the compu-\ntation of the rotation update. Instead of directly computing the derivatives of the 3×3 rotation\nmatrix R(q) as a function of the unit quaternion entries, you can prepend the incremental ro-\ntation matrix ∆R(ω) given in Equation (2.35) to the current rotation matrix and compute the\n326\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 6.6 The VideoMouse can sense six degrees of freedom relative to a specially printed\nmouse pad using its embedded camera (Hinckley, Sinclair, Hanson et al. 1999) c⃝1999\nACM: (a) top view of the mouse; (b) view of the mouse showing the curved base for rocking;\n(c) moving the mouse pad with the other hand extends the interaction capabilities; (d) the\nresulting movement seen on the screen.\npartial derivative of the transform with respect to these parameters, which results in a simple\ncross product of the backward chaining partial derivative and the outgoing 3D vector (2.36).\n6.2.3 Application: Augmented reality\nA widely used application of pose estimation is augmented reality, where virtual 3D images\nor annotations are superimposed on top of a live video feed, either through the use of see-\nthrough glasses (a head-mounted display) or on a regular computer or mobile device screen\n(Azuma, Baillot, Behringer et al. 2001; Haller, Billinghurst, and Thomas 2007). In some\napplications, a special pattern printed on cards or in a book is tracked to perform the aug-\nmentation (Kato, Billinghurst, Poupyrev et al. 2000; Billinghurst, Kato, and Poupyrev 2001).\nFor a desktop application, a grid of dots printed on a mouse pad can be tracked by a camera\nembedded in an augmented mouse to give the user control of a full six degrees of freedom\nover their position and orientation in a 3D space (Hinckley, Sinclair, Hanson et al. 1999), as\nshown in Figure 6.6.\nSometimes, the scene itself provides a convenient object to track, such as the rectangle\ndeﬁning a desktop used in through-the-lens camera control (Gleicher and Witkin 1992). In\noutdoor locations, such as ﬁlm sets, it is more common to place special markers such as\nbrightly colored balls in the scene to make it easier to ﬁnd and track them (Bogart 1991). In\nolder applications, surveying techniques were used to determine the locations of these balls\nbefore ﬁlming. Today, it is more common to apply structure-from-motion directly to the ﬁlm\nfootage itself (Section 7.4.2).\nRapid pose estimation is also central to tracking the position and orientation of the hand-\nheld remote controls used in Nintendo’s Wii game systems. A high-speed camera embedded\nin the remote control is used to track the locations of the infrared (IR) LEDs in the bar that\n6.3 Geometric intrinsic calibration\n327\nis mounted on the TV monitor. Pose estimation is then used to infer the remote control’s\nlocation and orientation at very high frame rates. The Wii system can be extended to a variety\nof other user interaction applications by mounting the bar on a hand-held device, as described\nby Johnny Lee.11\nExercises 6.4 and 6.5 have you implement two different tracking and pose estimation sys-\ntems for augmented-reality applications. The ﬁrst system tracks the outline of a rectangular\nobject, such as a book cover or magazine page, and the second has you track the pose of a\nhand-held Rubik’s cube.\n6.3 Geometric intrinsic calibration\nAs described above in Equations (6.42–6.43), the computation of the internal (intrinsic) cam-\nera calibration parameters can occur simultaneously with the estimation of the (extrinsic)\npose of the camera with respect to a known calibration target. This, indeed, is the “classic”\napproach to camera calibration used in both the photogrammetry (Slama 1980) and the com-\nputer vision (Tsai 1987) communities. In this section, we look at alternative formulations\n(which may not involve the full solution of a non-linear regression problem), the use of alter-\nnative calibration targets, and the estimation of the non-linear part of camera optics such as\nradial distortion.12\n6.3.1 Calibration patterns\nThe use of a calibration pattern or set of markers is one of the more reliable ways to estimate\na camera’s intrinsic parameters. In photogrammetry, it is common to set up a camera in a\nlarge ﬁeld looking at distant calibration targets whose exact location has been precomputed\nusing surveying equipment (Slama 1980; Atkinson 1996; Kraus 1997). In this case, the trans-\nlational component of the pose becomes irrelevant and only the camera rotation and intrinsic\nparameters need to be recovered.\nIf a smaller calibration rig needs to be used, e.g., for indoor robotics applications or for\nmobile robots that carry their own calibration target, it is best if the calibration object can span\nas much of the workspace as possible (Figure 6.8a), as planar targets often fail to accurately\npredict the components of the pose that lie far away from the plane. A good way to determine\nif the calibration has been successfully performed is to estimate the covariance in the param-\neters (Section 6.1.4) and then project 3D points from various points in the workspace into the\nimage in order to estimate their 2D positional uncertainty.\n11 http://johnnylee.net/projects/wii/.\n12 In some applications, you can use the EXIF tags associated with a JPEG image to obtain a rough estimate of a\ncamera’s focal length but this technique should be used with caution as the results are often inaccurate.",
  "image_path": "page_348.jpg",
  "pages": [
    347,
    348,
    349
  ]
}