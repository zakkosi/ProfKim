{
  "doc_id": "pages_346_348",
  "text": "324\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nd2\ni can computed as ratios of successive d2n+2\ni\n/d2n\ni\nestimates and these can be averaged to\nobtain a ﬁnal estimate of d2\ni (and hence di).\nOnce the individual estimates of the di distances have been computed, we can generate\na 3D structure consisting of the scaled point directions diˆxi, which can then be aligned with\nthe 3D point cloud {pi} using absolute orientation (Section 6.1.5) to obtained the desired\npose estimate. Quan and Lan (1999) give accuracy results for this and other techniques,\nwhich use fewer points but require more complicated algebraic manipulations. The paper by\nMoreno-Noguer, Lepetit, and Fua (2007) reviews more recent alternatives and also gives a\nlower complexity algorithm that typically produces more accurate results.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 7.4.3), it is often preferable to use\nthe linear six-point algorithm to guess an initial pose and then optimize this estimate using\nthe iterative technique described in Section 6.2.2.\nAn alternative pose estimation algorithm involves starting with a scaled orthographic pro-\njection model and then iteratively reﬁning this initial estimate using a more accurate perspec-\ntive projection model (DeMenthon and Davis 1995). The attraction of this model, as stated\nin the paper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\n6.2.2 Iterative algorithms\nThe most accurate (and ﬂexible) way to estimate pose is to directly minimize the squared (or\nrobust) reprojection error for the 2D points as a function of the unknown pose parameters in\n(R, t) and optionally K using non-linear least squares (Tsai 1987; Bogart 1991; Gleicher\nand Witkin 1992). We can write the projection equations as\nxi = f(pi; R, t, K)\n(6.42)\nand iteratively minimize the robustiﬁed linearized reprojection errors\nENLP =\nX\ni\nρ\n\u0012 ∂f\n∂R∆R + ∂f\n∂t ∆t + ∂f\n∂K ∆K −ri\n\u0013\n,\n(6.43)\nwhere ri = ˜xi −ˆxi is the current residual vector (2D error in predicted position) and the\npartial derivatives are with respect to the unknown pose parameters (rotation, translation, and\noptionally calibration). Note that if full 2D covariance estimates are available for the 2D\nfeature locations, the above squared norm can be weighted by the inverse point covariance\nmatrix, as in Equation (6.11).\nAn easier to understand (and implement) version of the above non-linear regression prob-\nlem can be constructed by re-writing the projection equations as a concatenation of simpler\nsteps, each of which transforms a 4D homogeneous coordinate pi by a simple transformation\n6.2 Pose estimation\n325\nfC(x) = Kx\nk\nfP(x) = p/z\nfR(x) = Rx\nqj\nfT(x) = x-c\ncj\npi\nxi\ny(1)\ny(2)\ny(3)\nFigure 6.5 A set of chained transforms for projecting a 3D point pi to a 2D measurement xi\nthrough a series of transformations f (k), each of which is controlled by its own set of param-\neters. The dashed lines indicate the ﬂow of information as partial derivatives are computed\nduring a backward pass.\nsuch as translation, rotation, or perspective division (Figure 6.5). The resulting projection\nequations can be written as\ny(1)\n=\nf T(pi; cj) = pi −cj,\n(6.44)\ny(2)\n=\nf R(y(1); qj) = R(qj) y(1),\n(6.45)\ny(3)\n=\nf P(y(2)) = y(2)\nz(2) ,\n(6.46)\nxi\n=\nf C(y(3); k) = K(k) y(3).\n(6.47)\nNote that in these equations, we have indexed the camera centers cj and camera rotation\nquaternions qj by an index j, in case more than one pose of the calibration object is being\nused (see also Section 7.4.) We are also using the camera center cj instead of the world\ntranslation tj, since this is a more natural parameter to estimate.\nThe advantage of this chained set of transformations is that each one has a simple partial\nderivative with respect both to its parameters and to its input. Thus, once the predicted value\nof ˜xi has been computed based on the 3D point location pi and the current values of the pose\nparameters (cj, qj, k), we can obtain all of the required partial derivatives using the chain\nrule\n∂ri\n∂p(k) =\n∂ri\n∂y(k)\n∂y(k)\n∂p(k) ,\n(6.48)\nwhere p(k) indicates one of the parameter vectors that is being optimized. (This same “trick”\nis used in neural networks as part of the backpropagation algorithm (Bishop 2006).)\nThe one special case in this formulation that can be considerably simpliﬁed is the compu-\ntation of the rotation update. Instead of directly computing the derivatives of the 3×3 rotation\nmatrix R(q) as a function of the unit quaternion entries, you can prepend the incremental ro-\ntation matrix ∆R(ω) given in Equation (2.35) to the current rotation matrix and compute the\n326\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 6.6 The VideoMouse can sense six degrees of freedom relative to a specially printed\nmouse pad using its embedded camera (Hinckley, Sinclair, Hanson et al. 1999) c⃝1999\nACM: (a) top view of the mouse; (b) view of the mouse showing the curved base for rocking;\n(c) moving the mouse pad with the other hand extends the interaction capabilities; (d) the\nresulting movement seen on the screen.\npartial derivative of the transform with respect to these parameters, which results in a simple\ncross product of the backward chaining partial derivative and the outgoing 3D vector (2.36).\n6.2.3 Application: Augmented reality\nA widely used application of pose estimation is augmented reality, where virtual 3D images\nor annotations are superimposed on top of a live video feed, either through the use of see-\nthrough glasses (a head-mounted display) or on a regular computer or mobile device screen\n(Azuma, Baillot, Behringer et al. 2001; Haller, Billinghurst, and Thomas 2007). In some\napplications, a special pattern printed on cards or in a book is tracked to perform the aug-\nmentation (Kato, Billinghurst, Poupyrev et al. 2000; Billinghurst, Kato, and Poupyrev 2001).\nFor a desktop application, a grid of dots printed on a mouse pad can be tracked by a camera\nembedded in an augmented mouse to give the user control of a full six degrees of freedom\nover their position and orientation in a 3D space (Hinckley, Sinclair, Hanson et al. 1999), as\nshown in Figure 6.6.\nSometimes, the scene itself provides a convenient object to track, such as the rectangle\ndeﬁning a desktop used in through-the-lens camera control (Gleicher and Witkin 1992). In\noutdoor locations, such as ﬁlm sets, it is more common to place special markers such as\nbrightly colored balls in the scene to make it easier to ﬁnd and track them (Bogart 1991). In\nolder applications, surveying techniques were used to determine the locations of these balls\nbefore ﬁlming. Today, it is more common to apply structure-from-motion directly to the ﬁlm\nfootage itself (Section 7.4.2).\nRapid pose estimation is also central to tracking the position and orientation of the hand-\nheld remote controls used in Nintendo’s Wii game systems. A high-speed camera embedded\nin the remote control is used to track the locations of the infrared (IR) LEDs in the bar that",
  "image_path": "page_347.jpg",
  "pages": [
    346,
    347,
    348
  ]
}