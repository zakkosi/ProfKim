{
  "doc_id": "pages_668_670",
  "text": "646\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn this case, it makes more sense to move the camera through the environment and play\nback the video as an interactive video-based walkthrough. In order to allow the viewer to\nlook around in all directions, it is preferable to use a panoramic video camera (Uyttendaele,\nCriminisi, Kang et al. 2004).11\nOne way to structure the acquisition process is to capture these images in a 2D horizontal\nplane, e.g., over a grid superimposed inside a room. The resulting sea of images (Aliaga,\nFunkhouser, Yanovsky et al. 2003) can be used to enable continuous motion between the\ncaptured locations.12 However, extending this idea to larger settings, e.g., beyond a single\nroom, can become tedious and data-intensive.\nInstead, a natural way to explore a space is often to just walk through it along some pre-\nspeciﬁed paths, just as museums or home tours guide users along a particular path, say down\nthe middle of each room.13 Similarly, city-level exploration can be achieved by driving down\nthe middle of each street and allowing the user to branch at each intersection. This idea dates\nback to the Aspen MovieMap project (Lippman 1980), which recorded analog video taken\nfrom moving cars onto videodiscs for later interactive playback.\nRecent improvements in video technology now enable the capture of panoramic (spheri-\ncal) video using a small co-located array of cameras, such as the Point Grey Ladybug cam-\nera14 (Figure 13.16b) developed by Uyttendaele, Criminisi, Kang et al. (2004) for their inter-\nactive video-based walkthrough project. In their system, the synchronized video streams from\nthe six cameras (Figure 13.16a) are stitched together into 360◦panoramas using a variety of\ntechniques developed speciﬁcally for this project.\nBecause the cameras do not share the same center of projection, parallax between the\ncameras can lead to ghosting in the overlapping ﬁelds of view (Figure 13.16c). To remove\nthis, a multi-perspective plane sweep stereo algorithm is used to estimate per-pixel depths at\neach column in the overlap area. To calibrate the cameras relative to each other, the camera\nis spun in place and a constrained structure from motion algorithm (Figure 7.8) is used to\nestimate the relative camera poses and intrinsics. Feature tracking is then run on the walk-\nthrough video in order to stabilize the video sequence—Liu, Gleicher, Jin et al. (2009) have\ncarried out more recent work along these lines.\nIndoor environments with windows, as well as sunny outdoor environments with strong\nshadows, often have a dynamic range that exceeds the capabilities of video sensors. For\nthis reason, the Ladybug camera has a programmable exposure capability that enables the\nbracketing of exposures at subsequent video frames. In order to merge the resulting video\n11 See http://www.cis.upenn.edu/∼kostas/omni.html for descriptions of panoramic (omnidirectional) vision sys-\ntems and associated workshops.\n12 (The Photo Tourism system of Snavely, Seitz, and Szeliski (2006) applies this idea to less structured collections.\n13 In computer games, restricting a player to forward and backward motion along predetermined paths is called\nrail-based gaming.\n14 http://www.ptgrey.com/.\n13.5 Video-based rendering\n647\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 13.16 Video-based walkthroughs (Uyttendaele, Criminisi, Kang et al. 2004) c⃝2004\nIEEE: (a) system diagram of video pre-processing; (b) the Point Grey Ladybug camera; (c)\nghost removal using multi-perspective plane sweep; (d) point tracking, used both for calibra-\ntion and stabilization; (e) interactive garden walkthrough with map below; (f) overhead map\nauthoring and sound placement; (g) interactive home walkthrough with navigation bar (top)\nand icons of interest (bottom).\n648\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nframes into high dynamic range (HDR) video, pixels from adjacent frames need to be motion-\ncompensated before being merged (Kang, Uyttendaele, Winder et al. 2003).\nThe interactive walk-through experience becomes much richer and more navigable if an\noverview map is available as part of the experience. In Figure 13.16f, the map has annotations,\nwhich can show up during the tour, and localized sound sources, which play (with different\nvolumes) when the viewer is nearby. The process of aligning the video sequence with the\nmap can be automated using a process called map correlation (Levin and Szeliski 2004).\nAll of these elements combine to provide the user with a rich, interactive, and immersive\nexperience. Figure 13.16e shows a walk through the Bellevue Botanical Gardens, with an\noverview map in perspective below the live video window. Arrows on the ground are used to\nindicate potential directions of travel. The viewer simply orients his view towards one of the\narrows (the experience can be driven using a game controller) and “walks” forward along the\ndesired path.\nFigure 13.16g shows an indoor home tour experience. In addition to a schematic map\nin the lower left corner and adjacent room names along the top navigation bar, icons appear\nalong the bottom whenever items of interest, such as a homeowner’s art pieces, are visible\nin the main window. These icons can then be clicked to provide more information and 3D\nviews.\nThe development of interactive video tours spurred a renewed interest in 360◦video-based\nvirtual travel and mapping experiences, as evidenced by commercial sites such as Google’s\nStreet View and Bing Maps. The same videos can also be used to generate turn-by-turn driv-\ning directions, taking advantage of both expanded ﬁelds of view and image-based rendering\nto enhance the experience (Chen, Neubert, Ofek et al. 2009).\nAs we continue to capture more and more of our real world with large amounts of high-\nquality imagery and video, the interactive modeling, exploration, and rendering techniques\ndescribed in this chapter will play an even bigger role in bringing virtual experiences based\non remote areas of the world closer to everyone.\n13.6 Additional reading\nTwo good recent surveys of image-based rendering are by Kang, Li, Tong et al. (2006) and\nShum, Chan, and Kang (2007), with earlier surveys available from Kang (1999), McMillan\nand Gortler (1999), and Debevec (1999). The term image-based rendering was introduced by\nMcMillan and Bishop (1995), although the seminal paper in the ﬁeld is the view interpolation\npaper by Chen and Williams (1993). Debevec, Taylor, and Malik (1996) describe their Fac¸ade\nsystem, which not only created a variety of image-based modeling tools but also introduced\nthe widely used technique of view-dependent texture mapping.",
  "image_path": "page_669.jpg",
  "pages": [
    668,
    669,
    670
  ]
}