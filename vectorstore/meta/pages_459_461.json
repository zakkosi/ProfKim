{
  "doc_id": "pages_459_461",
  "text": "9.1 Motion models\n437\n+\n+\n+ · · · +\n=\nFigure 9.6 Video stitching the background scene to create a single sprite image that can be\ntransmitted and used to re-create the background in each frame (Lee, ge Chen, lung Bruce Lin\net al. 1997) c⃝1997 IEEE.\n(Massey and Bender 1996) and sometimes as video overlays (Irani and Anandan 1998).\nVideos can also be used to create animated panoramic video textures (Section 13.5.2), in\nwhich different portions of a panoramic scene are animated with independently moving video\nloops (Agarwala, Zheng, Pal et al. 2005; Rav-Acha, Pritch, Lischinski et al. 2005), or to shine\n“video ﬂashlights” onto a composite mosaic of a scene (Sawhney, Arpa, Kumar et al. 2002).\nVideo can also provide an interesting source of content for creating panoramas taken from\nmoving cameras. While this invalidates the usual assumption of a single point of view (opti-\ncal center), interesting results can still be obtained. For example, the VideoBrush system of\nSawhney, Kumar, Gendel et al. (1998) uses thin strips taken from the center of the image to\ncreate a panorama taken from a horizontally moving camera. This idea can be generalized\nto other camera motions and compositing surfaces using the concept of mosaics on adap-\ntive manifold (Peleg, Rousso, Rav-Acha et al. 2000), and also used to generate panoramic\nstereograms (Peleg, Ben-Ezra, and Pritch 2001). Related ideas have been used to create\npanoramic matte paintings for multi-plane cel animation (Wood, Finkelstein, Hughes et al.\n1997), for creating stitched images of scenes with parallax (Kumar, Anandan, Irani et al.\n1995), and as 3D representations of more complex scenes using multiple-center-of-projection\nimages (Rademacher and Bishop 1998) and multi-perspective panoramas (Rom´an, Garg, and\nLevoy 2004; Rom´an and Lensch 2006; Agarwala, Agrawala, Cohen et al. 2006).\nAnother interesting variant on video-based panoramas are concentric mosaics (Section 13.3.3)\n(Shum and He 1999). Here, rather than trying to produce a single panoramic image, the com-\nplete original video is kept and used to re-synthesize views (from different camera origins)\nusing ray remapping (light ﬁeld rendering), thus endowing the panorama with a sense of 3D\n438\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z)\nx = (sinθ,h,cosθ)\nθ\nh\nx\ny\np = (X,Y,Z)\nx = (sinθ cosφ, sinφ,\ncosθ cosφ)\nθ\nφ\nx\ny\n(a)\n(b)\nFigure 9.7 Projection from 3D to (a) cylindrical and (b) spherical coordinates.\ndepth. The same data set can also be used to explicitly reconstruct the depth using multi-\nbaseline stereo (Peleg, Ben-Ezra, and Pritch 2001; Li, Shum, Tang et al. 2004; Zheng, Kang,\nCohen et al. 2007).\n9.1.6 Cylindrical and spherical coordinates\nAn alternative to using homographies or 3D motions to align images is to ﬁrst warp the images\ninto cylindrical coordinates and then use a pure translational model to align them (Chen 1995;\nSzeliski 1996). Unfortunately, this only works if the images are all taken with a level camera\nor with a known tilt angle.\nAssume for now that the camera is in its canonical position, i.e., its rotation matrix is the\nidentity, R = I, so that the optical axis is aligned with the z axis and the y axis is aligned\nvertically. The 3D ray corresponding to an (x, y) pixel is therefore (x, y, f).\nWe wish to project this image onto a cylindrical surface of unit radius (Szeliski 1996).\nPoints on this surface are parameterized by an angle θ and a height h, with the 3D cylindrical\ncoordinates corresponding to (θ, h) given by\n(sin θ, h, cos θ) ∝(x, y, f),\n(9.12)\nas shown in Figure 9.7a. From this correspondence, we can compute the formula for the\nwarped or mapped coordinates (Szeliski and Shum 1997),\nx′\n=\nsθ = s tan−1 x\nf ,\n(9.13)\ny′\n=\nsh = s\ny\np\nx2 + f 2 ,\n(9.14)\nwhere s is an arbitrary scaling factor (sometimes called the radius of the cylinder) that can be\nset to s = f to minimize the distortion (scaling) near the center of the image.5 The inverse of\n5 The scale can also be set to a larger or smaller value for the ﬁnal compositing surface, depending on the desired\noutput panorama resolution—see Section 9.3.\n9.1 Motion models\n439\nthis mapping equation is given by\nx\n=\nf tan θ = f tan x′\ns ,\n(9.15)\ny\n=\nh\np\nx2 + f 2 = y′\ns f\nq\n1 + tan2 x′/s = f y′\ns sec x′\ns .\n(9.16)\nImages can also be projected onto a spherical surface (Szeliski and Shum 1997), which\nis useful if the ﬁnal panorama includes a full sphere or hemisphere of views, instead of just\na cylindrical strip. In this case, the sphere is parameterized by two angles (θ, φ), with 3D\nspherical coordinates given by\n(sin θ cos φ, sin φ, cos θ cos φ) ∝(x, y, f),\n(9.17)\nas shown in Figure 9.7b.6 The correspondence between coordinates is now given by (Szeliski\nand Shum 1997):\nx′\n=\nsθ = s tan−1 x\nf ,\n(9.18)\ny′\n=\nsφ = s tan−1\ny\np\nx2 + f 2 ,\n(9.19)\nwhile the inverse is given by\nx\n=\nf tan θ = f tan x′\ns ,\n(9.20)\ny\n=\np\nx2 + f 2 tan φ = tan y′\ns f\nq\n1 + tan2 x′/s = f tan y′\ns sec x′\ns .\n(9.21)\nNote that it may be simpler to generate a scaled (x, y, z) direction from Equation (9.17)\nfollowed by a perspective division by z and a scaling by f.\nCylindrical image stitching algorithms are most commonly used when the camera is\nknown to be level and only rotating around its vertical axis (Chen 1995). Under these condi-\ntions, images at different rotations are related by a pure horizontal translation.7 This makes\nit attractive as an initial class project in an introductory computer vision course, since the\nfull complexity of the perspective alignment algorithm (Sections 6.1, 8.2, and 9.1.3) can be\navoided. Figure 9.8 shows how two cylindrically warped images from a leveled rotational\npanorama are related by a pure translation (Szeliski and Shum 1997).\nProfessional panoramic photographers often use pan-tilt heads that make it easy to control\nthe tilt and to stop at speciﬁc detents in the rotation angle. Motorized rotation heads are also\n6 Note that these are not the usual spherical coordinates, ﬁrst presented in Equation (2.8). Here, the y axis points\nat the north pole instead of the z axis, since we are used to viewing images taken horizontally, i.e., with the y axis\npointing in the direction of the gravity vector.\n7Small vertical tilts can sometimes be compensated for with vertical translations.",
  "image_path": "page_460.jpg",
  "pages": [
    459,
    460,
    461
  ]
}