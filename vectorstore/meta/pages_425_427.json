{
  "doc_id": "pages_425_427",
  "text": "8.2 Parametric motion\n403\nChellappa 1997; Srinivasan, Chellappa, Veeraraghavan et al. 2005). Algorithms for stabiliza-\ntion run inside both hardware devices, such as camcorders and still cameras, and software\npackages for improving the visual quality of shaky videos.\nIn their paper on full-frame video stabilization, Matsushita, Ofek, Ge et al. (2006) give\na nice overview of the three major stages of stabilization, namely motion estimation, motion\nsmoothing, and image warping. Motion estimation algorithms often use a similarity trans-\nform to handle camera translations, rotations, and zooming. The tricky part is getting these\nalgorithms to lock onto the background motion, which is a result of the camera movement,\nwithout getting distracted by independent moving foreground objects. Motion smoothing al-\ngorithms recover the low-frequency (slowly varying) part of the motion and then estimate\nthe high-frequency shake component that needs to be removed. Finally, image warping algo-\nrithms apply the high-frequency correction to render the original frames as if the camera had\nundergone only the smooth motion.\nThe resulting stabilization algorithms can greatly improve the appearance of shaky videos\nbut they often still contain visual artifacts. For example, image warping can result in missing\nborders around the image, which must be cropped, ﬁlled using information from other frames,\nor hallucinated using inpainting techniques (Section 10.5.1). Furthermore, video frames cap-\ntured during fast motion are often blurry. Their appearance can be improved either using\ndeblurring techniques (Section 10.3) or stealing sharper pixels from other frames with less\nmotion or better focus (Matsushita, Ofek, Ge et al. 2006). Exercise 8.3 has you implement\nand test some of these ideas.\nIn situations where the camera is translating a lot in 3D, e.g., when the videographer is\nwalking, an even better approach is to compute a full structure from motion reconstruction\nof the camera motion and 3D scene. A smooth 3D camera path can then be computed and\nthe original video re-rendered using view interpolation with the interpolated 3D point cloud\nserving as the proxy geometry while preserving salient features (Liu, Gleicher, Jin et al.\n2009). If you have access to a camera array instead of a single video camera, you can do even\nbetter using a light ﬁeld rendering approach (Section 13.3) (Smith, Zhang, Jin et al. 2009).\n8.2.2 Learned motion models\nAn alternative to parameterizing the motion ﬁeld with a geometric deformation such as an\nafﬁne transform is to learn a set of basis functions tailored to a particular application (Black,\nYacoob, Jepson et al. 1997). First, a set of dense motion ﬁelds (Section 8.4) is computed from\na set of training videos. Next, singular value decomposition (SVD) is applied to the stack of\nmotion ﬁelds ut(x) to compute the ﬁrst few singular vectors vk(x). Finally, for a new test\nsequence, a novel ﬂow ﬁeld is computed using a coarse-to-ﬁne algorithm that estimates the\n404\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 8.6\nLearned parameterized motion ﬁelds for a walking sequence (Black, Yacoob,\nJepson et al. 1997) c⃝1997 IEEE: (a) learned basis ﬂow ﬁelds; (b) plots of motion coefﬁcients\nover time and corresponding estimated motion ﬁelds.\nunknown coefﬁcient ak in the parameterized ﬂow ﬁeld\nu(x) =\nX\nk\nakvk(x).\n(8.66)\nFigure 8.6a shows a set of basis ﬁelds learned by observing videos of walking motions.\nFigure 8.6b shows the temporal evolution of the basis coefﬁcients as well as a few of the\nrecovered parametric motion ﬁelds. Note that similar ideas can also be applied to feature\ntracks (Torresani, Hertzmann, and Bregler 2008), which is a topic we discuss in more detail\nin Sections 4.1.4 and 12.6.4.\n8.3 Spline-based motion\nWhile parametric motion models are useful in a wide variety of applications (such as video\nstabilization and mapping onto planar surfaces), most image motion is too complicated to be\ncaptured by such low-dimensional models.\nTraditionally, optical ﬂow algorithms (Section 8.4) compute an independent motion esti-\nmate for each pixel, i.e., the number of ﬂow vectors computed is equal to the number of input\npixels. The general optical ﬂow analog to Equation (8.1) can thus be written as\nESSD−OF({ui}) =\nX\ni\n[I1(xi + ui) −I0(xi)]2.\n(8.67)\n8.3 Spline-based motion\n405\nFigure 8.7 Spline motion ﬁeld: the displacement vectors ui = (ui, vi) are shown as pluses\n(+) and are controlled by the smaller number of control vertices ˆuj = (ˆui, ˆvj), which are\nshown as circles (◦).\nNotice how in the above equation, the number of variables {ui} is twice the number of\nmeasurements, so the problem is underconstrained.\nThe two classic approaches to this problem, which we study in Section 8.4, are to perform\nthe summation over overlapping regions (the patch-based or window-based approach) or to\nadd smoothness terms on the {ui} ﬁeld using regularization or Markov random ﬁelds (Sec-\ntion 3.7). In this section, we describe an alternative approach that lies somewhere between\ngeneral optical ﬂow (independent ﬂow at each pixel) and parametric ﬂow (a small number of\nglobal parameters). The approach is to represent the motion ﬁeld as a two-dimensional spline\ncontrolled by a smaller number of control vertices {ˆuj} (Figure 8.7),\nui =\nX\nj\nˆujBj(xi) =\nX\nj\nˆujwi,j,\n(8.68)\nwhere the Bj(xi) are called the basis functions and are only non-zero over a small ﬁnite sup-\nport interval (Szeliski and Coughlan 1997). We call the wij = Bj(xi) weights to emphasize\nthat the {ui} are known linear combinations of the {ˆuj}. Some commonly used spline basis\nfunctions are shown in Figure 8.8.\nSubstituting the formula for the individual per-pixel ﬂow vectors ui (8.68) into the SSD\nerror metric (8.67) yields a parametric motion formula similar to Equation (8.50). The biggest\ndifference is that the Jacobian J1(x′\ni) (8.52) now consists of the sparse entries in the weight\nmatrix W = [wij].\nIn situations where we know something more about the motion ﬁeld, e.g., when the mo-\ntion is due to a camera moving in a static scene, we can use more specialized motion models.\nFor example, the plane plus parallax model (Section 2.1.5) can be naturally combined with\na spline-based motion representation, where the in-plane motion is represented by a homog-\nraphy (6.19) and the out-of-plane parallax d is represented by a scalar variable at each spline",
  "image_path": "page_426.jpg",
  "pages": [
    425,
    426,
    427
  ]
}