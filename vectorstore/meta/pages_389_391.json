{
  "doc_id": "pages_389_391",
  "text": "7.4 Bundle adjustment\n367\na video camera or moving vehicle (Nist´er, Naroditsky, and Bergen 2006; Pollefeys, Nist´er,\nFrahm et al. 2008).) A Kalman ﬁlter can be used to incrementally update estimates as new\ninformation is acquired. Unfortunately, such sequential updating is only statistically optimal\nfor linear least squares problems.\nFor non-linear problems such as structure from motion, an extended Kalman ﬁlter, which\nlinearizes measurement and update equations around the current estimate, needs to be used\n(Gelb 1974; Vi´eville and Faugeras 1990). To overcome this limitation, several passes can\nbe made through the data (Azarbayejani and Pentland 1995). Because points disappear from\nview (and old cameras become irrelevant), a variable state dimension ﬁlter (VSDF) can be\nused to adjust the set of state variables over time, for example, by keeping only cameras and\npoint tracks seen in the last k frames (McLauchlan 2000). A more ﬂexible approach to using\na ﬁxed number of frames is to propagate corrections backwards through points and cameras\nuntil the changes on parameters are below a threshold (Steedly and Essa 2001). Variants of\nthese techniques, including methods that use a ﬁxed window for bundle adjustment (Engels,\nStew´enius, and Nist´er 2006) or select keyframes for doing full bundle adjustment (Klein and\nMurray 2008) are now commonly used in real-time tracking and augmented-reality applica-\ntions, as discussed in Section 7.4.2.\nWhen maximum accuracy is required, it is still preferable to perform a full bundle ad-\njustment over all the frames. In order to control the resulting computational complexity, one\napproach is to lock together subsets of frames into locally rigid conﬁgurations and to optimize\nthe relative positions of these cluster (Steedly, Essa, and Dellaert 2003). A different approach\nis to select a smaller number of frames to form a skeletal set that still spans the whole dataset\nand produces reconstructions of comparable accuracy (Snavely, Seitz, and Szeliski 2008b).\nWe describe this latter technique in more detail in Section 7.4.4, where we discuss applica-\ntions of structure from motion to large image sets.\nWhile bundle adjustment and other robust non-linear least squares techniques are the\nmethods of choice for most structure-from-motion problems, they suffer from initialization\nproblems, i.e., they can get stuck in local energy minima if not started sufﬁciently close\nto the global optimum. Many systems try to mitigate this by being conservative in what\nreconstruction they perform early on and which cameras and points they add to the solution\n(Section 7.4.4). An alternative, however, is to re-formulate the problem using a norm that\nsupports the computation of global optima.\nKahl and Hartley (2008) describe techniques for using L∞norms in geometric recon-\nstruction problems. The advantage of such norms is that globally optimal solutions can be\nefﬁciently computed using second-order cone programming (SOCP). The disadvantage is that\nL∞norms are particularly sensitive to outliers and so must be combined with good outlier\nrejection techniques before they can be used.\n368\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7.4.2 Application: Match move and augmented reality\nOne of the neatest applications of structure from motion is to estimate the 3D motion of a\nvideo or ﬁlm camera, along with the geometry of a 3D scene, in order to superimpose 3D\ngraphics or computer-generated images (CGI) on the scene. In the visual effects industry,\nthis is known as the match move problem (Roble 1999), since the motion of the synthetic 3D\ncamera used to render the graphics must be matched to that of the real-world camera. For\nvery small motions, or motions involving pure camera rotations, one or two tracked points can\nsufﬁce to compute the necessary visual motion. For planar surfaces moving in 3D, four points\nare needed to compute the homography, which can then be used to insert planar overlays, e.g.,\nto replace the contents of advertising billboards during sporting events.\nThe general version of this problem requires the estimation of the full 3D camera pose\nalong with the focal length (zoom) of the lens and potentially its radial distortion parameters\n(Roble 1999). When the 3D structure of the scene is known ahead of time, pose estima-\ntion techniques such as view correlation (Bogart 1991) or through-the-lens camera control\n(Gleicher and Witkin 1992) can be used, as described in Section 6.2.3.\nFor more complex scenes, it is usually preferable to recover the 3D structure simultane-\nously with the camera motion using structure-from-motion techniques. The trick with using\nsuch techniques is that in order to prevent any visible jitter between the synthetic graph-\nics and the actual scene, features must be tracked to very high accuracy and ample feature\ntracks must be available in the vicinity of the insertion location. Some of today’s best known\nmatch move software packages, such as the boujou package from 2d3,15 which won an Emmy\naward in 2002, originated in structure-from-motion research in the computer vision commu-\nnity (Fitzgibbon and Zisserman 1998).\nClosely related to the match move problem is robotics navigation, where a robot must es-\ntimate its location relative to its environment, while simultaneously avoiding any dangerous\nobstacles. This problem is often known as simultaneous localization and mapping (SLAM)\n(Thrun, Burgard, and Fox 2005) or visual odometry (Levin and Szeliski 2004; Nist´er, Nar-\noditsky, and Bergen 2006; Maimone, Cheng, and Matthies 2007). Early versions of such\nalgorithms used range-sensing techniques, such as ultrasound, laser range ﬁnders, or stereo\nmatching, to estimate local 3D geometry, which could then be fused into a 3D model. Newer\ntechniques can perform the same task based purely on visual feature tracking, sometimes not\neven requiring a stereo camera rig (Davison, Reid, Molton et al. 2007).\nAnother closely related application is augmented reality, where 3D objects are inserted\ninto a video feed in real time, often to annotate or help users understand a scene (Azuma,\nBaillot, Behringer et al. 2001). While traditional systems require prior knowledge about the\nscene or object being visually tracked (Rosten and Drummond 2005), newer systems can\n15 http://www.2d3.com/.\n7.4 Bundle adjustment\n369\n(a)\n(b)\nFigure 7.10\n3D augmented reality: (a) Darth Vader and a horde of Ewoks battle it out\non a table-top recovered using real-time, keyframe-based structure from motion (Klein and\nMurray 2007) c⃝2007 IEEE; (b) a virtual teapot is ﬁxed to the top of a real-world coffee cup,\nwhose pose is re-recognized at each time frame (Gordon and Lowe 2006) c⃝2007 Springer.\nsimultaneously build up a model of the 3D environment and then track it, so that graphics can\nbe superimposed.\nKlein and Murray (2007) describe a parallel tracking and mapping (PTAM) system,\nwhich simultaneously applies full bundle adjustment to keyframes selected from a video\nstream, while performing robust real-time pose estimation on intermediate frames.\nFig-\nure 7.10a shows an example of their system in use. Once an initial 3D scene has been\nreconstructed, a dominant plane is estimated (in this case, the table-top) and 3D animated\ncharacters are virtually inserted. Klein and Murray (2008) extend their previous system to\nhandle even faster camera motion by adding edge features, which can still be detected even\nwhen interest points become too blurred. They also use a direct (intensity-based) rotation\nestimation algorithm for even faster motions.\nInstead of modeling the whole scene as one rigid reference frame, Gordon and Lowe\n(2006) ﬁrst build a 3D model of an individual object using feature matching and structure\nfrom motion. Once the system has been initialized, for every new frame, they ﬁnd the object\nand its pose using a 3D instance recognition algorithm, and then superimpose a graphical\nobject onto that model, as shown in Figure 7.10b.\nWhile reliably tracking such objects and environments is now a well-solved problem,\ndetermining which pixels should be occluded by foreground scene elements still remains an\nopen problem (Chuang, Agarwala, Curless et al. 2002; Wang and Cohen 2007a).",
  "image_path": "page_390.jpg",
  "pages": [
    389,
    390,
    391
  ]
}