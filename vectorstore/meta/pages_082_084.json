{
  "doc_id": "pages_082_084",
  "text": "60\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmay become necessary to model the 3D line (as opposed to direction) corresponding to each\npixel separately (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Sautot et al.\n1992; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm, Trudeau et\nal. 2009). Some of these techniques are described in more detail in Section 6.3.5, which\ndiscusses how to calibrate lens distortions.\nThere is one subtle issue associated with the simple radial distortion model that is often\nglossed over. We have introduced a non-linearity between the perspective projection and ﬁnal\nsensor array projection steps. Therefore, we cannot, in general, post-multiply an arbitrary 3×\n3 matrix K with a rotation to put it into upper-triangular form and absorb this into the global\nrotation. However, this situation is not as bad as it may at ﬁrst appear. For many applications,\nkeeping the simpliﬁed diagonal form of (2.59) is still an adequate model. Furthermore, if we\ncorrect radial and other distortions to an accuracy where straight lines are preserved, we have\nessentially converted the sensor back into a linear imager and the previous decomposition still\napplies.\n2.2 Photometric image formation\nIn modeling the image formation process, we have described how 3D geometric features in\nthe world are projected into 2D features in an image. However, images are not composed of\n2D features. Instead, they are made up of discrete color or intensity values. Where do these\nvalues come from? How do they relate to the lighting in the environment, surface properties\nand geometry, camera optics, and sensor properties (Figure 2.14)? In this section, we develop\na set of models to describe these interactions and formulate a generative process of image\nformation. A more detailed treatment of these topics can be found in other textbooks on\ncomputer graphics and image synthesis (Glassner 1995; Weyrich, Lawrence, Lensch et al.\n2008; Foley, van Dam, Feiner et al. 1995; Watt 1995; Cohen and Wallace 1993; Sillion and\nPuech 1994).\n2.2.1 Lighting\nImages cannot exist without light. To produce an image, the scene must be illuminated with\none or more light sources. (Certain modalities such as ﬂuorescent microscopy and X-ray\ntomography do not ﬁt this model, but we do not deal with them in this book.) Light sources\ncan generally be divided into point and area light sources.\nA point light source originates at a single location in space (e.g., a small light bulb),\npotentially at inﬁnity (e.g., the sun). (Note that for some applications such as modeling soft\nshadows (penumbras), the sun may have to be treated as an area light source.) In addition to\nits location, a point light source has an intensity and a color spectrum, i.e., a distribution over\n2.2 Photometric image formation\n61\nn^\nsurface\nlight \nsource\nimage plane\nsensor \nplane\noptics\nFigure 2.14\nA simpliﬁed model of photometric image formation. Light is emitted by one\nor more light sources and is then reﬂected from an object’s surface. A portion of this light is\ndirected towards the camera. This simpliﬁed model ignores multiple reﬂections, which often\noccur in real-world scenes.\nwavelengths L(λ). The intensity of a light source falls off with the square of the distance\nbetween the source and the object being lit, because the same light is being spread over a\nlarger (spherical) area. A light source may also have a directional falloff (dependence), but\nwe ignore this in our simpliﬁed model.\nArea light sources are more complicated. A simple area light source such as a ﬂuorescent\nceiling light ﬁxture with a diffuser can be modeled as a ﬁnite rectangular area emitting light\nequally in all directions (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995).\nWhen the distribution is strongly directional, a four-dimensional lightﬁeld can be used instead\n(Ashdown 1993).\nA more complex light distribution that approximates, say, the incident illumination on an\nobject sitting in an outdoor courtyard, can often be represented using an environment map\n(Greene 1986) (originally called a reﬂection map (Blinn and Newell 1976)). This representa-\ntion maps incident light directions ˆv to color values (or wavelengths, λ),\nL(ˆv; λ),\n(2.80)\nand is equivalent to assuming that all light sources are at inﬁnity. Environment maps can be\nrepresented as a collection of cubical faces (Greene 1986), as a single longitude–latitude map\n(Blinn and Newell 1976), or as the image of a reﬂecting sphere (Watt 1995). A convenient\nway to get a rough model of a real-world environment map is to take an image of a reﬂective\nmirrored sphere and to unwrap this image onto the desired environment map (Debevec 1998).\nWatt (1995) gives a nice discussion of environment mapping, including the formulas needed\nto map directions to pixels for the three most commonly used representations.\n62\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nn^\nvi\ndx\nn^\nvr\n^\ndy^\nθi\nφi\nφr\nθr\n^\n^\n(a)\n(b)\nFigure 2.15\n(a) Light scatters when it hits a surface. (b) The bidirectional reﬂectance\ndistribution function (BRDF) f(θi, φi, θr, φr) is parameterized by the angles that the inci-\ndent, ˆvi, and reﬂected, ˆvr, light ray directions make with the local surface coordinate frame\n( ˆdx, ˆdy, ˆn).\n2.2.2 Reﬂectance and shading\nWhen light hits an object’s surface, it is scattered and reﬂected (Figure 2.15a). Many different\nmodels have been developed to describe this interaction. In this section, we ﬁrst describe the\nmost general form, the bidirectional reﬂectance distribution function, and then look at some\nmore specialized models, including the diffuse, specular, and Phong shading models. We also\ndiscuss how these models can be used to compute the global illumination corresponding to a\nscene.\nThe Bidirectional Reﬂectance Distribution Function (BRDF)\nThe most general model of light scattering is the bidirectional reﬂectance distribution func-\ntion (BRDF).5 Relative to some local coordinate frame on the surface, the BRDF is a four-\ndimensional function that describes how much of each wavelength arriving at an incident\ndirection ˆvi is emitted in a reﬂected direction ˆvr (Figure 2.15b). The function can be written\nin terms of the angles of the incident and reﬂected directions relative to the surface frame as\nfr(θi, φi, θr, φr; λ).\n(2.81)\nThe BRDF is reciprocal, i.e., because of the physics of light transport, you can interchange\nthe roles of ˆvi and ˆvr and still get the same answer (this is sometimes called Helmholtz\nreciprocity).\n5 Actually, even more general models of light transport exist, including some that model spatial variation along\nthe surface, sub-surface scattering, and atmospheric effects—see Section 12.7.1—(Dorsey, Rushmeier, and Sillion\n2007; Weyrich, Lawrence, Lensch et al. 2008).",
  "image_path": "page_083.jpg",
  "pages": [
    82,
    83,
    84
  ]
}