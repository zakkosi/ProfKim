{
  "doc_id": "pages_665_667",
  "text": "13.5 Video-based rendering\n643\n    displacement map\n...\n(a)\n(b)\n(c)\n(d)\n(e)\n...\n...\n=\n=\n=\n=\n=\n \nL1\nL2\nLl-2\nLl-1\nLl\nL  (t)\n1\nL  (t)\n2\nL    (t)\nl-2\nL    (t)\nl-1\nL (t)\nl\n    displacement map\n    displacement map\n    displacement map\n    displacement map\nd    (t)\n l-1\nd (t)\n l\nd    (t)\n l-2\nd  (t)\n 2\nd  (t)\n 1\ntype=“boat”\ntype=“still”\ntype=“tree”\ntype=“cloud”\ntype=“water”\nFigure 13.14 Animating still pictures (Chuang, Goldman, Zheng et al. 2005) c⃝2005 ACM.\n(a) The input still image is manually segmented into (b) several layers. (c) Each layer is\nthen animated with a different stochastic motion texture (d) The animated layers are then\ncomposited to produce (e) the ﬁnal animation\n13.5.3 Application: Animating pictures\nWhile video textures can turn a short video clip into an inﬁnitely long video, can the same\nthing be done with a single still image? The answer is yes, if you are willing to ﬁrst segment\nthe image into different layers and then animate each layer separately.\nChuang, Goldman, Zheng et al. (2005) describe how an image can be decomposed into\nseparate layers using interactive matting techniques. Each layer is then animated using a\nclass-speciﬁc synthetic motion. As shown in Figure 13.14, boats rock back and forth, trees\nsway in the wind, clouds move horizontally, and water ripples, using a shaped noise displace-\nment map. All of these effects can be tied to some global control parameters, such as the\nvelocity and direction of a virtual wind. After being individually animated, the layers can be\ncomposited to create a ﬁnal dynamic rendering.\n13.5.4 3D Video\nIn recent years, the popularity of 3D movies has grown dramatically, with recent releases\nranging from Hannah Montana, through U2’s 3D concert movie, to James Cameron’s Avatar.\nCurrently, such releases are ﬁlmed using stereoscopic camera rigs and displayed in theaters\n(or at home) to viewers wearing polarized glasses.8 In the future, however, home audiences\nmay wish to view such movies with multi-zone auto-stereoscopic displays, where each person\ngets his or her own customized stereo stream and can move around a scene to see it from\n8 http://www.3d-summit.com/.\n644\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nRender\nbackground\nBi\nRender\nforeground\nFi\nOver\ncomposite\nCamera i\nRender\nbackground\nBi+1\nRender\nforeground\nFi+1\nOver\ncomposite\nBlend\nCamera i+1\n(a)\n(b)\ndi\nMi\nBi\nstrip\nwidth\nstrip\nwidth\ndepth\ndiscontinuity\nmatte\n(c)\n(d)\n(e)\n(f)\nFigure 13.15\nVideo view interpolation (Zitnick, Kang, Uyttendaele et al. 2004) c⃝2004\nACM: (a) the capture hardware consists of eight synchronized cameras; (b) the background\nand foreground images from each camera are rendered and composited before blending; (c)\nthe two-layer representation, before and after boundary matting; (d) background color esti-\nmates; (e) background depth estimates; (f) foreground color estimates.\ndifferent perspectives.9\nThe stereo matching techniques developed in the computer vision community along with\nimage-based rendering (view interpolation) techniques from graphics are both essential com-\nponents in such scenarios, which are sometimes called free-viewpoint video (Carranza, Theobalt,\nMagnor et al. 2003) or virtual viewpoint video (Zitnick, Kang, Uyttendaele et al. 2004). In\naddition to solving a series of per-frame reconstruction and view interpolation problems, the\ndepth maps or proxies produced by the analysis phase must be temporally consistent in order\nto avoid ﬂickering artifacts.\nShum, Chan, and Kang (2007) and Magnor (2005) present nice overviews of various\nvideo view interpolation techniques and systems. These include the Virtualized Reality sys-\ntem of Kanade, Rander, and Narayanan (1997) and Vedula, Baker, and Kanade (2005), Im-\nmersive Video (Moezzi, Katkere, Kuramura et al. 1996), Image-Based Visual Hulls (Matusik,\nBuehler, Raskar et al. 2000; Matusik, Buehler, and McMillan 2001), and Free-Viewpoint\nVideo (Carranza, Theobalt, Magnor et al. 2003), which all use global 3D geometric models\n(surface-based (Section 12.3) or volumetric (Section 12.5)) as their proxies for rendering.\nThe work of Vedula, Baker, and Kanade (2005) also computes scene ﬂow, i.e., the 3D motion\nbetween corresponding surface elements, which can then be used to perform spatio-temporal\ninterpolation of the multi-view video stream.\nThe Virtual Viewpoint Video system of Zitnick, Kang, Uyttendaele et al. (2004), on the\n9 http://www.siggraph.org/s2008/attendees/caf/3d/.\n13.5 Video-based rendering\n645\nother hand, associates a two-layer depth map with each input image, which allows them to\naccurately model occlusion effects such as the mixed pixels that occur at object boundaries.\nTheir system, which consists of eight synchronized video cameras connected to a disk array\n(Figure 13.15a), ﬁrst uses segmentation-based stereo to extract a depth map for each input\nimage (Figure 13.15e). Near object boundaries (depth discontinuities), the background layer\nis extended along a strip behind the foreground object (Figure 13.15c) and its color is es-\ntimated from the neighboring images where it is not occluded (Figure 13.15d). Automated\nmatting techniques (Section 10.4) are then used to estimate the fractional opacity and color\nof boundary pixels in the foreground layer (Figure 13.15f).\nAt render time, given a new virtual camera that lies between two of the original cameras,\nthe layers in the neighboring cameras are rendered as texture-mapped triangles and the fore-\nground layer (which may have fractional opacities) is then composited over the background\nlayer (Figure 13.15b). The resulting two images are merged and blended by comparing their\nrespective z-buffer values. (Whenever the two z-values are sufﬁciently close, a linear blend of\nthe two colors is computed.) The interactive rendering system runs in real time using regular\ngraphics hardware. It can therefore be used to change the observer’s viewpoint while playing\nthe video or to freeze the scene and explore it in 3D. More recently, Rogmans, Lu, Bekaert\net al. (2009) have developed GPU implementations of both real-time stereo matching and\nreal-time rendering algorithms, which enable them to explore algorithmic alternatives in a\nreal-time setting.\nAt present, the depth maps computed from the eight stereo cameras using off-line stereo\nmatching have produced the highest quality depth maps associated with live video.10 They\nare therefore often used in studies of 3D video compression, which is an active area of re-\nsearch (Smolic and Kauff 2005; Gotchev and Rosenhahn 2009). Active video-rate depth\nsensing cameras, such as the 3DV Zcam (Iddan and Yahav 2001), which we discussed in\nSection 12.2.1, are another potential source of such data.\nWhen large numbers of closely spaced cameras are available, as in the Stanford Light\nField Camera (Wilburn, Joshi, Vaish et al. 2005), it may not always be necessary to compute\nexplicit depth maps to create video-based rendering effects, although the results are usually\nof higher quality if you do (Vaish, Szeliski, Zitnick et al. 2006).\n13.5.5 Application: Video-based walkthroughs\nVideo camera arrays enable the simultaneous capture of 3D dynamic scenes from multiple\nviewpoints, which can then enable the viewer to explore the scene from viewpoints near the\noriginal capture locations. What if instead we wish to capture an extended area, such as a\nhome, a movie set, or even an entire city?\n10 http://research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/.",
  "image_path": "page_666.jpg",
  "pages": [
    665,
    666,
    667
  ]
}