{
  "doc_id": "pages_557_559",
  "text": "11 Stereo correspondence\n535\nStereo matching is the process of taking two or more images and estimating a 3D model of\nthe scene by ﬁnding matching pixels in the images and converting their 2D positions into\n3D depths. In Chapters 6–7, we described techniques for recovering camera positions and\nbuilding sparse 3D models of scenes or objects. In this chapter, we address the question\nof how to build a more complete 3D model, e.g., a sparse or dense depth map that assigns\nrelative depths to pixels in the input images. We also look at the topic of multi-view stereo\nalgorithms that produce complete 3D volumetric or surface-based object models.\nWhy are people interested in stereo matching? From the earliest inquiries into visual per-\nception, it was known that we perceive depth based on the differences in appearance between\nthe left and right eye.1 As a simple experiment, hold your ﬁnger vertically in front of your\neyes and close each eye alternately. You will notice that the ﬁnger jumps left and right relative\nto the background of the scene. The same phenomenon is visible in the image pair shown in\nFigure 11.1a–b, in which the foreground objects shift left and right relative to the background.\nAs we will shortly see, under simple imaging conﬁgurations (both eyes or cameras look-\ning straight ahead), the amount of horizontal motion or disparity is inversely proportional to\nthe distance from the observer. While the basic physics and geometry relating visual disparity\nto scene structure are well understood (Section 11.1), automatically measuring this disparity\nby establishing dense and accurate inter-image correspondences is a challenging task.\nThe earliest stereo matching algorithms were developed in the ﬁeld of photogrammetry\nfor automatically constructing topographic elevation maps from overlapping aerial images.\nPrior to this, operators would use photogrammetric stereo plotters, which displayed shifted\nversions of such images to each eye and allowed the operator to ﬂoat a dot cursor around con-\nstant elevation contours (Figure 11.1g). The development of fully automated stereo matching\nalgorithms was a major advance in this ﬁeld, enabling much more rapid and less expensive\nprocessing of aerial imagery (Hannah 1974; Hsieh, McKeown, and Perlant 1992).\nIn computer vision, the topic of stereo matching has been one of the most widely stud-\nied and fundamental problems (Marr and Poggio 1976; Barnard and Fischler 1982; Dhond\nand Aggarwal 1989; Scharstein and Szeliski 2002; Brown, Burschka, and Hager 2003; Seitz,\nCurless, Diebel et al. 2006), and continues to be one of the most active research areas. While\nphotogrammetric matching concentrated mainly on aerial imagery, computer vision applica-\ntions include modeling the human visual system (Marr 1982), robotic navigation and manip-\nulation (Moravec 1983; Konolige 1997; Thrun, Montemerlo, Dahlkamp et al. 2006), as well\nas view interpolation and image-based rendering (Figure 11.2a–d), 3D model building (Fig-\nure 11.2e–f and h–j), and mixing live action with computer-generated imagery (Figure 11.2g).\nIn this chapter, we describe the fundamental principles behind stereo matching, following\n1 The word stereo comes from the Greek for solid; stereo vision is how we perceive solid shape (Koenderink\n1990).\n536\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\nFigure 11.2 Applications of stereo vision: (a) input image, (b) computed depth map, and (c)\nnew view generation from multi-view stereo (Matthies, Kanade, and Szeliski 1989) c⃝1989\nSpringer; (d) view morphing between two images (Seitz and Dyer 1996) c⃝1996 ACM; (e–f)\n3D face modeling (images courtesy of Fr´ed´eric Devernay); (g) z-keying live and computer-\ngenerated imagery (Kanade, Yoshida, Oda et al. 1996) c⃝1996 IEEE; (h–j) building 3D\nsurface models from multiple video streams in Virtualized Reality (Kanade, Rander, and\nNarayanan 1997).\n11.1 Epipolar geometry\n537\nthe general taxonomy proposed by Scharstein and Szeliski (2002). We begin in Section 11.1\nwith a review of the geometry of stereo image matching, i.e., how to compute for a given\npixel in one image the range of possible locations the pixel might appear at in the other\nimage, i.e., its epipolar line. We describe how to pre-warp images so that corresponding\nepipolar lines are coincident (rectiﬁcation). We also describe a general resampling algorithm\ncalled plane sweep that can be used to perform multi-image stereo matching with arbitrary\ncamera conﬁgurations.\nNext, we brieﬂy survey techniques for the sparse stereo matching of interest points and\nedge-like features (Section 11.2). We then turn to the main topic of this chapter, namely the\nestimation of a dense set of pixel-wise correspondences in the form of a disparity map (Fig-\nure 11.1c). This involves ﬁrst selecting a pixel matching criterion (Section 11.3) and then\nusing either local area-based aggregation (Section 11.4) or global optimization (Section 11.5)\nto help disambiguate potential matches. In Section 11.6, we discuss multi-view stereo meth-\nods that aim to reconstruct a complete 3D model instead of just a single disparity image\n(Figure 11.1d–f).\n11.1 Epipolar geometry\nGiven a pixel in one image, how can we compute its correspondence in the other image? In\nChapter 8, we saw that a variety of search techniques can be used to match pixels based on\ntheir local appearance as well as the motions of neighboring pixels. In the case of stereo\nmatching, however, we have some additional information available, namely the positions and\ncalibration data for the cameras that took the pictures of the same static scene (Section 7.2).\nHow can we exploit this information to reduce the number of potential correspondences,\nand hence both speed up the matching and increase its reliability? Figure 11.3a shows how a\npixel in one image x0 projects to an epipolar line segment in the other image. The segment\nis bounded at one end by the projection of the original viewing ray at inﬁnity p∞and at the\nother end by the projection of the original camera center c0 into the second camera, which\nis known as the epipole e1. If we project the epipolar line in the second image back into the\nﬁrst, we get another line (segment), this time bounded by the other corresponding epipole\ne0. Extending both line segments to inﬁnity, we get a pair of corresponding epipolar lines\n(Figure 11.3b), which are the intersection of the two image planes with the epipolar plane\nthat passes through both camera centers c0 and c1 as well as the point of interest p (Faugeras\nand Luong 2001; Hartley and Zisserman 2004).",
  "image_path": "page_558.jpg",
  "pages": [
    557,
    558,
    559
  ]
}