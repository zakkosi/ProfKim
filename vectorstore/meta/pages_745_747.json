{
  "doc_id": "pages_745_747",
  "text": "14.7 Additional reading\n723\nIn order to boost the performance of what are essentially 2D appearance-based models,\na variety of shape and pose deformation models have been developed (Beymer 1996; Vet-\nter and Poggio 1997), including Active Shape Models (Lanitis, Taylor, and Cootes 1997;\nCootes, Cooper, Taylor et al. 1995; Davies, Twining, and Taylor 2008), Elastic Bunch Graph\nMatching (Wiskott, Fellous, Kr¨uger et al. 1997), 3D Morphable Models (Blanz and Vetter\n1999), and Active Appearance Models (Costen, Cootes, Edwards et al. 1999; Cootes, Ed-\nwards, and Taylor 2001; Gross, Baker, Matthews et al. 2005; Gross, Matthews, and Baker\n2006; Matthews, Xiao, and Baker 2007; Liang, Xiao, Wen et al. 2008; Ramnath, Koterba,\nXiao et al. 2008). The topic of head pose estimation, in particular, is covered in a recent\nsurvey by Murphy-Chutorian and Trivedi (2009).\nAdditional information about face recognition can be found in a number of surveys and\nbooks on this topic (Chellappa, Wilson, and Sirohey 1995; Zhao, Chellappa, Phillips et al.\n2003; Li and Jain 2005) as well as on the Face Recognition Web site.23 Databases for face\nrecognition are discussed by Phillips, Moon, Rizvi et al. (2000), Sim, Baker, and Bsat (2003),\nGross, Shi, and Cohn (2005), Huang, Ramesh, Berg et al. (2007), and Phillips, Scruggs,\nO’Toole et al. (2010).\nAlgorithms for instance recognition, i.e., the detection of static man-made objects that\nonly vary slightly in appearance but may vary in 3D pose, are mostly based on detecting\n2D points of interest and describing them using viewpoint-invariant descriptors (Lowe 2004;\nRothganger, Lazebnik, Schmid et al. 2006; Ferrari, Tuytelaars, and Van Gool 2006b; Gordon\nand Lowe 2006; Obdrˇz´alek and Matas 2006; Kannala, Rahtu, Brandt et al. 2008; Sivic and\nZisserman 2009).\nAs the size of the database being matched increases, it becomes more efﬁcient to quantize\nthe visual descriptors into words (Sivic and Zisserman 2003; Schindler, Brown, and Szeliski\n2007; Sivic and Zisserman 2009; Turcot and Lowe 2009), and to then use information-\nretrieval techniques, such as inverted indices (Nist´er and Stew´enius 2006; Philbin, Chum,\nIsard et al. 2007; Philbin, Chum, Sivic et al. 2008), query expansion (Chum, Philbin, Sivic\net al. 2007; Agarwal, Snavely, Simon et al. 2009), and min hashing (Philbin and Zisserman\n2008; Li, Wu, Zach et al. 2008; Chum, Philbin, and Zisserman 2008; Chum and Matas 2010)\nto perform efﬁcient retrieval and clustering.\nA number of surveys, collections of papers, and course notes have been written on the\ntopic of category recognition (Pinz 2005; Ponce, Hebert, Schmid et al. 2006; Dickinson,\nLeonardis, Schiele et al. 2007; Fei-Fei, Fergus, and Torralba 2009). Some of the seminal\npapers on the bag of words (bag of keypoints) approach to whole-image category recognition\nhave been written by Csurka, Dance, Fan et al. (2004), Lazebnik, Schmid, and Ponce (2006),\nCsurka, Dance, Perronnin et al. (2006), Grauman and Darrell (2007b), and Zhang, Marszalek,\n23 http://www.face-rec.org/.\n724\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLazebnik et al. (2007). Additional and more recent papers in this area include Sivic, Russell,\nEfros et al. (2005), Serre, Wolf, and Poggio (2005), Opelt, Pinz, Fussenegger et al. (2006),\nGrauman and Darrell (2007a), Torralba, Murphy, and Freeman (2007), Boiman, Shechtman,\nand Irani (2008), Ferencz, Learned-Miller, and Malik (2008), and Mutch and Lowe (2008).\nIt is also possible to recognize objects based on their contours, e.g., using shape contexts\n(Belongie, Malik, and Puzicha 2002) or other techniques (Jurie and Schmid 2004; Shotton,\nBlake, and Cipolla 2005; Opelt, Pinz, and Zisserman 2006; Ferrari, Tuytelaars, and Van Gool\n2006a).\nMany object recognition algorithms use part-based decompositions to provide greater in-\nvariance to articulation and pose. Early algorithms focused on the relative positions of the\nparts (Fischler and Elschlager 1973; Kanade 1977; Yuille 1991) while newer algorithms use\nmore sophisticated models of appearance (Felzenszwalb and Huttenlocher 2005; Fergus, Per-\nona, and Zisserman 2007; Felzenszwalb, McAllester, and Ramanan 2008). Good overviews\non part-based models for recognition can be found in the course notes of Fergus 2007b; 2009.\nCarneiro and Lowe (2006) discuss a number of graphical models used for part-based\nrecognition, which include trees and stars (Felzenszwalb and Huttenlocher 2005; Fergus, Per-\nona, and Zisserman 2005; Felzenszwalb, McAllester, and Ramanan 2008), k-fans (Crandall,\nFelzenszwalb, and Huttenlocher 2005; Crandall and Huttenlocher 2006), and constellations\n(Burl, Weber, and Perona 1998; Weber, Welling, and Perona 2000; Fergus, Perona, and Zis-\nserman 2007). Other techniques that use part-based recognition include those developed by\nDork´o and Schmid (2003) and Bar-Hillel, Hertz, and Weinshall (2005).\nCombining object recognition with scene segmentation can yield strong beneﬁts. One\napproach is to pre-segment the image into pieces and then match the pieces to portions of\nthe model (Mori, Ren, Efros et al. 2004; Mori 2005; He, Zemel, and Ray 2006; Russell,\nEfros, Sivic et al. 2006; Borenstein and Ullman 2008; Csurka and Perronnin 2008; Gu, Lim,\nArbelaez et al. 2009). Another is to vote for potential object locations and scales based on\nobject detection (Leibe, Leonardis, and Schiele 2008). One of the currently most popular\napproaches is to use conditional random ﬁelds (Kumar and Hebert 2006; He, Zemel, and\nCarreira-Perpi˜n´an 2004; He, Zemel, and Ray 2006; Levin and Weiss 2006; Winn and Shotton\n2006; Hoiem, Rother, and Winn 2007; Rabinovich, Vedaldi, Galleguillos et al. 2007; Verbeek\nand Triggs 2007; Yang, Meer, and Foran 2007; Batra, Sukthankar, and Chen 2008; Larlus\nand Jurie 2008; He and Zemel 2008; Shotton, Winn, Rother et al. 2009; Kumar, Torr, and\nZisserman 2010), which produce some of the best results on the difﬁcult PASCAL VOC seg-\nmentation challenge (Shotton, Johnson, and Cipolla 2008; Kohli, Ladick´y, and Torr 2009).\nMore and more recognition algorithms are starting to use scene context as part of their\nrecognition strategy. Representative papers in this area include those by Torralba (2003),\nTorralba, Murphy, Freeman et al. (2003), Murphy, Torralba, and Freeman (2003), Torralba,\nMurphy, and Freeman (2004), Crandall and Huttenlocher (2007), Rabinovich, Vedaldi, Gal-\n14.8 Exercises\n725\nleguillos et al. (2007), Russell, Torralba, Liu et al. (2007), Hoiem, Efros, and Hebert (2008a),\nHoiem, Efros, and Hebert (2008b), Sudderth, Torralba, Freeman et al. (2008), and Divvala,\nHoiem, Hays et al. (2009).\nSophisticated machine learning techniques are also becoming a key component of suc-\ncessful object detection and recognition algorithms (Varma and Ray 2007; Felzenszwalb,\nMcAllester, and Ramanan 2008; Fritz and Schiele 2008; Sivic, Russell, Zisserman et al.\n2008; Vedaldi, Gulshan, Varma et al. 2009), as is exploiting large human-labeled databases\n(Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros 2008; Torralba, Freeman, and Fer-\ngus 2008; Liu, Yuen, and Torralba 2009). Rough three-dimensional models are also making\na comeback for recognition, as evidenced in some recent papers (Savarese and Fei-Fei 2007,\n2008; Sun, Su, Savarese et al. 2009; Su, Sun, Fei-Fei et al. 2009). As always, the latest con-\nferences on computer vision are your best reference for the newest algorithms in this rapidly\nevolving ﬁeld.\n14.8 Exercises\nEx 14.1: Face detection\nBuild and test one of the face detectors presented in Section 14.1.1.\n1. Download one or more of the labeled face detection databases in Table 14.2.\n2. Generate your own negative examples by ﬁnding photographs that do not contain any\npeople.\n3. Implement one of the following face detectors (or devise one of your own):\n• boosting (Algorithm 14.1) based on simple area features, with an optional cascade\nof detectors (Viola and Jones 2004);\n• PCA face subspace (Moghaddam and Pentland 1997);\n• distances to clustered face and non-face prototypes, followed by a neural network\n(Sung and Poggio 1998) or SVM (Osuna, Freund, and Girosi 1997) classiﬁer;\n• a multi-resolution neural network trained directly on normalized gray-level patches\n(Rowley, Baluja, and Kanade 1998a).\n4. Test the performance of your detector on the database by evaluating the detector at ev-\nery location in a sub-octave pyramid. Optionally retrain your detector on false positive\nexamples you get on non-face images.\nEx 14.2: Determining the threshold for AdaBoost\nGiven a set of function evaluations on\nthe training examples xi, fi = f(xi) ∈±1, training labels yi ∈±1, and weights wi ∈(0, 1),",
  "image_path": "page_746.jpg",
  "pages": [
    745,
    746,
    747
  ]
}