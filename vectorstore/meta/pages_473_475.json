{
  "doc_id": "pages_473_475",
  "text": "9.3 Compositing\n451\nwhich pixels contribute to the ﬁnal composite and how to optimally blend these pixels to\nminimize visible seams, blur, and ghosting.\nIn this section, we review techniques that address these problems, namely compositing\nsurface parameterization, pixel and seam selection, blending, and exposure compensation.\nMy emphasis is on fully automated approaches to the problem. Since the creation of high-\nquality panoramas and composites is as much an artistic endeavor as a computational one,\nvarious interactive tools have been developed to assist this process (Agarwala, Dontcheva,\nAgrawala et al. 2004; Li, Sun, Tang et al. 2004; Rother, Kolmogorov, and Blake 2004).\nSome of these are covered in more detail in Section 10.4.\n9.3.1 Choosing a compositing surface\nThe ﬁrst choice to be made is how to represent the ﬁnal image. If only a few images are\nstitched together, a natural approach is to select one of the images as the reference and to\nthen warp all of the other images into its reference coordinate system. The resulting com-\nposite is sometimes called a ﬂat panorama, since the projection onto the ﬁnal surface is still\na perspective projection, and hence straight lines remain straight (which is often a desirable\nattribute).12\nFor larger ﬁelds of view, however, we cannot maintain a ﬂat representation without ex-\ncessively stretching pixels near the border of the image. (In practice, ﬂat panoramas start\nto look severely distorted once the ﬁeld of view exceeds 90◦or so.) The usual choice for\ncompositing larger panoramas is to use a cylindrical (Chen 1995; Szeliski 1996) or spherical\n(Szeliski and Shum 1997) projection, as described in Section 9.1.6. In fact, any surface used\nfor environment mapping in computer graphics can be used, including a cube map, which\nrepresents the full viewing sphere with the six square faces of a cube (Greene 1986; Szeliski\nand Shum 1997). Cartographers have also developed a number of alternative methods for\nrepresenting the globe (Bugayevskiy and Snyder 1995).\nThe choice of parameterization is somewhat application dependent and involves a trade-\noff between keeping the local appearance undistorted (e.g., keeping straight lines straight)\nand providing a reasonably uniform sampling of the environment. Automatically making\nthis selection and smoothly transitioning between representations based on the extent of the\npanorama is an active area of current research (Kopf, Uyttendaele, Deussen et al. 2007).\nAn interesting recent development in panoramic photography has been the use of stereo-\ngraphic projections looking down at the ground (in an outdoor scene) to create “little planet”\nrenderings.13\n12 Recently, some techniques have been developed to straighten curved lines in cylindrical and spherical panora-\nmas (Carroll, Agrawala, and Agarwala 2009; Kopf, Lischinski, Deussen et al. 2009).\n13 These are inspired by The Little Prince by Antoine De Saint-Exupery. Go to http://www.ﬂickr.com and search\n452\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nView selection.\nOnce we have chosen the output parameterization, we still need to deter-\nmine which part of the scene will be centered in the ﬁnal view. As mentioned above, for a ﬂat\ncomposite, we can choose one of the images as a reference. Often, a reasonable choice is the\none that is geometrically most central. For example, for rotational panoramas represented as\na collection of 3D rotation matrices, we can choose the image whose z-axis is closest to the\naverage z-axis (assuming a reasonable ﬁeld of view). Alternatively, we can use the average\nz-axis (or quaternion, but this is trickier) to deﬁne the reference rotation matrix.\nFor larger, e.g., cylindrical or spherical, panoramas, we can use the same heuristic if a\nsubset of the viewing sphere has been imaged. In the case of full 360◦panoramas, a better\nchoice might be to choose the middle image from the sequence of inputs, or sometimes the\nﬁrst image, assuming this contains the object of greatest interest. In all of these cases, having\nthe user control the ﬁnal view is often highly desirable. If the “up vector” computation de-\nscribed in Section 9.2.1 is working correctly, this can be as simple as panning over the image\nor setting a vertical “center line” for the ﬁnal panorama.\nCoordinate transformations.\nAfter selecting the parameterization and reference view, we\nstill need to compute the mappings between the input and output pixels coordinates.\nIf the ﬁnal compositing surface is ﬂat (e.g., a single plane or the face of a cube map)\nand the input images have no radial distortion, the coordinate transformation is the simple\nhomography described by (9.5). This kind of warping can be performed in graphics hardware\nby appropriately setting texture mapping coordinates and rendering a single quadrilateral.\nIf the ﬁnal composite surface has some other analytic form (e.g., cylindrical or spherical),\nwe need to convert every pixel in the ﬁnal panorama into a viewing ray (3D point) and then\nmap it back into each image according to the projection (and optionally radial distortion)\nequations. This process can be made more efﬁcient by precomputing some lookup tables,\ne.g., the partial trigonometric functions needed to map cylindrical or spherical coordinates to\n3D coordinates or the radial distortion ﬁeld at each pixel. It is also possible to accelerate this\nprocess by computing exact pixel mappings on a coarser grid and then interpolating these\nvalues.\nWhen the ﬁnal compositing surface is a texture-mapped polyhedron, a slightly more so-\nphisticated algorithm must be used. Not only do the 3D and texture map coordinates have to\nbe properly handled, but a small amount of overdraw outside the triangle footprints in the tex-\nture map is necessary, to ensure that the texture pixels being interpolated during 3D rendering\nhave valid values (Szeliski and Shum 1997).\nSampling issues.\nWhile the above computations can yield the correct (fractional) pixel\naddresses in each input image, we still need to pay attention to sampling issues. For example,\nfor “little planet projection”.\n9.3 Compositing\n453\nif the ﬁnal panorama has a lower resolution than the input images, pre-ﬁltering the input\nimages is necessary to avoid aliasing. These issues have been extensively studied in both the\nimage processing and computer graphics communities. The basic problem is to compute the\nappropriate pre-ﬁlter, which depends on the distance (and arrangement) between neighboring\nsamples in a source image. As discussed in Sections 3.5.2 and 3.6.1, various approximate\nsolutions, such as MIP mapping (Williams 1983) or elliptically weighted Gaussian averaging\n(Greene and Heckbert 1986) have been developed in the graphics community. For highest\nvisual quality, a higher order (e.g., cubic) interpolator combined with a spatially adaptive pre-\nﬁlter may be necessary (Wang, Kang, Szeliski et al. 2001). Under certain conditions, it may\nalso be possible to produce images with a higher resolution than the input images using the\nprocess of super-resolution (Section 10.3).\n9.3.2 Pixel selection and weighting (de-ghosting)\nOnce the source pixels have been mapped onto the ﬁnal composite surface, we must still\ndecide how to blend them in order to create an attractive-looking panorama. If all of the\nimages are in perfect registration and identically exposed, this is an easy problem, i.e., any\npixel or combination will do. However, for real images, visible seams (due to exposure\ndifferences), blurring (due to mis-registration), or ghosting (due to moving objects) can occur.\nCreating clean, pleasing-looking panoramas involves both deciding which pixels to use\nand how to weight or blend them. The distinction between these two stages is a little ﬂuid,\nsince per-pixel weighting can be thought of as a combination of selection and blending. In\nthis section, we discuss spatially varying weighting, pixel selection (seam placement), and\nthen more sophisticated blending.\nFeathering and center-weighting.\nThe simplest way to create a ﬁnal composite is to sim-\nply take an average value at each pixel,\nC(x) =\nX\nk\nwk(x)˜Ik(x)\n,X\nk\nwk(x) ,\n(9.37)\nwhere ˜Ik(x) are the warped (re-sampled) images and wk(x) is 1 at valid pixels and 0 else-\nwhere. On computer graphics hardware, this kind of summation can be performed in an\naccumulation buffer (using the A channel as the weight).\nSimple averaging usually does not work very well, since exposure differences, mis-\nregistrations, and scene movement are all very visible (Figure 9.14a). If rapidly moving\nobjects are the only problem, taking a median ﬁlter (which is a kind of pixel selection opera-\ntor) can often be used to remove them (Figure 9.14b) (Irani and Anandan 1998). Conversely,\ncenter-weighting (discussed below) and minimum likelihood selection (Agarwala, Dontcheva,",
  "image_path": "page_474.jpg",
  "pages": [
    473,
    474,
    475
  ]
}