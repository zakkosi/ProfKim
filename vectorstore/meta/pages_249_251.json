{
  "doc_id": "pages_249_251",
  "text": "4.1 Points and patches\n227\nFigure 4.21 Recognizing objects in a cluttered scene (Lowe 2004) c⃝2004 Springer. Two of\nthe training images in the database are shown on the left. These are matched to the cluttered\nscene in the middle using SIFT features, shown as small squares in the right image. The afﬁne\nwarp of each recognized database image onto the scene is shown as a larger parallelogram in\nthe right image.\n1\n1\n2\n1\n3\n4\nFigure 4.22\nFalse positives and negatives: The black digits 1 and 2 are features being\nmatched against a database of features in other images. At the current threshold setting (the\nsolid circles), the green 1 is a true positive (good match), the blue 1 is a false negative (failure\nto match), and the red 3 is a false positive (incorrect match). If we set the threshold higher\n(the dashed circles), the blue 1 becomes a true positive but the brown 4 becomes an additional\nfalse positive.\n228\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPredicted matches\nTP = 18\nFP = 4\nP' = 22\nPPV = 0.82\nPredicted non-matches\nFN = 2\nTN = 76\nN' = 78\nP = 20\nN = 80\nTotal = 100\nTPR = 0.90\nFPR = 0.05\nACC = 0.94\nTrue matches\nTrue non-matches\nTable 4.1 The number of matches correctly and incorrectly estimated by a feature matching\nalgorithm, showing the number of true positives (TP), false positives (FP), false negatives\n(FN) and true negatives (TN). The columns sum up to the actual number of positives (P) and\nnegatives (N), while the rows sum up to the predicted number of positives (P’) and negatives\n(N’). The formulas for the true positive rate (TPR), the false positive rate (FPR), the positive\npredictive value (PPV), and the accuracy (ACC) are given in the text.\nﬁrst counting the number of true and false matches and match failures, using the following\ndeﬁnitions (Fawcett 2006):\n• TP: true positives, i.e., number of correct matches;\n• FN: false negatives, matches that were not correctly detected;\n• FP: false positives, proposed matches that are incorrect;\n• TN: true negatives, non-matches that were correctly rejected.\nTable 4.1 shows a sample confusion matrix (contingency table) containing such numbers.\nWe can convert these numbers into unit rates by deﬁning the following quantities (Fawcett\n2006):\n• true positive rate (TPR),\nTPR =\nTP\nTP+FN = TP\nP ;\n(4.14)\n• false positive rate (FPR),\nFPR =\nFP\nFP+TN = FP\nN ;\n(4.15)\n• positive predictive value (PPV),\nPPV =\nTP\nTP+FP = TP\nP’ ;\n(4.16)\n• accuracy (ACC),\nACC = TP+TN\nP+N .\n(4.17)\n4.1 Points and patches\n229\nfalse positive rate\ntrue positive rate\n0.1\n0.8\n0\n1\n1\nequal error\n \nrate\nrandom chance\nTP\nFP FN\nTN\nθ\nd\n#\n(a)\n(b)\nFigure 4.23 ROC curve and its related rates: (a) The ROC curve plots the true positive rate\nagainst the false positive rate for a particular combination of feature extraction and match-\ning algorithms. Ideally, the true positive rate should be close to 1, while the false positive\nrate is close to 0. The area under the ROC curve (AUC) is often used as a single (scalar)\nmeasure of algorithm performance. Alternatively, the equal error rate is sometimes used. (b)\nThe distribution of positives (matches) and negatives (non-matches) as a function of inter-\nfeature distance d. As the threshold θ is increased, the number of true positives (TP) and false\npositives (FP) increases.\nIn the information retrieval (or document retrieval) literature (Baeza-Yates and Ribeiro-\nNeto 1999; Manning, Raghavan, and Sch¨utze 2008), the term precision (how many returned\ndocuments are relevant) is used instead of PPV and recall (what fraction of relevant docu-\nments was found) is used instead of TPR.\nAny particular matching strategy (at a particular threshold or parameter setting) can be\nrated by the TPR and FPR numbers; ideally, the true positive rate will be close to 1 and the\nfalse positive rate close to 0. As we vary the matching threshold, we obtain a family of such\npoints, which are collectively known as the receiver operating characteristic (ROC curve)\n(Fawcett 2006) (Figure 4.23a). The closer this curve lies to the upper left corner, i.e., the\nlarger the area under the curve (AUC), the better its performance. Figure 4.23b shows how\nwe can plot the number of matches and non-matches as a function of inter-feature distance d.\nThese curves can then be used to plot an ROC curve (Exercise 4.3). The ROC curve can also\nbe used to calculate the mean average precision, which is the average precision (PPV) as you\nvary the threshold to select the best results, then the two top results, etc.\nThe problem with using a ﬁxed threshold is that it is difﬁcult to set; the useful range",
  "image_path": "page_250.jpg",
  "pages": [
    249,
    250,
    251
  ]
}