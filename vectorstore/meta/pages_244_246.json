{
  "doc_id": "pages_244_246",
  "text": "222\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.17\nMOPS descriptors are formed using an 8 × 8 sampling of bias and gain nor-\nmalized intensity values, with a sample spacing of ﬁve pixels relative to the detection scale\n(Brown, Szeliski, and Winder 2005) c⃝2005 IEEE. This low frequency sampling gives the\nfeatures some robustness to interest point location error and is achieved by sampling at a\nhigher pyramid level than the detection scale.\n4.1.2 Feature descriptors\nAfter detecting features (keypoints), we must match them, i.e., we must determine which\nfeatures come from corresponding locations in different images. In some situations, e.g., for\nvideo sequences (Shi and Tomasi 1994) or for stereo pairs that have been rectiﬁed (Zhang,\nDeriche, Faugeras et al. 1995; Loop and Zhang 1999; Scharstein and Szeliski 2002), the lo-\ncal motion around each feature point may be mostly translational. In this case, simple error\nmetrics, such as the sum of squared differences or normalized cross-correlation, described\nin Section 8.1 can be used to directly compare the intensities in small patches around each\nfeature point. (The comparative study by Mikolajczyk and Schmid (2005), discussed below,\nuses cross-correlation.) Because feature points may not be exactly located, a more accurate\nmatching score can be computed by performing incremental motion reﬁnement as described\nin Section 8.1.3 but this can be time consuming and can sometimes even decrease perfor-\nmance (Brown, Szeliski, and Winder 2005).\nIn most cases, however, the local appearance of features will change in orientation and\nscale, and sometimes even undergo afﬁne deformations. Extracting a local scale, orientation,\nor afﬁne frame estimate and then using this to resample the patch before forming the feature\ndescriptor is thus usually preferable (Figure 4.17).\nEven after compensating for these changes, the local appearance of image patches will\nusually still vary from image to image. How can we make image descriptors more invariant to\nsuch changes, while still preserving discriminability between different (non-corresponding)\npatches (Figure 4.16)? Mikolajczyk and Schmid (2005) review some recently developed\nview-invariant local image descriptors and experimentally compare their performance. Be-\nlow, we describe a few of these descriptors in more detail.\n4.1 Points and patches\n223\nBias and gain normalization (MOPS).\nFor tasks that do not exhibit large amounts of fore-\nshortening, such as image stitching, simple normalized intensity patches perform reasonably\nwell and are simple to implement (Brown, Szeliski, and Winder 2005) (Figure 4.17). In or-\nder to compensate for slight inaccuracies in the feature point detector (location, orientation,\nand scale), these multi-scale oriented patches (MOPS) are sampled at a spacing of ﬁve pixels\nrelative to the detection scale, using a coarser level of the image pyramid to avoid aliasing.\nTo compensate for afﬁne photometric variations (linear exposure changes or bias and gain,\n(3.3)), patch intensities are re-scaled so that their mean is zero and their variance is one.\nScale invariant feature transform (SIFT).\nSIFT features are formed by computing the\ngradient at each pixel in a 16×16 window around the detected keypoint, using the appropriate\nlevel of the Gaussian pyramid at which the keypoint was detected. The gradient magnitudes\nare downweighted by a Gaussian fall-off function (shown as a blue circle in (Figure 4.18a) in\norder to reduce the inﬂuence of gradients far from the center, as these are more affected by\nsmall misregistrations.\nIn each 4 × 4 quadrant, a gradient orientation histogram is formed by (conceptually)\nadding the weighted gradient value to one of eight orientation histogram bins. To reduce the\neffects of location and dominant orientation misestimation, each of the original 256 weighted\ngradient magnitudes is softly added to 2 × 2 × 2 histogram bins using trilinear interpolation.\nSoftly distributing values to adjacent histogram bins is generally a good idea in any appli-\ncation where histograms are being computed, e.g., for Hough transforms (Section 4.3.2) or\nlocal histogram equalization (Section 3.1.4).\nThe resulting 128 non-negative values form a raw version of the SIFT descriptor vector.\nTo reduce the effects of contrast or gain (additive variations are already removed by the gra-\ndient), the 128-D vector is normalized to unit length. To further make the descriptor robust to\nother photometric variations, values are clipped to 0.2 and the resulting vector is once again\nrenormalized to unit length.\nPCA-SIFT.\nKe and Sukthankar (2004) propose a simpler way to compute descriptors in-\nspired by SIFT; it computes the x and y (gradient) derivatives over a 39 × 39 patch and\nthen reduces the resulting 3042-dimensional vector to 36 using principal component analysis\n(PCA) (Section 14.2.1 and Appendix A.1.2). Another popular variant of SIFT is SURF (Bay,\nTuytelaars, and Van Gool 2006), which uses box ﬁlters to approximate the derivatives and\nintegrals used in SIFT.\nGradient location-orientation histogram (GLOH).\nThis descriptor, developed by Miko-\nlajczyk and Schmid (2005), is a variant on SIFT that uses a log-polar binning structure instead\nof the four quadrants used by Lowe (2004) (Figure 4.19). The spatial bins are of radius 6,\n224\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.18 A schematic representation of Lowe’s (2004) scale invariant feature transform\n(SIFT): (a) Gradient orientations and magnitudes are computed at each pixel and weighted\nby a Gaussian fall-off function (blue circle). (b) A weighted gradient orientation histogram\nis then computed in each subregion, using trilinear interpolation. While this ﬁgure shows an\n8 × 8 pixel patch and a 2 × 2 descriptor array, Lowe’s actual implementation uses 16 × 16\npatches and a 4 × 4 array of eight-bin histograms.\n11, and 15, with eight angular bins (except for the central region), for a total of 17 spa-\ntial bins and 16 orientation bins. The 272-dimensional histogram is then projected onto\na 128-dimensional descriptor using PCA trained on a large database. In their evaluation,\nMikolajczyk and Schmid (2005) found that GLOH, which has the best performance overall,\noutperforms SIFT by a small margin.\nSteerable ﬁlters.\nSteerable ﬁlters (Section 3.2.3) are combinations of derivative of Gaus-\nsian ﬁlters that permit the rapid computation of even and odd (symmetric and anti-symmetric)\nedge-like and corner-like features at all possible orientations (Freeman and Adelson 1991).\nBecause they use reasonably broad Gaussians, they too are somewhat insensitive to localiza-\ntion and orientation errors.\nPerformance of local descriptors.\nAmong the local descriptors that Mikolajczyk and Schmid\n(2005) compared, they found that GLOH performed best, followed closely by SIFT (see Fig-\nure 4.25). They also present results for many other descriptors not covered in this book.\nThe ﬁeld of feature descriptors continues to evolve rapidly, with some of the newer tech-\nniques looking at local color information (van de Weijer and Schmid 2006; Abdel-Hakim\nand Farag 2006). Winder and Brown (2007) develop a multi-stage framework for feature\ndescriptor computation that subsumes both SIFT and GLOH (Figure 4.20a) and also allows\nthem to learn optimal parameters for newer descriptors that outperform previous hand-tuned",
  "image_path": "page_245.jpg",
  "pages": [
    244,
    245,
    246
  ]
}