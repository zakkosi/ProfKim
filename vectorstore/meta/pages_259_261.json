{
  "doc_id": "pages_259_261",
  "text": "4.1 Points and patches\n237\nFigure 4.29 Real-time head tracking using the fast trained classiﬁers of Lepetit, Pilet, and\nFua (2004) c⃝2004 IEEE.\n4.1.5 Application: Performance-driven animation\nOne of the most compelling applications of fast feature tracking is performance-driven an-\nimation, i.e., the interactive deformation of a 3D graphics model based on tracking a user’s\nmotions (Williams 1990; Litwinowicz and Williams 1994; Lepetit, Pilet, and Fua 2004).\nBuck, Finkelstein, Jacobs et al. (2000) present a system that tracks a user’s facial expres-\nsions and head motions and then uses them to morph among a series of hand-drawn sketches.\nAn animator ﬁrst extracts the eye and mouth regions of each sketch and draws control lines\nover each image (Figure 4.30a). At run time, a face-tracking system (Toyama 1998) deter-\nmines the current location of these features (Figure 4.30b). The animation system decides\nwhich input images to morph based on nearest neighbor feature appearance matching and\ntriangular barycentric interpolation. It also computes the global location and orientation of\nthe head from the tracked features. The resulting morphed eye and mouth regions are then\ncomposited back into the overall head model to yield a frame of hand-drawn animation (Fig-\nure 4.30d).\nIn more recent work, Barnes, Jacobs, Sanders et al. (2008) watch users animate paper\ncutouts on a desk and then turn the resulting motions and drawings into seamless 2D anima-\ntions.\n238\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 4.30\nPerformance-driven, hand-drawn animation (Buck, Finkelstein, Jacobs et al.\n2000) c⃝2000 ACM: (a) eye and mouth portions of hand-drawn sketch with their overlaid\ncontrol lines; (b) an input video frame with the tracked features overlaid; (c) a different input\nvideo frame along with its (d) corresponding hand-drawn animation.\n4.2 Edges\nWhile interest points are useful for ﬁnding image locations that can be accurately matched\nin 2D, edge points are far more plentiful and often carry important semantic associations.\nFor example, the boundaries of objects, which also correspond to occlusion events in 3D, are\nusually delineated by visible contours. Other kinds of edges correspond to shadow boundaries\nor crease edges, where surface orientation changes rapidly. Isolated edge points can also be\ngrouped into longer curves or contours, as well as straight line segments (Section 4.3). It\nis interesting that even young children have no difﬁculty in recognizing familiar objects or\nanimals from such simple line drawings.\n4.2.1 Edge detection\nGiven an image, how can we ﬁnd the salient edges? Consider the color images in Figure 4.31.\nIf someone asked you to point out the most “salient” or “strongest” edges or the object bound-\naries (Martin, Fowlkes, and Malik 2004; Arbel´aez, Maire, Fowlkes et al. 2010), which ones\nwould you trace? How closely do your perceptions match the edge images shown in Fig-\nure 4.31?\nQualitatively, edges occur at boundaries between regions of different color, intensity, or\ntexture. Unfortunately, segmenting an image into coherent regions is a difﬁcult task, which\nwe address in Chapter 5. Often, it is preferable to detect edges using only purely local infor-\nmation.\nUnder such conditions, a reasonable approach is to deﬁne an edge as a location of rapid\n4.2 Edges\n239\nFigure 4.31 Human boundary detection (Martin, Fowlkes, and Malik 2004) c⃝2004 IEEE.\nThe darkness of the edges corresponds to how many human subjects marked an object bound-\nary at that location.\nintensity variation.3 Think of an image as a height ﬁeld. On such a surface, edges occur\nat locations of steep slopes, or equivalently, in regions of closely packed contour lines (on a\ntopographic map).\nA mathematical way to deﬁne the slope and direction of a surface is through its gradient,\nJ(x) = ∇I(x) = (∂I\n∂x, ∂I\n∂y )(x).\n(4.19)\nThe local gradient vector J points in the direction of steepest ascent in the intensity function.\nIts magnitude is an indication of the slope or strength of the variation, while its orientation\npoints in a direction perpendicular to the local contour.\nUnfortunately, taking image derivatives accentuates high frequencies and hence ampliﬁes\nnoise, since the proportion of noise to signal is larger at high frequencies. It is therefore\nprudent to smooth the image with a low-pass ﬁlter prior to computing the gradient. Because\nwe would like the response of our edge detector to be independent of orientation, a circularly\nsymmetric smoothing ﬁlter is desirable. As we saw in Section 3.2, the Gaussian is the only\nseparable circularly symmetric ﬁlter and so it is used in most edge detection algorithms.\nCanny (1986) discusses alternative ﬁlters and a number of researcher review alternative edge\ndetection algorithms and compare their performance (Davis 1975; Nalwa and Binford 1986;\nNalwa 1987; Deriche 1987; Freeman and Adelson 1991; Nalwa 1993; Heath, Sarkar, Sanocki\net al. 1998; Crane 1997; Ritter and Wilson 2000; Bowyer, Kranenburg, and Dougherty 2001;\nArbel´aez, Maire, Fowlkes et al. 2010).\nBecause differentiation is a linear operation, it commutes with other linear ﬁltering oper-\n3 We defer the topic of edge detection in color images.",
  "image_path": "page_260.jpg",
  "pages": [
    259,
    260,
    261
  ]
}