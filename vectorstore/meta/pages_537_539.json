{
  "doc_id": "pages_537_539",
  "text": "10.4 Image matting and compositing\n515\nFigure 10.44 Comparative matting results for a medium accuracy trimap. Wang and Cohen\n(2007a) describe the individual techniques being compared.\nauthors call their technique a closed-form solution to natural image matting. Once α has\nbeen computed, the foreground and background colors are estimated using a least squares\nminimization of the compositing equation (10.30) regularized with a spatially varying ﬁrst-\norder smoothness,\nEB,F =\nX\ni\n∥Ci −[α + Fi + (1 −αi)Bi]∥2 + λ|∇αi|(∥∇Fi∥2 + ∥∇Bi∥2),\n(10.41)\nwhere the |∇αi| weight is applied separately for the x and y components of the F and B\nderivatives (Levin, Lischinski, and Weiss 2008).\nLaplacian (closed-form) matting is just one of many optimization-based techniques sur-\nveyed and compared by Wang and Cohen (2007a). Some of these techniques use alternative\nformulations for the afﬁnities or smoothness terms on the α matte, alternative estimation\ntechniques such as belief propagation, or alternative representations (e.g., local histograms)\nfor modeling local foreground and background color distributions (Wang and Cohen 2005,\n2007b,c). Some of these techniques also provide real-time results as the user draws a contour\nline or sparse set of scribbles (Wang, Agrawala, and Cohen 2007; Rhemann, Rother, Rav-\nAcha et al. 2008) or even pre-segment the image into a small number of mattes that the user\ncan select with simple clicks (Levin, Acha, and Lischinski 2008).\nFigure 10.44 shows the results of running a number of the surveyed algorithms on a region\nof toy animal fur where a trimap has been speciﬁed, while Figure 10.45 shows results for\ntechniques that can produce mattes with only a few scribbles as input. Figure 10.46 shows\na result for an even more recent algorithm (Rhemann, Rother, Rav-Acha et al. 2008) that\nclaims to outperform all of the techniques surveyed by Wang and Cohen (2007a).\nPasting.\nOnce a matte has been pulled from an image, it is usually composited directly\nover the new background, unless the seams between the cutout and background regions are\n516\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.45\nComparative matting results with scribble-based inputs. Wang and Cohen\n(2007a) describe the individual techniques being compared.\nFigure 10.46\nStroke-based segmentation result (Rhemann, Rother, Rav-Acha et al. 2008)\nc⃝2008 IEEE.\nto be hidden, in which case Poisson blending (P´erez, Gangnet, and Blake 2003) can be used\n(Section 9.3.4).\nIn the latter case, it is helpful if the matte boundary passes through regions that either\nhave little texture or look similar in the old and new images. Papers by Jia, Sun, Tang et al.\n(2006) and Wang and Cohen (2007c) explain how to do this.\n10.4.4 Smoke, shadow, and ﬂash matting\nIn addition to matting out solid objects with fractional boundaries, it is also possible to matte\nout translucent media such as smoke (Chuang, Agarwala, Curless et al. 2002). Starting with\na video sequence, each pixel is modeled as a linear combination of its (unknown) background\ncolor and a constant foreground (smoke) color that is common to all pixels. Voting in color\n10.4 Image matting and compositing\n517\n(a)\n(b)\n(c)\n(d)\nFigure 10.47\nSmoke matting (Chuang, Agarwala, Curless et al. 2002) c⃝2002 ACM: (a)\ninput video frame; (b) after removing the foreground object; (c) estimated alpha matte; (d)\ninsertion of new objects into the background.\nFigure 10.48 Shadow matting (Chuang, Goldman, Curless et al. 2003) c⃝2003 ACM. In-\nstead of simply darkening the new scene with the shadow (c), shadow matting correctly dims\nthe lit scene with the new shadow and drapes the shadow over 3D geometry (d).\nspace is used to estimate this foreground color and the distance along each color line is used\nto estimate the per-pixel temporally varying alpha (Figure 10.47).\nExtracting and re-inserting shadows is also possible using a related technique (Chuang,\nGoldman, Curless et al. 2003). Here, instead of assuming a constant foreground color, each\npixel is assumed to vary between its fully lit and fully shadowed colors, which can be esti-\nmated by taking (robust) minimum and maximum values over time as a shadow passes over\nthe scene (Exercise 10.9). The resulting fractional shadow matte can be used to re-project\nthe shadow into a new scene. If the destination scene has a non-planar geometry, it can be\nscanned by waving a straight stick shadow across the scene. The new shadow matte can then\nbe warped with the computed deformation ﬁeld to have it drape correctly over the new scene\n(Figure 10.48).\nThe quality and reliability of matting algorithms can also be enhanced using more sophis-\nticated acquisition systems. For example, taking a ﬂash and non-ﬂash image pair supports\nthe reliable extraction of foreground mattes, which show up as regions of large illumination\nchange between the two images (Sun, Li, Kang et al. 2006). Taking simultaneous video\nstreams focused at different distances (McGuire, Matusik, Pﬁster et al. 2005) or using multi-\ncamera arrays (Joshi, Matusik, and Avidan 2006) are also good approaches to producing",
  "image_path": "page_538.jpg",
  "pages": [
    537,
    538,
    539
  ]
}