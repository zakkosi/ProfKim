{
  "doc_id": "pages_484_486",
  "text": "462\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nExposure compensation.\nPyramid and gradient domain blending can do a good job of\ncompensating for moderate amounts of exposure differences between images.\nHowever,\nwhen the exposure differences become large, alternative approaches may be necessary.\nUyttendaele, Eden, and Szeliski (2001) iteratively estimate a local correction between\neach source image and a blended composite. First, a block-based quadratic transfer function is\nﬁt between each source image and an initial feathered composite. Next, transfer functions are\naveraged with their neighbors to get a smoother mapping and per-pixel transfer functions are\ncomputed by splining (interpolating) between neighboring block values. Once each source\nimage has been smoothly adjusted, a new feathered composite is computed and the process is\nrepeated (typically three times). The results shown by Uyttendaele, Eden, and Szeliski (2001)\ndemonstrate that this does a better job of exposure compensation than simple feathering and\ncan handle local variations in exposure due to effects such as lens vignetting.\nUltimately, however, the most principled way to deal with exposure differences is to stitch\nimages in the radiance domain, i.e., to convert each image into a radiance image using its\nexposure value and then create a stitched, high dynamic range image, as discussed in Sec-\ntion 10.2 (Eden, Uyttendaele, and Szeliski 2006).\n9.4 Additional reading\nThe literature on image stitching dates back to work in the photogrammetry community in\nthe 1970s (Milgram 1975, 1977; Slama 1980). In computer vision, papers started appearing\nin the early 1980s (Peleg 1981), while the development of fully automated techniques came\nabout a decade later (Mann and Picard 1994; Chen 1995; Szeliski 1996; Szeliski and Shum\n1997; Sawhney and Kumar 1999; Shum and Szeliski 2000). Those techniques used direct\npixel-based alignment but feature-based approaches are now the norm (Zoghlami, Faugeras,\nand Deriche 1997; Capel and Zisserman 1998; Cham and Cipolla 1998; Badra, Qumsieh, and\nDudek 1998; McLauchlan and Jaenicke 2002; Brown and Lowe 2007). A collection of some\nof these papers can be found in the book by Benosman and Kang (2001). Szeliski (2006a)\nprovides a comprehensive survey of image stitching, on which the material in this chapter is\nbased.\nHigh-quality techniques for optimal seam ﬁnding and blending are another important\ncomponent of image stitching systems. Important developments in this ﬁeld include work by\nMilgram (1977), Burt and Adelson (1983b), Davis (1998), Uyttendaele, Eden, and Szeliski\n(2001),P´erez, Gangnet, and Blake (2003), Levin, Zomet, Peleg et al. (2004), Agarwala,\nDontcheva, Agrawala et al. (2004), Eden, Uyttendaele, and Szeliski (2006), and Kopf, Uyt-\ntendaele, Deussen et al. (2007).\nIn addition to the merging of multiple overlapping photographs taken for aerial or ter-\n9.5 Exercises\n463\nrestrial panoramic image creation, stitching techniques can be used for automated white-\nboard scanning (He and Zhang 2005; Zhang and He 2007), scanning with a mouse (Nakao,\nKashitani, and Kaneyoshi 1998), and retinal image mosaics (Can, Stewart, Roysam et al.\n2002). They can also be applied to video sequences (Teodosio and Bender 1993; Irani, Hsu,\nand Anandan 1995; Kumar, Anandan, Irani et al. 1995; Sawhney and Ayer 1996; Massey\nand Bender 1996; Irani and Anandan 1998; Sawhney, Arpa, Kumar et al. 2002; Agarwala,\nZheng, Pal et al. 2005; Rav-Acha, Pritch, Lischinski et al. 2005; Steedly, Pal, and Szeliski\n2005; Baudisch, Tan, Steedly et al. 2006) and can even be used for video compression (Lee,\nge Chen, lung Bruce Lin et al. 1997).\n9.5 Exercises\nEx 9.1: Direct pixel-based alignment\nTake a pair of images, compute a coarse-to-ﬁne afﬁne\nalignment (Exercise 8.2) and then blend them using either averaging (Exercise 6.2) or a Lapla-\ncian pyramid (Exercise 3.20). Extend your motion model from afﬁne to perspective (homog-\nraphy) to better deal with rotational mosaics and planar surfaces seen under arbitrary motion.\nEx 9.2: Featured-based stitching\nExtend your feature-based alignment technique from Ex-\nercise 6.2 to use a full perspective model and then blend the resulting mosaic using either\naveraging or more sophisticated distance-based feathering (Exercise 9.9).\nEx 9.3: Cylindrical strip panoramas\nTo generate cylindrical or spherical panoramas from\na horizontally panning (rotating) camera, it is best to use a tripod. Set your camera up to take\na series of 50% overlapped photos and then use the following steps to create your panorama:\n1. Estimate the amount of radial distortion by taking some pictures with lots of long\nstraight lines near the edges of the image and then using the plumb-line method from\nExercise 6.10.\n2. Compute the focal length either by using a ruler and paper, as in Figure 6.7 (Debevec,\nWenger, Tchou et al. 2002) or by rotating your camera on the tripod, overlapping the\nimages by exactly 0% and counting the number of images it takes to make a 360◦\npanorama.\n3. Convert each of your images to cylindrical coordinates using (9.12–9.16).\n4. Line up the images with a translational motion model using either a direct pixel-based\ntechnique, such as coarse-to-ﬁne incremental or an FFT, or a feature-based technique.\n5. (Optional) If doing a complete 360◦panorama, align the ﬁrst and last images. Compute\nthe amount of accumulated vertical mis-registration and re-distribute this among the\nimages.\n464\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n6. Blend the resulting images using feathering or some other technique.\nEx 9.4: Coarse alignment\nUse FFT or phase correlation (Section 8.1.2) to estimate the\ninitial alignment between successive images. How well does this work? Over what range of\noverlaps? If it does not work, does aligning sub-sections (e.g., quarters) do better?\nEx 9.5: Automated mosaicing\nUse feature-based alignment with four-point RANSAC for\nhomographies (Section 6.1.3, Equations (6.19–6.23)) or three-point RANSAC for rotational\nmotions (Brown, Hartley, and Nist´er 2007) to match up all pairs of overlapping images.\nMerge these pairwise estimates together by ﬁnding a spanning tree of pairwise relations.\nVisualize the resulting global alignment, e.g., by displaying a blend of each image with all\nother images that overlap it.\nFor greater robustness, try multiple spanning trees (perhaps randomly sampled based on\nthe conﬁdence in pairwise alignments) to see if you can recover from bad pairwise matches\n(Zach, Klopschitz, and Pollefeys 2010). As a measure of ﬁtness, count how many pairwise\nestimates are consistent with the global alignment.\nEx 9.6: Global optimization\nUse the initialization from the previous algorithm to perform\na full bundle adjustment over all of the camera rotations and focal lengths, as described in\nSection 7.4 and by Shum and Szeliski (2000). Optionally, estimate radial distortion parame-\nters as well or support ﬁsheye lenses (Section 2.1.6).\nAs in the previous exercise, visualize the quality of your registration by creating compos-\nites of each input image with its neighbors, optionally blinking between the original image\nand the composite to better see mis-alignment artifacts.\nEx 9.7: De-ghosting\nUse the results of the previous bundle adjustment to predict the loca-\ntion of each feature in a consensus geometry. Use the difference between the predicted and\nactual feature locations to correct for small mis-registrations, as described in Section 9.2.2\n(Shum and Szeliski 2000).\nEx 9.8: Compositing surface\nChoose a compositing surface (Section 9.3.1), e.g., a single\nreference image extended to a larger plane, a sphere represented using cylindrical or spherical\ncoordinates, a stereographic “little planet” projection, or a cube map.\nProject all of your images onto this surface and blend them with equal weighting, for now\n(just to see where the original image seams are).\nEx 9.9: Feathering and blending\nCompute a feather (distance) map for each warped source\nimage and use these maps to blend the warped images.\nAlternatively, use Laplacian pyramid blending (Exercise 3.20) or gradient domain blend-\ning.",
  "image_path": "page_485.jpg",
  "pages": [
    484,
    485,
    486
  ]
}