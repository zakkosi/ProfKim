{
  "doc_id": "pages_643_645",
  "text": "13.1 View interpolation\n621\nOver the last two decades, image-based rendering has emerged as one of the most exciting\napplications of computer vision (Kang, Li, Tong et al. 2006; Shum, Chan, and Kang 2007).\nIn image-based rendering, 3D reconstruction techniques from computer vision are combined\nwith computer graphics rendering techniques that use multiple views of a scene to create inter-\nactive photo-realistic experiences, such as the Photo Tourism system shown in Figure 13.1a.\nCommercial versions of such systems include immersive street-level navigation in on-line\nmapping systems1 and the creation of 3D Photosynths2 from large collections of casually\nacquired photographs.\nIn this chapter, we explore a variety of image-based rendering techniques, such as those\nillustrated in Figure 13.1. We begin with view interpolation (Section 13.1), which creates a\nseamless transition between a pair of reference images using one or more pre-computed depth\nmaps. Closely related to this idea are view-dependent texture maps (Section 13.1.1), which\nblend multiple texture maps on a 3D model’s surface. The representations used for both the\ncolor imagery and the 3D geometry in view interpolation include a number of clever variants\nsuch as layered depth images (Section 13.2) and sprites with depth (Section 13.2.1).\nWe continue our exploration of image-based rendering with the light ﬁeld and Lumigraph\nfour-dimensional representations of a scene’s appearance (Section 13.3), which can be used\nto render the scene from any arbitrary viewpoint. Variants on these representations include\nthe unstructured Lumigraph (Section 13.3.1), surface light ﬁelds (Section 13.3.2), concentric\nmosaics (Section 13.3.3), and environment mattes (Section 13.4).\nThe last part of this chapter explores the topic of video-based rendering, which uses one\nor more videos in order to create novel video-based experiences (Section 13.5). The topics\nwe cover include video-based facial animation (Section 13.5.1), as well as video textures\n(Section 13.5.2), in which short video clips can be seamlessly looped to create dynamic real-\ntime video-based renderings of a scene. We close with a discussion of 3D videos created from\nmultiple video streams (Section 13.5.4), as well as video-based walkthroughs of environments\n(Section 13.5.5), which have found widespread application in immersive outdoor mapping\nand driving direction systems.\n13.1 View interpolation\nWhile the term image-based rendering ﬁrst appeared in the papers by Chen (1995) and\nMcMillan and Bishop (1995), the work on view interpolation by Chen and Williams (1993)\nis considered as the seminal paper in the ﬁeld. In view interpolation, pairs of rendered color\nimages are combined with their pre-computed depth maps to generate interpolated views that\n1 http://maps.bing.com and http://maps.google.com.\n2 http://photosynth.net.\n622\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 13.2\nView interpolation (Chen and Williams 1993) c⃝1993 ACM: (a) holes from\none source image (shown in blue); (b) holes after combining two widely spaced images; (c)\nholes after combining two closely spaced images; (d) after interpolation (hole ﬁlling).\nmimic what a virtual camera would see in between the two reference views.\nView interpolation combines two ideas that were previously used in computer vision and\ncomputer graphics. The ﬁrst is the idea of pairing a recovered depth map with the refer-\nence image used in its computation and then using the resulting texture-mapped 3D model\nto generate novel views (Figure 11.1). The second is the idea of morphing (Section 3.6.3)\n(Figure 3.53), where correspondences between pairs of images are used to warp each refer-\nence image to an in-between location while simultaneously cross-dissolving between the two\nwarped images.\nFigure 13.2 illustrates this process in more detail. First, both source images are warped\nto the novel view, using both the knowledge of the reference and virtual 3D camera pose\nalong with each image’s depth map (2.68–2.70). In the paper by Chen and Williams (1993),\na forward warping algorithm (Algorithm 3.1 and Figure 3.46) is used. The depth maps are\nrepresented as quadtrees for both space and rendering time efﬁciency (Samet 1989).\nDuring the forward warping process, multiple pixels (which occlude one another) may\nland on the same destination pixel. To resolve this conﬂict, either a z-buffer depth value can\nbe associated with each destination pixel or the images can be warped in back-to-front order,\nwhich can be computed based on the knowledge of epipolar geometry (Chen and Williams\n1993; Laveau and Faugeras 1994; McMillan and Bishop 1995).\nOnce the two reference images have been warped to the novel view (Figure 13.2a–b), they\ncan be merged to create a coherent composite (Figure 13.2c). Whenever one of the images\nhas a hole (illustrated as a cyan pixel), the other image is used as the ﬁnal value. When both\nimages have pixels to contribute, these can be blended as in usual morphing, i.e., according\nto the relative distances between the virtual and source cameras. Note that if the two images\nhave very different exposures, which can happen when performing view interpolation on real\nimages, the hole-ﬁlled regions and the blended regions will have different exposures, leading\n13.1 View interpolation\n623\nto subtle artifacts.\nThe ﬁnal step in view interpolation (Figure 13.2d) is to ﬁll any remaining holes or cracks\ndue to the forward warping process or lack of source data (scene visibility). This can be done\nby copying pixels from the further pixels adjacent to the hole. (Otherwise, foreground objects\nare subject to a “fattening effect”.)\nThe above process works well for rigid scenes, although its visual quality (lack of alias-\ning) can be improved using a two-pass, forward–backward algorithm (Section 13.2.1) (Shade,\nGortler, He et al. 1998) or full 3D rendering (Zitnick, Kang, Uyttendaele et al. 2004). In the\ncase where the two reference images are views of a non-rigid scene, e.g., a person smiling\nin one image and frowning in the other, view morphing, which combines ideas from view\ninterpolation with regular morphing, can be used (Seitz and Dyer 1996).\nWhile the original view interpolation paper describes how to generate novel views based\non similar pre-computed (linear perspective) images, the plenoptic modeling paper of McMil-\nlan and Bishop (1995) argues that cylindrical images should be used to store the pre-computed\nrendering or real-world images. (Chen 1995) also propose using environment maps (cylin-\ndrical, cubic, or spherical) as source images for view interpolation.\n13.1.1 View-dependent texture maps\nView-dependent texture maps (Debevec, Taylor, and Malik 1996) are closely related to view\ninterpolation. Instead of associating a separate depth map with each input image, a single 3D\nmodel is created for the scene, but different images are used as texture map sources depending\non the virtual camera’s current position (Figure 13.3a).3\nIn more detail, given a new virtual camera position, the similarity of this camera’s view of\neach polygon (or pixel) is compared to that of potential source images. The images are then\nblended using a weighting that is inversely proportional to the angles αi between the virtual\nview and the source views (Figure 13.3a). Even though the geometric model can be fairly\ncoarse (Figure 13.3b), blending between different views gives a strong sense of more detailed\ngeometry because of the parallax (visual motion) between corresponding pixels. While the\noriginal paper performs the weighted blend computation separately at each pixel or coarsened\npolygon face, follow-on work by Debevec, Yu, and Borshukov (1998) presents a more efﬁ-\ncient implementation based on precomputing contributions for various portions of viewing\nspace and then using projective texture mapping (OpenGL-ARB 1997).\nThe idea of view-dependent texture mapping has been used in a large number of sub-\nsequent image-based rendering systems, including facial modeling and animation (Pighin,\n3 The term image-based modeling, which is now commonly used to describe the creation of texture-mapped 3D\nmodels from multiple images, appears to have ﬁrst been used by Debevec, Taylor, and Malik (1996), who also used\nthe term photogrammetric modeling to describe the same process.",
  "image_path": "page_644.jpg",
  "pages": [
    643,
    644,
    645
  ]
}