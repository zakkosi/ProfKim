{
  "doc_id": "pages_089_091",
  "text": "2.2 Photometric image formation\n67\nThe Phong shading model has been superseded in terms of physical accuracy by a number\nof more recently developed models in computer graphics, including the model developed by\nCook and Torrance (1982) based on the original micro-facet model of Torrance and Sparrow\n(1967). Until recently, most computer graphics hardware implemented the Phong model but\nthe recent advent of programmable pixel shaders makes the use of more complex models\nfeasible.\nDi-chromatic reﬂection model\nThe Torrance and Sparrow (1967) model of reﬂection also forms the basis of Shafer’s (1985)\ndi-chromatic reﬂection model, which states that the apparent color of a uniform material lit\nfrom a single source depends on the sum of two terms,\nLr(ˆvr; λ)\n=\nLi(ˆvr, ˆvi, ˆn; λ) + Lb(ˆvr, ˆvi, ˆn; λ)\n(2.94)\n=\nci(λ)mi(ˆvr, ˆvi, ˆn) + cb(λ)mb(ˆvr, ˆvi, ˆn),\n(2.95)\ni.e., the radiance of the light reﬂected at the interface, Li, and the radiance reﬂected at the sur-\nface body, Lb. Each of these, in turn, is a simple product between a relative power spectrum\nc(λ), which depends only on wavelength, and a magnitude m(ˆvr, ˆvi, ˆn), which depends only\non geometry. (This model can easily be derived from a generalized version of Phong’s model\nby assuming a single light source and no ambient illumination, and re-arranging terms.) The\ndi-chromatic model has been successfully used in computer vision to segment specular col-\nored objects with large variations in shading (Klinker 1993) and more recently has inspired\nlocal two-color models for applications such Bayer pattern demosaicing (Bennett, Uytten-\ndaele, Zitnick et al. 2006).\nGlobal illumination (ray tracing and radiosity)\nThe simple shading model presented thus far assumes that light rays leave the light sources,\nbounce off surfaces visible to the camera, thereby changing in intensity or color, and arrive\nat the camera. In reality, light sources can be shadowed by occluders and rays can bounce\nmultiple times around a scene while making their trip from a light source to the camera.\nTwo methods have traditionally been used to model such effects. If the scene is mostly\nspecular (the classic example being scenes made of glass objects and mirrored or highly pol-\nished balls), the preferred approach is ray tracing or path tracing (Glassner 1995; Akenine-\nM¨oller and Haines 2002; Shirley 2005), which follows individual rays from the camera across\nmultiple bounces towards the light sources (or vice versa). If the scene is composed mostly\nof uniform albedo simple geometry illuminators and surfaces, radiosity (global illumination)\ntechniques are preferred (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995).\n68\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nCombinations of the two techniques have also been developed (Wallace, Cohen, and Green-\nberg 1987), as well as more general light transport techniques for simulating effects such as\nthe caustics cast by rippling water.\nThe basic ray tracing algorithm associates a light ray with each pixel in the camera im-\nage and ﬁnds its intersection with the nearest surface. A primary contribution can then be\ncomputed using the simple shading equations presented previously (e.g., Equation (2.93))\nfor all light sources that are visible for that surface element. (An alternative technique for\ncomputing which surfaces are illuminated by a light source is to compute a shadow map,\nor shadow buffer, i.e., a rendering of the scene from the light source’s perspective, and then\ncompare the depth of pixels being rendered with the map (Williams 1983; Akenine-M¨oller\nand Haines 2002).) Additional secondary rays can then be cast along the specular direction\ntowards other objects in the scene, keeping track of any attenuation or color change that the\nspecular reﬂection induces.\nRadiosity works by associating lightness values with rectangular surface areas in the scene\n(including area light sources). The amount of light interchanged between any two (mutually\nvisible) areas in the scene can be captured as a form factor, which depends on their relative\norientation and surface reﬂectance properties, as well as the 1/r2 fall-off as light is distributed\nover a larger effective sphere the further away it is (Cohen and Wallace 1993; Sillion and\nPuech 1994; Glassner 1995). A large linear system can then be set up to solve for the ﬁnal\nlightness of each area patch, using the light sources as the forcing function (right hand side).\nOnce the system has been solved, the scene can be rendered from any desired point of view.\nUnder certain circumstances, it is possible to recover the global illumination in a scene from\nphotographs using computer vision techniques (Yu, Debevec, Malik et al. 1999).\nThe basic radiosity algorithm does not take into account certain near ﬁeld effects, such\nas the darkening inside corners and scratches, or the limited ambient illumination caused\nby partial shadowing from other surfaces. Such effects have been exploited in a number of\ncomputer vision algorithms (Nayar, Ikeuchi, and Kanade 1991; Langer and Zucker 1994).\nWhile all of these global illumination effects can have a strong effect on the appearance\nof a scene, and hence its 3D interpretation, they are not covered in more detail in this book.\n(But see Section 12.7.1 for a discussion of recovering BRDFs from real scenes and objects.)\n2.2.3 Optics\nOnce the light from a scene reaches the camera, it must still pass through the lens before\nreaching the sensor (analog ﬁlm or digital silicon). For many applications, it sufﬁces to\ntreat the lens as an ideal pinhole that simply projects all rays through a common center of\nprojection (Figures 2.8 and 2.9).\nHowever, if we want to deal with issues such as focus, exposure, vignetting, and aber-\n2.2 Photometric image formation\n69\nzi=102 mm\nf = 100 mm\nW=35mm\nzo=5 m\nf.o.v.\nc\nΔzi\nP\nd\nFigure 2.19\nA thin lens of focal length f focuses the light from a plane a distance zo in front\nof the lens at a distance zi behind the lens, where\n1\nzo + 1\nzi = 1\nf . If the focal plane (vertical\ngray line next to c) is moved forward, the images are no longer in focus and the circle of\nconfusion c (small thick line segments) depends on the distance of the image plane motion\n∆zi relative to the lens aperture diameter d. The ﬁeld of view (f.o.v.) depends on the ratio\nbetween the sensor width W and the focal length f (or, more precisely, the focusing distance\nzi, which is usually quite close to f).\nration, we need to develop a more sophisticated model, which is where the study of optics\ncomes in (M¨oller 1988; Hecht 2001; Ray 2002).\nFigure 2.19 shows a diagram of the most basic lens model, i.e., the thin lens composed\nof a single piece of glass with very low, equal curvature on both sides. According to the\nlens law (which can be derived using simple geometric arguments on light ray refraction), the\nrelationship between the distance to an object zo and the distance behind the lens at which a\nfocused image is formed zi can be expressed as\n1\nzo\n+ 1\nzi\n= 1\nf ,\n(2.96)\nwhere f is called the focal length of the lens. If we let zo →∞, i.e., we adjust the lens (move\nthe image plane) so that objects at inﬁnity are in focus, we get zi = f, which is why we can\nthink of a lens of focal length f as being equivalent (to a ﬁrst approximation) to a pinhole a\ndistance f from the focal plane (Figure 2.10), whose ﬁeld of view is given by (2.60).\nIf the focal plane is moved away from its proper in-focus setting of zi (e.g., by twisting\nthe focus ring on the lens), objects at zo are no longer in focus, as shown by the gray plane in\nFigure 2.19. The amount of mis-focus is measured by the circle of confusion c (shown as short\nthick blue line segments on the gray plane).7 The equation for the circle of confusion can be\nderived using similar triangles; it depends on the distance of travel in the focal plane ∆zi\nrelative to the original focus distance zi and the diameter of the aperture d (see Exercise 2.4).\n7 If the aperture is not completely circular, e.g., if it is caused by a hexagonal diaphragm, it is sometimes possible\nto see this effect in the actual blur function (Levin, Fergus, Durand et al. 2007; Joshi, Szeliski, and Kriegman 2008)\nor in the “glints” that are seen when shooting into the sun.",
  "image_path": "page_090.jpg",
  "pages": [
    89,
    90,
    91
  ]
}