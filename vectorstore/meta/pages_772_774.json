{
  "doc_id": "pages_772_774",
  "text": "750\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ni.e., if the u and v descent directions satisfy uT Cv = 0. In practice, conjugate gradient\ndescent outperforms other kinds of gradient descent algorithm because its convergence rate\nis proportional to the square root of the condition number of C instead of the condition\nnumber itself.9 Shewchuk (1994) provides a nice introduction to this topic, with clear intuitive\nexplanations of the reasoning behind the conjugate gradient algorithm and its performance.\nAlgorithm A.3 describes the conjugate gradient algorithm and its related least squares\ncounterpart, which can be used when the original set of least squares linear equations are\navailable in the form of Ax = b (A.28). While it is easy to convince yourself that the two\nforms are mathematically equivalent, the least squares form is preferable if rounding errors\nstart to affect the results because of poor conditioning. It may also be preferable if, due to\nthe sparsity structure of A, multiplies with the original A matrix are faster or more space\nefﬁcient than multiplies with C.\nThe conjugate gradient algorithm starts by computing the current residual r0 = d−Cx0,\nwhich is the direction of steepest descent of the energy function (A.28). It sets the original\ndescent direction p0 = r0. Next, it multiplies the descent direction by the quadratic form\n(Hessian) matrix C and combines this with the residual to estimate the optimal step size αk.\nThe solution vector xk and the residual vector rk are then updated using this step size. (No-\ntice how the least squares variant of the conjugate gradient algorithm splits the multiplication\nby the C = AT A matrix across steps 4 and 8.) Finally, a new search direction is calculated\nby ﬁrst computing a factor β as the ratio of current to previous residual magnitudes. The\nnew search direction pk+1 is then set to the residual plus β times the old search direction pk,\nwhich keeps the directions conjugate with respect to C.\nIt turns out that conjugate gradient descent can also be directly applied to non-quadratic\nenergy functions, e.g., those arising from non-linear least squares (Appendix A.3). Instead\nof explicitly forming a local quadratic approximation C and then computing residuals rk,\nnon-linear conjugate gradient descent computes the gradient of the energy function E (A.45)\ndirectly inside each iteration and uses it to set the search direction (Nocedal and Wright 2006).\nSince the quadratic approximation to the energy function may not exist or may be inaccurate,\nline search is often used to determine the step size αk. Furthermore, to compensate for errors\nin ﬁnding the true function minimum, alternative formulas for βk+1 such as Polak–Ribi`ere,\nβk+1 = ∇E(xk+1)[∇E(xk+1) −∇E(xk)]\n∥∇E(xk)∥2\n(A.51)\nare often used (Nocedal and Wright 2006).\n9 The condition number κ(C) is the ratio of the largest and smallest eigenvalues of C. The actual convergence\nrate depends on the clustering of the eigenvalues, as discussed in the references cited in this section.\nA.5 Iterative techniques\n751\nConjugateGradient(C, d, x0)\n1. r0 = d −Cx0\n2. p0 = r0\n3. for k = 0 . . .\n4.\nwk = Cpk\n5.\nαk = ∥rk∥2/(pk · wk)\n6.\nxk+1 = xk + αkpk\n7.\nrk+1 = rk −αkwk\n8.\n9.\nβk+1 = ∥rk+1∥2/∥rk∥2\n10.\npk+1 = rk+1 + βkpk\nConjugateGradientLS(A, b, x0)\n1. q0 = b −Ax0, r0 = AT q0\n2. p0 = r0\n3. for k = 0 . . .\n4.\nvk = Apk\n5.\nαk = ∥rk∥2/∥vk∥2\n6.\nxk+1 = xk + αkpk\n7.\nqk+1 = qk −αkvk\n8.\nrk+1 = AT qk+1\n9.\nβk+1 = ∥rk+1∥2/∥rk∥2\n10.\npk+1 = rk+1 + βkpk\nAlgorithm A.3\nConjugate gradient and conjugate gradient least squares algorithms. The\nalgorithm is described in more detail in the text, but in brief, they choose descent directions\npk that are conjugate to each other with respect to C by computing a factor β by which to\ndiscount the previous search direction pk−1. They then ﬁnd the optimal step size α and take\na downhill step by an amount αkpk.\nA.5.2 Preconditioning\nAs we mentioned previously, the rate of convergence of the conjugate gradient algorithm\nis governed in large part by the condition number κ(C). Its effectiveness can therefore be\nincreased dramatically by reducing this number, e.g., by rescaling elements in x, which cor-\nresponds to rescaling rows and columns in C.\nIn general, preconditioning is usually thought of as a change of basis from the vector x to\na new vector\nˆx = Sx.\n(A.52)\nThe corresponding linear system being solved then becomes\nAS−1ˆx = S−1b\nor\nˆ\nAˆx = ˆb,\n(A.53)\n752\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwith a corresponding least squares energy (A.29) of the form\nEPLS = ˆxT (S−T CS−1)ˆx −2ˆxT (S−T d) + ∥ˆb∥2.\n(A.54)\nThe actual preconditioned matrix ˆC = S−T CS−1 is usually not explicitly computed. In-\nstead, Algorithm A.3 is extended to insert S−T and ST operations at the appropriate places\n(Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Saad 2003; Nocedal and\nWright 2006).\nA good preconditioner S is easy and cheap to compute, but is also a decent approximation\nto a square root of C, so that κ(S−T CS−1) is closer to 1. The simplest such choice is the\nsquare root of the diagonal matrix S = D1/2, with D = diag(C). This has the advantage\nthat any scalar change in variables (e.g., using radians instead of degrees for angular measure-\nments) has no effect on the range of convergence of the iterative technique. For problems that\nare naturally block-structured, e.g., for structure from motion, where 3D point positions or\n6D camera poses are being estimated, a block diagonal preconditioner is often a good choice.\nA wide variety of more sophisticated preconditioners have been developed over the years\n(Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Saad 2003; Nocedal and\nWright 2006), many of which can be directly applied to problems in computer vision (Byr¨od\nand øAstr¨om 2009; Jeong, Nist´er, Steedly et al. 2010; Agarwal, Snavely, Seitz et al. 2010).\nSome of these are based on an incomplete Cholesky factorization of C, i.e., one in which the\namount of ﬁll-in in R is strictly limited, e.g., to just the original non-zero elements in C.10\nOther preconditioners are based on a sparsiﬁed, e.g., tree-based or clustered, approximation\nto C (Koutis 2007; Koutis and Miller 2008; Grady 2008; Koutis, Miller, and Tolliver 2009),\nsince these are known to have efﬁcient inversion properties.\nFor grid-based image-processing applications, parallel or hierarchical preconditioners\noften perform extremely well (Yserentant 1986; Szeliski 1990b; Pentland 1994; Saad 2003;\nSzeliski 2006b). These approaches use a change of basis transformation S that resembles\nthe pyramidal or wavelet representations discussed in Section 3.5, and are hence amenable\nto parallel and GPU-based implementations. Coarser elements in the new representation\nquickly converge to the low-frequency components in the solution, while ﬁner-level elements\nencode the higher-frequency components. Some of the relationships between hierarchical\npreconditioners, incomplete Cholesky factorization, and multigrid techniques are explored\nby Saad (2003) and Szeliski (2006b).\n10 If a complete Cholesky factorization C = RT R is used, we get ˆC = R−T CR−1 = I and all iterative\nalgorithms converge in a single step, thereby obviating the need to use them, but the complete factorization is often\ntoo expensive. Note that incomplete factorization can also beneﬁt from reordering.",
  "image_path": "page_773.jpg",
  "pages": [
    772,
    773,
    774
  ]
}