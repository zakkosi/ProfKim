{
  "doc_id": "pages_786_788",
  "text": "764\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure B.1\nGraphical model for an N4 neighborhood Markov random ﬁeld. The white\ncircles are the unknowns f(i, j), while the dark circles are the input data d(i, j). The sx(i, j)\nand sy(i, j) black boxes denote arbitrary interaction potentials between adjacent nodes in the\nrandom ﬁeld, and the w(i, j) denote the data penalty functions. They are all examples of the\ngeneral potentials Vi,j,k,l(f(i, j), f(k, l)) used in Equation (B.24).\nsley–Clifford Theorem) can be written as a sum of pairwise interaction potentials,\nEp(x) =\nX\n{(i,j),(k,l)}∈N\nVi,j,k,l(f(i, j), f(k, l)),\n(B.24)\nwhere N(i, j) denotes the neighbors of pixel (i, j). In the more general case, MRFs can also\ncontain unary potentials, as well as higher-order potentials deﬁned over larger cardinality\ncliques (Kindermann and Snell 1980; Geman and Geman 1984; Bishop 2006; Potetz and Lee\n2008; Kohli, Kumar, and Torr 2009; Kohli, Ladick´y, and Torr 2009; Rother, Kohli, Feng et\nal. 2009; Alahari, Kohli, and Torr 2011). They can also contain line processes, i.e., additional\nbinary variables that mediate discontinuities between adjacent elements (Geman and Geman\n1984). Black and Rangarajan (1996) show how independent line process variables can be\neliminated and incorporated into regular MRFs using robust pairwise penalty functions.\nThe most commonly used neighborhood in Markov random ﬁeld modeling is the N4\nneighborhood, where each pixel in the ﬁeld f(i, j) interacts only with its immediate neighbors—\nFigure B.1 shows such an N4 MRF. The sx(i, j) and sy(i, j) black boxes denote arbitrary\ninteraction potentials between adjacent nodes in the random ﬁeld and the w(i, j) denote the\nelemental data penalty terms in Ed (B.23). These square nodes can also be interpreted as fac-\ntors in a factor graph version of the undirected graphical model (Bishop 2006; Wainwright\nand Jordan 2008; Koller and Friedman 2009), which is another name for interaction poten-\ntials. (Strictly speaking, the factors are improper probability functions whose product is the\nun-normalized posterior distribution.)\nMore complex and higher-dimensional interaction models and neighborhoods are also\nB.5 Markov random ﬁelds\n765\npossible. For example, 2D grids can be enhanced with the addition of diagonal connections\n(an N8 neighborhood) or even larger numbers of pairwise terms (Boykov and Kolmogorov\n2003; Rother, Kolmogorov, Lempitsky et al. 2007). 3D grids can be used to compute glob-\nally optimal segmentations in 3D volumetric medical images (Boykov and Funka-Lea 2006)\n(Section 5.5.1). Higher-order cliques can also be used to develop more sophisticated models\n(Potetz and Lee 2008; Kohli, Ladick´y, and Torr 2009; Kohli, Kumar, and Torr 2009).\nOne of the biggest challenges in using MRF models is to develop efﬁcient inference algo-\nrithms that will ﬁnd low-energy solutions (Veksler 1999; Boykov, Veksler, and Zabih 2001;\nKohli 2007; Kumar 2008). Over the years, a large variety of such algorithms have been de-\nveloped, including simulated annealing, graph cuts, and loopy belief propagation. The choice\nof inference technique can greatly affect the overall performance of a vision system. For\nexample, most of the top-performing algorithms on the Middlebury Stereo Evaluation page\neither use belief propagation or graph cuts.\nIn the next few subsections, we review some of the more widely used MRF inference\ntechniques. More in-depth descriptions of most of these algorithms can be found in a re-\ncently published book on advances in MRF techniques (Blake, Kohli, and Rother 2010).\nExperimental comparisons, along with test datasets and reference software, are provided by\nSzeliski, Zabih, Scharstein et al. (2008).6\nB.5.1 Gradient descent and simulated annealing\nThe simplest optimization technique is gradient descent, which minimizes the energy by\nchanging independent subsets of nodes to take on lower-energy conﬁgurations. Such tech-\nniques go under a variety of names, including contextual classiﬁcation (Kittler and F¨oglein\n1984) and iterated conditional modes (ICM) (Besag 1986).7 Variables can either be updated\nsequentially, e.g., in raster scan, or in parallel, e.g., using red–black coloring on a checker-\nboard. Chou and Brown (1990) suggests using highest conﬁdence ﬁrst (HCF), i.e., choosing\nvariables based on how large a difference they make in reducing the energy.\nThe problem with gradient descent is that it is prone to getting stuck in local minima,\nwhich is almost always the case with MRF problems. One way around this is to use stochastic\ngradient descent or Markov chain Monte Carlo (MCMC) (Metropolis, Rosenbluth, Rosen-\nbluth et al. 1953), i.e., to randomly take occasional uphill steps in order to get out of such\nminima. One popular update rule is the Gibbs sampler (Geman and Geman 1984); rather\nthan choosing the lowest energy state for a variable being updated, it chooses the state with\n6 http://vision.middlebury.edu/MRF/.\n7 The name comes from iteratively setting variables to the mode (most likely, i.e., lowest energy) state conditioned\non its currently ﬁxed neighbors.\n766\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprobability\np(x) ∝e−E(x)/T ,\n(B.25)\nwhere T is called the temperature and controls how likely the system is to choose a more\nrandom update. Stochastic gradient descent is usually combined with simulated annealing\n(Kirkpatrick, Gelatt, and Vecchi 1983), which starts at a relatively high temperature, thereby\nrandomly exploring a large part of the state space, and gradually cools (anneals) the tem-\nperature to ﬁnd a good local minimum. During the late 1980s, simulated annealing was the\nmethod of choice for solving MRF inference problems (Szeliski 1986; Marroquin, Mitter,\nand Poggio 1985; Barnard 1989).\nAnother variant on simulated annealing is the Swendsen–Wang algorithm (Swendsen and\nWang 1987; Barbu and Zhu 2003, 2005). Here, instead of “ﬂipping” (changing) single vari-\nables, a connected subset of variables, chosen using a random walk based on MRF connec-\ntively strengths, is selected as the basic update unit. This can sometimes help make larger\nstate changes, and hence ﬁnd better-quality solutions in less time.\nWhile simulated annealing has largely been superseded by the newer graph cuts and loopy\nbelief propagation techniques, it still occasionally ﬁnds use, especially in highly connected\nand highly non-submodular graphs (Rother, Kolmogorov, Lempitsky et al. 2007).\nB.5.2 Dynamic programming\nDynamic programming (DP) is an efﬁcient inference procedure that works for any tree-\nstructured graphical model, i.e., one that does not have any cycles. Given such a tree, pick\nany node as the root r and ﬁguratively pick up the tree by its root. The depth or distance of all\nthe other nodes from this root induces a partial ordering over the vertices, from which a total\nordering can be obtained by arbitrarily breaking ties. Let us now lay out this graph as a tree\nwith the root on the right and indices increasing from left to right, as shown in Figure B.2a.\nBefore describing the DP algorithm, let us re-write the potential function of Equation (B.24)\nin a more general but succinct form,\nE(x) =\nX\n(i,j)∈N\nVi,j(xi, xj) +\nX\ni\nVi(xi),\n(B.26)\nwhere instead of using pixel indices (i, j) and (k, l), we just use scalar index variables i\nand j. We also replace the function value f(i, j) with the more succinct notation xi, with\nthe {xi} variables making up the state vector x. We can simplify this function even further\nby adding dummy nodes (vertices) i−for every node that has a non-zero Vi(xi) and setting\nVi,i−(xi, xi−) = Vi(xi), which lets us drop the Vi terms from (B.26).\nDynamic programming proceeds by computing partial sums in a left-to-right fashion, i.e.,\nin order of increasing variable index. Let Ck be the children of k, i.e., i < k, (i, k) ∈N).",
  "image_path": "page_787.jpg",
  "pages": [
    786,
    787,
    788
  ]
}