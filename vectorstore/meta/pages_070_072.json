{
  "doc_id": "pages_070_072",
  "text": "48\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ni.e., we drop the z component but keep the w component. Orthography is an approximate\nmodel for long focal length (telephoto) lenses and objects whose depth is shallow relative\nto their distance to the camera (Sawhney and Hanson 1991). It is exact only for telecentric\nlenses (Baker and Nayar 1999, 2001).\nIn practice, world coordinates (which may measure dimensions in meters) need to be\nscaled to ﬁt onto an image sensor (physically measured in millimeters, but ultimately mea-\nsured in pixels). For this reason, scaled orthography is actually more commonly used,\nx = [sI2×2|0] p.\n(2.48)\nThis model is equivalent to ﬁrst projecting the world points onto a local fronto-parallel image\nplane and then scaling this image using regular perspective projection. The scaling can be the\nsame for all parts of the scene (Figure 2.7b) or it can be different for objects that are being\nmodeled independently (Figure 2.7c). More importantly, the scaling can vary from frame to\nframe when estimating structure from motion, which can better model the scale change that\noccurs as an object approaches the camera.\nScaled orthography is a popular model for reconstructing the 3D shape of objects far away\nfrom the camera, since it greatly simpliﬁes certain computations. For example, pose (camera\norientation) can be estimated using simple least squares (Section 6.2.1). Under orthography,\nstructure and motion can simultaneously be estimated using factorization (singular value de-\ncomposition), as discussed in Section 7.3 (Tomasi and Kanade 1992).\nA closely related projection model is para-perspective (Aloimonos 1990; Poelman and\nKanade 1997). In this model, object points are again ﬁrst projected onto a local reference\nparallel to the image plane. However, rather than being projected orthogonally to this plane,\nthey are projected parallel to the line of sight to the object center (Figure 2.7d). This is\nfollowed by the usual projection onto the ﬁnal image plane, which again amounts to a scaling.\nThe combination of these two projections is therefore afﬁne and can be written as\n˜x =\n\n\na00\na01\na02\na03\na10\na11\na12\na13\n0\n0\n0\n1\n\n˜p.\n(2.49)\nNote how parallel lines in 3D remain parallel after projection in Figure 2.7b–d. Para-perspective\nprovides a more accurate projection model than scaled orthography, without incurring the\nadded complexity of per-pixel perspective division, which invalidates traditional factoriza-\ntion methods (Poelman and Kanade 1997).\nPerspective\nThe most commonly used projection in computer graphics and computer vision is true 3D\nperspective (Figure 2.7e). Here, points are projected onto the image plane by dividing them\n2.1 Geometric primitives and transformations\n49\nby their z component. Using inhomogeneous coordinates, this can be written as\n¯x = Pz(p) =\n\n\nx/z\ny/z\n1\n\n.\n(2.50)\nIn homogeneous coordinates, the projection has a simple linear form,\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n˜p,\n(2.51)\ni.e., we drop the w component of p. Thus, after projection, it is not possible to recover the\ndistance of the 3D point from the image, which makes sense for a 2D imaging sensor.\nA form often seen in computer graphics systems is a two-step projection that ﬁrst projects\n3D coordinates into normalized device coordinates in the range (x, y, z) ∈[−1, −1] ×\n[−1, 1] × [0, 1], and then rescales these coordinates to integer pixel coordinates using a view-\nport transformation (Watt 1995; OpenGL-ARB 1997).\nThe (initial) perspective projection\nis then represented using a 4 × 4 matrix\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n−zfar/zrange\nznearzfar/zrange\n0\n0\n1\n0\n\n˜p,\n(2.52)\nwhere znear and zfar are the near and far z clipping planes and zrange = zfar −znear. Note\nthat the ﬁrst two rows are actually scaled by the focal length and the aspect ratio so that\nvisible rays are mapped to (x, y, z) ∈[−1, −1]2. The reason for keeping the third row, rather\nthan dropping it, is that visibility operations, such as z-buffering, require a depth for every\ngraphical element that is being rendered.\nIf we set znear = 1, zfar →∞, and switch the sign of the third row, the third element\nof the normalized screen vector becomes the inverse depth, i.e., the disparity (Okutomi and\nKanade 1993). This can be quite convenient in many cases since, for cameras moving around\noutdoors, the inverse depth to the camera is often a more well-conditioned parameterization\nthan direct 3D distance.\nWhile a regular 2D image sensor has no way of measuring distance to a surface point,\nrange sensors (Section 12.2) and stereo matching algorithms (Chapter 11) can compute such\nvalues. It is then convenient to be able to map from a sensor-based depth or disparity value d\ndirectly back to a 3D location using the inverse of a 4 × 4 matrix (Section 2.1.5). We can do\nthis if we represent perspective projection using a full-rank 4 × 4 matrix, as in (2.64).\n50\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\ncs\nyc\nxs\nys\nsx\nsy\npc\np\nOc\nFigure 2.8 Projection of a 3D camera-centered point pc onto the sensor planes at location\np. Oc is the camera center (nodal point), cs is the 3D origin of the sensor plane coordinate\nsystem, and sx and sy are the pixel spacings.\nCamera intrinsics\nOnce we have projected a 3D point through an ideal pinhole using a projection matrix, we\nmust still transform the resulting coordinates according to the pixel sensor spacing and the\nrelative position of the sensor plane to the origin. Figure 2.8 shows an illustration of the\ngeometry involved. In this section, we ﬁrst present a mapping from 2D pixel coordinates to\n3D rays using a sensor homography M s, since this is easier to explain in terms of physically\nmeasurable quantities. We then relate these quantities to the more commonly used camera in-\ntrinsic matrix K, which is used to map 3D camera-centered points pc to 2D pixel coordinates\n˜xs.\nImage sensors return pixel values indexed by integer pixel coordinates (xs, ys), often\nwith the coordinates starting at the upper-left corner of the image and moving down and to\nthe right. (This convention is not obeyed by all imaging libraries, but the adjustment for\nother coordinate systems is straightforward.) To map pixel centers to 3D coordinates, we ﬁrst\nscale the (xs, ys) values by the pixel spacings (sx, sy) (sometimes expressed in microns for\nsolid-state sensors) and then describe the orientation of the sensor array relative to the camera\nprojection center Oc with an origin cs and a 3D rotation Rs (Figure 2.8).\nThe combined 2D to 3D projection can then be written as\np =\nh\nRs\ncs\ni\n\n\nsx\n0\n0\n0\nsy\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\nxs\nys\n1\n\n= M s¯xs.\n(2.53)\nThe ﬁrst two columns of the 3 × 3 matrix M s are the 3D vectors corresponding to unit steps\nin the image pixel array along the xs and ys directions, while the third column is the 3D\nimage array origin cs.",
  "image_path": "page_071.jpg",
  "pages": [
    70,
    71,
    72
  ]
}