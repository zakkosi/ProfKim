{
  "doc_id": "pages_757_759",
  "text": "Appendix A\nLinear algebra and numerical\ntechniques\nA.1\nMatrix decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 736\nA.1.1\nSingular value decomposition\n. . . . . . . . . . . . . . . . . . . . . 736\nA.1.2\nEigenvalue decomposition . . . . . . . . . . . . . . . . . . . . . . . 737\nA.1.3\nQR factorization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 740\nA.1.4\nCholesky factorization . . . . . . . . . . . . . . . . . . . . . . . . . 741\nA.2\nLinear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742\nA.2.1\nTotal least squares\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 744\nA.3\nNon-linear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 746\nA.4\nDirect sparse matrix techniques . . . . . . . . . . . . . . . . . . . . . . . . . 747\nA.4.1\nVariable reordering . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\nA.5\nIterative techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\nA.5.1\nConjugate gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . 749\nA.5.2\nPreconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 751\nA.5.3\nMultigrid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753\n736\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn this appendix, we introduce some elements of linear algebra and numerical techniques that\nare used elsewhere in the book. We start with some basic decompositions in matrix algebra,\nincluding the singular value decomposition (SVD), eigenvalue decompositions, and other\nmatrix decompositions (factorizations). Next, we look at the problem of linear least squares,\nwhich can be solved using either the QR decomposition or normal equations. This is followed\nby non-linear least squares, which arise when the measurement equations are not linear in the\nunknowns or when robust error functions are used. Such problems require iteration to ﬁnd\na solution. Next, we look at direct solution (factorization) techniques for sparse problems,\nwhere the ordering of the variables can have a large inﬂuence on the computation and memory\nrequirements. Finally, we discuss iterative techniques for solving large linear (or linearized)\nleast squares problems. Good general references for much of this material include the work\nby Bj¨orck (1996), Golub and Van Loan (1996), Trefethen and Bau (1997), Meyer (2000),\nNocedal and Wright (2006), and Bj¨orck and Dahlquist (2010).\nA note on vector and matrix indexing.\nTo be consistent with the rest of the book and\nwith the general usage in the computer science and computer vision communities, I adopt\na 0-based indexing scheme for vector and matrix element indexing. Please note that most\nmathematical textbooks and papers use 1-based indexing, so you need to be aware of the\ndifferences when you read this book.\nSoftware implementations.\nHighly optimized and tested libraries corresponding to the al-\ngorithms described in this appendix are readily available and are listed in Appendix C.2.\nA.1 Matrix decompositions\nIn order to better understand the structure of matrices and more stably perform operations\nsuch as inversion and system solving, a number of decompositions (or factorizations) can be\nused. In this section, we review singular value decomposition (SVD), eigenvalue decomposi-\ntion, QR factorization, and Cholesky factorization.\nA.1.1 Singular value decomposition\nOne of the most useful decompositions in matrix algebra is the singular value decomposition\n(SVD), which states that any real-valued M × N matrix A can be written as\nAM×N\n=\nU M×P ΣP ×P V T\nP ×N\n(A.1)\nA.1 Matrix decompositions\n737\n=\n\nu0\n· · ·\nup−1\n\n\n\n\nσ0\n...\nσp−1\n\n\n\n\nvT\n0\n· · ·\nvT\np−1\n\n,\nwhere P = min(M, N). The matrices U and V are orthonormal, i.e., U T U = I and\nV T V = I, and so are their column vectors,\nui · uj = vi · vj = δij.\n(A.2)\nThe singular values are all non-negative and can be ordered in decreasing order\nσ0 ≥σ1 ≥· · · ≥σp−1 ≥0.\n(A.3)\nA geometric intuition for the SVD of a matrix A can be obtained by re-writing A =\nUΣV T in (A.2) as\nAV = UΣ\nor\nAvj = σjuj.\n(A.4)\nThis formula says that the matrix A takes any basis vector vj and maps it to a direction uj\nwith length σj, as shown in Figure A.1\nIf only the ﬁrst r singular values are positive, the matrix A is of rank r and the index p\nin the SVD decomposition (A.2) can be replaced by r. (In other words, we can drop the last\np −r columns of U and V .)\nAn important property of the singular value decomposition of a matrix (also true for\nthe eigenvalue decomposition of a real symmetric non-negative deﬁnite matrix) is that if we\ntruncate the expansion\nA =\nt\nX\nj=0\nσjujvT\nj ,\n(A.5)\nwe obtain the best possible least squares approximation to the original matrix A. This is\nused both in eigenface-based face recognition systems (Section 14.2.1) and in the separable\napproximation of convolution kernels (3.21).\nA.1.2 Eigenvalue decomposition\nIf the matrix C is symmetric (m = n),1 it can be written as an eigenvalue decomposition,\nC\n=\nUΛU T =\n\nu0\n· · ·\nun−1\n\n\n\n\nλ0\n...\nλn−1\n\n\n\n\nuT\n0\n· · ·\nuT\nn−1\n\n\n=\nn−1\nX\ni=0\nλiuiuT\ni .\n(A.6)\n1 In this appendix, we denote symmetric matrices using C and general rectangular matrices using A.",
  "image_path": "page_758.jpg",
  "pages": [
    757,
    758,
    759
  ]
}