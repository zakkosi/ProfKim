{
  "doc_id": "pages_738_740",
  "text": "716\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.52 Recognition using tiny images (Torralba, Freeman, and Fergus 2008) c⃝2008\nIEEE: columns (a) and (c) show sample input images and columns (b) and (d) show the\ncorresponding 16 nearest neighbors in the database of 80 million tiny images.\nsimultaneous recognition and segmentation (Liu, Yuen, and Torralba 2009).\nWhen the database of images becomes large enough, it is even possible to directly match\ncomplete images with the expectation of ﬁnding a good match. Torralba, Freeman, and Fergus\n(2008) start with a database of 80 million tiny (32 × 32) images and compensate for the poor\naccuracy in their image labels, which are collected automatically from the Internet, by using\na semantic taxonomy (Wordnet) to infer the most likely labels for a new image. Somewhere\nin the 80 million images, there are enough examples to associate some set of images with\neach of the 75,000 non-abstract nouns in Wordnet that they use in their system. Some sample\nrecognition results are shown in Figure 14.52.\nAnother example of a large labeled database of images is ImageNet (Deng, Dong, Socher\net al. 2009), which is collecting images for the 80,000 nouns (synonym sets) in WordNet\n(Fellbaum 1998). As of April 2010, about 500–1000 carefully vetted examples for 14841\n14.5 Context and scene understanding\n717\nFigure 14.53\nImageNet (Deng, Dong, Socher et al. 2009) c⃝2009 IEEE. This database\ncontains over 500 carefully vetted images for each of 14,841 (as of April, 2010) nouns from\nthe WordNet hierarchy.\nsynsets have been collected (Figure 14.53). The paper by Deng, Dong, Socher et al. (2009)\nalso has a nice review of related databases.\nAs we mentioned in Section 14.4.3, the existence of large databases of partially labeled\nInternet imagery has given rise to a new sub-ﬁeld of Internet computer vision, with its own\nworkshops21 and a special journal issue (Avidan, Baker, and Shan 2010).\n14.5.2 Application: Image search\nEven though visual recognition algorithms are by some measures still in their infancy, they\nare already starting to have some impact on image search, i.e., the retrieval of images from the\nWeb using combinations of keywords and visual similarity. Today, most image search engines\nrely mostly on textual keywords found in captions, nearby text, and ﬁlenames, augmented by\nuser click-through data (Craswell and Szummer 2007). As recognition algorithms continue\nto improve, however, visual features and visual similarity will start being used to recognize\nimages with missing or erroneous keywords.\nThe topic of searching by visual similarity has a long history and goes by a variety of\nnames, including content-based image retrieval (CBIR) (Smeulders, Worring, Santini et al.\n2000; Lew, Sebe, Djeraba et al. 2006; Vasconcelos 2007; Datta, Joshi, Li et al. 2008) and\nquery by image content (QBIC) (Flickner, Sawhney, Niblack et al. 1995). Original publica-\ntions in these ﬁelds were based primarily on simple whole-image similarity metrics, such as\ncolor and texture (Swain and Ballard 1991; Jacobs, Finkelstein, and Salesin 1995; Manjunathi\nand Ma 1996).\n21 http://www.internetvisioner.org/.\n718\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn more recent work, Fergus, Perona, and Zisserman (2004) use a feature-based learning\nand recognition algorithm to re-rank the outputs from a traditional keyword-based image\nsearch engine. In follow-on work, Fergus, Fei-Fei, Perona et al. (2005) cluster the results\nreturned by image search using an extension of probabilistic latest semantic analysis (PLSA)\n(Hofmann 1999) and then select the clusters associated with the highest ranked results as the\nrepresentative images for that category.\nEven more recent work relies on carefully annotated image databases such as LabelMe\n(Russell, Torralba, Murphy et al. 2008). For example, Malisiewicz and Efros (2008) describe\na system that, given a query image, can ﬁnd similar LabelMe images, whereas Liu, Yuen, and\nTorralba (2009) combine feature-based correspondence algorithms with the labeled database\nto perform simultaneous recognition and segmentation.\n14.6 Recognition databases and test sets\nIn addition to rapid advances in machine learning and statistical modeling techniques, one\nof the key ingredients in the continued improvement of recognition algorithms has been the\nincreased availability and quality of image recognition databases.\nTables 14.1 and 14.2, which are based on similar tables in Fei-Fei, Fergus, and Torralba\n(2009), updated with more recent entries and URLs, show some of the mostly widely used\nrecognition databases. Some of these databases, such as the ones for face recognition and\nlocalization, date back over a decade. The most recent ones, such as the PASCAL database,\nare refreshed annually with ever more challenging problems. Table 14.1 shows examples of\ndatabases used primarily for (whole image) recognition while Table 14.2 shows databases\nwhere more accurate localization or segmentation information is available and expected.\nPonce, Berg, Everingham et al. (2006) discuss some of the problems with earlier datasets\nand describe how the latest PASCAL Visual Object Classes Challenge aims to overcome\nthese. Some examples of the 20 visual classes in the 2008 challenge are shown in Fig-\nure 14.54. The slides from the VOC workshops,22 are a great source for pointers to the\nbest recognition techniques currently available.\nTwo of the most recent trends in recognition databases are the emergence of Web-based\nannotation and data collection tools, and the use of search and recognition algorithms to build\nup databases (Ponce, Berg, Everingham et al. 2006). Some of the most interesting work in\nhuman annotation of images comes from a series of interactive multi-person games such as\nESP (von Ahn and Dabbish 2004) and Peekaboom (von Ahn, Liu, and Blum 2006). In these\ngames, people help each other guess the identity of a hidden image by giving textual clues\nas to its contents, which implicitly labels either the whole image or just regions. A more\n22 http://pascallin.ecs.soton.ac.uk/challenges/VOC/.",
  "image_path": "page_739.jpg",
  "pages": [
    738,
    739,
    740
  ]
}