{
  "doc_id": "pages_648_650",
  "text": "626\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncan be used to ﬁnd the most iconic (commonly photographed) objects in the collection, along\nwith their related tags. In follow-on work, Simon and Seitz (2008) show how such tags can\nbe propagated to sub-regions of each image, using an analysis of which 3D points appear\nin the central portions of photographs. Extensions of these techniques to all of the world’s\nimages, including the use of GPS tags where available, have been investigated as well (Li,\nWu, Zach et al. 2008; Quack, Leibe, and Van Gool 2008; Crandall, Backstrom, Huttenlocher\net al. 2009; Li, Crandall, and Huttenlocher 2009; Zheng, Zhao, Song et al. 2009).\n13.2 Layered depth images\nTraditional view interpolation techniques associate a single depth map with each source or\nreference image. Unfortunately, when such a depth map is warped to a novel view, holes and\ncracks inevitably appear behind the foreground objects. One way to alleviate this problem is\nto keep several depth and color values (depth pixels) at every pixel in a reference image (or,\nat least for pixels near foreground–background transitions) (Figure 13.5). The resulting data\nstructure, which is called a layered depth image (LDI), can be used to render new views using\na back-to-front forward warping (splatting) algorithm (Shade, Gortler, He et al. 1998).\n13.2.1 Impostors, sprites, and layers\nAn alternative to keeping lists of color-depth values at each pixel, as is done in the LDI, is\nto organize objects into different layers or sprites. The term sprite originates in the computer\ngame industry, where it is used to designate ﬂat animated characters in games such as Pac-\nMan or Mario Bros. When put into a 3D setting, such objects are often called impostors,\nbecause they use a piece of ﬂat, alpha-matted geometry to represent simpliﬁed versions of 3D\nobjects that are far away from the camera (Shade, Lischinski, Salesin et al. 1996; Lengyel and\nSnyder 1997; Torborg and Kajiya 1996). In computer vision, such representations are usually\ncalled layers (Wang and Adelson 1994; Baker, Szeliski, and Anandan 1998; Torr, Szeliski,\nand Anandan 1999; Birchﬁeld, Natarajan, and Tomasi 2007). Section 8.5.2 discusses the\ntopics of transparent layers and reﬂections, which occur on specular and transparent surfaces\nsuch as glass.\nWhile ﬂat layers can often serve as an adequate representation of geometry and appear-\nance for far-away objects, better geometric ﬁdelity can be achieved by also modeling the\nper-pixel offsets relative to a base plane, as shown in Figures 13.5 and 13.6a–b. Such repre-\nsentations are called plane plus parallax in the computer vision literature (Kumar, Anandan,\nand Hanna 1994; Sawhney 1994; Szeliski and Coughlan 1997; Baker, Szeliski, and Anandan\n1998), as discussed in Section 8.5 (Figure 8.16). In addition to fully automated stereo tech-\nniques, it is also possible to paint in depth layers (Kang 1998; Oh, Chen, Dorsey et al. 2001;\n13.2 Layered depth images\n627\nFigure 13.5\nA variety of image-based rendering primitives, which can be used depending\non the distance between the camera and the object of interest (Shade, Gortler, He et al. 1998)\nc⃝1998 ACM. Closer objects may require more detailed polygonal representations, while\nmid-level objects can use a layered depth image (LDI), and far-away objects can use sprites\n(potentially with depth) and environment maps.\n(a)\n(b)\n(c)\n(d)\nFigure 13.6\nSprites with depth (Shade, Gortler, He et al. 1998) c⃝1998 ACM: (a) alpha-\nmatted color sprite; (b) corresponding relative depth or parallax; (c) rendering without relative\ndepth; (d) rendering with depth (note the curved object boundaries).\n628\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nShum, Sun, Yamazaki et al. 2004) or to infer their 3D structure from monocular image cues\n(Section 14.4.4) (Hoiem, Efros, and Hebert 2005b; Saxena, Sun, and Ng 2009).\nHow can we render a sprite with depth from a novel viewpoint? One possibility, as with\na regular depth map, is to just forward warp each pixel to its new location, which can cause\naliasing and cracks. A better way, which we already mentioned in Section 3.6.2, is to ﬁrst\nwarp the depth (or (u, v) displacement) map to the novel view, ﬁll in the cracks, and then use\nhigher-quality inverse warping to resample the color image (Shade, Gortler, He et al. 1998).\nFigure 13.6d shows the results of applying such a two-pass rendering algorithm. From this\nstill image, you can appreciate that the foreground sprites look more rounded; however, to\nfully appreciate the improvement in realism, you would have to look at the actual animated\nsequence.\nSprites with depth can also be rendered using conventional graphics hardware, as de-\nscribed in (Zitnick, Kang, Uyttendaele et al. 2004). Rogmans, Lu, Bekaert et al. (2009)\ndescribe GPU implementations of both real-time stereo matching and real-time forward and\ninverse rendering algorithms.\n13.3 Light ﬁelds and Lumigraphs\nWhile image-based rendering approaches can synthesize scene renderings from novel view-\npoints, they raise the following more general question:\nIs is possible to capture and render the appearance of a scene from all possible\nviewpoints and, if so, what is the complexity of the resulting structure?\nLet us assume that we are looking at a static scene, i.e., one where the objects and illu-\nminants are ﬁxed, and only the observer is moving around. Under these conditions, we can\ndescribe each image by the location and orientation of the virtual camera (6 dof) as well as\nits intrinsics (e.g., its focal length). However, if we capture a two-dimensional spherical im-\nage around each possible camera location, we can re-render any view from this information.4\nThus, taking the cross-product of the three-dimensional space of camera positions with the\n2D space of spherical images, we obtain the 5D plenoptic function of Adelson and Bergen\n(1991), which forms the basis of the image-based rendering system of McMillan and Bishop\n(1995).\nNotice, however, that when there is no light dispersion in the scene, i.e., no smoke or fog,\nall the coincident rays along a portion of free space (between solid or refractive objects) have\nthe same color value. Under these conditions, we can reduce the 5D plenoptic function to\n4 Since we are counting dimensions, we ignore for now any sampling or resolution issues.",
  "image_path": "page_649.jpg",
  "pages": [
    648,
    649,
    650
  ]
}