{
  "doc_id": "pages_230_232",
  "text": "208\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.2\nTwo pairs of images to be matched. What kinds of feature might one use to\nestablish a set of correspondences between these images?\nmatching (Hannah 1974; Moravec 1983; Hannah 1988) and have more recently gained pop-\nularity for image-stitching applications (Zoghlami, Faugeras, and Deriche 1997; Brown and\nLowe 2007) as well as fully automated 3D modeling (Beardsley, Torr, and Zisserman 1996;\nSchaffalitzky and Zisserman 2002; Brown and Lowe 2003; Snavely, Seitz, and Szeliski 2006).\nThere are two main approaches to ﬁnding feature points and their correspondences. The\nﬁrst is to ﬁnd features in one image that can be accurately tracked using a local search tech-\nnique, such as correlation or least squares (Section 4.1.4). The second is to independently\ndetect features in all the images under consideration and then match features based on their\nlocal appearance (Section 4.1.3). The former approach is more suitable when images are\ntaken from nearby viewpoints or in rapid succession (e.g., video sequences), while the lat-\nter is more suitable when a large amount of motion or appearance change is expected, e.g.,\nin stitching together panoramas (Brown and Lowe 2007), establishing correspondences in\nwide baseline stereo (Schaffalitzky and Zisserman 2002), or performing object recognition\n(Fergus, Perona, and Zisserman 2007).\nIn this section, we split the keypoint detection and matching pipeline into four separate\nstages. During the feature detection (extraction) stage (Section 4.1.1), each image is searched\nfor locations that are likely to match well in other images. At the feature description stage\n(Section 4.1.2), each region around detected keypoint locations is converted into a more com-\npact and stable (invariant) descriptor that can be matched against other descriptors. The\n4.1 Points and patches\n209\nFigure 4.3\nImage pairs with extracted patches below. Notice how some patches can be\nlocalized or matched with higher accuracy than others.\nfeature matching stage (Section 4.1.3) efﬁciently searches for likely matching candidates in\nother images. The feature tracking stage (Section 4.1.4) is an alternative to the third stage\nthat only searches a small neighborhood around each detected feature and is therefore more\nsuitable for video processing.\nA wonderful example of all of these stages can be found in David Lowe’s (2004) paper,\nwhich describes the development and reﬁnement of his Scale Invariant Feature Transform\n(SIFT). Comprehensive descriptions of alternative techniques can be found in a series of\nsurvey and evaluation papers covering both feature detection (Schmid, Mohr, and Bauck-\nhage 2000; Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuytelaars and Mikolajczyk 2007)\nand feature descriptors (Mikolajczyk and Schmid 2005). Shi and Tomasi (1994) and Triggs\n(2004) also provide nice reviews of feature detection techniques.\n4.1.1 Feature detectors\nHow can we ﬁnd image locations where we can reliably ﬁnd correspondences with other\nimages, i.e., what are good features to track (Shi and Tomasi 1994; Triggs 2004)? Look again\nat the image pair shown in Figure 4.3 and at the three sample patches to see how well they\nmight be matched or tracked. As you may notice, textureless patches are nearly impossible\nto localize. Patches with large contrast changes (gradients) are easier to localize, although\nstraight line segments at a single orientation suffer from the aperture problem (Horn and\nSchunck 1981; Lucas and Kanade 1981; Anandan 1989), i.e., it is only possible to align\nthe patches along the direction normal to the edge direction (Figure 4.4b). Patches with\n210\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nxi\nxi+u\nu\ni\n(a)\n(b)\n(c)\nFigure 4.4\nAperture problems for different image patches: (a) stable (“corner-like”) ﬂow;\n(b) classic aperture problem (barber-pole illusion); (c) textureless region. The two images I0\n(yellow) and I1 (red) are overlaid. The red vector u indicates the displacement between the\npatch centers and the w(xi) weighting function (patch window) is shown as a dark circle.\ngradients in at least two (signiﬁcantly) different orientations are the easiest to localize, as\nshown schematically in Figure 4.4a.\nThese intuitions can be formalized by looking at the simplest possible matching criterion\nfor comparing two image patches, i.e., their (weighted) summed square difference,\nEWSSD(u) =\nX\ni\nw(xi)[I1(xi + u) −I0(xi)]2,\n(4.1)\nwhere I0 and I1 are the two images being compared, u = (u, v) is the displacement vector,\nw(x) is a spatially varying weighting (or window) function, and the summation i is over all\nthe pixels in the patch. Note that this is the same formulation we later use to estimate motion\nbetween complete images (Section 8.1).\nWhen performing feature detection, we do not know which other image locations the\nfeature will end up being matched against. Therefore, we can only compute how stable this\nmetric is with respect to small variations in position ∆u by comparing an image patch against\nitself, which is known as an auto-correlation function or surface\nEAC(∆u) =\nX\ni\nw(xi)[I0(xi + ∆u) −I0(xi)]2\n(4.2)\n(Figure 4.5).1 Note how the auto-correlation surface for the textured ﬂower bed (Figure 4.5b\nand the red cross in the lower right quadrant of Figure 4.5a) exhibits a strong minimum,\nindicating that it can be well localized. The correlation surface corresponding to the roof\nedge (Figure 4.5c) has a strong ambiguity along one direction, while the correlation surface\ncorresponding to the cloud region (Figure 4.5d) has no stable minimum.\n1 Strictly speaking, a correlation is the product of two patches (3.12); I’m using the term here in a more qualitative\nsense. The weighted sum of squared differences is often called an SSD surface (Section 8.1).",
  "image_path": "page_231.jpg",
  "pages": [
    230,
    231,
    232
  ]
}