{
  "doc_id": "pages_294_296",
  "text": "272\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 5.3 Elastic net: The open squares indicate the cities and the closed squares linked by\nstraight line segments are the tour points. The blue circles indicate the approximate extent of\nthe attraction force of each city, which is reduced over time. Under the Bayesian interpretation\nof the elastic net, the blue circles correspond to one standard deviation of the circular Gaussian\nthat generates each city from some unknown tour point.\ntem solution and the linearization of non-linear constraints such as edge energy. A more direct\nway to ﬁnd a global energy minimum is to use dynamic programming (Amini, Weymouth,\nand Jain 1990; Williams and Shah 1992), but this is not often used in practice, since it has\nbeen superseded by even more efﬁcient or interactive algorithms such as intelligent scissors\n(Section 5.1.3) and GrabCut (Section 5.5).\nElastic nets and slippery springs\nAn interesting variant on snakes, ﬁrst proposed by Durbin and Willshaw (1987) and later\nre-formulated in an energy-minimizing framework by Durbin, Szeliski, and Yuille (1989), is\nthe elastic net formulation of the Traveling Salesman Problem (TSP). Recall that in a TSP,\nthe salesman must visit each city once while minimizing the total distance traversed. A snake\nthat is constrained to pass through each city could solve this problem (without any optimality\nguarantees) but it is impossible to tell ahead of time which snake control point should be\nassociated with each city.\nInstead of having a ﬁxed constraint between snake nodes and cities, as in (5.6), a city is\nassumed to pass near some point along the tour (Figure 5.3). In a probabilistic interpretation,\neach city is generated as a mixture of Gaussians centered at each tour point,\np(d(j)) =\nX\ni\npij with pij = e−d2\nij/(2σ2)\n(5.7)\nwhere σ is the standard deviation of the Gaussian and\ndij = ∥f(i) −d(j)∥\n(5.8)\n5.1 Active contours\n273\nis the Euclidean distance between a tour point f(i) and a city location d(j). The correspond-\ning data ﬁtting energy (negative log likelihood) is\nEslippery = −\nX\nj\nlog p(d(j)) = −\nX\nj\nlog\nhX\ne−∥f (i)−d(j)∥2/2σ2i\n.\n(5.9)\nThis energy derives its name from the fact that, unlike a regular spring, which couples a\ngiven snake point to a given constraint (5.6), this alternative energy deﬁnes a slippery spring\nthat allows the association between constraints (cities) and curve (tour) points to evolve over\ntime (Szeliski 1989). Note that this is a soft variant of the popular iterated closest point\ndata constraint that is often used in ﬁtting or aligning surfaces to data points or to each other\n(Section 12.2.1) (Besl and McKay 1992; Zhang 1994).\nTo compute a good solution to the TSP, the slippery spring data association energy is\ncombined with a regular ﬁrst-order internal smoothness energy (5.3) to deﬁne the cost of a\ntour. The tour f(s) is initialized as a small circle around the mean of the city points and σ is\nprogressively lowered (Figure 5.3). For large σ values, the tour tries to stay near the centroid\nof the points but as σ decreases each city pulls more and more strongly on its closest tour\npoints (Durbin, Szeliski, and Yuille 1989). In the limit as σ →0, each city is guaranteed to\ncapture at least one tour point and the tours between subsequent cites become straight lines.\nSplines and shape priors\nWhile snakes can be very good at capturing the ﬁne and irregular detail in many real-world\ncontours, they sometimes exhibit too many degrees of freedom, making it more likely that\nthey can get trapped in local minima during their evolution.\nOne solution to this problem is to control the snake with fewer degrees of freedom through\nthe use of B-spline approximations (Menet, Saint-Marc, and Medioni 1990b,a; Cipolla and\nBlake 1990). The resulting B-snake can be written as\nf(s) =\nX\nk\nBk(s)xk\n(5.10)\nor in discrete form as\nF = BX\n(5.11)\nwith\nF =\n\n\nf T (0)\n...\nf T (N)\n\n, B =\n\n\nB0(s0)\n. . .\nBK(s0)\n...\n...\n...\nB0(sN)\n. . .\nBK(sN)\n\n, and X =\n\n\nxT (0)\n...\nxT (K)\n\n.\n(5.12)\n274\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 5.4\nPoint distribution model for a set of resistors (Cootes, Cooper, Taylor et al.\n1995) c⃝1995 Elsevier: (a) set of input resistor shapes; (b) assignment of control points\nto the boundary; (c) distribution (scatter plot) of point locations; (d) ﬁrst (largest) mode of\nvariation in the ensemble shapes.\nIf the object being tracked or recognized has large variations in location, scale, or ori-\nentation, these can be modeled as an additional transformation on the control points, e.g.,\nx′\nk = sRxk + t (2.18), which can be estimated at the same time as the values of the control\npoints. Alternatively, separate detection and alignment stages can be run to ﬁrst localize and\norient the objects of interest (Cootes, Cooper, Taylor et al. 1995).\nIn a B-snake, because the snake is controlled by fewer degrees of freedom, there is less\nneed for the internal smoothness forces used with the original snakes, although these can still\nbe derived and implemented using ﬁnite element analysis, i.e., taking derivatives and integrals\nof the B-spline basis functions (Terzopoulos 1983; Bathe 2007).\nIn practice, it is more common to estimate a set of shape priors on the typical distribution\nof the control points {xk} (Cootes, Cooper, Taylor et al. 1995). Consider the set of resistor\nshapes shown in Figure 5.4a. If we describe each contour with the set of control points\nshown in Figure 5.4b, we can plot the distribution of each point in a scatter plot, as shown in\nFigure 5.4c.\nOne potential way of describing this distribution would be by the location ¯xk and 2D\ncovariance Ck of each individual point xk. These could then be turned into a quadratic\npenalty (prior energy) on the point location,\nEloc(xk) = 1\n2(xk −¯xk)T C−1\nk (xk −¯xk).\n(5.13)\nIn practice, however, the variation in point locations is usually highly correlated.\nA preferable approach is to estimate the joint covariance of all the points simultaneously.\nFirst, concatenate all of the point locations {xk} into a single vector x, e.g., by interleaving\nthe x and y locations of each point. The distribution of these vectors across all training",
  "image_path": "page_295.jpg",
  "pages": [
    294,
    295,
    296
  ]
}