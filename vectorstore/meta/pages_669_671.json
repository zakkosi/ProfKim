{
  "doc_id": "pages_669_671",
  "text": "13.5 Video-based rendering\n647\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 13.16 Video-based walkthroughs (Uyttendaele, Criminisi, Kang et al. 2004) c⃝2004\nIEEE: (a) system diagram of video pre-processing; (b) the Point Grey Ladybug camera; (c)\nghost removal using multi-perspective plane sweep; (d) point tracking, used both for calibra-\ntion and stabilization; (e) interactive garden walkthrough with map below; (f) overhead map\nauthoring and sound placement; (g) interactive home walkthrough with navigation bar (top)\nand icons of interest (bottom).\n648\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nframes into high dynamic range (HDR) video, pixels from adjacent frames need to be motion-\ncompensated before being merged (Kang, Uyttendaele, Winder et al. 2003).\nThe interactive walk-through experience becomes much richer and more navigable if an\noverview map is available as part of the experience. In Figure 13.16f, the map has annotations,\nwhich can show up during the tour, and localized sound sources, which play (with different\nvolumes) when the viewer is nearby. The process of aligning the video sequence with the\nmap can be automated using a process called map correlation (Levin and Szeliski 2004).\nAll of these elements combine to provide the user with a rich, interactive, and immersive\nexperience. Figure 13.16e shows a walk through the Bellevue Botanical Gardens, with an\noverview map in perspective below the live video window. Arrows on the ground are used to\nindicate potential directions of travel. The viewer simply orients his view towards one of the\narrows (the experience can be driven using a game controller) and “walks” forward along the\ndesired path.\nFigure 13.16g shows an indoor home tour experience. In addition to a schematic map\nin the lower left corner and adjacent room names along the top navigation bar, icons appear\nalong the bottom whenever items of interest, such as a homeowner’s art pieces, are visible\nin the main window. These icons can then be clicked to provide more information and 3D\nviews.\nThe development of interactive video tours spurred a renewed interest in 360◦video-based\nvirtual travel and mapping experiences, as evidenced by commercial sites such as Google’s\nStreet View and Bing Maps. The same videos can also be used to generate turn-by-turn driv-\ning directions, taking advantage of both expanded ﬁelds of view and image-based rendering\nto enhance the experience (Chen, Neubert, Ofek et al. 2009).\nAs we continue to capture more and more of our real world with large amounts of high-\nquality imagery and video, the interactive modeling, exploration, and rendering techniques\ndescribed in this chapter will play an even bigger role in bringing virtual experiences based\non remote areas of the world closer to everyone.\n13.6 Additional reading\nTwo good recent surveys of image-based rendering are by Kang, Li, Tong et al. (2006) and\nShum, Chan, and Kang (2007), with earlier surveys available from Kang (1999), McMillan\nand Gortler (1999), and Debevec (1999). The term image-based rendering was introduced by\nMcMillan and Bishop (1995), although the seminal paper in the ﬁeld is the view interpolation\npaper by Chen and Williams (1993). Debevec, Taylor, and Malik (1996) describe their Fac¸ade\nsystem, which not only created a variety of image-based modeling tools but also introduced\nthe widely used technique of view-dependent texture mapping.\n13.6 Additional reading\n649\nEarly work on planar impostors and layers was carried out by Shade, Lischinski, Salesin\net al. (1996), Lengyel and Snyder (1997), and Torborg and Kajiya (1996), while newer work\nbased on sprites with depth is described by Shade, Gortler, He et al. (1998).\nThe two foundational papers in image-based rendering are Light ﬁeld rendering by Levoy\nand Hanrahan (1996) and The Lumigraph by Gortler, Grzeszczuk, Szeliski et al. (1996).\nBuehler, Bosse, McMillan et al. (2001) generalize the Lumigraph approach to irregularly\nspaced collections of images, while Levoy (2006) provides a survey and more gentle intro-\nduction to the topic of light ﬁeld and image-based rendering.\nSurface light ﬁelds (Wood, Azuma, Aldinger et al. 2000) provide an alternative param-\neterization for light ﬁelds with accurately known surface geometry and support both better\ncompression and the possibility of editing surface properties. Concentric mosaics (Shum and\nHe 1999; Shum, Wang, Chai et al. 2002) and panoramas with depth (Peleg, Ben-Ezra, and\nPritch 2001; Li, Shum, Tang et al. 2004; Zheng, Kang, Cohen et al. 2007), provide useful\nparameterizations for light ﬁelds captured with panning cameras. Multi-perspective images\n(Rademacher and Bishop 1998) and manifold projections (Peleg and Herman 1997), although\nnot true light ﬁelds, are also closely related to these ideas.\nAmong the possible extensions of light ﬁelds to higher-dimensional structures, environ-\nment mattes (Zongker, Werner, Curless et al. 1999; Chuang, Zongker, Hindorff et al. 2000)\nare the most useful, especially for placing captured objects into new scenes.\nVideo-based rendering, i.e., the re-use of video to create new animations or virtual ex-\nperiences, started with the seminal work of Szummer and Picard (1996), Bregler, Covell,\nand Slaney (1997), and Sch¨odl, Szeliski, Salesin et al. (2000). Important follow-on work\nto these basic re-targeting approaches was carried out by Sch¨odl and Essa (2002), Kwatra,\nSch¨odl, Essa et al. (2003), Doretto, Chiuso, Wu et al. (2003), Wang and Zhu (2003), Zhong\nand Sclaroff (2003), Yuan, Wen, Liu et al. (2004), Doretto and Soatto (2006), Zhao and\nPietik¨ainen (2007), and Chan and Vasconcelos (2009).\nSystems that allow users to change their 3D viewpoint based on multiple synchronized\nvideo streams include those by Moezzi, Katkere, Kuramura et al. (1996), Kanade, Ran-\nder, and Narayanan (1997), Matusik, Buehler, Raskar et al. (2000), Matusik, Buehler, and\nMcMillan (2001), Carranza, Theobalt, Magnor et al. (2003), Zitnick, Kang, Uyttendaele et\nal. (2004), Magnor (2005), and Vedula, Baker, and Kanade (2005). 3D (multiview) video\ncoding and compression is also an active area of research (Smolic and Kauff 2005; Gotchev\nand Rosenhahn 2009), with 3D Blu-Ray discs, encoded using the multiview video coding\n(MVC) extension to H.264/MPEG-4 AVC, expected by the end of 2010.",
  "image_path": "page_670.jpg",
  "pages": [
    669,
    670,
    671
  ]
}