{
  "doc_id": "pages_110_112",
  "text": "88\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen color television was invented, it was decided to separately pass the red, green, and\nblue signals through the same gamma non-linearity before combining them for encoding.\nToday, even though we no longer have analog noise in our transmission systems, signals are\nstill quantized during compression (see Section 2.3.3), so applying inverse gamma to sensed\nvalues is still useful.\nUnfortunately, for both computer vision and computer graphics, the presence of gamma\nin images is often problematic. For example, the proper simulation of radiometric phenomena\nsuch as shading (see Section 2.2 and Equation (2.87)) occurs in a linear radiance space. Once\nall of the computations have been performed, the appropriate gamma should be applied before\ndisplay. Unfortunately, many computer graphics systems (such as shading models) operate\ndirectly on RGB values and display these values directly. (Fortunately, newer color imaging\nstandards such as the 16-bit scRGB use a linear space, which makes this less of a problem\n(Glassner 1995).)\nIn computer vision, the situation can be even more daunting. The accurate determination\nof surface normals, using a technique such as photometric stereo (Section 12.1.1) or even a\nsimpler operation such as accurate image deblurring, require that the measurements be in a\nlinear space of intensities. Therefore, it is imperative when performing detailed quantitative\ncomputations such as these to ﬁrst undo the gamma and the per-image color re-balancing\nin the sensed color values. Chakrabarti, Scharstein, and Zickler (2009) develop a sophisti-\ncated 24-parameter model that is a good match to the processing performed by today’s digital\ncameras; they also provide a database of color images you can use for your own testing.23\nFor other vision applications, however, such as feature detection or the matching of sig-\nnals in stereo and motion estimation, this linearization step is often not necessary. In fact,\ndetermining whether it is necessary to undo gamma can take some careful thinking, e.g., in\nthe case of compensating for exposure variations in image stitching (see Exercise 2.7).\nIf all of these processing steps sound confusing to model, they are. Exercise 2.10 has you\ntry to tease apart some of these phenomena using empirical investigation, i.e., taking pictures\nof color charts and comparing the RAW and JPEG compressed color values.\nOther color spaces\nWhile RGB and XYZ are the primary color spaces used to describe the spectral content (and\nhence tri-stimulus response) of color signals, a variety of other representations have been\ndeveloped both in video and still image coding and in computer graphics.\nThe earliest color representation developed for video transmission was the YIQ standard\ndeveloped for NTSC video in North America and the closely related YUV standard developed\nfor PAL in Europe. In both of these cases, it was desired to have a luma channel Y (so called\n23 http://vision.middlebury.edu/color/.\n2.3 The digital camera\n89\nsince it only roughly mimics true luminance) that would be comparable to the regular black-\nand-white TV signal, along with two lower frequency chroma channels.\nIn both systems, the Y signal (or more appropriately, the Y’ luma signal since it is gamma\ncompressed) is obtained from\nY ′\n601 = 0.299R′ + 0.587G′ + 0.114B′,\n(2.112)\nwhere R’G’B’ is the triplet of gamma-compressed color components. When using the newer\ncolor deﬁnitions for HDTV in BT.709, the formula is\nY ′\n709 = 0.2125R′ + 0.7154G′ + 0.0721B′.\n(2.113)\nThe UV components are derived from scaled versions of (B′−Y ′) and (R′−Y ′), namely,\nU = 0.492111(B′ −Y ′) and V = 0.877283(R′ −Y ′),\n(2.114)\nwhereas the IQ components are the UV components rotated through an angle of 33◦. In\ncomposite (NTSC and PAL) video, the chroma signals were then low-pass ﬁltered horizon-\ntally before being modulated and superimposed on top of the Y’ luma signal. Backward\ncompatibility was achieved by having older black-and-white TV sets effectively ignore the\nhigh-frequency chroma signal (because of slow electronics) or, at worst, superimposing it as\na high-frequency pattern on top of the main signal.\nWhile these conversions were important in the early days of computer vision, when frame\ngrabbers would directly digitize the composite TV signal, today all digital video and still\nimage compression standards are based on the newer YCbCr conversion. YCbCr is closely\nrelated to YUV (the Cb and Cr signals carry the blue and red color difference signals and have\nmore useful mnemonics than UV) but uses different scale factors to ﬁt within the eight-bit\nrange available with digital signals.\nFor video, the Y’ signal is re-scaled to ﬁt within the [16 . . . 235] range of values, while\nthe Cb and Cr signals are scaled to ﬁt within [16 . . . 240] (Gomes and Velho 1997; Fairchild\n2005). For still images, the JPEG standard uses the full eight-bit range with no reserved\nvalues,\n\n\nY ′\nCb\nCr\n\n=\n\n\n0.299\n0.587\n0.114\n−0.168736\n−0.331264\n0.5\n0.5\n−0.418688\n−0.081312\n\n\n\n\nR′\nG′\nB′\n\n+\n\n\n0\n128\n128\n\n,\n(2.115)\nwhere the R’G’B’ values are the eight-bit gamma-compressed color components (i.e., the\nactual RGB values we obtain when we open up or display a JPEG image). For most appli-\ncations, this formula is not that important, since your image reading software will directly\n90\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprovide you with the eight-bit gamma-compressed R’G’B’ values. However, if you are trying\nto do careful image deblocking (Exercise 3.30), this information may be useful.\nAnother color space you may come across is hue, saturation, value (HSV), which is a pro-\njection of the RGB color cube onto a non-linear chroma angle, a radial saturation percentage,\nand a luminance-inspired value. In more detail, value is deﬁned as either the mean or maxi-\nmum color value, saturation is deﬁned as scaled distance from the diagonal, and hue is deﬁned\nas the direction around a color wheel (the exact formulas are described by Hall (1989); Foley,\nvan Dam, Feiner et al. (1995)). Such a decomposition is quite natural in graphics applications\nsuch as color picking (it approximates the Munsell chart for color description). Figure 2.32l–\nn shows an HSV representation of a sample color image, where saturation is encoded using a\ngray scale (saturated = darker) and hue is depicted as a color.\nIf you want your computer vision algorithm to only affect the value (luminance) of an\nimage and not its saturation or hue, a simpler solution is to use either the Y xy (luminance +\nchromaticity) coordinates deﬁned in (2.104) or the even simpler color ratios,\nr =\nR\nR + G + B , g =\nG\nR + G + B , b =\nB\nR + G + B\n(2.116)\n(Figure 2.32e–h). After manipulating the luma (2.112), e.g., through the process of histogram\nequalization (Section 3.1.4), you can multiply each color ratio by the ratio of the new to old\nluma to obtain an adjusted RGB triplet.\nWhile all of these color systems may sound confusing, in the end, it often may not mat-\nter that much which one you use. Poynton, in his Color FAQ, http://www.poynton.com/\nColorFAQ.html, notes that the perceptually motivated L*a*b* system is qualitatively similar\nto the gamma-compressed R’G’B’ system we mostly deal with, since both have a fractional\npower scaling (which approximates a logarithmic response) between the actual intensity val-\nues and the numbers being manipulated. As in all cases, think carefully about what you are\ntrying to accomplish before deciding on a technique to use.24\n2.3.3 Compression\nThe last stage in a camera’s processing pipeline is usually some form of image compression\n(unless you are using a lossless compression scheme such as camera RAW or PNG).\nAll color video and image compression algorithms start by converting the signal into\nYCbCr (or some closely related variant), so that they can compress the luminance signal with\nhigher ﬁdelity than the chrominance signal. (Recall that the human visual system has poorer\n24 If you are at a loss for questions at a conference, you can always ask why the speaker did not use a perceptual\ncolor space, such as L*a*b*. Conversely, if they did use L*a*b*, you can ask if they have any concrete evidence that\nthis works better than regular colors.",
  "image_path": "page_111.jpg",
  "pages": [
    110,
    111,
    112
  ]
}