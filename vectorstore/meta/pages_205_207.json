{
  "doc_id": "pages_205_207",
  "text": "3.7 Global optimization\n183\ngradient descent (Metropolis, Rosenbluth, Rosenbluth et al. 1953; Geman and Geman 1984).\nWhen the amount of noise is decreased over time, this technique is known as simulated an-\nnealing (Kirkpatrick, Gelatt, and Vecchi 1983; Carnevali, Coletti, and Patarnello 1985; Wol-\nberg and Pavlidis 1985; Swendsen and Wang 1987) and was ﬁrst popularized in computer\nvision by Geman and Geman (1984) and later applied to stereo matching by Barnard (1989),\namong others.\nEven this technique, however, does not perform that well (Boykov, Veksler, and Zabih\n2001). For binary images, a much better technique, introduced to the computer vision com-\nmunity by Boykov, Veksler, and Zabih (2001) is to re-formulate the energy minimization as\na max-ﬂow/min-cut graph optimization problem (Greig, Porteous, and Seheult 1989). This\ntechnique has informally come to be known as graph cuts in the computer vision community\n(Boykov and Kolmogorov 2010). For simple energy functions, e.g., those where the penalty\nfor non-identical neighboring pixels is a constant, this algorithm is guaranteed to produce the\nglobal minimum. Kolmogorov and Zabih (2004) formally characterize the class of binary\nenergy potentials (regularity conditions) for which these results hold, while newer work by\nKomodakis, Tziritas, and Paragios (2008) and Rother, Kolmogorov, Lempitsky et al. (2007)\nprovide good algorithms for the cases when they do not.\nIn addition to the above mentioned techniques, a number of other optimization approaches\nhave been developed for MRF energy minimization, such as (loopy) belief propagation and\ndynamic programming (for one-dimensional problems). These are discussed in more detail\nin Appendix B.5 as well as the comparative survey paper by Szeliski, Zabih, Scharstein et al.\n(2008).\nOrdinal-valued MRFs\nIn addition to binary images, Markov random ﬁelds can be applied to ordinal-valued labels\nsuch as grayscale images or depth maps. The term ”ordinal” indicates that the labels have an\nimplied ordering, e.g., that higher values are lighter pixels. In the next section, we look at\nunordered labels, such as source image labels for image compositing.\nIn many cases, it is common to extend the binary data and smoothness prior terms as\nEd(i, j) = w(i, j)ρd(f(i, j) −d(i, j))\n(3.112)\nand\nEp(i, j) = sx(i, j)ρp(f(i, j) −f(i + 1, j)) + sy(i, j)ρp(f(i, j) −f(i, j + 1)),\n(3.113)\nwhich are robust generalizations of the quadratic penalty terms (3.101) and (3.100), ﬁrst\nintroduced in (3.105). As before, the w(i, j), sx(i, j) and sy(i, j) weights can be used to\nlocally control the data weighting and the horizontal and vertical smoothness. Instead of\n184\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 3.57\nGrayscale image denoising and inpainting: (a) original image; (b) image\ncorrupted by noise and with missing data (black bar); (c) image restored using loopy be-\nlief propagation; (d) image restored using expansion move graph cuts. Images are from\nhttp://vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharstein et al. 2008).\nusing a quadratic penalty, however, a general monotonically increasing penalty function ρ()\nis used. (Different functions can be used for the data and smoothness terms.) For example,\nρp can be a hyper-Laplacian penalty\nρp(d) = |d|p, p < 1,\n(3.114)\nwhich better encodes the distribution of gradients (mainly edges) in an image than either a\nquadratic or linear (total variation) penalty.24\nLevin and Weiss (2007) use such a penalty\nto separate a transmitted and reﬂected image (Figure 8.17) by encouraging gradients to lie in\none or the other image, but not both. More recently, Levin, Fergus, Durand et al. (2007) use\nthe hyper-Laplacian as a prior for image deconvolution (deblurring) and Krishnan and Fergus\n(2009) develop a faster algorithm for solving such problems. For the data penalty, ρd can be\nquadratic (to model Gaussian noise) or the log of a contaminated Gaussian (Appendix B.3).\nWhen ρp is a quadratic function, the resulting Markov random ﬁeld is called a Gaussian\nMarkov random ﬁeld (GMRF) and its minimum can be found by sparse linear system solving\n(3.103). When the weighting functions are uniform, the GMRF becomes a special case of\nWiener ﬁltering (Section 3.4.3). Allowing the weighting functions to depend on the input\nimage (a special kind of conditional random ﬁeld, which we describe below) enables quite\nsophisticated image processing algorithms to be performed, including colorization (Levin,\nLischinski, and Weiss 2004), interactive tone mapping (Lischinski, Farbman, Uyttendaele et\nal. 2006a), natural image matting (Levin, Lischinski, and Weiss 2008), and image restoration\n(Tappen, Liu, Freeman et al. 2007).\n24 Note that, unlike a quadratic penalty, the sum of the horizontal and vertical derivative p-norms is not rotationally\ninvariant. A better approach may be to locally estimate the gradient direction and to impose different norms on the\nperpendicular and parallel components, which Roth and Black (2007b) call a steerable random ﬁeld.\n3.7 Global optimization\n185\n(a) initial labeling\n(b) standard move\n(c) α-β-swap\n(d) α-expansion\nFigure 3.58\nMulti-level graph optimization from (Boykov, Veksler, and Zabih 2001) c⃝\n2001 IEEE: (a) initial problem conﬁguration; (b) the standard move only changes one pixel;\n(c) the α-β-swap optimally exchanges all α and β-labeled pixels; (d) the α-expansion move\noptimally selects among current pixel values and the α label.\nWhen ρd or ρp are non-quadratic functions, gradient descent techniques such as non-\nlinear least squares or iteratively re-weighted least squares can sometimes be used (Ap-\npendix A.3). However, if the search space has lots of local minima, as is the case for stereo\nmatching (Barnard 1989; Boykov, Veksler, and Zabih 2001), more sophisticated techniques\nare required.\nThe extension of graph cut techniques to multi-valued problems was ﬁrst proposed by\nBoykov, Veksler, and Zabih (2001). In their paper, they develop two different algorithms,\ncalled the swap move and the expansion move, which iterate among a series of binary labeling\nsub-problems to ﬁnd a good solution (Figure 3.58). Note that a global solution is generally not\nachievable, as the problem is provably NP-hard for general energy functions. Because both\nthese algorithms use a binary MRF optimization inside their inner loop, they are subject to the\nkind of constraints on the energy functions that occur in the binary labeling case (Kolmogorov\nand Zabih 2004). Appendix B.5.4 discusses these algorithms in more detail, along with some\nmore recently developed approaches to this problem.\nAnother MRF inference technique is belief propagation (BP). While belief propagation\nwas originally developed for inference over trees, where it is exact (Pearl 1988), it has more\nrecently been applied to graphs with loops such as Markov random ﬁelds (Freeman, Pasz-\ntor, and Carmichael 2000; Yedidia, Freeman, and Weiss 2001). In fact, some of the better\nperforming stereo-matching algorithms use loopy belief propagation (LBP) to perform their\ninference (Sun, Zheng, and Shum 2003). LBP is discussed in more detail in Appendix B.5.3\nas well as the comparative survey paper on MRF optimization (Szeliski, Zabih, Scharstein et\nal. 2008).\nFigure 3.57 shows an example of image denoising and inpainting (hole ﬁlling) using a\nnon-quadratic energy function (non-Gaussian MRF). The original image has been corrupted\nby noise and a portion of the data has been removed (the black bar). In this case, the loopy",
  "image_path": "page_206.jpg",
  "pages": [
    205,
    206,
    207
  ]
}