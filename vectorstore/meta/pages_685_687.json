{
  "doc_id": "pages_685_687",
  "text": "14.1 Object detection\n663\n(a)\n(b)\nFigure 14.6\nSimple features used in boosting-based face detector (Viola and Jones 2004)\nc⃝2004 Springer: (a) difference of rectangle feature composed of 2–4 different rectangles\n(pixels inside the white rectangles are subtracted from the gray ones); (b) the ﬁrst and second\nfeatures selected by AdaBoost. The ﬁrst feature measures the differences in intensity between\nthe eyes and the cheeks, the second one between the eyes and the bridge of the nose.\nHeisele, Serre, and Poggio 2007) and are a widely used tool in object recognition in general.\nBoosting.\nOf all the face detectors currently in use, the one introduced by Viola and Jones\n(2004) is probably the best known and most widely used. Their technique was the ﬁrst to\nintroduce the concept of boosting to the computer vision community, which involves train-\ning a series of increasingly discriminating simple classiﬁers and then blending their outputs\n(Hastie, Tibshirani, and Friedman 2001; Bishop 2006).\nIn more detail, boosting involves constructing a classiﬁer h(x) as a sum of simple weak\nlearners,\nh(x) = sign\n\n\nm−1\nX\nj=0\nαjhj(x)\n\n,\n(14.1)\nwhere each of the weak learners hj(x) is an extremely simple function of the input, and hence\nis not expected to contribute much (in isolation) to the classiﬁcation performance.\nIn most variants of boosting, the weak learners are threshold functions,\nhj(x) = aj[fj < θj] + bj[fj ≥θj] =\n(\naj\nif fj < θj\nbj\notherwise,\n(14.2)\nwhich are also known as decision stumps (basically, the simplest possible version of decision\ntrees). In most cases, it is also traditional (and simpler) to set aj and bj to ±1, i.e., aj = −sj,\nbj = +sj, so that only the feature fj, the threshold value θj, and the polarity of the threshold\nsj ∈±1 need to be selected.4\n4Some variants, such as that of Viola and Jones (2004), use (aj, bj) ∈[0, 1] and adjust the learning algorithm\n664\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWeak classifier 1\nWeights increased\nWeak classifier 2\nWeights increased\nWeak classifier 3\nFinal classifier\nFigure 14.7 Schematic illustration of boosting, courtesy of Svetlana Lazebnik, after origi-\nnal illustrations from Paul Viola and David Lowe. After each weak classiﬁer (decision stump\nor hyperplane) is selected, data points that are erroneously classiﬁed have their weights in-\ncreased. The ﬁnal classiﬁer is a linear combination of the simple weak classiﬁers.\nIn many applications of boosting, the features are simply coordinate axes xk, i.e., the\nboosting algorithm selects one of the input vector components as the best one to threshold. In\nViola and Jones’ face detector, the features are differences of rectangular regions in the input\npatch, as shown in Figure 14.6. The advantage of using these features is that, while they are\nmore discriminating than single pixels, they are extremely fast to compute once a summed\narea table has been pre-computed, as described in Section 3.2.3 (3.31–3.32). Essentially, for\nthe cost of an O(N) pre-computation phase (where N is the number of pixels in the image),\nsubsequent differences of rectangles can be computed in 4r additions or subtractions, where\nr ∈{2, 3, 4} is the number of rectangles in the feature.\nThe key to the success of boosting is the method for incrementally selecting the weak\nlearners and for re-weighting the training examples after each stage (Figure 14.7). The Ad-\naBoost (Adaptive Boosting) algorithm (Hastie, Tibshirani, and Friedman 2001; Bishop 2006)\ndoes this by re-weighting each sample as a function of whether it is correctly classiﬁed at each\nstage, and using the stage-wise average classiﬁcation error to determine the ﬁnal weightings\nαj among the weak classiﬁers, as described in Algorithm 14.1. While the resulting classi-\nﬁer is extremely fast in practice, the training time can be quite slow (in the order of weeks),\nbecause of the large number of feature (difference of rectangle) hypotheses that need to be\nexamined at each stage.\nTo further increase the speed of the detector, it is possible to create a cascade of classiﬁers,\nwhere each classiﬁer uses a small number of tests (say, a two-term AdaBoost classiﬁer) to\nreject a large fraction of non-faces while trying to pass through all potential face candidates\n(Fleuret and Geman 2001; Viola and Jones 2004). An even faster algorithm for performing\ncascade learning has recently been developed by Brubaker, Wu, Sun et al. (2008).\naccordingly.\n14.1 Object detection\n665\n1. Input the positive and negative training examples along with their labels {(xi, yi)},\nwhere yi = 1 for positive (face) examples and yi = −1 for negative examples.\n2. Initialize all the weights to wi,1 ←\n1\nN , where N is the number of training exam-\nples. (Viola and Jones (2004) use a separate N1 and N2 for positive and negative\nexamples.)\n3. For each training stage j = 1 . . . M:\n(a) Renormalize the weights so that they sum up to 1 (divide them by their sum).\n(b) Select the best classiﬁer hj(x; fj, θj, sj) by ﬁnding the one that minimizes\nthe weighted classiﬁcation error\nej\n=\nN−1\nX\ni=0\nwi,jei,j,\n(14.3)\nei,j\n=\n1 −δ(yi, hj(xi; fj, θj, sj)).\n(14.4)\nFor any given fj function, the optimal values of (θj, sj) can be found in\nlinear time using a variant of weighted median computation (Exercise 14.2).\n(c) Compute the modiﬁed error rate βj and classiﬁer weight αj,\nβj =\nej\n1 −ej\nand\nαj = −log βj.\n(14.5)\n(d) Update the weights according to the classiﬁcation errors ei,j\nwi,j+1 ←wi,jβ1−ei,j\nj\n,\n(14.6)\ni.e., downweight the training samples that were correctly classiﬁed in pro-\nportion to the overall classiﬁcation error.\n4. Set the ﬁnal classiﬁer to\nh(x) = sign\n\n\nm−1\nX\nj=0\nαjhj(x)\n\n.\n(14.7)\nAlgorithm 14.1\nThe AdaBoost training algorithm, adapted from Hastie, Tibshirani, and\nFriedman (2001), Viola and Jones (2004), and Bishop (2006).",
  "image_path": "page_686.jpg",
  "pages": [
    685,
    686,
    687
  ]
}