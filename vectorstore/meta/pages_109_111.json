{
  "doc_id": "pages_109_111",
  "text": "2.3 The digital camera\n87\nY\nY’\nY’ = Y1/γ\nY’\nY\nY = Y’γ\nquantization \nnoise\nvisible \nnoise\nFigure 2.31 Gamma compression: (a) The relationship between the input signal luminance\nY and the transmitted signal Y ′ is given by Y ′ = Y 1/γ. (b) At the receiver, the signal Y ′ is\nexponentiated by the factor γ, ˆY = Y ′γ. Noise introduced during transmission is squashed in\nthe dark regions, which corresponds to the more noise-sensitive region of the visual system.\nactually perform a color twist, i.e., they use a general 3 × 3 color transform matrix.21 Exer-\ncise 2.9 has you explore some of these issues.\nGamma\nIn the early days of black and white television, the phosphors in the CRT used to display\nthe TV signal responded non-linearly to their input voltage. The relationship between the\nvoltage and the resulting brightness was characterized by a number called gamma (γ), since\nthe formula was roughly\nB = V γ,\n(2.110)\nwith a γ of about 2.2. To compensate for this effect, the electronics in the TV camera would\npre-map the sensed luminance Y through an inverse gamma,\nY ′ = Y\n1\nγ ,\n(2.111)\nwith a typical value of 1\nγ = 0.45.\nThe mapping of the signal through this non-linearity before transmission had a beneﬁcial\nside effect: noise added during transmission (remember, these were analog days!) would be\nreduced (after applying the gamma at the receiver) in the darker regions of the signal where\nit was more visible (Figure 2.31).22 (Remember that our visual system is roughly sensitive to\nrelative differences in luminance.)\n21 Those of you old enough to remember the early days of color television will naturally think of the hue adjustment\nknob on the television set, which could produce truly bizarre results.\n22 A related technique called companding was the basis of the Dolby noise reduction systems used with audio\ntapes.\n88\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen color television was invented, it was decided to separately pass the red, green, and\nblue signals through the same gamma non-linearity before combining them for encoding.\nToday, even though we no longer have analog noise in our transmission systems, signals are\nstill quantized during compression (see Section 2.3.3), so applying inverse gamma to sensed\nvalues is still useful.\nUnfortunately, for both computer vision and computer graphics, the presence of gamma\nin images is often problematic. For example, the proper simulation of radiometric phenomena\nsuch as shading (see Section 2.2 and Equation (2.87)) occurs in a linear radiance space. Once\nall of the computations have been performed, the appropriate gamma should be applied before\ndisplay. Unfortunately, many computer graphics systems (such as shading models) operate\ndirectly on RGB values and display these values directly. (Fortunately, newer color imaging\nstandards such as the 16-bit scRGB use a linear space, which makes this less of a problem\n(Glassner 1995).)\nIn computer vision, the situation can be even more daunting. The accurate determination\nof surface normals, using a technique such as photometric stereo (Section 12.1.1) or even a\nsimpler operation such as accurate image deblurring, require that the measurements be in a\nlinear space of intensities. Therefore, it is imperative when performing detailed quantitative\ncomputations such as these to ﬁrst undo the gamma and the per-image color re-balancing\nin the sensed color values. Chakrabarti, Scharstein, and Zickler (2009) develop a sophisti-\ncated 24-parameter model that is a good match to the processing performed by today’s digital\ncameras; they also provide a database of color images you can use for your own testing.23\nFor other vision applications, however, such as feature detection or the matching of sig-\nnals in stereo and motion estimation, this linearization step is often not necessary. In fact,\ndetermining whether it is necessary to undo gamma can take some careful thinking, e.g., in\nthe case of compensating for exposure variations in image stitching (see Exercise 2.7).\nIf all of these processing steps sound confusing to model, they are. Exercise 2.10 has you\ntry to tease apart some of these phenomena using empirical investigation, i.e., taking pictures\nof color charts and comparing the RAW and JPEG compressed color values.\nOther color spaces\nWhile RGB and XYZ are the primary color spaces used to describe the spectral content (and\nhence tri-stimulus response) of color signals, a variety of other representations have been\ndeveloped both in video and still image coding and in computer graphics.\nThe earliest color representation developed for video transmission was the YIQ standard\ndeveloped for NTSC video in North America and the closely related YUV standard developed\nfor PAL in Europe. In both of these cases, it was desired to have a luma channel Y (so called\n23 http://vision.middlebury.edu/color/.\n2.3 The digital camera\n89\nsince it only roughly mimics true luminance) that would be comparable to the regular black-\nand-white TV signal, along with two lower frequency chroma channels.\nIn both systems, the Y signal (or more appropriately, the Y’ luma signal since it is gamma\ncompressed) is obtained from\nY ′\n601 = 0.299R′ + 0.587G′ + 0.114B′,\n(2.112)\nwhere R’G’B’ is the triplet of gamma-compressed color components. When using the newer\ncolor deﬁnitions for HDTV in BT.709, the formula is\nY ′\n709 = 0.2125R′ + 0.7154G′ + 0.0721B′.\n(2.113)\nThe UV components are derived from scaled versions of (B′−Y ′) and (R′−Y ′), namely,\nU = 0.492111(B′ −Y ′) and V = 0.877283(R′ −Y ′),\n(2.114)\nwhereas the IQ components are the UV components rotated through an angle of 33◦. In\ncomposite (NTSC and PAL) video, the chroma signals were then low-pass ﬁltered horizon-\ntally before being modulated and superimposed on top of the Y’ luma signal. Backward\ncompatibility was achieved by having older black-and-white TV sets effectively ignore the\nhigh-frequency chroma signal (because of slow electronics) or, at worst, superimposing it as\na high-frequency pattern on top of the main signal.\nWhile these conversions were important in the early days of computer vision, when frame\ngrabbers would directly digitize the composite TV signal, today all digital video and still\nimage compression standards are based on the newer YCbCr conversion. YCbCr is closely\nrelated to YUV (the Cb and Cr signals carry the blue and red color difference signals and have\nmore useful mnemonics than UV) but uses different scale factors to ﬁt within the eight-bit\nrange available with digital signals.\nFor video, the Y’ signal is re-scaled to ﬁt within the [16 . . . 235] range of values, while\nthe Cb and Cr signals are scaled to ﬁt within [16 . . . 240] (Gomes and Velho 1997; Fairchild\n2005). For still images, the JPEG standard uses the full eight-bit range with no reserved\nvalues,\n\n\nY ′\nCb\nCr\n\n=\n\n\n0.299\n0.587\n0.114\n−0.168736\n−0.331264\n0.5\n0.5\n−0.418688\n−0.081312\n\n\n\n\nR′\nG′\nB′\n\n+\n\n\n0\n128\n128\n\n,\n(2.115)\nwhere the R’G’B’ values are the eight-bit gamma-compressed color components (i.e., the\nactual RGB values we obtain when we open up or display a JPEG image). For most appli-\ncations, this formula is not that important, since your image reading software will directly",
  "image_path": "page_110.jpg",
  "pages": [
    109,
    110,
    111
  ]
}