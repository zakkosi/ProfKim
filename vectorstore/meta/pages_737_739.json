{
  "doc_id": "pages_737_739",
  "text": "14.5 Context and scene understanding\n715\n(a)\n(b)\n(c)\nFigure 14.51 Recognition by scene alignment (Russell, Torralba, Liu et al. 2007): (a) input\nimage; (b) matched images with similar scene conﬁgurations; (c) ﬁnal labeling of the input\nimage.\ntion 14.1.1). Some of the best-performing techniques on challenging recognition benchmarks\n(Varma and Ray 2007; Felzenszwalb, McAllester, and Ramanan 2008; Fritz and Schiele 2008;\nVedaldi, Gulshan, Varma et al. 2009) rely heavily on the latest machine learning techniques,\nwhose development is often being driven by challenging vision problems (Freeman, Perona,\nand Sch¨olkopf 2008).\nA distinction sometimes made in the recognition community is between problems where\nmost of the variables of interest (say, parts) are already (partially) labeled and systems that\nlearn more of the problem structure with less supervision (Fergus, Perona, and Zisserman\n2007; Fei-Fei, Fergus, and Perona 2006). In fact, recent work by Sivic, Russell, Zisserman et\nal. (2008) has demonstrated the ability to learn visual hierarchies (hierarchies of object parts\nwith related visual appearance) and scene segmentations in a totally unsupervised framework.\nPerhaps the most dramatic change in the recognition community has been the appearance\nof very large databases of training images.20 Early learning-based algorithms, such as those\nfor face and pedestrian detection (Section 14.1), used relatively few (in the hundreds) labeled\nexamples to train recognition algorithm parameters (say, the thresholds used in boosting). To-\nday, some recognition algorithms use databases such as LabelMe (Russell, Torralba, Murphy\net al. 2008), which contain tens of thousands of labeled examples.\nThe existence of such large databases opens up the possibility of matching directly against\nthe training images rather than using them to learn the parameters of recognition algorithms.\nRussell, Torralba, Liu et al. (2007) describe a system where a new image is matched against\neach of the training images, from which a consensus labeling for the unknown objects in\nthe scene can be inferred, as shown in Figure 14.51. Malisiewicz and Efros (2008) start\nby over-segmenting each image and then use the LabelMe database to search for similar\nimages and conﬁgurations in order to obtain per-pixel category labelings. It is also possible\nto combine feature-based correspondence algorithms with large labeled databases to perform\n20 We have already seen some computational photography applications of such databases in Section 14.4.4.\n716\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.52 Recognition using tiny images (Torralba, Freeman, and Fergus 2008) c⃝2008\nIEEE: columns (a) and (c) show sample input images and columns (b) and (d) show the\ncorresponding 16 nearest neighbors in the database of 80 million tiny images.\nsimultaneous recognition and segmentation (Liu, Yuen, and Torralba 2009).\nWhen the database of images becomes large enough, it is even possible to directly match\ncomplete images with the expectation of ﬁnding a good match. Torralba, Freeman, and Fergus\n(2008) start with a database of 80 million tiny (32 × 32) images and compensate for the poor\naccuracy in their image labels, which are collected automatically from the Internet, by using\na semantic taxonomy (Wordnet) to infer the most likely labels for a new image. Somewhere\nin the 80 million images, there are enough examples to associate some set of images with\neach of the 75,000 non-abstract nouns in Wordnet that they use in their system. Some sample\nrecognition results are shown in Figure 14.52.\nAnother example of a large labeled database of images is ImageNet (Deng, Dong, Socher\net al. 2009), which is collecting images for the 80,000 nouns (synonym sets) in WordNet\n(Fellbaum 1998). As of April 2010, about 500–1000 carefully vetted examples for 14841\n14.5 Context and scene understanding\n717\nFigure 14.53\nImageNet (Deng, Dong, Socher et al. 2009) c⃝2009 IEEE. This database\ncontains over 500 carefully vetted images for each of 14,841 (as of April, 2010) nouns from\nthe WordNet hierarchy.\nsynsets have been collected (Figure 14.53). The paper by Deng, Dong, Socher et al. (2009)\nalso has a nice review of related databases.\nAs we mentioned in Section 14.4.3, the existence of large databases of partially labeled\nInternet imagery has given rise to a new sub-ﬁeld of Internet computer vision, with its own\nworkshops21 and a special journal issue (Avidan, Baker, and Shan 2010).\n14.5.2 Application: Image search\nEven though visual recognition algorithms are by some measures still in their infancy, they\nare already starting to have some impact on image search, i.e., the retrieval of images from the\nWeb using combinations of keywords and visual similarity. Today, most image search engines\nrely mostly on textual keywords found in captions, nearby text, and ﬁlenames, augmented by\nuser click-through data (Craswell and Szummer 2007). As recognition algorithms continue\nto improve, however, visual features and visual similarity will start being used to recognize\nimages with missing or erroneous keywords.\nThe topic of searching by visual similarity has a long history and goes by a variety of\nnames, including content-based image retrieval (CBIR) (Smeulders, Worring, Santini et al.\n2000; Lew, Sebe, Djeraba et al. 2006; Vasconcelos 2007; Datta, Joshi, Li et al. 2008) and\nquery by image content (QBIC) (Flickner, Sawhney, Niblack et al. 1995). Original publica-\ntions in these ﬁelds were based primarily on simple whole-image similarity metrics, such as\ncolor and texture (Swain and Ballard 1991; Jacobs, Finkelstein, and Salesin 1995; Manjunathi\nand Ma 1996).\n21 http://www.internetvisioner.org/.",
  "image_path": "page_738.jpg",
  "pages": [
    737,
    738,
    739
  ]
}