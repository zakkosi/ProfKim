{
  "doc_id": "pages_198_200",
  "text": "176\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nsecond-order norm is called the thin-plate spline, since it approximates the behavior of thin\nplates (e.g., ﬂexible steel) under small deformations. A blend of the two is called the thin-\nplate spline under tension; versions of these formulas where each derivative term is mul-\ntiplied by a local weighting function are called controlled-continuity splines (Terzopoulos\n1988). Figure 3.54 shows a simple example of a controlled-continuity interpolator ﬁt to nine\nscattered data points. In practice, it is more common to ﬁnd ﬁrst-order smoothness terms\nused with images and ﬂow ﬁelds (Section 8.4) and second-order smoothness associated with\nsurfaces (Section 12.3.1).\nIn addition to the smoothness term, regularization also requires a data term (or data\npenalty). For scattered data interpolation (Nielson 1993), the data term measures the dis-\ntance between the function f(x, y) and a set of data points di = d(xi, yi),\nEd =\nX\ni\n[f(xi, yi) −di]2.\n(3.96)\nFor a problem like noise removal, a continuous version of this measure can be used,\nEd =\nZ\n[f(x, y) −d(x, y)]2 dx dy.\n(3.97)\nTo obtain a global energy that can be minimized, the two energy terms are usually added\ntogether,\nE = Ed + λEs,\n(3.98)\nwhere Es is the smoothness penalty (E1, E2 or some weighted blend) and λ is the regulariza-\ntion parameter, which controls how smooth the solution should be.\nIn order to ﬁnd the minimum of this continuous problem, the function f(x, y) is usually\nﬁrst discretized on a regular grid.21 The most principled way to perform this discretization is\nto use ﬁnite element analysis, i.e., to approximate the function with a piecewise continuous\nspline, and then perform the analytic integration (Bathe 2007).\nFortunately, for both the ﬁrst-order and second-order smoothness functionals, the judi-\ncious selection of appropriate ﬁnite elements results in particularly simple discrete forms\n(Terzopoulos 1983). The corresponding discrete smoothness energy functions become\nE1\n=\nX\ni,j\nsx(i, j)[f(i + 1, j) −f(i, j) −gx(i, j)]2\n(3.99)\n+ sy(i, j)[f(i, j + 1) −f(i, j) −gy(i, j)]2\nand\nE2\n=\nh−2 X\ni,j\ncx(i, j)[f(i + 1, j) −2f(i, j) + f(i −1, j)]2\n(3.100)\n21 The alternative of using kernel basis functions centered on the data points (Boult and Kender 1986; Nielson\n1993) is discussed in more detail in Section 12.3.1.\n3.7 Global optimization\n177\n+ 2cm(i, j)[f(i + 1, j + 1) −f(i + 1, j) −f(i, j + 1) + f(i, j)]2\n+ cy(i, j)[f(i, j + 1) −2f(i, j) + f(i, j −1)]2,\nwhere h is the size of the ﬁnite element grid. The h factor is only important if the energy is\nbeing discretized at a variety of resolutions, as in coarse-to-ﬁne or multigrid techniques.\nThe optional smoothness weights sx(i, j) and sy(i, j) control the location of horizon-\ntal and vertical tears (or weaknesses) in the surface. For other problems, such as coloriza-\ntion (Levin, Lischinski, and Weiss 2004) and interactive tone mapping (Lischinski, Farbman,\nUyttendaele et al. 2006a), they control the smoothness in the interpolated chroma or expo-\nsure ﬁeld and are often set inversely proportional to the local luminance gradient strength.\nFor second-order problems, the crease variables cx(i, j), cm(i, j), and cy(i, j) control the\nlocations of creases in the surface (Terzopoulos 1988; Szeliski 1990a).\nThe data values gx(i, j) and gy(i, j) are gradient data terms (constraints) used by al-\ngorithms, such as photometric stereo (Section 12.1.1), HDR tone mapping (Section 10.2.1)\n(Fattal, Lischinski, and Werman 2002), Poisson blending (Section 9.3.4) (P´erez, Gangnet,\nand Blake 2003), and gradient-domain blending (Section 9.3.4) (Levin, Zomet, Peleg et al.\n2004). They are set to zero when just discretizing the conventional ﬁrst-order smoothness\nfunctional (3.94).\nThe two-dimensional discrete data energy is written as\nEd =\nX\ni,j\nw(i, j)[f(i, j) −d(i, j)]2,\n(3.101)\nwhere the local weights w(i, j) control how strongly the data constraint is enforced. These\nvalues are set to zero where there is no data and can be set to the inverse variance of the data\nmeasurements when there is data (as discussed by Szeliski (1989) and in Section 3.7.2).\nThe total energy of the discretized problem can now be written as a quadratic form\nE = Ed + λEs = xT Ax −2xT b + c,\n(3.102)\nwhere x = [f(0, 0) . . . f(m −1, n −1)] is called the state vector.22\nThe sparse symmetric positive-deﬁnite matrix A is called the Hessian since it encodes the\nsecond derivative of the energy function.23 For the one-dimensional, ﬁrst-order problem, A\nis tridiagonal; for the two-dimensional, ﬁrst-order problem, it is multi-banded with ﬁve non-\nzero entries per row. We call b the weighted data vector. Minimizing the above quadratic\nform is equivalent to solving the sparse linear system\nAx = b,\n(3.103)\n22 We use x instead of f because this is the more common form in the numerical analysis literature (Golub and\nVan Loan 1996).\n23 In numerical analysis, A is called the coefﬁcient matrix (Saad 2003); in ﬁnite element analysis (Bathe 2007), it\nis called the stiffness matrix.\n178\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure 3.55\nGraphical model interpretation of ﬁrst-order regularization. The white circles\nare the unknowns f(i, j) while the dark circles are the input data d(i, j). In the resistive grid\ninterpretation, the d and f values encode input and output voltages and the black squares\ndenote resistors whose conductance is set to sx(i, j), sy(i, j), and w(i, j). In the spring-mass\nsystem analogy, the circles denote elevations and the black squares denote springs. The same\ngraphical model can be used to depict a ﬁrst-order Markov random ﬁeld (Figure 3.56).\nwhich can be done using a variety of sparse matrix techniques, such as multigrid (Briggs,\nHenson, and McCormick 2000) and hierarchical preconditioners (Szeliski 2006b), as de-\nscribed in Appendix A.5.\nWhile regularization was ﬁrst introduced to the vision community by Poggio, Torre, and\nKoch (1985) and Terzopoulos (1986b) for problems such as surface interpolation, it was\nquickly adopted by other vision researchers for such varied problems as edge detection (Sec-\ntion 4.2), optical ﬂow (Section 8.4), and shape from shading (Section 12.1) (Poggio, Torre,\nand Koch 1985; Horn and Brooks 1986; Terzopoulos 1986b; Bertero, Poggio, and Torre 1988;\nBrox, Bruhn, Papenberg et al. 2004). Poggio, Torre, and Koch (1985) also showed how the\ndiscrete energy deﬁned by Equations (3.100–3.101) could be implemented in a resistive grid,\nas shown in Figure 3.55. In computational photography (Chapter 10), regularization and its\nvariants are commonly used to solve problems such as high-dynamic range tone mapping\n(Fattal, Lischinski, and Werman 2002; Lischinski, Farbman, Uyttendaele et al. 2006a), Pois-\nson and gradient-domain blending (P´erez, Gangnet, and Blake 2003; Levin, Zomet, Peleg et\nal. 2004; Agarwala, Dontcheva, Agrawala et al. 2004), colorization (Levin, Lischinski, and\nWeiss 2004), and natural image matting (Levin, Lischinski, and Weiss 2008).\nRobust regularization\nWhile regularization is most commonly formulated using quadratic (L2) norms (compare\nwith the squared derivatives in (3.92–3.95) and squared differences in (3.100–3.101)), it can",
  "image_path": "page_199.jpg",
  "pages": [
    198,
    199,
    200
  ]
}