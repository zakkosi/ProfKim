{
  "doc_id": "pages_735_737",
  "text": "14.5 Context and scene understanding\n713\n(a)\n(b)\n(c)\nFigure 14.50 Contextual scene models for object recognition (Sudderth, Torralba, Freeman\net al. 2008) c⃝2008 Springer: (a) some street scenes and their corresponding labels (magenta\n= buildings, red = cars, green = trees, blue = road); (b) some ofﬁce scenes (red = computer\nscreen, green = keyboard, blue = mouse); (c) learned contextual models built from these\nlabeled scenes. The top row shows a sample label image and the distribution of the objects\nrelative to the center red (car or screen) object. The bottom rows show the distributions of\nparts that make up each object.\nrecognition architecture (Murphy, Torralba, and Freeman 2003; Sudderth, Torralba, Freeman\net al. 2008; Crandall and Huttenlocher 2007).\nConsider the street and ofﬁce scenes shown in Figure 14.50a–b. If we have enough train-\ning images with labeled regions, such as buildings, cars, and roads or monitors, keyboards,\nand mice, we can develop a geometric model for describing their relative positions. Sud-\nderth, Torralba, Freeman et al. (2008) develop such a model, which can be thought of as a\ntwo-level constellation model. At the top level, the distributions of objects relative to each\nother (say, buildings with respect to cars) is modeled as a Gaussian (Figure 14.50c, upper\nright corners). At the bottom level, the distribution of parts (afﬁne covariant features) with\nrespect to the object center is modeled using a mixture of Gaussians (Figure 14.50c, lower\ntwo rows). However, since the number of objects in the scene and parts in each object is\nunknown, a latent Dirichlet process (LDP) is used to model object and part creation in a gen-\nerative framework. The distributions for all of the objects and parts are learned from a large\nlabeled database and then later used during inference (recognition) to label the elements of a\nscene.\nAnother example of context is in simultaneous segmentation and recognition (Section 14.4.3)\n714\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Figures 14.44–14.45), where the arrangements of various objects in a scene are used as part\nof the labeling process. Torralba, Murphy, and Freeman (2004) describe a conditional random\nﬁeld where the estimated locations of building and roads inﬂuence the detection of cars, and\nwhere boosting is used to learn the structure of the CRF. Rabinovich, Vedaldi, Galleguillos\net al. (2007) use context to improve the results of CRF segmentation by noting that certain\nadjacencies (relationships) are more likely than others, e.g., a person is more likely to be on\na horse than on a dog.\nContext also plays an important role in 3D inference from single images (Figure 14.47),\nusing computer vision techniques for labeling pixels as belonging to the ground, vertical\nsurfaces, or sky (Hoiem, Efros, and Hebert 2005a,b). This line of work has been extended to\na more holistic approach that simultaneously reasons about object identity, location, surface\norientations, occlusions, and camera viewing parameters (Hoiem, Efros, and Hebert 2008a,b).\nA number of approaches use the gist of a scene (Torralba 2003; Torralba, Murphy, Free-\nman et al. 2003) to determine where instances of particular objects are likely to occur. For\nexample, Murphy, Torralba, and Freeman (2003) train a regressor to predict the vertical loca-\ntions of objects such as pedestrians, cars, and buildings (or screens and keyboard for indoor\nofﬁce scenes) based on the gist of an image. These location distributions are then used with\nclassic object detectors to improve the performance of the detectors. Gists can also be used to\ndirectly match complete images, as we saw in the scene completion work of Hays and Efros\n(2007).\nFinally, some of the most recent work in scene understanding exploits the existence of\nlarge numbers of labeled (or even unlabeled) images to perform matching directly against\nwhole images, where the images themselves implicitly encode the expected relationships\nbetween objects (Figure 14.51) (Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros\n2008). We discuss such techniques in the next section, where we look at the inﬂuence that\nlarge image databases have had on object recognition and scene understanding.\n14.5.1 Learning and large image collections\nGiven how learning techniques are widely used in recognition algorithms, you may wonder\nwhether the topic of learning deserves its own section (or even chapter), or whether it is just\npart of the basic fabric of all recognition tasks. In fact, trying to build a recognition system\nwithout lots of training data for anything other than a basic pattern such as a UPC code has\nproven to be a dismal failure.\nIn this chapter, we have already seen lots of techniques borrowed from the machine learn-\ning, statistics, and pattern recognition communities. These include principal component, sub-\nspace, and discriminant analysis (Section 14.2.1) and more sophisticated discriminative clas-\nsiﬁcation algorithms such as neural networks, support vector machines, and boosting (Sec-\n14.5 Context and scene understanding\n715\n(a)\n(b)\n(c)\nFigure 14.51 Recognition by scene alignment (Russell, Torralba, Liu et al. 2007): (a) input\nimage; (b) matched images with similar scene conﬁgurations; (c) ﬁnal labeling of the input\nimage.\ntion 14.1.1). Some of the best-performing techniques on challenging recognition benchmarks\n(Varma and Ray 2007; Felzenszwalb, McAllester, and Ramanan 2008; Fritz and Schiele 2008;\nVedaldi, Gulshan, Varma et al. 2009) rely heavily on the latest machine learning techniques,\nwhose development is often being driven by challenging vision problems (Freeman, Perona,\nand Sch¨olkopf 2008).\nA distinction sometimes made in the recognition community is between problems where\nmost of the variables of interest (say, parts) are already (partially) labeled and systems that\nlearn more of the problem structure with less supervision (Fergus, Perona, and Zisserman\n2007; Fei-Fei, Fergus, and Perona 2006). In fact, recent work by Sivic, Russell, Zisserman et\nal. (2008) has demonstrated the ability to learn visual hierarchies (hierarchies of object parts\nwith related visual appearance) and scene segmentations in a totally unsupervised framework.\nPerhaps the most dramatic change in the recognition community has been the appearance\nof very large databases of training images.20 Early learning-based algorithms, such as those\nfor face and pedestrian detection (Section 14.1), used relatively few (in the hundreds) labeled\nexamples to train recognition algorithm parameters (say, the thresholds used in boosting). To-\nday, some recognition algorithms use databases such as LabelMe (Russell, Torralba, Murphy\net al. 2008), which contain tens of thousands of labeled examples.\nThe existence of such large databases opens up the possibility of matching directly against\nthe training images rather than using them to learn the parameters of recognition algorithms.\nRussell, Torralba, Liu et al. (2007) describe a system where a new image is matched against\neach of the training images, from which a consensus labeling for the unknown objects in\nthe scene can be inferred, as shown in Figure 14.51. Malisiewicz and Efros (2008) start\nby over-segmenting each image and then use the LabelMe database to search for similar\nimages and conﬁgurations in order to obtain per-pixel category labelings. It is also possible\nto combine feature-based correspondence algorithms with large labeled databases to perform\n20 We have already seen some computational photography applications of such databases in Section 14.4.4.",
  "image_path": "page_736.jpg",
  "pages": [
    735,
    736,
    737
  ]
}