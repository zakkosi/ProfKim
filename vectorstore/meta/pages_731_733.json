{
  "doc_id": "pages_731_733",
  "text": "14.4 Category recognition\n709\n(a)\n(b)\n(c)\n(d)\nFigure 14.46\nScene completion using millions of photographs (Hays and Efros 2007) c⃝\n2007 ACM: (a) original image; (b) after unwanted foreground removal; (c) plausible scene\nmatches, with the one the user selected highlighted in red; (d) output image after replacement\nand blending.\n2008; He and Zemel 2008; Kumar, Torr, and Zisserman 2010), producing some of the best\nresults on the difﬁcult PASCAL VOC segmentation challenge (Shotton, Johnson, and Cipolla\n2008; Kohli, Ladick´y, and Torr 2009). Approaches that ﬁrst segment the image into unique\nor multiple segmentations (Borenstein and Ullman 2008; He, Zemel, and Ray 2006; Russell,\nEfros, Sivic et al. 2006) (potentially combined with CRF models) also do quite well: Csurka\nand Perronnin (2008) have one of the top algorithms in the VOC segmentation challenge.\nHierarchical (multi-scale) and grammar (parsing) models are also sometimes used (Tu, Chen,\nYuille et al. 2005; Zhu, Chen, Lin et al. 2008).\n14.4.4 Application: Intelligent photo editing\nRecent advances in object recognition and scene understanding have greatly increased the\npower of intelligent (semi-automated) photo editing applications. One example is the Photo\nClip Art system of Lalonde, Hoiem, Efros et al. (2007), which recognizes and segments\nobjects of interest, such as pedestrians, in Internet photo collections and then allows users to\npaste them into their own photos. Another is the scene completion system of Hays and Efros\n(2007), which tackles the same inpainting problem we studied in Section 10.5. Given an\nimage in which we wish to erase and ﬁll in a large section (Figure 14.46a–b), where do you\nget the pixels to ﬁll in the gaps in the edited image? Traditional approaches either use smooth\ncontinuation (Bertalmio, Sapiro, Caselles et al. 2000) or borrowing pixels from other parts of\nthe image (Efros and Leung 1999; Criminisi, P´erez, and Toyama 2004; Efros and Freeman\n2001). With the advent of huge repositories of images on the Web (a topic we return to in\nSection 14.5.1), it often makes more sense to ﬁnd a different image to serve as the source of\nthe missing pixels.\nIn their system, Hays and Efros (2007) compute the gist of each image (Oliva and Tor-\n710\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.47\nAutomatic photo pop-up (Hoiem, Efros, and Hebert 2005a) c⃝2005 ACM:\n(a) input image; (b) superpixels are grouped into (c) multiple regions; (d) labelings indicating\nground (green), vertical (red), and sky (blue); (e) novel view of resulting piecewise-planar 3D\nmodel.\nralba 2001; Torralba, Murphy, Freeman et al. 2003) to ﬁnd images with similar colors and\ncomposition. They then run a graph cut algorithm that minimizes image gradient differences\nand composite the new replacement piece into the original image using Poisson image blend-\ning (Section 9.3.4) (P´erez, Gangnet, and Blake 2003). Figure 14.46d shows the resulting\nimage with the erased foreground rooftops region replaced with sailboats.\nA different application of image recognition and segmentation is to infer 3D structure\nfrom a single photo by recognizing certain scene structures. For example, Criminisi, Reid,\nand Zisserman (2000) detect vanishing points and have the user draw basic structures, such\nas walls, in order infer the 3D geometry (Section 6.3.3). Hoiem, Efros, and Hebert (2005a)\non the other hand, work with more “organic” scenes such as the one shown in Figure 14.47.\nTheir system uses a variety of classiﬁers and statistics learned from labeled images to classify\neach pixel as either ground, vertical, or sky (Figure 14.47d). To do this, they begin by com-\nputing superpixels (Figure 14.47b) and then group them into plausible regions that are likely\nto share similar geometric labels (Figure 14.47c). After all the pixels have been labeled, the\nboundaries between the vertical and ground pixels can be used to infer 3D lines along which\nthe image can be folded into a “pop-up” (after removing the sky pixels), as shown in Fig-\nure 14.47e. In related work, Saxena, Sun, and Ng (2009) develop a system that directly infers\nthe depth and orientation of each pixel instead of using just three geometric class labels.\nFace detection and localization can also be used in a variety of photo editing applications\n(in addition to being used in-camera to provide better focus, exposure, and ﬂash settings).\nZanella and Fuentes (2004) use active shape models (Section 14.2.2) to register facial features\nfor creating automated morphs. Rother, Bordeaux, Hamadi et al. (2006) use face and sky\ndetection to determine regions of interest in order to decide which pieces from a collection\nof images to stitch into a collage. Bitouk, Kumar, Dhillon et al. (2008) describe a system\nthat matches a given face image to a large collection of Internet face images, which can\nthen be used (with careful relighting algorithms) to replace the face in the original image.\nApplications they describe include de-identiﬁcation and getting the best possible smile from\n14.4 Category recognition\n711\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.48\nThe importance of context (images courtesy of Antonio Torralba). Can you\nname all of the objects in images (a–b), especially those that are circled in (c–d). Look\ncarefully at the circled objects. Did you notice that they all have the same shape (after being\nrotated), as shown in column (e)?\neveryone in a “burst mode” group shot. Leyvand, Cohen-Or, Dror et al. (2008) show how\naccurately locating facial features using an active shape model (Cootes, Edwards, and Taylor\n2001; Zhou, Gu, and Zhang 2003) can be used to warp such features (and hence the image)\ntowards conﬁgurations resembling those found in images whose facial attractiveness was\nhighly rated, thereby “beautifying” the image without completely losing a person’s identity.\nMost of these techniques rely either on a set of labeled training images, which is an\nessential component of all learning techniques, or the even more recent explosion in images\navailable on the Internet. The assumption in some of this work (and in recognition systems\nbased on such very large databases (Section 14.5.1)) is that as the collection of accessible (and\npotentially partially labeled) images gets larger, ﬁnding a close match gets easier. As Hays\nand Efros (2007) state in their abstract “Our chief insight is that while the space of images is\neffectively inﬁnite, the space of semantically differentiable scenes is actually not that large.”\nIn an interesting commentary on their paper, Levoy (2008) disputes this assertion, claiming\nthat “features in natural scenes form a heavy-tailed distribution, meaning that while some\nfeatures in photographs are more common than others, the relative occurrence of less common\nfeatures drops slowly. In other words, there are many unusual photographs in the world.” He\ndoes, however agree that in computational photography, as in many other applications such\nas speech recognition, synthesis, and translation, “simple machine learning algorithms often\noutperform more sophisticated ones if trained on large enough databases.” He also goes on\nto point out both the potential advantages of such systems, such as better automatic color\nbalancing, and potential issues and pitfalls with the kind of image fakery that these new\napproaches enable.\nFor additional examples of photo editing and computational photography applications",
  "image_path": "page_732.jpg",
  "pages": [
    731,
    732,
    733
  ]
}