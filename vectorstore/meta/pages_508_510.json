{
  "doc_id": "pages_508_510",
  "text": "486\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.18 Fuji SuperCCD high dynamic range image sensor. The paired large and small\nactive areas provide two different effective exposures.\nhave widely different exposures, care must be taken when estimating the motions, which must\nthemselves be checked for consistency to avoid the creation of ghosts and object fragments.\nEven this approach, however, may not work when the camera is simultaneously undergo-\ning large panning motions and exposure changes, which is a common occurrence in casually\nacquired panoramas. Under such conditions, different parts of the image may be seen at one\nor more exposures. Devising a method to blend all of these different sources while avoid-\ning sharp transitions and dealing with scene motion is a challenging problem. One approach\nis to ﬁrst ﬁnd a consensus mosaic and to then selectively compute radiances in under- and\nover-exposed regions (Eden, Uyttendaele, and Szeliski 2006), as shown in Figure 10.17.\nRecently, some cameras, such as the Sony α550 and Pentax K-7, have started integrating\nmultiple exposure merging and tone mapping directly into the camera body. In the future,\nthe need to compute high dynamic range images from multiple exposures may be eliminated\nby advances in camera sensor technology (Figure 10.18) (Yang, El Gamal, Fowler et al.\n1999; Nayar and Mitsunaga 2000; Nayar and Branzoi 2003; Kang, Uyttendaele, Winder et\nal. 2003; Narasimhan and Nayar 2005; Tumblin, Agrawal, and Raskar 2005). However, the\nneed to blend such images and to tone map them to lower-gamut displays is likely to remain.\nHDR image formats.\nBefore we discuss techniques for mapping HDR images back to a\ndisplayable gamut, we should discuss the commonly used formats for storing HDR images.\nIf storage space is not an issue, storing each of the R, G, and B values as a 32-bit IEEE\nﬂoat is the best solution. The commonly used Portable PixMap (.ppm) format, which supports\nboth uncompressed ASCII and raw binary encodings of values, can be extended to a Portable\nFloatMap (.pfm) format by modifying the header. TIFF also supports full ﬂoating point\nvalues.\nA more compact representation is the Radiance format (.pic, .hdr) (Ward 1994), which\nuses a single common exponent and per-channel mantissas (10.19b). An intermediate encod-\n10.2 High dynamic range imaging\n487\n96 bits / pixel\nsign\nexponent\nmantissa\n(a)\n32 bits / pixel\nred\ngreen\nblue\nexponent\n(b)\n48 bits / pixel\nsign exponent\nmantissa\n(c)\nFigure 10.19\nHDR image encoding formats: (a) Portable PixMap (.ppm); (b) Radiance\n(.pic, .hdr); (c) OpenEXR (.exr).\ning, OpenEXR from ILM,12 uses 16-bit ﬂoats for each channel (10.19c), which is a format\nsupported natively on most modern GPUs. Ward (2004) describes these and other data for-\nmats such as LogLuv (Larson 1998) in more detail, as do the books by Reinhard, Ward,\nPattanaik et al. (2005) and Freeman (2008). An even more recent HDR image format is the\nJPEG XR standard.13\n10.2.1 Tone mapping\nOnce a radiance map has been computed, it is usually necessary to display it on a lower gamut\n(i.e., eight-bit) screen or printer. A variety of tone mapping techniques has been developed for\nthis purpose, which involve either computing spatially varying transfer functions or reducing\nimage gradients to ﬁt the available dynamic range (Reinhard, Ward, Pattanaik et al. 2005).\nThe simplest way to compress a high dynamic range radiance image into a low dynamic\nrange gamut is to use a global transfer curve (Larson, Rushmeier, and Piatko 1997). Fig-\nure 10.20 shows one such example, where a gamma curve is used to map an HDR image back\n12 http://www.openexr.net/.\n13 http://www.itu.int/rec/T-REC-T.832-200903-I/en.\n488\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 10.20\nGlobal tone mapping: (a) input HDR image, linearly mapped; (b) gamma\napplied to each color channel independently; (c) gamma applied to intensity (colors are\nless washed out). Original HDR image courtesy of Paul Debevec, http://ict.debevec.org/\n∼debevec/Research/HDR/. Processed images courtesy of Fr´edo Durand, MIT 6.815/6.865\ncourse on Computational Photography.\ninto a displayable gamut. If gamma is applied separately to each channel (Figure 10.20b), the\ncolors become muted (less saturated), since higher-valued color channels contribute less (pro-\nportionately) to the ﬁnal color. Splitting the image up into its luminance and chrominance\n(say, L*a*b*) components (Section 2.3.2), applying the global mapping to the luminance\nchannel, and then reconstituting a color image works better (Figure 10.20c).\nUnfortunately, when the image has a really wide range of exposures, this global approach\nstill fails to preserve details in regions with widely varying exposures. What is needed, in-\nstead, is something akin to the dodging and burning performed by photographers in the dark-\nroom. Mathematically, this is similar to dividing each pixel by the average brightness in a\nregion around that pixel.\nFigure 10.21 shows how this process works. As before, the image is split into its lumi-\nnance and chrominance channels. The log luminance image\nH(x, y) = log L(x, y)\n(10.12)\nis then low-pass ﬁltered to produce a base layer\nHL(x, y) = B(x, y) ∗H(x, y),\n(10.13)\nand a high-pass detail layer\nHH(x, y) = H(x, y) −HL(x, y).\n(10.14)\nThe base layer is then contrast reduced by scaling to the desired log-luminance range,\nH′\nH(x, y) = s HH(x, y)\n(10.15)",
  "image_path": "page_509.jpg",
  "pages": [
    508,
    509,
    510
  ]
}