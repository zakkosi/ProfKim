{
  "doc_id": "pages_699_701",
  "text": "14.2 Face recognition\n677\n(a)\n(b)\nFigure 14.17 “Dual” eigenfaces (Moghaddam, Jebara, and Pentland 2000) c⃝2000 Elsevier:\n(a) intrapersonal and (b) extrapersonal.\nwhere pN is a normal (Gaussian) distribution with covariance ΣI and\n|2πΣI|1/2 = (2π)M/2\nM\nY\nj=1\nλ1/2\nj\n(14.25)\nis its volume. The Mahalanobis distance\n∥∆i∥2\nΣ\n−1\nI\n= ∆T\ni Σ−1\nI ∆i = ∥aI −aI\ni ∥2\n(14.26)\ncan be computed more efﬁciently by ﬁrst projecting the new image x into the whitened in-\ntrapersonal face space (14.15)\naI = ˆ\nU Ix\n(14.27)\nand then computing a Euclidean distance to the training image vector aI\ni , which can be pre-\ncomputed ofﬂine. The extrapersonal likelihood pE(∆i) can be computed in a similar fashion.\nOnce the intrapersonal and extrapersonal likelihoods have been computed, we can com-\npute the Bayesian likelihood of a new image x matching a training image xi as\np(∆i) =\npI(∆i)lI\npI(∆i)lI + pE(∆i)lE\n,\n(14.28)\nwhere lI and lE are the prior probabilities of two images being in the same or in different\nclasses (Moghaddam, Jebara, and Pentland 2000). A simpler approach, which does not re-\nquire the evaluation of extrapersonal probabilities, is to simply choose the training image with\n678\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.18\nModular eigenspace for face recognition (Moghaddam and Pentland 1997)\nc⃝1997 IEEE. (a) By detecting separate features in the faces (eyes, nose, mouth), separate\neigenspaces can be estimated for each one. (b) The relative positions of each feature can be\ndetected at recognition time, thus allowing for more ﬂexibility in viewpoint and expression.\nthe highest likelihood pI(∆i). In this case, nearest neighbor search techniques in the space\nspanned by the precomputed {aI\ni } vectors could be used to speed up ﬁnding the best match.13\nAnother way to improve the performance of eigenface-based approaches is to break up\nthe image into separate regions such as the eyes, nose, and mouth (Figure 14.18) and to match\neach of these modular eigenspaces independently (Moghaddam and Pentland 1997; Heisele,\nHo, Wu et al. 2003; Heisele, Serre, and Poggio 2007). The advantage of such a modular\napproach is that it can tolerate a wider range of viewpoints, because each part can move\nrelative to the others. It also supports a larger variety of combinations, e.g., we can model one\nperson as having a narrow nose and bushy eyebrows, without requiring the eigenfaces to span\nall possible combinations of nose, mouth, and eyebrows. (If you remember the cardboard\nchildren’s books where you can select different top and bottom faces, or Mr. Potato Head,\nyou get the idea.)\nAnother approach to dealing with large variability in appearance is to create view-based\n(view-speciﬁc) eigenspaces, as shown in Figure 14.19 (Moghaddam and Pentland 1997). We\ncan think of these view-based eigenspaces as local descriptors that select different axes de-\npending on which part of the face space you are in. Note that such approaches, however,\npotentially require large amounts of training data, i.e., pictures of every person in every pos-\nsible pose or expression. This is in contrast to the shape and appearance models we study in\n13 Note that while the covariance matrices ΣI and ΣE are computed by looking at differences between all pairs of\nimages, the run-time evaluation selects the nearest image to determine the facial identity. Whether this is statistically\ncorrect is explored in Exercise 14.4.\n14.2 Face recognition\n679\n(a)\n(b)\nFigure 14.19 View-based eigenspace (Moghaddam and Pentland 1997) c⃝1997 IEEE. (a)\nComparison between a regular (parametric) eigenspace reconstruction (middle column) and\na view-based eigenspace reconstruction (right column) corresponding to the input image (left\ncolumn). The top row is from a training image, the bottom row is from the test set. (b) A\nschematic representation of the two approaches, showing how each view computes its own\nlocal basis representation.\nSection 14.2.2, which can learn deformations across all individuals.\nIt is also possible to generalize the bilinear factorization implicit in PCA and SVD ap-\nproaches to multilinear (tensor) formulations that can model several interacting factors si-\nmultaneously (Vasilescu and Terzopoulos 2007). These ideas are related to currently active\ntopics in machine learning such as subspace learning (Cai, He, Hu et al. 2007), local distance\nfunctions (Frome, Singer, Sha et al. 2007), and metric learning (Ramanan and Baker 2009).\nLearning approaches play an increasingly important role in face recognition, e.g., in the work\nof Sivic, Everingham, and Zisserman (2009) and Guillaumin, Verbeek, and Schmid (2009).\n14.2.2 Active appearance and 3D shape models\nThe need to use modular or view-based eigenspaces for face recognition is symptomatic of\na more general observation, i.e., that facial appearance and identiﬁability depend as much\non shape as they do on color or texture (which is what eigenfaces capture). Furthermore,\nwhen dealing with 3D head rotations, the pose of a person’s head should be discounted when\nperforming recognition.\nIn fact, the earliest face recognition systems, such as those by Fischler and Elschlager\n(1973), Kanade (1977), and Yuille (1991), found distinctive feature points on facial images\nand performed recognition on the basis of their relative positions or distances. Newer tech-\nniques such as local feature analysis (Penev and Atick 1996) and elastic bunch graph match-\ning (Wiskott, Fellous, Kr¨uger et al. 1997) combine local ﬁlter responses (jets) at distinctive",
  "image_path": "page_700.jpg",
  "pages": [
    699,
    700,
    701
  ]
}