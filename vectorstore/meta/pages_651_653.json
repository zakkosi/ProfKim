{
  "doc_id": "pages_651_653",
  "text": "13.3 Light ﬁelds and Lumigraphs\n629\ns\nt\nu\nv\n(s,t)\n(u,v)\nCamera center\nImage plane\n pixel\n(a)\n(b)\nFigure 13.7 The Lumigraph (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996 ACM: (a) a\nray is represented by its 4D two-plane parameters (s, t) and (u, v); (b) a slice through the 3D\nlight ﬁeld subset (u, v, s).\nthe 4D light ﬁeld of all possible rays (Gortler, Grzeszczuk, Szeliski et al. 1996; Levoy and\nHanrahan 1996; Levoy 2006).5\nTo make the parameterization of this 4D function simpler, let us put two planes in the\n3D scene roughly bounding the area of interest, as shown in Figure 13.7a. Any light ray\nterminating at a camera that lives in front of the st plane (assuming that this space is empty)\npasses through the two planes at (s, t) and (u, v) and can be described by its 4D coordinate\n(s, t, u, v). This diagram (and parameterization) can be interpreted as describing a family of\ncameras living on the st plane with their image planes being the uv plane. The uv plane\ncan be placed at inﬁnity, which corresponds to all the virtual cameras looking in the same\ndirection.\nIn practice, if the planes are of ﬁnite extent, the ﬁnite light slab L(s, t, u, v) can be used to\ngenerate any synthetic view that a camera would see through a (ﬁnite) viewport in the st plane\nwith a view frustum that wholly intersects the far uv plane. To enable the camera to move\nall the way around an object, the 3D space surrounding the object can be split into multiple\ndomains, each with its own light slab parameterization. Conversely, if the camera is moving\ninside a bounded volume of free space looking outward, multiple cube faces surrounding the\ncamera can be used as (s, t) planes.\n5 Levoy and Hanrahan (1996) borrowed the term light ﬁeld from a paper by Gershun (1939). Another name for\nthis representation is the photic ﬁeld (Moon and Spencer 1981).\n630\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 13.8\nDepth compensation in the Lumigraph (Gortler, Grzeszczuk, Szeliski et al.\n1996) c⃝1996 ACM. To resample the (s, u) dashed light ray, the u parameter corresponding\nto each discrete si camera location is modiﬁed according to the out-of-plane depth z to yield\nnew coordinates u and u′; in (u, s) ray space, the original sample (△) is resampled from\nthe (si, u′) and (si+1, u′′) samples, which are themselves linear blends of their adjacent (◦)\nsamples.\nThinking about 4D spaces is difﬁcult, so let us drop our visualization by one dimension.\nIf we ﬁx the row value t and constrain our camera to move along the s axis while looking\nat the uv plane, we can stack all of the stabilized images the camera sees to get the (u, v, s)\nepipolar volume, which we discussed in Section 11.6. A “horizontal” cross-section through\nthis volume is the well-known epipolar plane image (Bolles, Baker, and Marimont 1987),\nwhich is the us slice shown in Figure 13.7b.\nAs you can see in this slice, each color pixel moves along a linear track whose slope\nis related to its depth (parallax) from the uv plane. (Pixels exactly on the uv plane appear\n“vertical”, i.e., they do not move as the camera moves along s.) Furthermore, pixel tracks\nocclude one another as their corresponding 3D surface elements occlude. Translucent pixels,\nhowever, composite over background pixels (Section 3.1.3, (3.8)) rather than occluding them.\nThus, we can think of adjacent pixels sharing a similar planar geometry as EPI strips or EPI\ntubes (Criminisi, Kang, Swaminathan et al. 2005).\nThe equations mapping from pixels (x, y) in a virtual camera and the corresponding\n(s, t, u, v) coordinates are relatively straightforward to derive and are sketched out in Ex-\nercise 13.7. It is also possible to show that the set of pixels corresponding to a regular ortho-\ngraphic or perspective camera, i.e., one that has a linear projective relationship between 3D\npoints and (x, y) pixels (2.63), lie along a two-dimensional hyperplane in the (s, t, u, v) light\nﬁeld (Exercise 13.7).\n13.3 Light ﬁelds and Lumigraphs\n631\nWhile a light ﬁeld can be used to render a complex 3D scene from novel viewpoints, a\nmuch better rendering (with less ghosting) can be obtained if something is known about its\n3D geometry. The Lumigraph system of Gortler, Grzeszczuk, Szeliski et al. (1996) extends\nthe basic light ﬁeld rendering approach by taking into account the 3D location of surface\npoints corresponding to each 3D ray.\nConsider the ray (s, u) corresponding to the dashed line in Figure 13.8, which intersects\nthe object’s surface at a distance z from the uv plane. When we look up the pixel’s color in\ncamera si (assuming that the light ﬁeld is discretely sampled on a regular 4D (s, t, u, v) grid),\nthe actual pixel coordinate is u′, instead of the original u value speciﬁed by the (s, u) ray.\nSimilarly, for camera si+1 (where si ≤s ≤si+1), pixel address u′′ is used. Thus, instead of\nusing quadri-linear interpolation of the nearest sampled (s, t, u, v) values around a given ray\nto determine its color, the (u, v) values are modiﬁed for each discrete (si, ti) camera.\nFigure 13.8 also shows the same reasoning in ray space. Here, the original continuous-\nvalued (s, u) ray is represented by a triangle and the nearby sampled discrete values are\nshown as circles. Instead of just blending the four nearest samples, as would be indicated\nby the vertical and horizontal dashed lines, the modiﬁed (si, u′) and (si+1, u′′) values are\nsampled instead and their values are then blended.\nThe resulting rendering system produces images of much better quality than a proxy-free\nlight ﬁeld and is the method of choice whenever 3D geometry can be inferred. In subsequent\nwork, Isaksen, McMillan, and Gortler (2000) show how a planar proxy for the scene, which\nis a simpler 3D model, can be used to simplify the resampling equations. They also describe\nhow to create synthetic aperture photos, which mimic what might be seen by a wide-aperture\nlens, by blending more nearby samples (Levoy and Hanrahan 1996). A similar approach\ncan be used to re-focus images taken with a plenoptic (microlens array) camera (Ng, Levoy,\nBr´eedif et al. 2005; Ng 2005) or a light ﬁeld microscope (Levoy, Ng, Adams et al. 2006). It\ncan also be used to see through obstacles, using extremely large synthetic apertures focused\non a background that can blur out foreground objects and make them appear translucent\n(Wilburn, Joshi, Vaish et al. 2005; Vaish, Szeliski, Zitnick et al. 2006).\nNow that we understand how to render new images from a light ﬁeld, how do we go about\ncapturing such data sets? One answer is to move a calibrated camera with a motion control rig\nor gantry.6 Another approach is to take handheld photographs and to determine the pose and\nintrinsic calibration of each image using either a calibrated stage or structure from motion. In\nthis case, the images need to be rebinned into a regular 4D (s, t, u, v) space before they can\nbe used for rendering (Gortler, Grzeszczuk, Szeliski et al. 1996). Alternatively, the original\nimages can be used directly using a process called the unstructured Lumigraph, which we\n6 See http://lightﬁeld.stanford.edu/acq.html for a description of some of the gantries and camera arrays built at\nthe Stanford Computer Graphics Laboratory. This Web site also provides a number of light ﬁeld data sets that are a\ngreat source of research and project material.",
  "image_path": "page_652.jpg",
  "pages": [
    651,
    652,
    653
  ]
}