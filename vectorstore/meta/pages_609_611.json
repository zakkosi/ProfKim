{
  "doc_id": "pages_609_611",
  "text": "12.2 Active rangeﬁnding\n587\n2003) (Section 10.4.3).\nThe time it takes to scan an object using a light stripe technique is proportional to the\nnumber of depth planes used, which is usually comparable to the number of pixels across\nan image. A much faster scanner can be constructed by turning different projector pixels on\nand off in a structured manner, e.g., using a binary or Gray code (Besl 1989). For example,\nlet us assume that the LCD projector we are using has 1024 columns of pixels. Taking the\n10-bit binary code corresponding to each column’s address (0 . . . 1023), we project the ﬁrst\nbit, then the second, etc. After 10 projections (e.g., a third of a second for a synchronized\n30Hz camera-projector system), each pixel in the camera knows which of the 1024 columns\nof projector light it is seeing. A similar approach can also be used to estimate the refractive\nproperties of an object by placing a monitor behind the object (Zongker, Werner, Curless et al.\n1999; Chuang, Zongker, Hindorff et al. 2000) (Section 13.4). Very fast scanners can also be\nconstructed with a single laser beam, i.e., a real-time ﬂying spot optical triangulation scanner\n(Rioux, Bechthold, Taylor et al. 1987).\nIf even faster, i.e., frame-rate, scanning is required, we can project a single textured pat-\ntern into the scene. Proesmans, Van Gool, and Defoort (1998) describe a system where a\ncheckerboard grid is projected onto an object (e.g., a person’s face) and the deformation of\nthe grid is used to infer 3D shape. Unfortunately, such a technique only works if the surface\nis continuous enough to link all of the grid points.\nA much better system can be constructed using high-speed custom illumination and sens-\ning hardware. Iddan and Yahav (2001) describe the construction of their 3DV Zcam video-\nrate depth sensing camera, which projects a pulsed plane of light onto the scene and then\nintegrates the returning light for a short interval, essentially obtaining time-of-ﬂight mea-\nsurement for the distance to individual pixels in the scene. A good description of earlier\ntime-of-ﬂight systems, including amplitude and frequency modulation schemes for LIDAR,\ncan be found in (Besl 1989).\nInstead of using a single camera, it is also possible to construct an active illumination\nrange sensor using stereo imaging setups. The simplest way to do this is to just project ran-\ndom stripe patterns onto the scene to create synthetic texture, which helps match textureless\nsurfaces (Kang, Webb, Zitnick et al. 1995). Projecting a known series of stripes, just as in\ncoded pattern single-camera rangeﬁnding, makes the correspondence between pixels unam-\nbiguous and allows for the recovery of depth estimates at pixels only seen in a single camera\n(Scharstein and Szeliski 2003). This technique has been used to produce large numbers of\nhighly accurate registered multi-image stereo pairs and depth maps for the purpose of eval-\nuating stereo correspondence algorithms (Scharstein and Szeliski 2002; Hirschm¨uller and\nScharstein 2009) and learning depth map priors and parameters (Scharstein and Pal 2007).\nWhile projecting multiple patterns usually requires the scene or object to remain still, ad-\nditional processing can enable the production of real-time depth maps for dynamic scenes.\n588\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 12.7 Real-time dense 3D face capture using spacetime stereo (Zhang, Snavely, Cur-\nless et al. 2004) c⃝2004 ACM: (a) set of ﬁve consecutive video frames from one of two stereo\ncameras (every ﬁfth frame is free of stripe patterns, in order to extract texture); (b) resulting\nhigh-quality 3D surface model (depth map visualized as a shaded rendering).\nThe basic idea (Davis, Ramamoorthi, and Rusinkiewicz 2003; Zhang, Curless, and Seitz\n2003) is to assume that depth is nearly constant within a 3D space–time window around\neach pixel and to use the 3D window for matching and reconstruction. Depending on the\nsurface shape and motion, this assumption may be error-prone, as shown in (Davis, Nahab,\nRamamoorthi et al. 2005). To model shapes more accurately, Zhang, Curless, and Seitz\n(2003) model the linear disparity variation within the space–time window and show that bet-\nter results can be obtained by globally optimizing disparity and disparity gradient estimates\nover video volumes (Zhang, Snavely, Curless et al. 2004). Figure 12.7 shows the results of\napplying this system to a person’s face; the frame-rate 3D surface model can then be used for\nfurther model-based ﬁtting and computer graphics manipulation (Section 12.6.2).\n12.2.1 Range data merging\nWhile individual range images can be useful for applications such as real-time z-keying or fa-\ncial motion capture, they are often used as building blocks for more complete 3D object mod-\neling. In such applications, the next two steps in processing are the registration (alignment) of\npartial 3D surface models and their integration into coherent 3D surfaces (Curless 1999). If\ndesired, this can be followed by a model ﬁtting stage using either parametric representations\nsuch as generalized cylinders (Agin and Binford 1976; Nevatia and Binford 1977; Marr and\nNishihara 1978; Brooks 1981), superquadrics (Pentland 1986; Solina and Bajcsy 1990; Ter-\nzopoulos and Metaxas 1991), or non-parametric models such as triangular meshes (Boissonat\n1984) or physically-based models (Terzopoulos, Witkin, and Kass 1988; Delingette, Hebert,\nand Ikeuichi 1992; Terzopoulos and Metaxas 1991; McInerney and Terzopoulos 1993; Ter-\nzopoulos 1999). A number of techniques have also been developed for segmenting range\nimages into simpler constituent surfaces (Hoover, Jean-Baptiste, Jiang et al. 1996).\nThe most widely used 3D registration technique is the iterated closest point (ICP) algo-\n12.2 Active rangeﬁnding\n589\nrithm, which alternates between ﬁnding the closest point matches between the two surfaces\nbeing aligned and then solving a 3D absolute orientation problem (Section 6.1.5, (6.31–6.32)\n(Besl and McKay 1992; Chen and Medioni 1992; Zhang 1994; Szeliski and Lavall´ee 1996;\nGold, Rangarajan, Lu et al. 1998; David, DeMenthon, Duraiswami et al. 2004; Li and Hart-\nley 2007; Enqvist, Josephson, and Kahl 2009).3 Since the two surfaces being aligned usually\nonly have partial overlap and may also have outliers, robust matching criteria (Section 6.1.4\nand Appendix B.3) are typically used. In order to speed up the determination of the closest\npoint, and also to make the distance-to-surface computation more accurate, one of the two\npoint sets (e.g., the current merged model) can be converted into a signed distance function,\noptionally represented using an octree spline for compactness (Lavall´ee and Szeliski 1995).\nVariants on the basic ICP algorithm can be used to register 3D point sets under non-rigid de-\nformations, e.g., for medical applications (Feldmar and Ayache 1996; Szeliski and Lavall´ee\n1996). Color values associated with the points or range measurements can also be used as\npart of the registration process to improve robustness (Johnson and Kang 1997; Pulli 1999).\nUnfortunately, the ICP algorithm and its variants can only ﬁnd a locally optimal alignment\nbetween 3D surfaces. If this is not known a priori, more global correspondence or search\ntechniques, based on local descriptors invariant to 3D rigid transformations, need to be used.\nAn example of such a descriptor is the spin image, which is a local circular projection of a\n3D surface patch around the local normal axis (Johnson and Hebert 1999). Another (earlier)\nexample is the splash representation introduced by Stein and Medioni (1992).\nOnce two or more 3D surfaces have been aligned, they can be merged into a single model.\nOne approach is to represent each surface using a triangulated mesh and combine these\nmeshes using a process that is sometimes called zippering (Soucy and Laurendeau 1992;\nTurk and Levoy 1994). Another, now more widely used, approach is to compute a signed\ndistance function that ﬁts all of the 3D data points (Hoppe, DeRose, Duchamp et al. 1992;\nCurless and Levoy 1996; Hilton, Stoddart, Illingworth et al. 1996; Wheeler, Sato, and Ikeuchi\n1998).\nFigure 12.8 shows one such approach, the volumetric range image processing (VRIP)\ntechnique developed by Curless and Levoy (1996), which ﬁrst computes a weighted signed\ndistance function from each range image and then merges them using a weighted averaging\nprocess. To make the representation more compact, run-length coding is used to encode\nthe empty, seen, and varying (signed distance) voxels, and only the signed distance values\nnear each surface are stored.4 Once the merged signed distance function has been computed,\na zero-crossing surface extraction algorithm, such as marching cubes (Lorensen and Cline\n1987), can be used to recover a meshed surface model. Figure 12.9 shows an example of the\n3 Some techniques, such as the one developed by Chen and Medioni (1992), use local surface tangent planes to\nmake this computation more accurate and to accelerate convergence.\n4 An alternative, even more compact, representation could be to use octrees (Lavall´ee and Szeliski 1995).",
  "image_path": "page_610.jpg",
  "pages": [
    609,
    610,
    611
  ]
}