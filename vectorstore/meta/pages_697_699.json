{
  "doc_id": "pages_697_699",
  "text": "14.2 Face recognition\n675\n-2\n-1\n0\n1\n2\n-3\n-2\n-1\n0\n1\n2\n3\nFigure 14.16\nSimple example of Fisher linear discriminant analysis. The samples come\nfrom three different classes, shown in different colors along with their principal axes, which\nare scaled to 2σi. (The intersections of the tilted axes are the class means mk.) The dashed\nline is the (dominant) Fisher linear discriminant direction and the dotted lines are the linear\ndiscriminants between the classes. Note how the discriminant direction is a blend between\nthe principal directions of the between-class and within-class scatter matrices.\nwhich is aligned with the black x and y axes. We can compute the total within-class scatter\nmatrix as\nSW =\nK−1\nX\nk=0\nSk =\nK−1\nX\nk=0\nX\ni∈Ck\n(xi −mk)(xi −mk)T ,\n(14.17)\nwhere mk is the mean of class k and Sk is its within-class scatter matrix.11 Similarly, we\ncan compute the between-class scatter as\nSB =\nK−1\nX\nk=0\nNk(mk −m)(mk −m)T ,\n(14.18)\nwhere Nk are the number of exemplars in each class and m is the overall mean. For the three\ndistributions shown in Figure 14.16, we have\nSW = 3N\n\"\n0.246\n0.183\n0.183\n0.457\n#\nand\nSB = N\n\"\n6.125\n0\n0\n0.375\n#\n,\n(14.19)\n11 To be consistent with Belhumeur, Hespanha, and Kriegman (1997), we use SW and SB to denote the scatter\nmatrices, even though we use C elsewhere (14.9).\n676\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhere N = Nk = 13 is the number of samples in each class.\nTo compute the most discriminating direction, Fisher’s linear discriminant (FLD) (Bel-\nhumeur, Hespanha, and Kriegman 1997; Hastie, Tibshirani, and Friedman 2001; Bishop\n2006), which is also known as linear discriminant analysis (LDA), selects the direction u\nthat results in the largest ratio between the projected between-class and within-class varia-\ntions\nu∗= arg max\nu\nuT SBu\nuT SWu,\n(14.20)\nwhich is equivalent to ﬁnding the eigenvector corresponding to the largest eigenvalue of the\ngeneralized eigenvalue problem\nSBu = λSWu\nor\nλu = S−1\nW SBu.\n(14.21)\nFor the problem shown in Figure 14.16,\nS−1\nW SB =\n\"\n11.796\n−0.289\n−4.715\n0.3889\n#\nand\nu =\n\"\n0.926\n−0.379\n#\n(14.22)\nAs you can see, using this direction results in a better separation between the classes than\nusing the dominant PCA direction, which is the horizontal axis. In their paper, Belhumeur,\nHespanha, and Kriegman (1997) show that Fisherfaces signiﬁcantly outperform the original\neigenfaces algorithm, especially when faces have large amounts of illumination variation, as\nin Figure 14.15.\nAn alternative for modeling within-class (intrapersonal) and between-class (extraper-\nsonal) variations is to model each distribution separately and then use Bayesian techniques\nto ﬁnd the closest exemplar (Moghaddam, Jebara, and Pentland 2000). Instead of computing\nthe mean for each class and then the within-class and between-class distributions, consider\nevaluating the difference images\n∆ij = xi −xj\n(14.23)\nbetween all pairs of training images (xi, xj). The differences between pairs that are in the\nsame class (the same person) are used to estimate the intrapersonal covariance matrix ΣI,\nwhile differences between different people are used to estimate the extrapersonal covariance\nΣE.12 The principal components (eigenfaces) corresponding to these two classes are shown\nin Figure 14.17.\nAt recognition time, we can compute the distance ∆i between a new face x and a stored\ntraining image xi and evaluate its intrapersonal likelihood as\npI(∆i) = pN (∆i; ΣI) =\n1\n|2πΣI|1/2 exp −∥∆i∥2\nΣ\n−1\nI ,\n(14.24)\n12 Note that the difference distributions are zero mean because for every ∆ij there corresponds a negative ∆ji.\n14.2 Face recognition\n677\n(a)\n(b)\nFigure 14.17 “Dual” eigenfaces (Moghaddam, Jebara, and Pentland 2000) c⃝2000 Elsevier:\n(a) intrapersonal and (b) extrapersonal.\nwhere pN is a normal (Gaussian) distribution with covariance ΣI and\n|2πΣI|1/2 = (2π)M/2\nM\nY\nj=1\nλ1/2\nj\n(14.25)\nis its volume. The Mahalanobis distance\n∥∆i∥2\nΣ\n−1\nI\n= ∆T\ni Σ−1\nI ∆i = ∥aI −aI\ni ∥2\n(14.26)\ncan be computed more efﬁciently by ﬁrst projecting the new image x into the whitened in-\ntrapersonal face space (14.15)\naI = ˆ\nU Ix\n(14.27)\nand then computing a Euclidean distance to the training image vector aI\ni , which can be pre-\ncomputed ofﬂine. The extrapersonal likelihood pE(∆i) can be computed in a similar fashion.\nOnce the intrapersonal and extrapersonal likelihoods have been computed, we can com-\npute the Bayesian likelihood of a new image x matching a training image xi as\np(∆i) =\npI(∆i)lI\npI(∆i)lI + pE(∆i)lE\n,\n(14.28)\nwhere lI and lE are the prior probabilities of two images being in the same or in different\nclasses (Moghaddam, Jebara, and Pentland 2000). A simpler approach, which does not re-\nquire the evaluation of extrapersonal probabilities, is to simply choose the training image with",
  "image_path": "page_698.jpg",
  "pages": [
    697,
    698,
    699
  ]
}