{
  "doc_id": "pages_194_196",
  "text": "172\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 3.52 Line-based image warping (Beier and Neely 1992) c⃝1992 ACM: (a) distance\ncomputation and position transfer; (b) rendering algorithm; (c) two intermediate warps used\nfor morphing.\nits new location, ﬁll small holes in the resulting map, and then use inverse warping to perform\nthe resampling (Shade, Gortler, He et al. 1998). The reason that this generally works better\nthan forward warping is that displacement ﬁelds tend to be much smoother than images, so\nthe aliasing introduced during the forward warping of the displacement ﬁeld is much less\nnoticeable.\nA second approach to specifying displacements for local deformations is to use corre-\nsponding oriented line segments (Beier and Neely 1992), as shown in Figures 3.51c and 3.52.\nPixels along each line segment are transferred from source to destination exactly as speciﬁed,\nand other pixels are warped using a smooth interpolation of these displacements. Each line\nsegment correspondence speciﬁes a translation, rotation, and scaling, i.e., a similarity trans-\nform (Table 3.5), for pixels in its vicinity, as shown in Figure 3.52a. Line segments inﬂuence\nthe overall displacement of the image using a weighting function that depends on the mini-\nmum distance to the line segment (v in Figure 3.52a if u ∈[0, 1], else the shorter of the two\ndistances to P and Q).\nFor each pixel X, the target location X′ for each line correspondence is computed along\nwith a weight that depends on the distance and the line segment length (Figure 3.52b). The\nweighted average of all target locations X′\ni then becomes the ﬁnal destination location. Note\nthat while Beier and Neely describe this algorithm as a forward warp, an equivalent algorithm\ncan be written by sequencing through the destination pixels. The resulting warps are not\nidentical because line lengths or distances to lines may be different. Exercise 3.23 has you\nimplement the Beier–Neely (line-based) warp and compare it to a number of other local\ndeformation methods.\nYet another way of specifying correspondences in order to create image warps is to use\nsnakes (Section 5.1.1) combined with B-splines (Lee, Wolberg, Chwa et al. 1996). This tech-\nnique is used in Apple’s Shake software and is popular in the medical imaging community.\n3.6 Geometric transformations\n173\nFigure 3.53 Image morphing (Gomes, Darsa, Costa et al. 1999) c⃝1999 Morgan Kaufmann.\nTop row: if the two images are just blended, visible ghosting results. Bottom row: both\nimages are ﬁrst warped to the same intermediate location (e.g., halfway towards the other\nimage) and the resulting warped images are then blended resulting in a seamless morph.\nOne ﬁnal possibility for specifying displacement ﬁelds is to use a mesh speciﬁcally\nadapted to the underlying image content, as shown in Figure 3.51d. Specifying such meshes\nby hand can involve a fair amount of work; Gomes, Darsa, Costa et al. (1999) describe an\ninteractive system for doing this. Once the two meshes have been speciﬁed, intermediate\nwarps can be generated using linear interpolation and the displacements at mesh nodes can\nbe interpolated using splines.\n3.6.3 Application: Feature-based morphing\nWhile warps can be used to change the appearance of or to animate a single image, even\nmore powerful effects can be obtained by warping and blending two or more images using a\nprocess now commonly known as morphing (Beier and Neely 1992; Lee, Wolberg, Chwa et\nal. 1996; Gomes, Darsa, Costa et al. 1999).\nFigure 3.53 shows the essence of image morphing. Instead of simply cross-dissolving\nbetween two images, which leads to ghosting as shown in the top row, each image is warped\ntoward the other image before blending, as shown in the bottom row. If the correspondences\nhave been set up well (using any of the techniques shown in Figure 3.51), corresponding\nfeatures are aligned and no ghosting results.\nThe above process is repeated for each intermediate frame being generated during a\n174\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmorph, using different blends (and amounts of deformation) at each interval. Let t ∈[0, 1] be\nthe time parameter that describes the sequence of interpolated frames. The weighting func-\ntions for the two warped images in the blend go as (1 −t) and t. Conversely, the amount of\nmotion that image 0 undergoes at time t is t of the total amount of motion that is speciﬁed\nby the correspondences. However, some care must be taken in deﬁning what it means to par-\ntially warp an image towards a destination, especially if the desired motion is far from linear\n(Sederberg, Gao, Wang et al. 1993). Exercise 3.25 has you implement a morphing algorithm\nand test it out under such challenging conditions.\n3.7 Global optimization\nSo far in this chapter, we have covered a large number of image processing operators that\ntake as input one or more images and produce some ﬁltered or transformed version of these\nimages. In many applications, it is more useful to ﬁrst formulate the goals of the desired\ntransformation using some optimization criterion and then ﬁnd or infer the solution that best\nmeets this criterion.\nIn this ﬁnal section, we present two different (but closely related) variants on this idea.\nThe ﬁrst, which is often called regularization or variational methods (Section 3.7.1), con-\nstructs a continuous global energy function that describes the desired characteristics of the\nsolution and then ﬁnds a minimum energy solution using sparse linear systems or related\niterative techniques. The second formulates the problem using Bayesian statistics, model-\ning both the noisy measurement process that produced the input images as well as prior\nassumptions about the solution space, which are often encoded using a Markov random ﬁeld\n(Section 3.7.2).\nExamples of such problems include surface interpolation from scattered data (Figure 3.54),\nimage denoising and the restoration of missing regions (Figure 3.57), and the segmentation\nof images into foreground and background regions (Figure 3.61).\n3.7.1 Regularization\nThe theory of regularization was ﬁrst developed by statisticians trying to ﬁt models to data\nthat severely underconstrained the solution space (Tikhonov and Arsenin 1977; Engl, Hanke,\nand Neubauer 1996). Consider, for example, ﬁnding a smooth surface that passes through\n(or near) a set of measured data points (Figure 3.54). Such a problem is described as ill-\nposed because many possible surfaces can ﬁt this data. Since small changes in the input can\nsometimes lead to large changes in the ﬁt (e.g., if we use polynomial interpolation), such\nproblems are also often ill-conditioned. Since we are trying to recover the unknown function\nf(x, y) from which the data point d(xi, yi) were sampled, such problems are also often called",
  "image_path": "page_195.jpg",
  "pages": [
    194,
    195,
    196
  ]
}