{
  "doc_id": "pages_575_577",
  "text": "11.5 Global optimization\n553\nto measuring only the differences between neighboring pixels’ disparities,\nEs(d) =\nX\n(x,y)\nρ(d(x, y) −d(x + 1, y)) + ρ(d(x, y) −d(x, y + 1)),\n(11.10)\nwhere ρ is some monotonically increasing function of disparity difference. It is also possi-\nble to use larger neighborhoods, such as N8, which can lead to better boundaries (Boykov\nand Kolmogorov 2003), or to use second-order smoothness terms (Woodford, Reid, Torr et\nal. 2008), but such terms require more complex optimization techniques. An alternative to\nsmoothness functionals is to use a lower-dimensional representation such as splines (Szeliski\nand Coughlan 1997).\nIn standard regularization (Section 3.7.1), ρ is a quadratic function, which makes d smooth\neverywhere and may lead to poor results at object boundaries. Energy functions that do not\nhave this problem are called discontinuity-preserving and are based on robust ρ functions\n(Terzopoulos 1986b; Black and Rangarajan 1996). The seminal paper by Geman and Ge-\nman (1984) gave a Bayesian interpretation of these kinds of energy functions and proposed a\ndiscontinuity-preserving energy function based on Markov random ﬁelds (MRFs) and addi-\ntional line processes, which are additional binary variables that control whether smoothness\npenalties are enforced or not. Black and Rangarajan (1996) show how independent line pro-\ncess variables can be replaced by robust pairwise disparity terms.\nThe terms in Es can also be made to depend on the intensity differences, e.g.,\nρd(d(x, y) −d(x + 1, y)) · ρI(∥I(x, y) −I(x + 1, y)∥),\n(11.11)\nwhere ρI is some monotonically decreasing function of intensity differences that lowers\nsmoothness costs at high-intensity gradients. This idea (Gamble and Poggio 1987; Fua 1993;\nBobick and Intille 1999; Boykov, Veksler, and Zabih 2001) encourages disparity discontinu-\nities to coincide with intensity or color edges and appears to account for some of the good\nperformance of global optimization approaches. While most researchers set these functions\nheuristically, Scharstein and Pal (2007) show how the free parameters in such conditional\nrandom ﬁelds (Section 3.7.2, (3.118)) can be learned from ground truth disparity maps.\nOnce the global energy has been deﬁned, a variety of algorithms can be used to ﬁnd a\n(local) minimum. Traditional approaches associated with regularization and Markov random\nﬁelds include continuation (Blake and Zisserman 1987), simulated annealing (Geman and\nGeman 1984; Marroquin, Mitter, and Poggio 1987; Barnard 1989), highest conﬁdence ﬁrst\n(Chou and Brown 1990), and mean-ﬁeld annealing (Geiger and Girosi 1991).\nMore recently, max-ﬂow and graph cut methods have been proposed to solve a special\nclass of global optimization problems (Roy and Cox 1998; Boykov, Veksler, and Zabih 2001;\nIshikawa 2003). Such methods are more efﬁcient than simulated annealing and have produced\ngood results, as have techniques based on loopy belief propagation (Sun, Zheng, and Shum\n554\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n2003; Tappen and Freeman 2003). Appendix B.5 and a recent survey paper on MRF inference\n(Szeliski, Zabih, Scharstein et al. 2008) discuss and compare such techniques in more detail.\nWhile global optimization techniques currently produce the best stereo matching results,\nthere are some alternative approaches worth studying.\nCooperative algorithms.\nCooperative algorithms, inspired by computational models of hu-\nman stereo vision, were among the earliest methods proposed for disparity computation (Dev\n1974; Marr and Poggio 1976; Marroquin 1983; Szeliski and Hinton 1985; Zitnick and Kanade\n2000). Such algorithms iteratively update disparity estimates using non-linear operations that\nresult in an overall behavior similar to global optimization algorithms. In fact, for some of\nthese algorithms, it is possible to explicitly state a global function that is being minimized\n(Scharstein and Szeliski 1998).\nCoarse-to-ﬁne and incremental warping.\nMost of today’s best algorithms ﬁrst enumer-\nate all possible matches at all possible disparities and then select the best set of matches in\nsome way. Faster approaches can sometimes be obtained using methods inspired by classic\n(inﬁnitesimal) optical ﬂow computation. Here, images are successively warped and disparity\nestimates incrementally updated until a satisfactory registration is achieved. These techniques\nare most often implemented within a coarse-to-ﬁne hierarchical reﬁnement framework (Quam\n1984; Bergen, Anandan, Hanna et al. 1992; Barron, Fleet, and Beauchemin 1994; Szeliski\nand Coughlan 1997).\n11.5.1 Dynamic programming\nA different class of global optimization algorithm is based on dynamic programming. While\nthe 2D optimization of Equation (11.8) can be shown to be NP-hard for common classes\nof smoothness functions (Veksler 1999), dynamic programming can ﬁnd the global mini-\nmum for independent scanlines in polynomial time. Dynamic programming was ﬁrst used\nfor stereo vision in sparse, edge-based methods (Baker and Binford 1981; Ohta and Kanade\n1985). More recent approaches have focused on the dense (intensity-based) scanline match-\ning problem (Belhumeur 1996; Geiger, Ladendorf, and Yuille 1992; Cox, Hingorani, Rao et\nal. 1996; Bobick and Intille 1999; Birchﬁeld and Tomasi 1999). These approaches work by\ncomputing the minimum-cost path through the matrix of all pairwise matching costs between\ntwo corresponding scanlines, i.e., through a horizontal slice of the DSI. Partial occlusion is\nhandled explicitly by assigning a group of pixels in one image to a single pixel in the other\nimage. Figure 11.11 schematically shows how DP works, while Figure 11.5f shows a real\nDSI slice over which the DP is applied.\n11.5 Global optimization\n555\nc\nd\ne\nf\ng\nk\na\nLeft scanline\ni\nRight scanline\na\nc\nf\ng\nj\nk\nh\nb\nM\nL\nR\nR\nR\nM\nL\nL\nM\nM\nM\nk\nd\n1\n2\n3\n2\n8\n10\nm\n1\n2\n3\n4\n0\nLeft\nn\n1\n2\n3\n4\nRight\nCyclopean\nDisparity\n6\n4\n(a)\n(b)\nFigure 11.11 Stereo matching using dynamic programming, as illustrated by (a) Scharstein\nand Szeliski (2002) c⃝2002 Springer and (b) Kolmogorov, Criminisi, Blake et al. (2006). c⃝\n2006 IEEE. For each pair of corresponding scanlines, a minimizing path through the matrix\nof all pairwise matching costs (DSI) is selected. Lowercase letters (a–k) symbolize the inten-\nsities along each scanline. Uppercase letters represent the selected path through the matrix.\nMatches are indicated by M, while partially occluded points (which have a ﬁxed cost) are\nindicated by L or R, corresponding to points only visible in the left or right images, respec-\ntively. Usually, only a limited disparity range is considered (0–4 in the ﬁgure, indicated by\nthe non-shaded squares). The representation in (a) allows for diagonal moves while the rep-\nresentation in (b) does not. Note that these diagrams, which use the Cyclopean representation\nof depth, i.e., depth relative to a camera between the two input cameras, show an “unskewed”\nx-d slice through the DSI.\nTo implement dynamic programming for a scanline y, each entry (state) in a 2D cost\nmatrix D(m, n) is computed by combining its DSI value\nC′(m, n) = C(m + n, m −n, y)\n(11.12)\nwith one of its predecessor cost values.\nUsing the representation shown in Figure 11.11a,\nwhich allows for “diagonal” moves, the aggregated match costs can be recursively computed\nas\nD(m, n, M)\n=\nmin(D(m−1, n−1, M), D(m−1, n, L), D(m−1, n−1, R)) + C′(m, n)\nD(m, n, L)\n=\nmin(D(m−1, n−1, M), D(m−1, n, L)) + O\n(11.13)\nD(m, n, R)\n=\nmin(D(m, n−1, M), D(m, n−1, R)) + O,\nwhere O is a per-pixel occlusion cost. The aggregation rules corresponding to Figure 11.11b\nare given by Kolmogorov, Criminisi, Blake et al. (2006), who also use a two-state foreground–\nbackground model for bi-layer segmentation.",
  "image_path": "page_576.jpg",
  "pages": [
    575,
    576,
    577
  ]
}