{
  "doc_id": "pages_468_470",
  "text": "446\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\net al. 2004), as discussed in Section 9.3.2. However, when the motion is reasonably small (on\nthe order of a few pixels), general 2D motion estimation (optical ﬂow) can be used to perform\nan appropriate correction before blending using a process called local alignment (Shum and\nSzeliski 2000; Kang, Uyttendaele, Winder et al. 2003). This same process can also be used\nto compensate for radial distortion and 3D parallax, although it uses a weaker motion model\nthan explicitly modeling the source of error and may, therefore, fail more often or introduce\nunwanted distortions.\nThe local alignment technique introduced by Shum and Szeliski (2000) starts with the\nglobal bundle adjustment (9.31) used to optimize the camera poses. Once these have been\nestimated, the desired location of a 3D point xi can be estimated as the average of the back-\nprojected 3D locations,\n¯xi ∼\nX\nj\ncij ˜xi(ˆxij; Rj, fj)\n,X\nj\ncij ,\n(9.35)\nwhich can be projected into each image j to obtain a target location ¯xij. The difference\nbetween the target locations ¯xij and the original features xij provide a set of local motion\nestimates\nuij = ¯xij −xij,\n(9.36)\nwhich can be interpolated to form a dense correction ﬁeld uj(xj). In their system, Shum and\nSzeliski (2000) use an inverse warping algorithm where the sparse −uij values are placed at\nthe new target locations ¯xij, interpolated using bilinear kernel functions (Nielson 1993) and\nthen added to the original pixel coordinates when computing the warped (corrected) image.\nIn order to get a reasonably dense set of features to interpolate, Shum and Szeliski (2000)\nplace a feature point at the center of each patch (the patch size controls the smoothness in\nthe local alignment stage), rather than relying of features extracted using an interest operator\n(Figure 9.10).\nAn alternative approach to motion-based de-ghosting was proposed by Kang, Uytten-\ndaele, Winder et al. (2003), who estimate dense optical ﬂow between each input image and a\ncentral reference image. The accuracy of the ﬂow vector is checked using a photo-consistency\nmeasure before a given warped pixel is considered valid and is used to compute a high dy-\nnamic range radiance estimate, which is the goal of their overall algorithm. The requirement\nfor a reference image makes their approach less applicable to general image mosaicing, al-\nthough an extension to this case could certainly be envisaged.\n9.2.3 Recognizing panoramas\nThe ﬁnal piece needed to perform fully automated image stitching is a technique to recognize\nwhich images actually go together, which Brown and Lowe (2007) call recognizing panora-\n9.2 Global alignment\n447\n(a)\n(b)\n(c)\nFigure 9.10\nDeghosting a mosaic with motion parallax (Shum and Szeliski 2000) c⃝2000\nIEEE: (a) composite with parallax; (b) after a single deghosting step (patch size 32); (c) after\nmultiple steps (sizes 32, 16 and 8).\nmas. If the user takes images in sequence so that each image overlaps its predecessor and\nalso speciﬁes the ﬁrst and last images to be stitched, bundle adjustment combined with the\nprocess of topology inference can be used to automatically assemble a panorama (Sawhney\nand Kumar 1999). However, users often jump around when taking panoramas, e.g., they\nmay start a new row on top of a previous one, jump back to take a repeat shot, or create\n360◦panoramas where end-to-end overlaps need to be discovered. Furthermore, the ability\nto discover multiple panoramas taken by a user over an extended period of time can be a big\nconvenience.\nTo recognize panoramas, Brown and Lowe (2007) ﬁrst ﬁnd all pairwise image overlaps\nusing a feature-based method and then ﬁnd connected components in the overlap graph to\n“recognize” individual panoramas (Figure 9.11). The feature-based matching stage ﬁrst ex-\ntracts scale invariant feature transform (SIFT) feature locations and feature descriptors (Lowe\n2004) from all the input images and places them in an indexing structure, as described in Sec-\ntion 4.1.3. For each image pair under consideration, the nearest matching neighbor is found\nfor each feature in the ﬁrst image, using the indexing structure to rapidly ﬁnd candidates and\nthen comparing feature descriptors to ﬁnd the best match. RANSAC is used to ﬁnd a set of in-\nlier matches; pairs of matches are used to hypothesize similarity motion models that are then\nused to count the number of inliers. (A more recent RANSAC algorithm tailored speciﬁcally\nfor rotational panoramas is described by Brown, Hartley, and Nist´er (2007).)\nIn practice, the most difﬁcult part of getting a fully automated stitching algorithm to\nwork is deciding which pairs of images actually correspond to the same parts of the scene.\nRepeated structures such as windows (Figure 9.12) can lead to false matches when using\na feature-based approach. One way to mitigate this problem is to perform a direct pixel-\nbased comparison between the registered images to determine if they actually are different\nviews of the same scene. Unfortunately, this heuristic may fail if there are moving objects\nin the scene (Figure 9.13). While there is no magic bullet for this problem, short of full\nscene understanding, further improvements can likely be made by applying domain-speciﬁc\n448\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 9.11\nRecognizing panoramas (Brown, Szeliski, and Winder 2005), ﬁgures cour-\ntesy of Matthew Brown: (a) input images with pairwise matches; (b) images grouped into\nconnected components (panoramas); (c) individual panoramas registered and blended into\nstitched composites.",
  "image_path": "page_469.jpg",
  "pages": [
    468,
    469,
    470
  ]
}