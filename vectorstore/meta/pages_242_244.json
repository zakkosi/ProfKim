{
  "doc_id": "pages_242_244",
  "text": "220\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.13\nAfﬁne region detectors used to match two images taken from dramatically\ndifferent viewpoints (Mikolajczyk and Schmid 2004) c⃝2004 Springer.\nx0 →\nA−1/2\n0\nx′\n0\nx′\n0 →\nRx′\n1\nA−1/2\n1\nx′\n1\n←x1\nFigure 4.14\nAfﬁne normalization using the second moment matrices, as described by Miko-\nlajczyk, Tuytelaars, Schmid et al. (2005) c⃝2005 Springer. After image coordinates are trans-\nformed using the matrices A−1/2\n0\nand A−1/2\n1\n, they are related by a pure rotation R, which\ncan be estimated using a dominant orientation technique.\nAfﬁne-invariant detectors not only respond at consistent locations after scale and orientation\nchanges, they also respond consistently across afﬁne deformations such as (local) perspective\nforeshortening (Figure 4.13). In fact, for a small enough patch, any continuous image warping\ncan be well approximated by an afﬁne deformation.\nTo introduce afﬁne invariance, several authors have proposed ﬁtting an ellipse to the auto-\ncorrelation or Hessian matrix (using eigenvalue analysis) and then using the principal axes\nand ratios of this ﬁt as the afﬁne coordinate frame (Lindeberg and Garding 1997; Baumberg\n2000; Mikolajczyk and Schmid 2004; Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuyte-\nlaars and Mikolajczyk 2007). Figure 4.14 shows how the square root of the moment matrix\ncan be used to transform local patches into a frame which is similar up to rotation.\nAnother important afﬁne invariant region detector is the maximally stable extremal region\n(MSER) detector developed by Matas, Chum, Urban et al. (2004). To detect MSERs, binary\nregions are computed by thresholding the image at all possible gray levels (the technique\ntherefore only works for grayscale images). This operation can be performed efﬁciently by\nﬁrst sorting all pixels by gray value and then incrementally adding pixels to each connected\ncomponent as the threshold is changed (Nist´er and Stew´enius 2008). As the threshold is\nchanged, the area of each component (region) is monitored; regions whose rate of change of\narea with respect to the threshold is minimal are deﬁned as maximally stable. Such regions\n4.1 Points and patches\n221\nFigure 4.15\nMaximally stable extremal regions (MSERs) extracted and matched from a\nnumber of images (Matas, Chum, Urban et al. 2004) c⃝2004 Elsevier.\nFigure 4.16\nFeature matching: how can we extract local descriptors that are invariant\nto inter-image variations and yet still discriminative enough to establish correct correspon-\ndences?\nare therefore invariant to both afﬁne geometric and photometric (linear bias-gain or smooth\nmonotonic) transformations (Figure 4.15). If desired, an afﬁne coordinate frame can be ﬁt to\neach detected region using its moment matrix.\nThe area of feature point detectors continues to be very active, with papers appearing ev-\nery year at major computer vision conferences (Xiao and Shah 2003; Koethe 2003; Carneiro\nand Jepson 2005; Kenney, Zuliani, and Manjunath 2005; Bay, Tuytelaars, and Van Gool 2006;\nPlatel, Balmachnova, Florack et al. 2006; Rosten and Drummond 2006). Mikolajczyk, Tuyte-\nlaars, Schmid et al. (2005) survey a number of popular afﬁne region detectors and provide\nexperimental comparisons of their invariance to common image transformations such as scal-\ning, rotations, noise, and blur. These experimental results, code, and pointers to the surveyed\npapers can be found on their Web site at http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/.\nOf course, keypoints are not the only features that can be used for registering images.\nZoghlami, Faugeras, and Deriche (1997) use line segments as well as point-like features to\nestimate homographies between pairs of images, whereas Bartoli, Coquerelle, and Sturm\n(2004) use line segments with local correspondences along the edges to extract 3D structure\nand motion. Tuytelaars and Van Gool (2004) use afﬁne invariant regions to detect corre-\nspondences for wide baseline stereo matching, whereas Kadir, Zisserman, and Brady (2004)\ndetect salient regions where patch entropy and its rate of change with scale are locally max-\nimal. Corso and Hager (2005) use a related technique to ﬁt 2D oriented Gaussian kernels\nto homogeneous regions. More details on techniques for ﬁnding and matching curves, lines,\nand regions can be found later in this chapter.\n222\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.17\nMOPS descriptors are formed using an 8 × 8 sampling of bias and gain nor-\nmalized intensity values, with a sample spacing of ﬁve pixels relative to the detection scale\n(Brown, Szeliski, and Winder 2005) c⃝2005 IEEE. This low frequency sampling gives the\nfeatures some robustness to interest point location error and is achieved by sampling at a\nhigher pyramid level than the detection scale.\n4.1.2 Feature descriptors\nAfter detecting features (keypoints), we must match them, i.e., we must determine which\nfeatures come from corresponding locations in different images. In some situations, e.g., for\nvideo sequences (Shi and Tomasi 1994) or for stereo pairs that have been rectiﬁed (Zhang,\nDeriche, Faugeras et al. 1995; Loop and Zhang 1999; Scharstein and Szeliski 2002), the lo-\ncal motion around each feature point may be mostly translational. In this case, simple error\nmetrics, such as the sum of squared differences or normalized cross-correlation, described\nin Section 8.1 can be used to directly compare the intensities in small patches around each\nfeature point. (The comparative study by Mikolajczyk and Schmid (2005), discussed below,\nuses cross-correlation.) Because feature points may not be exactly located, a more accurate\nmatching score can be computed by performing incremental motion reﬁnement as described\nin Section 8.1.3 but this can be time consuming and can sometimes even decrease perfor-\nmance (Brown, Szeliski, and Winder 2005).\nIn most cases, however, the local appearance of features will change in orientation and\nscale, and sometimes even undergo afﬁne deformations. Extracting a local scale, orientation,\nor afﬁne frame estimate and then using this to resample the patch before forming the feature\ndescriptor is thus usually preferable (Figure 4.17).\nEven after compensating for these changes, the local appearance of image patches will\nusually still vary from image to image. How can we make image descriptors more invariant to\nsuch changes, while still preserving discriminability between different (non-corresponding)\npatches (Figure 4.16)? Mikolajczyk and Schmid (2005) review some recently developed\nview-invariant local image descriptors and experimentally compare their performance. Be-\nlow, we describe a few of these descriptors in more detail.",
  "image_path": "page_243.jpg",
  "pages": [
    242,
    243,
    244
  ]
}