{
  "doc_id": "pages_607_609",
  "text": "12.2 Active rangeﬁnding\n585\nSurface\nCCD\nLaser\n(a)\nDirection of travel\nObject\nCCD\nCCD image\n    plane\nLaser\nCylindrical lens\nLaser \nsheet\nσz\nσx\n(b)\n(c)\n(d)\nFigure 12.5 Range data scanning (Curless and Levoy 1996) c⃝1996 ACM: (a) a laser dot\non a surface is imaged by a CCD sensor; (b) a laser stripe (sheet) is imaged by the sensor (the\ndeformation of the stripe encodes the distance to the object); (c) the resulting set of 3D points\nare turned into (d) a triangulated mesh.\ndistance settings (Pentland 1987; Nayar, Watanabe, and Noguchi 1996) or to translate\nthe object in depth and look for the point of maximum sharpness (Nayar and Nakagawa\n1994).\n• The magniﬁcation of the object can vary as the focus distance is changed or the object is\nmoved. This can be modeled either explicitly (making correspondence more difﬁcult)\nor using telecentric optics, which approximate an orthographic camera and require an\naperture in front of the lens (Nayar, Watanabe, and Noguchi 1996).\n• The amount of defocus must be reliably estimated. A simple approach is to average the\nsquared gradient in a region but this suffers from several problems, including the image\nmagniﬁcation problem mentioned above. A better solution is to use carefully designed\nrational ﬁlters (Watanabe and Nayar 1998).\nFigure 12.4 shows an example of a real-time depth from defocus sensor, which employs\ntwo imaging chips at slightly different depths sharing a common optical path, as well as an\nactive illumination system that projects a checkerboard pattern from the same direction. As\nyou can see in Figure 12.4b–g, the system produces high-accuracy real-time depth maps for\nboth static and dynamic scenes.\n12.2 Active rangeﬁnding\nAs we have seen in the previous section, actively lighting a scene, whether for the purpose\nof estimating normals using photometric stereo or for adding artiﬁcial texture for shape from\ndefocus, can greatly improve the performance of vision systems. This kind of active illu-\nmination has been used from the earliest days of machine vision to construct highly reliable\n586\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.6\nShape scanning using cast shadows (Bouguet and Perona 1999) c⃝1999\nSpringer: (a) camera setup with a point light source (a desk lamp without its reﬂector), a\nhand-held stick casting a shadow, and (b) the objects being scanned in front of two planar\nbackgrounds. (c) Real-time depth map using a pulsed illumination system (Iddan and Yahav\n2001) c⃝2001 SPIE.\nsensors for estimating 3D depth images using a variety of rangeﬁnding (or range sensing)\ntechniques (Besl 1989; Curless 1999; Hebert 2000).\nOne of the most popular active illumination sensors is a laser or light stripe sensor, which\nsweeps a plane of light across the scene or object while observing it from an offset viewpoint,\nas shown in Figure 12.5b (Rioux and Bird 1993; Curless and Levoy 1995). As the stripe falls\nacross the object, it deforms its shape according to the shape of the surface it is illuminating.\nIt is then a simple matter of using optical triangulation to estimate the 3D locations of all the\npoints seen in a particular stripe. In more detail, knowledge of the 3D plane equation of the\nlight stripe allows us to infer the 3D location corresponding to each illuminated pixel, as pre-\nviously discussed in (2.70–2.71). The accuracy of light striping techniques can be improved\nby ﬁnding the exact temporal peak in illumination for each pixel (Curless and Levoy 1995).\nThe ﬁnal accuracy of a scanner can be determined using slant edge modulation techniques,\ni.e., by imaging sharp creases in a calibration object (Goesele, Fuchs, and Seidel 2003).\nAn interesting variant on light stripe rangeﬁnding is presented by Bouguet and Perona\n(1999). Instead of projecting a light stripe, they simply wave a stick casting a shadow over a\nscene or object illuminated by a point light source such as a lamp or the sun (Figure 12.6a).\nAs the shadow falls across two background planes whose orientation relative to the cam-\nera is known (or inferred during pre-calibration), the plane equation for each stripe can be\ninferred from the two projected lines, whose 3D equations are known (Figure 12.6b). The\ndeformation of the shadow as it crosses the object being scanned then reveals its 3D shape,\nas with regular light stripe rangeﬁnding (Exercise 12.2). This technique can also be used to\nestimate the 3D geometry of a background scene and how its appearance varies as it moves\ninto shadow, in order to cast new shadows onto the scene (Chuang, Goldman, Curless et al.\n12.2 Active rangeﬁnding\n587\n2003) (Section 10.4.3).\nThe time it takes to scan an object using a light stripe technique is proportional to the\nnumber of depth planes used, which is usually comparable to the number of pixels across\nan image. A much faster scanner can be constructed by turning different projector pixels on\nand off in a structured manner, e.g., using a binary or Gray code (Besl 1989). For example,\nlet us assume that the LCD projector we are using has 1024 columns of pixels. Taking the\n10-bit binary code corresponding to each column’s address (0 . . . 1023), we project the ﬁrst\nbit, then the second, etc. After 10 projections (e.g., a third of a second for a synchronized\n30Hz camera-projector system), each pixel in the camera knows which of the 1024 columns\nof projector light it is seeing. A similar approach can also be used to estimate the refractive\nproperties of an object by placing a monitor behind the object (Zongker, Werner, Curless et al.\n1999; Chuang, Zongker, Hindorff et al. 2000) (Section 13.4). Very fast scanners can also be\nconstructed with a single laser beam, i.e., a real-time ﬂying spot optical triangulation scanner\n(Rioux, Bechthold, Taylor et al. 1987).\nIf even faster, i.e., frame-rate, scanning is required, we can project a single textured pat-\ntern into the scene. Proesmans, Van Gool, and Defoort (1998) describe a system where a\ncheckerboard grid is projected onto an object (e.g., a person’s face) and the deformation of\nthe grid is used to infer 3D shape. Unfortunately, such a technique only works if the surface\nis continuous enough to link all of the grid points.\nA much better system can be constructed using high-speed custom illumination and sens-\ning hardware. Iddan and Yahav (2001) describe the construction of their 3DV Zcam video-\nrate depth sensing camera, which projects a pulsed plane of light onto the scene and then\nintegrates the returning light for a short interval, essentially obtaining time-of-ﬂight mea-\nsurement for the distance to individual pixels in the scene. A good description of earlier\ntime-of-ﬂight systems, including amplitude and frequency modulation schemes for LIDAR,\ncan be found in (Besl 1989).\nInstead of using a single camera, it is also possible to construct an active illumination\nrange sensor using stereo imaging setups. The simplest way to do this is to just project ran-\ndom stripe patterns onto the scene to create synthetic texture, which helps match textureless\nsurfaces (Kang, Webb, Zitnick et al. 1995). Projecting a known series of stripes, just as in\ncoded pattern single-camera rangeﬁnding, makes the correspondence between pixels unam-\nbiguous and allows for the recovery of depth estimates at pixels only seen in a single camera\n(Scharstein and Szeliski 2003). This technique has been used to produce large numbers of\nhighly accurate registered multi-image stereo pairs and depth maps for the purpose of eval-\nuating stereo correspondence algorithms (Scharstein and Szeliski 2002; Hirschm¨uller and\nScharstein 2009) and learning depth map priors and parameters (Scharstein and Pal 2007).\nWhile projecting multiple patterns usually requires the scene or object to remain still, ad-\nditional processing can enable the production of real-time depth maps for dynamic scenes.",
  "image_path": "page_608.jpg",
  "pages": [
    607,
    608,
    609
  ]
}