{
  "doc_id": "pages_199_201",
  "text": "3.7 Global optimization\n177\n+ 2cm(i, j)[f(i + 1, j + 1) −f(i + 1, j) −f(i, j + 1) + f(i, j)]2\n+ cy(i, j)[f(i, j + 1) −2f(i, j) + f(i, j −1)]2,\nwhere h is the size of the ﬁnite element grid. The h factor is only important if the energy is\nbeing discretized at a variety of resolutions, as in coarse-to-ﬁne or multigrid techniques.\nThe optional smoothness weights sx(i, j) and sy(i, j) control the location of horizon-\ntal and vertical tears (or weaknesses) in the surface. For other problems, such as coloriza-\ntion (Levin, Lischinski, and Weiss 2004) and interactive tone mapping (Lischinski, Farbman,\nUyttendaele et al. 2006a), they control the smoothness in the interpolated chroma or expo-\nsure ﬁeld and are often set inversely proportional to the local luminance gradient strength.\nFor second-order problems, the crease variables cx(i, j), cm(i, j), and cy(i, j) control the\nlocations of creases in the surface (Terzopoulos 1988; Szeliski 1990a).\nThe data values gx(i, j) and gy(i, j) are gradient data terms (constraints) used by al-\ngorithms, such as photometric stereo (Section 12.1.1), HDR tone mapping (Section 10.2.1)\n(Fattal, Lischinski, and Werman 2002), Poisson blending (Section 9.3.4) (P´erez, Gangnet,\nand Blake 2003), and gradient-domain blending (Section 9.3.4) (Levin, Zomet, Peleg et al.\n2004). They are set to zero when just discretizing the conventional ﬁrst-order smoothness\nfunctional (3.94).\nThe two-dimensional discrete data energy is written as\nEd =\nX\ni,j\nw(i, j)[f(i, j) −d(i, j)]2,\n(3.101)\nwhere the local weights w(i, j) control how strongly the data constraint is enforced. These\nvalues are set to zero where there is no data and can be set to the inverse variance of the data\nmeasurements when there is data (as discussed by Szeliski (1989) and in Section 3.7.2).\nThe total energy of the discretized problem can now be written as a quadratic form\nE = Ed + λEs = xT Ax −2xT b + c,\n(3.102)\nwhere x = [f(0, 0) . . . f(m −1, n −1)] is called the state vector.22\nThe sparse symmetric positive-deﬁnite matrix A is called the Hessian since it encodes the\nsecond derivative of the energy function.23 For the one-dimensional, ﬁrst-order problem, A\nis tridiagonal; for the two-dimensional, ﬁrst-order problem, it is multi-banded with ﬁve non-\nzero entries per row. We call b the weighted data vector. Minimizing the above quadratic\nform is equivalent to solving the sparse linear system\nAx = b,\n(3.103)\n22 We use x instead of f because this is the more common form in the numerical analysis literature (Golub and\nVan Loan 1996).\n23 In numerical analysis, A is called the coefﬁcient matrix (Saad 2003); in ﬁnite element analysis (Bathe 2007), it\nis called the stiffness matrix.\n178\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure 3.55\nGraphical model interpretation of ﬁrst-order regularization. The white circles\nare the unknowns f(i, j) while the dark circles are the input data d(i, j). In the resistive grid\ninterpretation, the d and f values encode input and output voltages and the black squares\ndenote resistors whose conductance is set to sx(i, j), sy(i, j), and w(i, j). In the spring-mass\nsystem analogy, the circles denote elevations and the black squares denote springs. The same\ngraphical model can be used to depict a ﬁrst-order Markov random ﬁeld (Figure 3.56).\nwhich can be done using a variety of sparse matrix techniques, such as multigrid (Briggs,\nHenson, and McCormick 2000) and hierarchical preconditioners (Szeliski 2006b), as de-\nscribed in Appendix A.5.\nWhile regularization was ﬁrst introduced to the vision community by Poggio, Torre, and\nKoch (1985) and Terzopoulos (1986b) for problems such as surface interpolation, it was\nquickly adopted by other vision researchers for such varied problems as edge detection (Sec-\ntion 4.2), optical ﬂow (Section 8.4), and shape from shading (Section 12.1) (Poggio, Torre,\nand Koch 1985; Horn and Brooks 1986; Terzopoulos 1986b; Bertero, Poggio, and Torre 1988;\nBrox, Bruhn, Papenberg et al. 2004). Poggio, Torre, and Koch (1985) also showed how the\ndiscrete energy deﬁned by Equations (3.100–3.101) could be implemented in a resistive grid,\nas shown in Figure 3.55. In computational photography (Chapter 10), regularization and its\nvariants are commonly used to solve problems such as high-dynamic range tone mapping\n(Fattal, Lischinski, and Werman 2002; Lischinski, Farbman, Uyttendaele et al. 2006a), Pois-\nson and gradient-domain blending (P´erez, Gangnet, and Blake 2003; Levin, Zomet, Peleg et\nal. 2004; Agarwala, Dontcheva, Agrawala et al. 2004), colorization (Levin, Lischinski, and\nWeiss 2004), and natural image matting (Levin, Lischinski, and Weiss 2008).\nRobust regularization\nWhile regularization is most commonly formulated using quadratic (L2) norms (compare\nwith the squared derivatives in (3.92–3.95) and squared differences in (3.100–3.101)), it can\n3.7 Global optimization\n179\nalso be formulated using non-quadratic robust penalty functions (Appendix B.3). For exam-\nple, (3.100) can be generalized to\nE1r\n=\nX\ni,j\nsx(i, j)ρ(f(i + 1, j) −f(i, j))\n(3.104)\n+ sy(i, j)ρ(f(i, j + 1) −f(i, j)),\nwhere ρ(x) is some monotonically increasing penalty function. For example, the family of\nnorms ρ(x) = |x|p is called p-norms. When p < 2, the resulting smoothness terms become\nmore piecewise continuous than totally smooth, which can better model the discontinuous\nnature of images, ﬂow ﬁelds, and 3D surfaces.\nAn early example of robust regularization is the graduated non-convexity (GNC) algo-\nrithm introduced by Blake and Zisserman (1987). Here, the norms on the data and derivatives\nare clamped to a maximum value\nρ(x) = min(x2, V ).\n(3.105)\nBecause the resulting problem is highly non-convex (it has many local minima), a continua-\ntion method is proposed, where a quadratic norm (which is convex) is gradually replaced by\nthe non-convex robust norm (Allgower and Georg 2003). (Around the same time, Terzopou-\nlos (1988) was also using continuation to infer the tear and crease variables in his surface\ninterpolation problems.)\nToday, it is more common to use the L1 (p = 1) norm, which is often called total variation\n(Chan, Osher, and Shen 2001; Tschumperl´e and Deriche 2005; Tschumperl´e 2006; Kaftory,\nSchechner, and Zeevi 2007). Other norms, for which the inﬂuence (derivative) more quickly\ndecays to zero, are presented by Black and Rangarajan (1996); Black, Sapiro, Marimont et\nal. (1998) and discussed in Appendix B.3.\nEven more recently, hyper-Laplacian norms with p < 1 have gained popularity, based\non the observation that the log-likelihood distribution of image derivatives follows a p ≈\n0.5 −0.8 slope and is therefore a hyper-Laplacian distribution (Simoncelli 1999; Levin and\nWeiss 2007; Weiss and Freeman 2007; Krishnan and Fergus 2009). Such norms have an even\nstronger tendency to prefer large discontinuities over small ones. See the related discussion\nin Section 3.7.2 (3.114).\nWhile least squares regularized problems using L2 norms can be solved using linear sys-\ntems, other p-norms require different iterative techniques, such as iteratively reweighted least\nsquares (IRLS), Levenberg–Marquardt, or alternation between local non-linear subproblems\nand global quadratic regularization (Krishnan and Fergus 2009). Such techniques are dis-\ncussed in Section 6.1.3 and Appendices A.3 and B.3.",
  "image_path": "page_200.jpg",
  "pages": [
    199,
    200,
    201
  ]
}