{
  "doc_id": "pages_711_713",
  "text": "14.3 Instance recognition\n689\n(a)\n(b)\nFigure 14.29 Matching based on visual words (Sivic and Zisserman 2009) c⃝2009 IEEE.\n(a) Features in the query region on the left are matched to corresponding features in a highly\nranked video frame. (b) Results after removing the stop words and ﬁltering the results using\nspatial consistency.\nSivic and Zisserman (2003) perform this mapping using k-means clustering, while some of\nnewer methods discussed below (Nist´er and Stew´enius 2006; Philbin, Chum, Isard et al.\n2007) use alternative techniques, such as vocabulary trees or randomized forests. To keep the\nclustering time manageable, only a few hundred video frames are used to learn the cluster\ncenters, which still involves estimating several thousand clusters from about 300,000 descrip-\ntors. At visual query time, each feature in a new query region (e.g., Figure 14.28a, which is\na cropped region from a larger video frame) is mapped to its corresponding visual word. To\nkeep very common patterns from contaminating the results, a stop list of the most common\nvisual words is created and such words are dropped from further consideration.\nOnce a query image or region has been mapped into its constituent visual words, likely\nmatching images or video frames must then be retrieved from the database. Information\nretrieval systems do this by matching word distributions (term frequencies) nid/nd between\nthe query and target documents, where nid is how many times word i occurs in document d,\nand nd is the total number of words in document d. In order to downweight words that occur\nfrequently and to focus the search on rarer (and hence, more informative) terms, an inverse\ndocument frequency weighting log N/Ni is applied, where Ni is the number of documents\ncontaining word i, and N is the total number of documents in the database. The combination\nof these two factors results in the term frequency-inverse document frequency (tf-idf) measure,\nti = nid\nnd\nlog N\nNi\n.\n(14.33)\nAt match time, each document (or query region) is represented by its tf-idf vector,\nt = (t1, . . . , ti, . . . tm).\n(14.34)\nThe similarity between two documents is measured by the dot product between their corre-\nsponding normalized vectors ˆt = t/∥t∥, which means that their dissimilarity is proportional\nto their Euclidean distance. In their journal paper, Sivic and Zisserman (2009) compare this\n690\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Vocabulary construction (off-line)\n(a) Extract afﬁne covariant regions from each database image.\n(b) Compute descriptors and optionally whiten them to make Euclidean dis-\ntances meaningful (Sivic and Zisserman 2009).\n(c) Cluster the descriptors into visual words, either using k-means (Sivic and\nZisserman 2009), hierarchical clustering (Nist´er and Stew´enius 2006), or\nrandomized k-d trees (Philbin, Chum, Isard et al. 2007).\n(d) Decide which words are too common and put them in the stop list.\n2. Database construction (off-line)\n(a) Compute term frequencies for the visual word in each image, document fre-\nquencies for each word, and normalized tf-idf vectors for each document.\n(b) Compute inverted indices from visual words to images (with word counts).\n3. Image retrieval (on-line)\n(a) Extract regions, descriptors, and visual words, and compute a tf-idf vector\nfor the query image or region.\n(b) Retrieve the top image candidates, either by exhaustively comparing sparse\ntf-idf vectors (Sivic and Zisserman 2009) or by using inverted indices to ex-\namine only a subset of the images (Nist´er and Stew´enius 2006).\n(c) Optionally re-rank or verify all the candidate matches, using either spatial\nconsistency (Sivic and Zisserman 2009) or an afﬁne (or simpler) transforma-\ntion model (Philbin, Chum, Isard et al. 2007).\n(d) Optionally expand the answer set by re-submitting highly ranked matches as\nnew queries (Chum, Philbin, Sivic et al. 2007).\nAlgorithm 14.2 Image retrieval using visual words (Sivic and Zisserman 2009; Nist´er and\nStew´enius 2006; Philbin, Chum, Isard et al. 2007; Chum, Philbin, Sivic et al. 2007; Philbin,\nChum, Sivic et al. 2008).\n14.3 Instance recognition\n691\nsimple metric to a dozen other metrics and conclude that it performs just about as well as\nmore complicated metrics. Because the number of non-zero ti terms in a typical query or\ndocument is small (M ≈200) compared to the number of visual words (V ≈20, 000), the\ndistance between pairs of (sparse) tf-idf vectors can be computed quite quickly.\nAfter retrieving the top Ns = 500 documents based on word frequencies, Sivic and Zis-\nserman (2009) re-rank these results using spatial consistency. This step involves taking every\nmatching feature and counting the number of k = 15 nearest adjacent features that also match\nbetween the two documents. (This latter process is accelerated using inverted ﬁles, which we\ndiscuss in more detail below.) As shown in Figure 14.29, this step helps remove spurious false\npositive matches and produces a better estimate of which frames and regions in the video are\nactually true matches. Algorithm 14.2 summarizes the processing steps involved in image\nretrieval using visual words.\nWhile this approach works well for tens of thousand of visual words and thousands of\nkeyframes, as the size of the database continues to increase, both the time to quantize each\nfeature and to ﬁnd potential matching frames or images can become prohibitive. Nist´er and\nStew´enius (2006) address this problem by constructing a hierarchical vocabulary tree, where\nfeature vectors are hierarchically clustered into a k-way tree of prototypes. (This technique is\nalso known as tree-structured vector quantization (Gersho and Gray 1991).) At both database\nconstruction time and query time, each descriptor vector is compared to several prototypes\nat a given level in the vocabulary tree and the branch with the closest prototype is selected\nfor further reﬁnement (Figure 14.30). In this way, vocabularies with millions (106) of words\ncan be supported, which enables individual words to be far more discriminative, while only\nrequiring 10 · 6 comparisons for quantizing each descriptor.\nAt query time, each node in the vocabulary tree keeps its own inverted ﬁle index, so that\nfeatures that match a particular node in the tree can be rapidly mapped to potential matching\nimages. (Interior leaf nodes just use the inverted indices of their corresponding leaf-node\ndescendants.) To score a particular query tf-idf vector tq against all document vectors {tj}\nusing an Lp metric,18 the non-zero tiq entries in tq are used to fetch corresponding non-zero\ntij entries, and the Lp norm is efﬁciently computed as\n∥tq −tj∥p\np = 2 +\nX\ni|tiq>0∧tij>0\n(|tiq −tij|p −|tiq|p −|tij|p).\n(14.35)\nIn order to mitigate quantization errors due to noise in the descriptor vectors, Nist´er and\nStew´enius (2006) not only score leaf nodes in the vocabulary tree (corresponding to visual\nwords), but also score interior nodes in the tree, which correspond to clusters of similar visual\nwords.\n18 In their actual implementation, Nist´er and Stew´enius (2006) use an L1 metric.",
  "image_path": "page_712.jpg",
  "pages": [
    711,
    712,
    713
  ]
}