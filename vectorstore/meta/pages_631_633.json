{
  "doc_id": "pages_631_633",
  "text": "12.6 Model-based reconstruction\n609\nFigure 12.21 Estimating human shape and pose from a single image using a parametric 3D\nmodel (Guan, Weiss, Bˇalan et al. 2009) c⃝2009 IEEE.\n(Figures 5.6–5.8). It was subsequently applied to whole-body tracking (Deutscher, Blake,\nand Reid 2000; Sidenbladh, Black, and Fleet 2000; Deutscher and Reid 2005) and continues\nto be used in modern trackers (Ong, Micilotta, Bowden et al. 2006). Alternative approaches\nto handling the uncertainty inherent in tracking include multiple hypothesis tracking (Cham\nand Rehg 1999) and inﬂated covariances (Sminchisescu and Triggs 2001).\nFigure 12.20c–d shows an example of a sophisticated spatio-temporal probabilistic graph-\nical model called loose-limbed people, which models not only the geometric relationship be-\ntween various limbs, but also their likely temporal dynamics (Sigal, Bhatia, Roth et al. 2004).\nThe conditional probabilities relating various limbs and time instances are learned from train-\ning data, and particle ﬁltering is used to perform the ﬁnal pose inference.\nAdaptive shape modeling.\nAnother essential component of whole body modeling and\ntracking is the ﬁtting of parameterized shape models to visual data. As we saw in Sec-\ntion 12.6.3 (Figure 12.19), the availability of large numbers of registered 3D range scans can\nbe used to create morphable models of shape and appearance (Allen, Curless, and Popovi´c\n2003). Building on this work, Anguelov, Srinivasan, Koller et al. (2005) develop a sophis-\nticated system called SCAPE (Shape Completion and Animation for PEople), which ﬁrst\n610\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nacquires a large number of range scans of different people and of one person in different\nposes, and then registers these scans using semi-automated marker placement. The registered\ndatasets are used to model the variation in shape as a function of personal characteristics and\nskeletal pose, e.g., the bulging of muscles as certain joints are ﬂexed (Figure 12.21, top row).\nThe resulting system can then be used for shape completion, i.e., the recovery of a full 3D\nmesh model from a small number of captured markers, by ﬁnding the best model parameters\nin both shape and pose space that ﬁt the measured data.\nBecause it is constructed completely from scans of people in close-ﬁtting clothing and\nuses a parametric shape model, the SCAPE system cannot cope with people wearing loose-\nﬁtting clothing. B˘alan and Black (2008) overcome this limitation by estimating the body\nshape that ﬁts within the visual hull of the same person observed in multiple poses, while\nVlasic, Baran, Matusik et al. (2008) adapt an initial surface mesh ﬁtted with a parametric\nshape model to better match the visual hull.\nWhile the preceding body ﬁtting and pose estimation systems use multiple views to esti-\nmate body shape, even more recent work by Guan, Weiss, Bˇalan et al. (2009) can ﬁt a human\nshape and pose model to a single image of a person on a natural background. Manual ini-\ntialization is used to estimate a rough pose (skeleton) and height model, and this is then used\nto segment the person’s outline using the Grab Cut segmentation algorithm (Section 5.5).\nThe shape and pose estimate are then reﬁned using a combination of silhouette edge cues\nand shading information (Figure 12.21). The resulting 3D model can be used to create novel\nanimations.\nActivity recognition.\nThe ﬁnal widely studied topic in human modeling is motion, activity,\nand action recognition (Bobick 1997; Hu, Tan, Wang et al. 2004; Hilton, Fua, and Ronfard\n2006). Examples of actions that are commonly recognized include walking and running,\njumping, dancing, picking up objects, sitting down and standing up, and waving. Recent\nrepresentative papers on these topics have been written by Robertson and Reid (2006), Smin-\nchisescu, Kanaujia, and Metaxas (2006), Weinland, Ronfard, and Boyer (2006), Yilmaz and\nShah (2006), and Gorelick, Blank, Shechtman et al. (2007).\n12.7 Recovering texture maps and albedos\nAfter a 3D model of an object or person has been acquired, the ﬁnal step in modeling is\nusually to recover a texture map to describe the object’s surface appearance. This ﬁrst requires\nestablishing a parameterization for the (u, v) texture coordinates as a function of 3D surface\nposition. One simple way to do this is to associate a separate texture map with each triangle\n(or pair of triangles). More space-efﬁcient techniques involve unwrapping the surface onto\n12.7 Recovering texture maps and albedos\n611\none or more maps, e.g., using a subdivision mesh (Section 12.3.2) (Eck, DeRose, Duchamp\net al. 1995) or a geometry image (Section 12.3.3) (Gu, Gortler, and Hoppe 2002).\nOnce the (u, v) coordinates for each triangle have been ﬁxed, the perspective projec-\ntion equations mapping from texture (u, v) to an image j’s pixel (uj, vj) coordinates can be\nobtained by concatenating the afﬁne (u, v) →(X, Y, Z) mapping with the perspective ho-\nmography (X, Y, Z) →(uj, vj) (Szeliski and Shum 1997). The color values for the (u, v)\ntexture map can then be re-sampled and stored, or the original image can itself be used as the\ntexture source using projective texture mapping (OpenGL-ARB 1997).\nThe situation becomes more involved when more than one source image is available for\nappearance recovery, which is the usual case. One possibility is to use a view-dependent\ntexture map (Section 13.1.1), in which a different source image (or combination of source\nimages) is used for each polygonal face based on the angles between the virtual camera, the\nsurface normals, and the source images (Debevec, Taylor, and Malik 1996; Pighin, Hecker,\nLischinski et al. 1998). An alternative approach is to estimate a complete Surface Light Field\nfor each surface point (Wood, Azuma, Aldinger et al. 2000), as described in Section 13.3.2.\nIn some situations, e.g., when using models in traditional 3D games, it is preferable to\nmerge all of the source images into a single coherent texture map during pre-processing.\nIdeally, each surface triangle should select the source image where it is seen most directly\n(perpendicular to its normal) and at the resolution best matching the texture map resolution.14\nThis can be posed as a graph cut optimization problem, where the smoothness term encour-\nages adjacent triangles to use similar source images, followed by blending to compensate\nfor exposure differences (Lempitsky and Ivanov 2007; Sinha, Steedly, Szeliski et al. 2008).\nEven better results can be obtained by explicitly modeling geometric and photometric mis-\nalignments between the source images (Shum and Szeliski 2000; Gal, Wexler, Ofek et al.\n2010).\nThese kinds of approaches produce good results when the lighting stays ﬁxed with respect\nto the object, i.e., when the camera moves around the object or space. When the lighting is\nstrongly directional, however, and the object is being moved relative to this lighting, strong\nshading effects or specularities may be present, which will interfere with the reliable recov-\nery of a texture (albedo) map. In this case, it is preferable to explicitly undo the shading\neffects (Section 12.1) by modeling the light source directions and estimating the surface re-\nﬂectance properties while recovering the texture map (Sato and Ikeuchi 1996; Sato, Wheeler,\nand Ikeuchi 1997; Yu and Malik 1998; Yu, Debevec, Malik et al. 1999). Figure 12.22 shows\nthe results of one such approach, where the specularities are ﬁrst removed while estimat-\ning the matte reﬂectance component (albedo) and then later re-introduced by estimating the\nspecular component ks in a Torrance–Sparrow reﬂection model (2.91).\n14 When surfaces are seen at oblique viewing angles, it may be necessary to blend different images together to\nobtain the best resolution (Wang, Kang, Szeliski et al. 2001).",
  "image_path": "page_632.jpg",
  "pages": [
    631,
    632,
    633
  ]
}