{
  "doc_id": "pages_211_213",
  "text": "3.7 Global optimization\n189\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure 3.62\nGraphical model for a conditional random ﬁeld (CRF). The additional green\nedges show how combinations of sensed data inﬂuence the smoothness in the underlying\nMRF prior model, i.e., sx(i, j) and sy(i, j) in (3.113) depend on adjacent d(i, j) values.\nThese additional links (factors) enable the smoothness to depend on the input data. However,\nthey make sampling from this MRF more complex.\nis modiﬁed so that the smoothness terms sx(x, y) and sy(x, y) in Figure 3.56 and (3.113)\ndepend on the magnitude of the gradient between adjacent pixels.25\nSince the smoothness term now depends on the data, Bayes’ Rule (3.117) no longer ap-\nplies. Instead, we use a direct model for the posterior distribution p(x|y), whose negative log\nlikelihood can be written as\nE(x|y)\n=\nEd(x, y) + Es(x, y)\n=\nX\np\nVp(xp, y) +\nX\n(p,q)∈N\nVp,q(xp, xq, y),\n(3.118)\nusing the notation introduced in (3.116). The resulting probability distribution is called a\nconditional random ﬁeld (CRF) and was ﬁrst introduced to the computer vision ﬁeld by Ku-\nmar and Hebert (2003), based on earlier work in text modeling by Lafferty, McCallum, and\nPereira (2001).\nFigure 3.62 shows a graphical model where the smoothness terms depend on the data\nvalues. In this particular model, each smoothness term depends only on its adjacent pair of\ndata values, i.e., terms are of the form Vp,q(xp, xq, yp, yq) in (3.118).\nThe idea of modifying smoothness terms in response to input data is not new. For ex-\nample, Boykov and Jolly (2001) used this idea for interactive segmentation, as shown in\nFigure 3.61, and it is now widely used in image segmentation (Section 5.5) (Blake, Rother,\n25 An alternative formulation that also uses detected edges to modulate the smoothness of a depth or motion ﬁeld\nand hence to integrate multiple lower level vision modules is presented by Poggio, Gamble, and Little (1988).\n190\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nd (i, j+1)\nFigure 3.63\nGraphical model for a discriminative random ﬁeld (DRF). The additional green\nedges show how combinations of sensed data, e.g., d(i, j + 1), inﬂuence the data term for\nf(i, j). The generative model is therefore more complex, i.e., we cannot just apply a simple\nfunction to the unknown variables and add noise.\nBrown et al. 2004; Rother, Kolmogorov, and Blake 2004), denoising (Tappen, Liu, Freeman\net al. 2007), and object recognition (Section 14.4.3) (Winn and Shotton 2006; Shotton, Winn,\nRother et al. 2009).\nIn stereo matching, the idea of encouraging disparity discontinuities to coincide with\nintensity edges goes back even further to the early days of optimization and MRF-based\nalgorithms (Poggio, Gamble, and Little 1988; Fua 1993; Bobick and Intille 1999; Boykov,\nVeksler, and Zabih 2001) and is discussed in more detail in (Section 11.5).\nIn addition to using smoothness terms that adapt to the input data, Kumar and Hebert\n(2003) also compute a neighborhood function over the input data for each Vp(xp, y) term,\nas illustrated in Figure 3.63, instead of using the classic unary MRF data term Vp(xp, yp)\nshown in Figure 3.56.26 Because such neighborhood functions can be thought of as dis-\ncriminant functions (a term widely used in machine learning (Bishop 2006)), they call the\nresulting graphical model a discriminative random ﬁeld (DRF). In their paper, Kumar and\nHebert (2006) show that DRFs outperform similar CRFs on a number of applications, such\nas structure detection (Figure 3.64) and binary image denoising.\nHere again, one could argue that previous stereo correspondence algorithms also look at\na neighborhood of input data, either explicitly, because they compute correlation measures\n(Criminisi, Cross, Blake et al. 2006) as data terms, or implicitly, because even pixel-wise\ndisparity costs look at several pixels in either the left or right image (Barnard 1989; Boykov,\nVeksler, and Zabih 2001).\n26 Kumar and Hebert (2006) call the unary potentials Vp(xp, y) association potentials and the pairwise potentials\nVp,q(xp, yq, y) interaction potentials.\n3.7 Global optimization\n191\nFigure 3.64\nStructure detection results using an MRF (left) and a DRF (right) (Kumar and\nHebert 2006) c⃝2006 Springer.\nWhat, then are the advantages and disadvantages of using conditional or discriminative\nrandom ﬁelds instead of MRFs?\nClassic Bayesian inference (MRF) assumes that the prior distribution of the data is in-\ndependent of the measurements. This makes a lot of sense: if you see a pair of sixes when\nyou ﬁrst throw a pair of dice, it would be unwise to assume that they will always show up\nthereafter. However, if after playing for a long time you detect a statistically signiﬁcant bias,\nyou may want to adjust your prior. What CRFs do, in essence, is to select or modify the prior\nmodel based on observed data. This can be viewed as making a partial inference over addi-\ntional hidden variables or correlations between the unknowns (say, a label, depth, or clean\nimage) and the knowns (observed images).\nIn some cases, the CRF approach makes a lot of sense and is, in fact, the only plausi-\nble way to proceed. For example, in grayscale image colorization (Section 10.3.2) (Levin,\nLischinski, and Weiss 2004), the best way to transfer the continuity information from the\ninput grayscale image to the unknown color image is to modify local smoothness constraints.\nSimilarly, for simultaneous segmentation and recognition (Winn and Shotton 2006; Shotton,\nWinn, Rother et al. 2009), it makes a lot of sense to permit strong color edges to inﬂuence\nthe semantic image label continuities.\nIn other cases, such as image denoising, the situation is more subtle.\nUsing a non-\nquadratic (robust) smoothness term as in (3.113) plays a qualitatively similar role to setting\nthe smoothness based on local gradient information in a Gaussian MRF (GMRF) (Tappen,\nLiu, Freeman et al. 2007). (In more recent work, Tanaka and Okutomi (2008) use a larger\nneighborhood and full covariance matrix on a related Gaussian MRF.) The advantage of Gaus-\nsian MRFs, when the smoothness can be correctly inferred, is that the resulting quadratic\nenergy can be minimized in a single step. However, for situations where the discontinuities\nare not self-evident in the input data, such as for piecewise-smooth sparse data interpolation\n(Blake and Zisserman 1987; Terzopoulos 1988), classic robust smoothness energy minimiza-",
  "image_path": "page_212.jpg",
  "pages": [
    211,
    212,
    213
  ]
}