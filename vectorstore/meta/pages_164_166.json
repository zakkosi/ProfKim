{
  "doc_id": "pages_164_166",
  "text": "142\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nP\nn\nW\n(a)\n(b)\nFigure 3.25 One-dimensional Wiener ﬁlter: (a) power spectrum of signal Ps(f), noise level\nσ2, and Wiener ﬁlter transform W(f); (b) Wiener ﬁlter spatial kernel.\nwhich is the negative posterior log likelihood. The minimum of this quantity is easy to\ncompute,\nSopt =\nP −1\nn\nP −1\nn\n+ P −1\ns\nO =\nPs\nPs + Pn\nO =\n1\n1 + Pn/Ps\nO.\n(3.73)\nThe quantity\nW(ωx, ωy) =\n1\n1 + σ2n/Ps(ωx, ωy)\n(3.74)\nis the Fourier transform of the optimum Wiener ﬁlter needed to remove the noise from an\nimage whose power spectrum is Ps(ωx, ωy).\nNotice that this ﬁlter has the right qualitative properties, i.e., for low frequencies where\nPs ≫σ2\nn, it has unit gain, whereas for high frequencies, it attenuates the noise by a factor\nPs/σ2\nn. Figure 3.25 shows the one-dimensional transform W(f) and the corresponding ﬁlter\nkernel w(x) for the commonly assumed case of P(f) = f −2 (Field 1987). Exercise 3.16 has\nyou compare the Wiener ﬁlter as a denoising algorithm to hand-tuned Gaussian smoothing.\nThe methodology given above for deriving the Wiener ﬁlter can easily be extended to the\ncase where the observed image is a noisy blurred version of the original image,\no(x, y) = b(x, y) ∗s(x, y) + n(x, y),\n(3.75)\nwhere b(x, y) is the known blur kernel. Rather than deriving the corresponding Wiener ﬁl-\nter, we leave it as an exercise (Exercise 3.17), which also encourages you to compare your\nde-blurring results with unsharp masking and na¨ıve inverse ﬁltering. More sophisticated al-\ngorithms for blur removal are discussed in Sections 3.7 and 10.3.\nDiscrete cosine transform\nThe discrete cosine transform (DCT) is a variant of the Fourier transform particularly well-\nsuited to compressing images in a block-wise fashion. The one-dimensional DCT is com-\nputed by taking the dot product of each N-wide block of pixels with a set of cosines of\n3.4 Fourier transforms\n143\n-1.00\n-0.75\n-0.50\n-0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 3.26 Discrete cosine transform (DCT) basis functions: The ﬁrst DC (i.e., constant)\nbasis is the horizontal blue line, the second is the brown half-cycle waveform, etc. These\nbases are widely used in image and video compression standards such as JPEG.\ndifferent frequencies,\nF(k) =\nN−1\nX\ni=0\ncos\n\u0012 π\nN (i + 1\n2)k\n\u0013\nf(i),\n(3.76)\nwhere k is the coefﬁcient (frequency) index, and the 1/2-pixel offset is used to make the\nbasis coefﬁcients symmetric (Wallace 1991). Some of the discrete cosine basis functions are\nshown in Figure 3.26. As you can see, the ﬁrst basis function (the straight blue line) encodes\nthe average DC value in the block of pixels, while the second encodes a slightly curvy version\nof the slope.\nIn turns out that the DCT is a good approximation to the optimal Karhunen–Lo`eve decom-\nposition of natural image statistics over small patches, which can be obtained by performing\na principal component analysis (PCA) of images, as described in Section 14.2.1. The KL-\ntransform de-correlates the signal optimally (assuming the signal is described by its spectrum)\nand thus, theoretically, leads to optimal compression.\nThe two-dimensional version of the DCT is deﬁned similarly,\nF(k, l) =\nN−1\nX\ni=0\nN−1\nX\nj=0\ncos\n\u0012 π\nN (i + 1\n2)k\n\u0013\ncos\n\u0012 π\nN (j + 1\n2)l\n\u0013\nf(i, j).\n(3.77)\nLike the 2D Fast Fourier Transform, the 2D DCT can be implemented separably, i.e., ﬁrst\ncomputing the DCT of each line in the block and then computing the DCT of each resulting\ncolumn. Like the FFT, each of the DCTs can also be computed in O(N log N) time.\nAs we mentioned in Section 2.3.3, the DCT is widely used in today’s image and video\ncompression algorithms, although it is slowly being supplanted by wavelet algorithms (Si-\nmoncelli and Adelson 1990b), as discussed in Section 3.5.4, and overlapped variants of the\nDCT (Malvar 1990, 1998, 2000), which are used in the new JPEG XR standard.12 These\n12 http://www.itu.int/rec/T-REC-T.832-200903-I/en.\n144\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nnewer algorithms suffer less from the blocking artifacts (visible edge-aligned discontinuities)\nthat result from the pixels in each block (typically 8 × 8) being transformed and quantized\nindependently. See Exercise 3.30 for ideas on how to remove blocking artifacts from com-\npressed JPEG images.\n3.4.4 Application: Sharpening, blur, and noise removal\nAnother common application of image processing is the enhancement of images through the\nuse of sharpening and noise removal operations, which require some kind of neighborhood\nprocessing. Traditionally, these kinds of operation were performed using linear ﬁltering (see\nSections 3.2 and Section 3.4.3). Today, it is more common to use non-linear ﬁlters (Sec-\ntion 3.3.1), such as the weighted median or bilateral ﬁlter (3.34–3.37), anisotropic diffusion\n(3.39–3.40), or non-local means (Buades, Coll, and Morel 2008). Variational methods (Sec-\ntion 3.7.1), especially those using non-quadratic (robust) norms such as the L1 norm (which\nis called total variation), are also often used. Figure 3.19 shows some examples of linear and\nnon-linear ﬁlters being used to remove noise.\nWhen measuring the effectiveness of image denoising algorithms, it is common to report\nthe results as a peak signal-to-noise ratio (PSNR) measurement (2.119), where I(x) is the\noriginal (noise-free) image and ˆI(x) is the image after denoising; this is for the case where the\nnoisy image has been synthetically generated, so that the clean image is known. A better way\nto measure the quality is to use a perceptually based similarity metric, such as the structural\nsimilarity (SSIM) index (Wang, Bovik, Sheikh et al. 2004; Wang, Bovik, and Simoncelli\n2005).\nExercises 3.11, 3.16, 3.17, 3.21, and 3.28 have you implement some of these operations\nand compare their effectiveness. More sophisticated techniques for blur removal and the\nrelated task of super-resolution are discussed in Section 10.3.\n3.5 Pyramids and wavelets\nSo far in this chapter, all of the image transformations we have studied produce output images\nof the same size as the inputs. Often, however, we may wish to change the resolution of an\nimage before proceeding further. For example, we may need to interpolate a small image to\nmake its resolution match that of the output printer or computer screen. Alternatively, we\nmay want to reduce the size of an image to speed up the execution of an algorithm or to save\non storage space or transmission time.\nSometimes, we do not even know what the appropriate resolution for the image should\nbe. Consider, for example, the task of ﬁnding a face in an image (Section 14.1.1). Since we\ndo not know the scale at which the face will appear, we need to generate a whole pyramid",
  "image_path": "page_165.jpg",
  "pages": [
    164,
    165,
    166
  ]
}