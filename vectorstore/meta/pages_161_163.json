{
  "doc_id": "pages_161_163",
  "text": "3.4 Fourier transforms\n139\nName\nKernel\nTransform\nPlot\nbox-3\n1\n3\n1\n1\n1\n1\n3(1 + 2 cos ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nbox-5\n1\n5\n1\n1\n1\n1\n1\n1\n5(1 + 2 cos ω + 2 cos 2ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nlinear\n1\n4\n1\n2\n1\n1\n2(1 + cos ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nbinomial\n1\n16\n1\n4\n6\n4\n1\n1\n4(1 + cos ω)2\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nSobel\n1\n2\n−1\n0\n1\nsin ω\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\ncorner\n1\n2\n−1\n2\n−1\n1\n2(1 −cos ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nTable 3.3 Fourier transforms of the separable kernels shown in Figure 3.14.\n140\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3.4.2 Two-dimensional Fourier transforms\nThe formulas and insights we have developed for one-dimensional signals and their trans-\nforms translate directly to two-dimensional images. Here, instead of just specifying a hor-\nizontal or vertical frequency ωx or ωy, we can create an oriented sinusoid of frequency\n(ωx, ωy),\ns(x, y) = sin(ωxx + ωyy).\n(3.62)\nThe corresponding two-dimensional Fourier transforms are then\nH(ωx, ωy) =\nZ ∞\n−∞\nZ ∞\n−∞\nh(x, y)e−j(ωxx+ωyy)dx dy,\n(3.63)\nand in the discrete domain,\nH(kx, ky) =\n1\nMN\nM−1\nX\nx=0\nN−1\nX\ny=0\nh(x, y)e−j2π\nkxx+kyy\nMN\n,\n(3.64)\nwhere M and N are the width and height of the image.\nAll of the Fourier transform properties from Table 3.1 carry over to two dimensions if\nwe replace the scalar variables x, ω, x0 and a with their 2D vector counterparts x = (x, y),\nω = (ωx, ωy), x0 = (x0, y0), and a = (ax, ay), and use vector inner products instead of\nmultiplications.\n3.4.3 Wiener ﬁltering\nWhile the Fourier transform is a useful tool for analyzing the frequency characteristics of a\nﬁlter kernel or image, it can also be used to analyze the frequency spectrum of a whole class\nof images.\nA simple model for images is to assume that they are random noise ﬁelds whose expected\nmagnitude at each frequency is given by this power spectrum Ps(ωx, ωy), i.e.,\n\n[S(ωx, ωy)]2\u000b\n= Ps(ωx, ωy),\n(3.65)\nwhere the angle brackets ⟨·⟩denote the expected (mean) value of a random variable.9 To\ngenerate such an image, we simply create a random Gaussian noise image S(ωx, ωy) where\neach “pixel” is a zero-mean Gaussian10 of variance Ps(ωx, ωy) and then take its inverse FFT.\nThe observation that signal spectra capture a ﬁrst-order description of spatial statistics\nis widely used in signal and image processing. In particular, assuming that an image is a\n9 The notation E[·] is also commonly used.\n10 We set the DC (i.e., constant) component at S(0, 0) to the mean grey level. See Algorithm C.1 in Appendix C.2\nfor code to generate Gaussian noise.\n3.4 Fourier transforms\n141\nsample from a correlated Gaussian random noise ﬁeld combined with a statistical model of\nthe measurement process yields an optimum restoration ﬁlter known as the Wiener ﬁlter.11\nTo derive the Wiener ﬁlter, we analyze each frequency component of a signal’s Fourier\ntransform independently. The noisy image formation process can be written as\no(x, y) = s(x, y) + n(x, y),\n(3.66)\nwhere s(x, y) is the (unknown) image we are trying to recover, n(x, y) is the additive noise\nsignal, and o(x, y) is the observed noisy image. Because of the linearity of the Fourier trans-\nform, we can write\nO(ωx, ωy) = S(ωx, ωy) + N(ωx, ωy),\n(3.67)\nwhere each quantity in the above equation is the Fourier transform of the corresponding\nimage.\nAt each frequency (ωx, ωy), we know from our image spectrum that the unknown trans-\nform component S(ωx, ωy) has a prior distribution which is a zero-mean Gaussian with vari-\nance Ps(ωx, ωy). We also have noisy measurement O(ωx, ωy) whose variance is Pn(ωx, ωy),\ni.e., the power spectrum of the noise, which is usually assumed to be constant (white),\nPn(ωx, ωy) = σ2\nn.\nAccording to Bayes’ Rule (Appendix B.4), the posterior estimate of S can be written as\np(S|O) = p(O|S)p(S)\np(O)\n,\n(3.68)\nwhere p(O) =\nR\nS p(O|S)p(S) is a normalizing constant used to make the p(S|O) distribution\nproper (integrate to 1). The prior distribution p(S) is given by\np(S) = e−(S−µ)2\n2Ps\n,\n(3.69)\nwhere µ is the expected mean at that frequency (0 everywhere except at the origin) and the\nmeasurement distribution P(O|S) is given by\np(S) = e−(S−O)2\n2Pn\n.\n(3.70)\nTaking the negative logarithm of both sides of (3.68) and setting µ = 0 for simplicity, we\nget\n−log p(S|O)\n=\n−log p(O|S) −log p(S) + C\n(3.71)\n=\n1/2P −1\nn (S −O)2 + 1/2P −1\ns\nS2 + C,\n(3.72)\n11 Wiener is pronounced “veener” since, in German, the “w” is pronounced “v”. Remember that next time you\norder “Wiener schnitzel”.",
  "image_path": "page_162.jpg",
  "pages": [
    161,
    162,
    163
  ]
}