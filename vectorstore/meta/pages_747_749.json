{
  "doc_id": "pages_747_749",
  "text": "14.8 Exercises\n725\nleguillos et al. (2007), Russell, Torralba, Liu et al. (2007), Hoiem, Efros, and Hebert (2008a),\nHoiem, Efros, and Hebert (2008b), Sudderth, Torralba, Freeman et al. (2008), and Divvala,\nHoiem, Hays et al. (2009).\nSophisticated machine learning techniques are also becoming a key component of suc-\ncessful object detection and recognition algorithms (Varma and Ray 2007; Felzenszwalb,\nMcAllester, and Ramanan 2008; Fritz and Schiele 2008; Sivic, Russell, Zisserman et al.\n2008; Vedaldi, Gulshan, Varma et al. 2009), as is exploiting large human-labeled databases\n(Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros 2008; Torralba, Freeman, and Fer-\ngus 2008; Liu, Yuen, and Torralba 2009). Rough three-dimensional models are also making\na comeback for recognition, as evidenced in some recent papers (Savarese and Fei-Fei 2007,\n2008; Sun, Su, Savarese et al. 2009; Su, Sun, Fei-Fei et al. 2009). As always, the latest con-\nferences on computer vision are your best reference for the newest algorithms in this rapidly\nevolving ﬁeld.\n14.8 Exercises\nEx 14.1: Face detection\nBuild and test one of the face detectors presented in Section 14.1.1.\n1. Download one or more of the labeled face detection databases in Table 14.2.\n2. Generate your own negative examples by ﬁnding photographs that do not contain any\npeople.\n3. Implement one of the following face detectors (or devise one of your own):\n• boosting (Algorithm 14.1) based on simple area features, with an optional cascade\nof detectors (Viola and Jones 2004);\n• PCA face subspace (Moghaddam and Pentland 1997);\n• distances to clustered face and non-face prototypes, followed by a neural network\n(Sung and Poggio 1998) or SVM (Osuna, Freund, and Girosi 1997) classiﬁer;\n• a multi-resolution neural network trained directly on normalized gray-level patches\n(Rowley, Baluja, and Kanade 1998a).\n4. Test the performance of your detector on the database by evaluating the detector at ev-\nery location in a sub-octave pyramid. Optionally retrain your detector on false positive\nexamples you get on non-face images.\nEx 14.2: Determining the threshold for AdaBoost\nGiven a set of function evaluations on\nthe training examples xi, fi = f(xi) ∈±1, training labels yi ∈±1, and weights wi ∈(0, 1),\n726\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nas explained in Algorithm 14.1, devise an efﬁcient algorithm to ﬁnd values of θ and s = ±1\nthat maximize\nX\ni\nwiyih(sfi, θ),\n(14.43)\nwhere h(x, θ) = sign(x −θ).\nEx 14.3: Face recognition using eigenfaces\nCollect a set of facial photographs and then\nbuild a recognition system to re-recognize the same people.\n1. Take several photos of each of your classmates and store them.\n2. Align the images by automatically or manually detecting the corners of the eyes and\nusing a similarity transform to stretch and rotate each image to a canonical position.\n3. Compute the average image and a PCA subspace for the face images\n4. Take a new set of photographs a week later and use them as your test set.\n5. Compare each new image to each database image and select the nearest one as the\nrecognized identity. Verify that the distance in PCA space is close to the distance\ncomputed with a full SSD (sum of squared difference) measure.\n6. (Optional) Compute different principal components for identity and expression, and\nuse them to improve your recognition results.\nEx 14.4: Bayesian face recognition\nMoghaddam, Jebara, and Pentland (2000) compute\nseparate covariance matrices ΣI and ΣE by looking at differences between all pairs of im-\nages. At run time, they select the nearest image to determine the facial identity. Does it make\nsense to estimate statistics for all pairs of images and use them for testing the distance to the\nnearest exemplar? Discuss whether this is statistically correct.\nHow is the all-pair intrapersonal covariance matrix ΣI related to the within-class scatter\nmatrix SW? Does a similar relationship hold between ΣE and SB?\nEx 14.5: Modular eigenfaces\nExtend your face recognition system to separately match the\neye, nose, and mouth regions, as shown in Figure 14.18.\n1. After normalizing face images to a canonical scale and location, manually segment out\nsome of the eye, nose, and face regions.\n2. Build separate detectors for these three (or four) kinds of region, either using a subspace\n(PCA) approach or one of the techniques presented in Section 14.1.1.\n3. For each new image to be recognized, ﬁrst detect the locations of the facial features.\n14.8 Exercises\n727\n4. Then, match the individual features against your database and note the locations of\nthese features.\n5. Train and test a classiﬁer that uses the individual feature matching IDs as well as (op-\ntionally) the feature locations to perform face recognition.\nEx 14.6: Recognition-based color balancing\nBuild a system that recognizes the most im-\nportant color areas in common photographs (sky, grass, skin) and color balances the image\naccordingly. Some references and ideas for skin detection are given in Exercise 2.8 and\nby Forsyth and Fleck (1999), Jones and Rehg (2001), Vezhnevets, Sazonov, and Andreeva\n(2003), and Kakumanu, Makrogiannis, and Bourbakis (2007). These may give you ideas\nfor how to detect other regions or you can try more sophisticated MRF-based approaches\n(Shotton, Winn, Rother et al. 2009).\nEx 14.7: Pedestrian detection\nBuild and test one of the pedestrian detectors presented in\nSection 14.1.2.\nEx 14.8: Simple instance recognition\nUse the feature detection, matching, and alignment\nalgorithms you developed in Exercises 4.1–4.4 and 9.2 to ﬁnd matching images given a query\nimage or region (Figure 14.26).\nEvaluate several feature detectors, descriptors, and robust geometric veriﬁcation strate-\ngies, either on your own or by comparing your results with those of classmates.\nEx 14.9: Large databases and location recognition\nExtend the previous exercise to larger\ndatabases using quantized visual words and information retrieval techniques, as described in\nAlgorithm 14.2.\nTest your algorithm on a large database, such as the one used by Nist´er and Stew´enius\n(2006) or Philbin, Chum, Sivic et al. (2008), which are listed in Table 14.1. Alternatively,\nuse keyword search on the Web or in a photo sharing site (e.g., for a city) to create your own\ndatabase.\nEx 14.10: Bag of words\nAdapt the feature extraction and matching pipeline developed in\nExercise 14.8 to category (class) recognition, using some of the techniques described in Sec-\ntion 14.4.1.\n1. Download the training and test images from one or more of the databases listed in\nTables 14.1 and 14.2, e.g., Caltech 101, Caltech 256, or PASCAL VOC.\n2. Extract features from each of the training images, quantize them, and compute the tf-idf\nvectors (bag of words histograms).",
  "image_path": "page_748.jpg",
  "pages": [
    747,
    748,
    749
  ]
}