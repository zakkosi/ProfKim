{
  "doc_id": "pages_071_073",
  "text": "2.1 Geometric primitives and transformations\n49\nby their z component. Using inhomogeneous coordinates, this can be written as\n¯x = Pz(p) =\n\n\nx/z\ny/z\n1\n\n.\n(2.50)\nIn homogeneous coordinates, the projection has a simple linear form,\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n˜p,\n(2.51)\ni.e., we drop the w component of p. Thus, after projection, it is not possible to recover the\ndistance of the 3D point from the image, which makes sense for a 2D imaging sensor.\nA form often seen in computer graphics systems is a two-step projection that ﬁrst projects\n3D coordinates into normalized device coordinates in the range (x, y, z) ∈[−1, −1] ×\n[−1, 1] × [0, 1], and then rescales these coordinates to integer pixel coordinates using a view-\nport transformation (Watt 1995; OpenGL-ARB 1997).\nThe (initial) perspective projection\nis then represented using a 4 × 4 matrix\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n−zfar/zrange\nznearzfar/zrange\n0\n0\n1\n0\n\n˜p,\n(2.52)\nwhere znear and zfar are the near and far z clipping planes and zrange = zfar −znear. Note\nthat the ﬁrst two rows are actually scaled by the focal length and the aspect ratio so that\nvisible rays are mapped to (x, y, z) ∈[−1, −1]2. The reason for keeping the third row, rather\nthan dropping it, is that visibility operations, such as z-buffering, require a depth for every\ngraphical element that is being rendered.\nIf we set znear = 1, zfar →∞, and switch the sign of the third row, the third element\nof the normalized screen vector becomes the inverse depth, i.e., the disparity (Okutomi and\nKanade 1993). This can be quite convenient in many cases since, for cameras moving around\noutdoors, the inverse depth to the camera is often a more well-conditioned parameterization\nthan direct 3D distance.\nWhile a regular 2D image sensor has no way of measuring distance to a surface point,\nrange sensors (Section 12.2) and stereo matching algorithms (Chapter 11) can compute such\nvalues. It is then convenient to be able to map from a sensor-based depth or disparity value d\ndirectly back to a 3D location using the inverse of a 4 × 4 matrix (Section 2.1.5). We can do\nthis if we represent perspective projection using a full-rank 4 × 4 matrix, as in (2.64).\n50\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\ncs\nyc\nxs\nys\nsx\nsy\npc\np\nOc\nFigure 2.8 Projection of a 3D camera-centered point pc onto the sensor planes at location\np. Oc is the camera center (nodal point), cs is the 3D origin of the sensor plane coordinate\nsystem, and sx and sy are the pixel spacings.\nCamera intrinsics\nOnce we have projected a 3D point through an ideal pinhole using a projection matrix, we\nmust still transform the resulting coordinates according to the pixel sensor spacing and the\nrelative position of the sensor plane to the origin. Figure 2.8 shows an illustration of the\ngeometry involved. In this section, we ﬁrst present a mapping from 2D pixel coordinates to\n3D rays using a sensor homography M s, since this is easier to explain in terms of physically\nmeasurable quantities. We then relate these quantities to the more commonly used camera in-\ntrinsic matrix K, which is used to map 3D camera-centered points pc to 2D pixel coordinates\n˜xs.\nImage sensors return pixel values indexed by integer pixel coordinates (xs, ys), often\nwith the coordinates starting at the upper-left corner of the image and moving down and to\nthe right. (This convention is not obeyed by all imaging libraries, but the adjustment for\nother coordinate systems is straightforward.) To map pixel centers to 3D coordinates, we ﬁrst\nscale the (xs, ys) values by the pixel spacings (sx, sy) (sometimes expressed in microns for\nsolid-state sensors) and then describe the orientation of the sensor array relative to the camera\nprojection center Oc with an origin cs and a 3D rotation Rs (Figure 2.8).\nThe combined 2D to 3D projection can then be written as\np =\nh\nRs\ncs\ni\n\n\nsx\n0\n0\n0\nsy\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\nxs\nys\n1\n\n= M s¯xs.\n(2.53)\nThe ﬁrst two columns of the 3 × 3 matrix M s are the 3D vectors corresponding to unit steps\nin the image pixel array along the xs and ys directions, while the third column is the 3D\nimage array origin cs.\n2.1 Geometric primitives and transformations\n51\nThe matrix M s is parameterized by eight unknowns: the three parameters describing\nthe rotation Rs, the three parameters describing the translation cs, and the two scale factors\n(sx, sy). Note that we ignore here the possibility of skew between the two axes on the image\nplane, since solid-state manufacturing techniques render this negligible. In practice, unless\nwe have accurate external knowledge of the sensor spacing or sensor orientation, there are\nonly seven degrees of freedom, since the distance of the sensor from the origin cannot be\nteased apart from the sensor spacing, based on external image measurement alone.\nHowever, estimating a camera model M s with the required seven degrees of freedom\n(i.e., where the ﬁrst two columns are orthogonal after an appropriate re-scaling) is impractical,\nso most practitioners assume a general 3 × 3 homogeneous matrix form.\nThe relationship between the 3D pixel center p and the 3D camera-centered point pc is\ngiven by an unknown scaling s, p = spc. We can therefore write the complete projection\nbetween pc and a homogeneous version of the pixel address ˜xs as\n˜xs = αM −1\ns pc = Kpc.\n(2.54)\nThe 3 × 3 matrix K is called the calibration matrix and describes the camera intrinsics (as\nopposed to the camera’s orientation in space, which are called the extrinsics).\nFrom the above discussion, we see that K has seven degrees of freedom in theory and\neight degrees of freedom (the full dimensionality of a 3×3 homogeneous matrix) in practice.\nWhy, then, do most textbooks on 3D computer vision and multi-view geometry (Faugeras\n1993; Hartley and Zisserman 2004; Faugeras and Luong 2001) treat K as an upper-triangular\nmatrix with ﬁve degrees of freedom?\nWhile this is usually not made explicit in these books, it is because we cannot recover\nthe full K matrix based on external measurement alone. When calibrating a camera (Chap-\nter 6) based on external 3D points or other measurements (Tsai 1987), we end up estimating\nthe intrinsic (K) and extrinsic (R, t) camera parameters simultaneously using a series of\nmeasurements,\n˜xs = K\nh\nR\nt\ni\npw = P pw,\n(2.55)\nwhere pw are known 3D world coordinates and\nP = K[R|t]\n(2.56)\nis known as the camera matrix. Inspecting this equation, we see that we can post-multiply\nK by R1 and pre-multiply [R|t] by RT\n1 , and still end up with a valid calibration. Thus, it\nis impossible based on image measurements alone to know the true orientation of the sensor\nand the true camera intrinsics.\nThe choice of an upper-triangular form for K seems to be conventional. Given a full\n3 × 4 camera matrix P = K[R|t], we can compute an upper-triangular K matrix using QR",
  "image_path": "page_072.jpg",
  "pages": [
    71,
    72,
    73
  ]
}