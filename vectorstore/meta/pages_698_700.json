{
  "doc_id": "pages_698_700",
  "text": "676\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhere N = Nk = 13 is the number of samples in each class.\nTo compute the most discriminating direction, Fisher’s linear discriminant (FLD) (Bel-\nhumeur, Hespanha, and Kriegman 1997; Hastie, Tibshirani, and Friedman 2001; Bishop\n2006), which is also known as linear discriminant analysis (LDA), selects the direction u\nthat results in the largest ratio between the projected between-class and within-class varia-\ntions\nu∗= arg max\nu\nuT SBu\nuT SWu,\n(14.20)\nwhich is equivalent to ﬁnding the eigenvector corresponding to the largest eigenvalue of the\ngeneralized eigenvalue problem\nSBu = λSWu\nor\nλu = S−1\nW SBu.\n(14.21)\nFor the problem shown in Figure 14.16,\nS−1\nW SB =\n\"\n11.796\n−0.289\n−4.715\n0.3889\n#\nand\nu =\n\"\n0.926\n−0.379\n#\n(14.22)\nAs you can see, using this direction results in a better separation between the classes than\nusing the dominant PCA direction, which is the horizontal axis. In their paper, Belhumeur,\nHespanha, and Kriegman (1997) show that Fisherfaces signiﬁcantly outperform the original\neigenfaces algorithm, especially when faces have large amounts of illumination variation, as\nin Figure 14.15.\nAn alternative for modeling within-class (intrapersonal) and between-class (extraper-\nsonal) variations is to model each distribution separately and then use Bayesian techniques\nto ﬁnd the closest exemplar (Moghaddam, Jebara, and Pentland 2000). Instead of computing\nthe mean for each class and then the within-class and between-class distributions, consider\nevaluating the difference images\n∆ij = xi −xj\n(14.23)\nbetween all pairs of training images (xi, xj). The differences between pairs that are in the\nsame class (the same person) are used to estimate the intrapersonal covariance matrix ΣI,\nwhile differences between different people are used to estimate the extrapersonal covariance\nΣE.12 The principal components (eigenfaces) corresponding to these two classes are shown\nin Figure 14.17.\nAt recognition time, we can compute the distance ∆i between a new face x and a stored\ntraining image xi and evaluate its intrapersonal likelihood as\npI(∆i) = pN (∆i; ΣI) =\n1\n|2πΣI|1/2 exp −∥∆i∥2\nΣ\n−1\nI ,\n(14.24)\n12 Note that the difference distributions are zero mean because for every ∆ij there corresponds a negative ∆ji.\n14.2 Face recognition\n677\n(a)\n(b)\nFigure 14.17 “Dual” eigenfaces (Moghaddam, Jebara, and Pentland 2000) c⃝2000 Elsevier:\n(a) intrapersonal and (b) extrapersonal.\nwhere pN is a normal (Gaussian) distribution with covariance ΣI and\n|2πΣI|1/2 = (2π)M/2\nM\nY\nj=1\nλ1/2\nj\n(14.25)\nis its volume. The Mahalanobis distance\n∥∆i∥2\nΣ\n−1\nI\n= ∆T\ni Σ−1\nI ∆i = ∥aI −aI\ni ∥2\n(14.26)\ncan be computed more efﬁciently by ﬁrst projecting the new image x into the whitened in-\ntrapersonal face space (14.15)\naI = ˆ\nU Ix\n(14.27)\nand then computing a Euclidean distance to the training image vector aI\ni , which can be pre-\ncomputed ofﬂine. The extrapersonal likelihood pE(∆i) can be computed in a similar fashion.\nOnce the intrapersonal and extrapersonal likelihoods have been computed, we can com-\npute the Bayesian likelihood of a new image x matching a training image xi as\np(∆i) =\npI(∆i)lI\npI(∆i)lI + pE(∆i)lE\n,\n(14.28)\nwhere lI and lE are the prior probabilities of two images being in the same or in different\nclasses (Moghaddam, Jebara, and Pentland 2000). A simpler approach, which does not re-\nquire the evaluation of extrapersonal probabilities, is to simply choose the training image with\n678\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.18\nModular eigenspace for face recognition (Moghaddam and Pentland 1997)\nc⃝1997 IEEE. (a) By detecting separate features in the faces (eyes, nose, mouth), separate\neigenspaces can be estimated for each one. (b) The relative positions of each feature can be\ndetected at recognition time, thus allowing for more ﬂexibility in viewpoint and expression.\nthe highest likelihood pI(∆i). In this case, nearest neighbor search techniques in the space\nspanned by the precomputed {aI\ni } vectors could be used to speed up ﬁnding the best match.13\nAnother way to improve the performance of eigenface-based approaches is to break up\nthe image into separate regions such as the eyes, nose, and mouth (Figure 14.18) and to match\neach of these modular eigenspaces independently (Moghaddam and Pentland 1997; Heisele,\nHo, Wu et al. 2003; Heisele, Serre, and Poggio 2007). The advantage of such a modular\napproach is that it can tolerate a wider range of viewpoints, because each part can move\nrelative to the others. It also supports a larger variety of combinations, e.g., we can model one\nperson as having a narrow nose and bushy eyebrows, without requiring the eigenfaces to span\nall possible combinations of nose, mouth, and eyebrows. (If you remember the cardboard\nchildren’s books where you can select different top and bottom faces, or Mr. Potato Head,\nyou get the idea.)\nAnother approach to dealing with large variability in appearance is to create view-based\n(view-speciﬁc) eigenspaces, as shown in Figure 14.19 (Moghaddam and Pentland 1997). We\ncan think of these view-based eigenspaces as local descriptors that select different axes de-\npending on which part of the face space you are in. Note that such approaches, however,\npotentially require large amounts of training data, i.e., pictures of every person in every pos-\nsible pose or expression. This is in contrast to the shape and appearance models we study in\n13 Note that while the covariance matrices ΣI and ΣE are computed by looking at differences between all pairs of\nimages, the run-time evaluation selects the nearest image to determine the facial identity. Whether this is statistically\ncorrect is explored in Exercise 14.4.",
  "image_path": "page_699.jpg",
  "pages": [
    698,
    699,
    700
  ]
}