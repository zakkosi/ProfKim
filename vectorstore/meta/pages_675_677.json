{
  "doc_id": "pages_675_677",
  "text": "13.7 Exercises\n653\n7. Implement the unstructured Lumigraph rendering algorithm from Buehler, Bosse, McMil-\nlan et al. (2001).\nEx 13.9: Surface light ﬁelds\nConstruct a surface light ﬁeld (Wood, Azuma, Aldinger et al.\n2000) and see how well you can compress it.\n1. Acquire an interesting light ﬁeld of a specular scene or object, or download one from\nhttp://lightﬁeld.stanford.edu/.\n2. Build a 3D model of the object using a multi-view stereo algorithm that is robust to\noutliers due to specularities.\n3. Estimate the Lumisphere for each surface point on the object.\n4. Estimate its diffuse components. Is the median the best way to do this? Why not use\nthe minimum color value? What happens if there is Lambertian shading on the diffuse\ncomponent?\n5. Model and compress the remaining portion of the Lumisphere using one of the tech-\nniques suggested by Wood, Azuma, Aldinger et al. (2000) or invent one of your own.\n6. Study how well your compression algorithm works and what artifacts it produces.\n7. (Optional) Develop a system to edit and manipulate your surface light ﬁeld.\nEx 13.10: Handheld concentric mosaics\nDevelop a system to navigate a handheld con-\ncentric mosaic.\n1. Stand in the middle of a room with a camcorder held at arm’s length in front of you and\nspin in a circle.\n2. Use a structure from motion system to determine the camera pose and sparse 3D struc-\nture for each input frame.\n3. (Optional) Re-bin your image pixels into a more regular concentric mosaic structure.\n4. At view time, determine from the new camera’s view (which should be near the plane\nof your original capture) which source pixels to display. You can simplify your com-\nputations to determine a source column (and scaling) for each output column.\n5. (Optional) Use your sparse 3D structure, interpolated to a dense depth map, to improve\nyour rendering (Zheng, Kang, Cohen et al. 2007).\n654\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 13.11: Video textures\nCapture some videos of natural phenomena, such as a water\nfountain, ﬁre, or smiling face, and loop the video seamlessly into an inﬁnite length video\n(Sch¨odl, Szeliski, Salesin et al. 2000).\n1. Compare all the frames in the original clip using an L2 (sum of square difference)\nmetric. (This assumes the videos were shot on a tripod or have already been stabilized.)\n2. Filter the comparison table temporally to accentuate temporal sub-sequences that match\nwell together.\n3. Convert your similarity table into a jump probability table through some exponential\ndistribution. Be sure to modify transitions near the end so you do not get “stuck” in the\nlast frame.\n4. Starting with the ﬁrst frame, use your transition table to decide whether to jump for-\nward, backward, or continue to the next frame.\n5. (Optional) Add any of the other extensions to the original video textures idea, such\nas multiple moving regions, interactive control, or graph cut spatio-temporal texture\nseaming.\nChapter 14\nRecognition\n14.1 Object detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658\n14.1.1 Face detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658\n14.1.2 Pedestrian detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . 666\n14.2 Face recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668\n14.2.1 Eigenfaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671\n14.2.2 Active appearance and 3D shape models . . . . . . . . . . . . . . . . 679\n14.2.3 Application: Personal photo collections . . . . . . . . . . . . . . . . 684\n14.3 Instance recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685\n14.3.1 Geometric alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 686\n14.3.2 Large databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687\n14.3.3 Application: Location recognition . . . . . . . . . . . . . . . . . . . 693\n14.4 Category recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 696\n14.4.1 Bag of words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 697\n14.4.2 Part-based models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 701\n14.4.3 Recognition with segmentation . . . . . . . . . . . . . . . . . . . . . 704\n14.4.4 Application: Intelligent photo editing\n. . . . . . . . . . . . . . . . . 709\n14.5 Context and scene understanding . . . . . . . . . . . . . . . . . . . . . . . . 712\n14.5.1 Learning and large image collections\n. . . . . . . . . . . . . . . . . 714\n14.5.2 Application: Image search . . . . . . . . . . . . . . . . . . . . . . . 717\n14.6 Recognition databases and test sets . . . . . . . . . . . . . . . . . . . . . . . 718\n14.7 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722\n14.8 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 725",
  "image_path": "page_676.jpg",
  "pages": [
    675,
    676,
    677
  ]
}