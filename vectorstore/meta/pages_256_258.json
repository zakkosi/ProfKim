{
  "doc_id": "pages_256_258",
  "text": "234\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nrecursively splits this plane along axis-aligned (horizontal or vertical) cutting planes. Each\nsplit can be denoted using the dimension number and split value (Figure 4.27b). The splits are\narranged so as to try to balance the tree, i.e., to keep its maximum depth as small as possible.\nAt query time, a classic k-d tree search ﬁrst locates the query point (+) in its appropriate\nbin (D), and then searches nearby leaves in the tree (C, B, . . .) until it can guarantee that\nthe nearest neighbor has been found. The best bin ﬁrst (BBF) search (Beis and Lowe 1999)\nsearches bins in order of their spatial proximity to the query point and is therefore usually\nmore efﬁcient.\nMany additional data structures have been developed over the years for solving nearest\nneighbor problems (Arya, Mount, Netanyahu et al. 1998; Liang, Liu, Xu et al. 2001; Hjalta-\nson and Samet 2003). For example, Nene and Nayar (1997) developed a technique they call\nslicing that uses a series of 1D binary searches on the point list sorted along different dimen-\nsions to efﬁciently cull down a list of candidate points that lie within a hypercube of the query\npoint. Grauman and Darrell (2005) reweight the matches at different levels of an indexing\ntree, which allows their technique to be less sensitive to discretization errors in the tree con-\nstruction. Nist´er and Stew´enius (2006) use a metric tree, which compares feature descriptors\nto a small number of prototypes at each level in a hierarchy. The resulting quantized visual\nwords can then be used with classical information retrieval (document relevance) techniques\nto quickly winnow down a set of potential candidates from a database of millions of images\n(Section 14.3.2). Muja and Lowe (2009) compare a number of these approaches, introduce a\nnew one of their own (priority search on hierarchical k-means trees), and conclude that mul-\ntiple randomized k-d trees often provide the best performance. Despite all of this promising\nwork, the rapid computation of image feature correspondences remains a challenging open\nresearch problem.\nFeature match veriﬁcation and densiﬁcation\nOnce we have some hypothetical (putative) matches, we can often use geometric alignment\n(Section 6.1) to verify which matches are inliers and which ones are outliers. For example,\nif we expect the whole image to be translated or rotated in the matching view, we can ﬁt a\nglobal geometric transform and keep only those feature matches that are sufﬁciently close to\nthis estimated transformation. The process of selecting a small set of seed matches and then\nverifying a larger set is often called random sampling or RANSAC (Section 6.1.4). Once an\ninitial set of correspondences has been established, some systems look for additional matches,\ne.g., by looking for additional correspondences along epipolar lines (Section 11.1) or in the\nvicinity of estimated locations based on the global transform. These topics are discussed\nfurther in Sections 6.1, 11.2, and 14.3.1.\n4.1 Points and patches\n235\n4.1.4 Feature tracking\nAn alternative to independently ﬁnding features in all candidate images and then matching\nthem is to ﬁnd a set of likely feature locations in a ﬁrst image and to then search for their\ncorresponding locations in subsequent images. This kind of detect then track approach is\nmore widely used for video tracking applications, where the expected amount of motion and\nappearance deformation between adjacent frames is expected to be small.\nThe process of selecting good features to track is closely related to selecting good features\nfor more general recognition applications. In practice, regions containing high gradients in\nboth directions, i.e., which have high eigenvalues in the auto-correlation matrix (4.8), provide\nstable locations at which to ﬁnd correspondences (Shi and Tomasi 1994).\nIn subsequent frames, searching for locations where the corresponding patch has low\nsquared difference (4.1) often works well enough. However, if the images are undergo-\ning brightness change, explicitly compensating for such variations (8.9) or using normalized\ncross-correlation (8.11) may be preferable. If the search range is large, it is also often more\nefﬁcient to use a hierarchical search strategy, which uses matches in lower-resolution images\nto provide better initial guesses and hence speed up the search (Section 8.1.1). Alternatives\nto this strategy involve learning what the appearance of the patch being tracked should be and\nthen searching for it in the vicinity of its predicted position (Avidan 2001; Jurie and Dhome\n2002; Williams, Blake, and Cipolla 2003). These topics are all covered in more detail in\nSection 8.1.3.\nIf features are being tracked over longer image sequences, their appearance can undergo\nlarger changes. You then have to decide whether to continue matching against the originally\ndetected patch (feature) or to re-sample each subsequent frame at the matching location. The\nformer strategy is prone to failure as the original patch can undergo appearance changes such\nas foreshortening. The latter runs the risk of the feature drifting from its original location\nto some other location in the image (Shi and Tomasi 1994). (Mathematically, small mis-\nregistration errors compound to create a Markov Random Walk, which leads to larger drift\nover time.)\nA preferable solution is to compare the original patch to later image locations using an\nafﬁne motion model (Section 8.2). Shi and Tomasi (1994) ﬁrst compare patches in neigh-\nboring frames using a translational model and then use the location estimates produced by\nthis step to initialize an afﬁne registration between the patch in the current frame and the\nbase frame where a feature was ﬁrst detected (Figure 4.28). In their system, features are only\ndetected infrequently, i.e., only in regions where tracking has failed. In the usual case, an\narea around the current predicted location of the feature is searched with an incremental reg-\nistration algorithm (Section 8.1.3). The resulting tracker is often called the Kanade–Lucas–\nTomasi (KLT) tracker.\n236\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.28 Feature tracking using an afﬁne motion model (Shi and Tomasi 1994) c⃝1994\nIEEE, Top row: image patch around the tracked feature location. Bottom row: image patch\nafter warping back toward the ﬁrst frame using an afﬁne deformation. Even though the speed\nsign gets larger from frame to frame, the afﬁne transformation maintains a good resemblance\nbetween the original and subsequent tracked frames.\nSince their original work on feature tracking, Shi and Tomasi’s approach has generated a\nstring of interesting follow-on papers and applications. Beardsley, Torr, and Zisserman (1996)\nuse extended feature tracking combined with structure from motion (Chapter 7) to incremen-\ntally build up sparse 3D models from video sequences. Kang, Szeliski, and Shum (1997)\ntie together the corners of adjacent (regularly gridded) patches to provide some additional\nstability to the tracking, at the cost of poorer handling of occlusions. Tommasini, Fusiello,\nTrucco et al. (1998) provide a better spurious match rejection criterion for the basic Shi and\nTomasi algorithm, Collins and Liu (2003) provide improved mechanisms for feature selec-\ntion and dealing with larger appearance changes over time, and Shaﬁque and Shah (2005)\ndevelop algorithms for feature matching (data association) for videos with large numbers of\nmoving objects or points. Yilmaz, Javed, and Shah (2006) and Lepetit and Fua (2005) survey\nthe larger ﬁeld of object tracking, which includes not only feature-based techniques but also\nalternative techniques based on contour and region (Section 5.1).\nOne of the newest developments in feature tracking is the use of learning algorithms to\nbuild special-purpose recognizers to rapidly search for matching features anywhere in an\nimage (Lepetit, Pilet, and Fua 2006; Hinterstoisser, Benhimane, Navab et al. 2008; Rogez,\nRihan, Ramalingam et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010).2 By taking the time\nto train classiﬁers on sample patches and their afﬁne deformations, extremely fast and reliable\nfeature detectors can be constructed, which enables much faster motions to be supported\n(Figure 4.29). Coupling such features to deformable models (Pilet, Lepetit, and Fua 2008) or\nstructure-from-motion algorithms (Klein and Murray 2008) can result in even higher stability.\n2 See also my previous comment on earlier work in learning-based tracking (Avidan 2001; Jurie and Dhome\n2002; Williams, Blake, and Cipolla 2003).",
  "image_path": "page_257.jpg",
  "pages": [
    256,
    257,
    258
  ]
}