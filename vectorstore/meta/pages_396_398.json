{
  "doc_id": "pages_396_398",
  "text": "374\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nically ﬁnd and label locations and regions of interest (Simon, Snavely, and Seitz 2007; Simon\nand Seitz 2008; Gammeter, Bossard, Quack et al. 2009) and to cluster large image collections\nso that they can be automatically labeled (Li, Wu, Zach et al. 2008; Quack, Leibe, and Van\nGool 2008). Some of these application are discussed in more detail in Section 13.1.2.\n7.5 Constrained structure and motion\nThe most general algorithms for structure from motion make no prior assumptions about the\nobjects or scenes that they are reconstructing. In many cases, however, the scene contains\nhigher-level geometric primitives, such as lines and planes. These can provide information\ncomplementary to interest points and also serve as useful building blocks for 3D modeling\nand visualization. Furthermore, these primitives are often arranged in particular relationships,\ni.e., many lines and planes are either parallel or orthogonal to each other. This is particularly\ntrue of architectural scenes and models, which we study in more detail in Section 12.6.1.\nSometimes, instead of exploiting regularity in the scene structure, it is possible to take\nadvantage of a constrained motion model. For example, if the object of interest is rotating\non a turntable (Szeliski 1991b), i.e., around a ﬁxed but unknown axis, specialized techniques\ncan be used to recover this motion (Fitzgibbon, Cross, and Zisserman 1998). In other situa-\ntions, the camera itself may be moving in a ﬁxed arc around some center of rotation (Shum\nand He 1999). Specialized capture setups, such as mobile stereo camera rigs or moving ve-\nhicles equipped with multiple ﬁxed cameras, can also take advantage of the knowledge that\nindividual cameras are (mostly) ﬁxed with respect to the capture rig, as shown in Figure 7.8.19\n7.5.1 Line-based techniques\nIt is well known that pairwise epipolar geometry cannot be recovered from line matches\nalone, even if the cameras are calibrated. To see this, think of projecting the set of lines in\neach image into a set of 3D planes in space. You can move the two cameras around into any\nconﬁguration you like and still obtain a valid reconstruction for 3D lines.\nWhen lines are visible in three or more views, the trifocal tensor can be used to transfer\nlines from one pair of images to another (Hartley and Zisserman 2004). The trifocal tensor\ncan also be computed on the basis of line matches alone.\nSchmid and Zisserman (1997) describe a widely used technique for matching 2D lines\nbased on the average of 15 × 15 pixel correlation scores evaluated at all pixels along their\n19 Because of mechanical compliance and jitter, it may be prudent to allow for a small amount of individual camera\nrotation around a nominal position.\n7.5 Constrained structure and motion\n375\nFigure 7.14 Two images of a toy house along with their matched 3D line segments (Schmid\nand Zisserman 1997) c⃝1997 Springer.\ncommon line segment intersection.20 In their system, the epipolar geometry is assumed to be\nknown, e.g., computed from point matches. For wide baselines, all possible homographies\ncorresponding to planes passing through the 3D line are used to warp pixels and the maximum\ncorrelation score is used. For triplets of images, the trifocal tensor is used to verify that\nthe lines are in geometric correspondence before evaluating the correlations between line\nsegments. Figure 7.14 shows the results of using their system.\nBartoli and Sturm (2003) describe a complete system for extending three view relations\n(trifocal tensors) computed from manual line correspondences to a full bundle adjustment of\nall the line and camera parameters. The key to their approach is to use the Pl¨ucker coor-\ndinates (2.12) to parameterize lines and to directly minimize reprojection errors. It is also\npossible to represent 3D line segments by their endpoints and to measure either the reprojec-\ntion error perpendicular to the detected 2D line segments in each image or the 2D errors using\nan elongated uncertainty ellipse aligned with the line segment direction (Szeliski and Kang\n1994).\nInstead of reconstructing 3D lines, Bay, Ferrari, and Van Gool (2005) use RANSAC to\ngroup lines into likely coplanar subsets. Four lines are chosen at random to compute a homog-\nraphy, which is then veriﬁed for these and other plausible line segment matches by evaluating\ncolor histogram-based correlation scores. The 2D intersection points of lines belonging to the\nsame plane are then used as virtual measurements to estimate the epipolar geometry, which\nis more accurate than using the homographies directly.\nAn alternative to grouping lines into coplanar subsets is to group lines by parallelism.\nWhenever three or more 2D lines share a common vanishing point, there is a good likelihood\nthat they are parallel in 3D. By ﬁnding multiple vanishing points in an image (Section 4.3.3)\nand establishing correspondences between such vanishing points in different images, the rel-\native rotations between the various images (and often the camera intrinsics) can be directly\nestimated (Section 6.3.2).\n20 Because lines often occur at depth or orientation discontinuities, it may be preferable to compute correlation\nscores (or to match color histograms (Bay, Ferrari, and Van Gool 2005)) separately on each side of the line.\n376\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nShum, Han, and Szeliski (1998) describe a 3D modeling system which ﬁrst constructs\ncalibrated panoramas from multiple images (Section 7.4) and then has the user draw vertical\nand horizontal lines in the image to demarcate the boundaries of planar regions. The lines\nare initially used to establish an absolute rotation for each panorama and are later used (along\nwith the inferred vertices and planes) to infer a 3D structure, which can be recovered up to\nscale from one or more images (Figure 12.15).\nA fully automated approach to line-based structure from motion is presented vy Werner\nand Zisserman (2002). In their system, they ﬁrst ﬁnd lines and group them by common van-\nishing points in each image (Section 4.3.3). The vanishing points are then used to calibrate the\ncamera, i.e., to performa a “metric upgrade” (Section 6.3.2). Lines corresponding to common\nvanishing points are then matched using both appearance (Schmid and Zisserman 1997) and\ntrifocal tensors. The resulting set of 3D lines, color coded by common vanishing directions\n(3D orientations) is shown in Figure 12.16a. These lines are then used to infer planes and a\nblock-structured model for the scene, as described in more detail in Section 12.6.1.\n7.5.2 Plane-based techniques\nIn scenes that are rich in planar structures, e.g., in architecture and certain kinds of manu-\nfactured objects such as furniture, it is possible to directly estimate homographies between\ndifferent planes, using either feature-based or intensity-based methods. In principle, this in-\nformation can be used to simultaneously infer the camera poses and the plane equations, i.e.,\nto compute plane-based structure from motion.\nLuong and Faugeras (1996) show how a fundamental matrix can be directly computed\nfrom two or more homographies using algebraic manipulations and least squares. Unfortu-\nnately, this approach often performs poorly, since the algebraic errors do not correspond to\nmeaningful reprojection errors (Szeliski and Torr 1998).\nA better approach is to hallucinate virtual point correspondences within the areas from\nwhich each homography was computed and to feed them into a standard structure from mo-\ntion algorithm (Szeliski and Torr 1998). An even better approach is to use full bundle adjust-\nment with explicit plane equations, as well as additional constraints to force reconstructed\nco-planar features to lie exactly on their corresponding planes. (A principled way to do this\nis to establish a coordinate frame for each plane, e.g., at one of the feature points, and to use\n2D in-plane parameterizations for the other points.) The system developed by Shum, Han,\nand Szeliski (1998) shows an example of such an approach, where the directions of lines and\nnormals for planes in the scene are pre-speciﬁed by the user.",
  "image_path": "page_397.jpg",
  "pages": [
    396,
    397,
    398
  ]
}