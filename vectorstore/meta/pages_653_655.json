{
  "doc_id": "pages_653_655",
  "text": "13.3 Light ﬁelds and Lumigraphs\n631\nWhile a light ﬁeld can be used to render a complex 3D scene from novel viewpoints, a\nmuch better rendering (with less ghosting) can be obtained if something is known about its\n3D geometry. The Lumigraph system of Gortler, Grzeszczuk, Szeliski et al. (1996) extends\nthe basic light ﬁeld rendering approach by taking into account the 3D location of surface\npoints corresponding to each 3D ray.\nConsider the ray (s, u) corresponding to the dashed line in Figure 13.8, which intersects\nthe object’s surface at a distance z from the uv plane. When we look up the pixel’s color in\ncamera si (assuming that the light ﬁeld is discretely sampled on a regular 4D (s, t, u, v) grid),\nthe actual pixel coordinate is u′, instead of the original u value speciﬁed by the (s, u) ray.\nSimilarly, for camera si+1 (where si ≤s ≤si+1), pixel address u′′ is used. Thus, instead of\nusing quadri-linear interpolation of the nearest sampled (s, t, u, v) values around a given ray\nto determine its color, the (u, v) values are modiﬁed for each discrete (si, ti) camera.\nFigure 13.8 also shows the same reasoning in ray space. Here, the original continuous-\nvalued (s, u) ray is represented by a triangle and the nearby sampled discrete values are\nshown as circles. Instead of just blending the four nearest samples, as would be indicated\nby the vertical and horizontal dashed lines, the modiﬁed (si, u′) and (si+1, u′′) values are\nsampled instead and their values are then blended.\nThe resulting rendering system produces images of much better quality than a proxy-free\nlight ﬁeld and is the method of choice whenever 3D geometry can be inferred. In subsequent\nwork, Isaksen, McMillan, and Gortler (2000) show how a planar proxy for the scene, which\nis a simpler 3D model, can be used to simplify the resampling equations. They also describe\nhow to create synthetic aperture photos, which mimic what might be seen by a wide-aperture\nlens, by blending more nearby samples (Levoy and Hanrahan 1996). A similar approach\ncan be used to re-focus images taken with a plenoptic (microlens array) camera (Ng, Levoy,\nBr´eedif et al. 2005; Ng 2005) or a light ﬁeld microscope (Levoy, Ng, Adams et al. 2006). It\ncan also be used to see through obstacles, using extremely large synthetic apertures focused\non a background that can blur out foreground objects and make them appear translucent\n(Wilburn, Joshi, Vaish et al. 2005; Vaish, Szeliski, Zitnick et al. 2006).\nNow that we understand how to render new images from a light ﬁeld, how do we go about\ncapturing such data sets? One answer is to move a calibrated camera with a motion control rig\nor gantry.6 Another approach is to take handheld photographs and to determine the pose and\nintrinsic calibration of each image using either a calibrated stage or structure from motion. In\nthis case, the images need to be rebinned into a regular 4D (s, t, u, v) space before they can\nbe used for rendering (Gortler, Grzeszczuk, Szeliski et al. 1996). Alternatively, the original\nimages can be used directly using a process called the unstructured Lumigraph, which we\n6 See http://lightﬁeld.stanford.edu/acq.html for a description of some of the gantries and camera arrays built at\nthe Stanford Computer Graphics Laboratory. This Web site also provides a number of light ﬁeld data sets that are a\ngreat source of research and project material.\n632\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ndescribe below.\nBecause of the large number of images involved, light ﬁelds and Lumigraphs can be quite\nvoluminous to store and transmit. Fortunately, as you can tell from Figure 13.7b, there is\na tremendous amount of redundancy (coherence) in a light ﬁeld, which can be made even\nmore explicit by ﬁrst computing a 3D model, as in the Lumigraph. A number of techniques\nhave been developed to compress and progressively transmit such representations (Gortler,\nGrzeszczuk, Szeliski et al. 1996; Levoy and Hanrahan 1996; Rademacher and Bishop 1998;\nMagnor and Girod 2000; Wood, Azuma, Aldinger et al. 2000; Shum, Kang, and Chan 2003;\nMagnor, Ramanathan, and Girod 2003; Shum, Chan, and Kang 2007).\n13.3.1 Unstructured Lumigraph\nWhen the images in a Lumigraph are acquired in an unstructured (irregular) manner, it can be\ncounterproductive to resample the resulting light rays into a regularly binned (s, t, u, v) data\nstructure. This is both because resampling always introduces a certain amount of aliasing and\nbecause the resulting gridded light ﬁeld can be populated very sparsely or irregularly.\nThe alternative is to render directly from the acquired images, by ﬁnding for each light ray\nin a virtual camera the closest pixels in the original images. The unstructured Lumigraph ren-\ndering (ULR) system of Buehler, Bosse, McMillan et al. (2001) describes how to select such\npixels by combining a number of ﬁdelity criteria, including epipole consistency (distance of\nrays to a source camera’s center), angular deviation (similar incidence direction on the sur-\nface), resolution (similar sampling density along the surface), continuity (to nearby pixels),\nand consistency (along the ray). These criteria can all be combined to determine a weighting\nfunction between each virtual camera’s pixel and a number of candidate input cameras from\nwhich it can draw colors. To make the algorithm more efﬁcient, the computations are per-\nformed by discretizing the virtual camera’s image plane using a regular grid overlaid with the\npolyhedral object mesh model and the input camera centers of projection and interpolating\nthe weighting functions between vertices.\nThe unstructured Lumigraph generalizes previous work in both image-based rendering\nand light ﬁeld rendering. When the input cameras are gridded, the ULR behaves the same way\nas regular Lumigraph rendering. When fewer cameras are available but the geometry is accu-\nrate, the algorithm behaves similarly to view-dependent texture mapping (Section 13.1.1).\n13.3.2 Surface light ﬁelds\nOf course, using a two-plane parameterization for a light ﬁeld is not the only possible choice.\n(It is the one usually presented ﬁrst since the projection equations and visualizations are the\neasiest to draw and understand.) As we mentioned on the topic of light ﬁeld compression,\n13.3 Light ﬁelds and Lumigraphs\n633\n(a)\n(b)\nFigure 13.9\nSurface light ﬁelds (Wood, Azuma, Aldinger et al. 2000) c⃝2000 ACM: (a)\nexample of a highly specular object with strong inter-reﬂections; (b) the surface light ﬁeld\nstores the light emanating from each surface point in all visible directions as a “Lumisphere”.\nif we know the 3D shape of the object or scene whose light ﬁeld is being modeled, we can\neffectively compress the ﬁeld because nearby rays emanating from nearby surface elements\nhave similar color values.\nIn fact, if the object is totally diffuse, ignoring occlusions, which can be handled using\n3D graphics algorithms or z-buffering, all rays passing through a given surface point will\nhave the same color value. Hence, the light ﬁeld “collapses” to the usual 2D texture-map\ndeﬁned over an object’s surface. Conversely, if the surface is totally specular (e.g., mirrored),\neach surface point reﬂects a miniature copy of the environment surrounding that point. In the\nabsence of inter-reﬂections (e.g., a convex object in a large open space), each surface point\nsimply reﬂects the far-ﬁeld environment map (Section 2.2.1), which again is two-dimensional.\nTherefore, is seems that re-parameterizing the 4D light ﬁeld to lie on the object’s surface can\nbe extremely beneﬁcial.\nThese observations underlie the surface light ﬁeld representation introduced by Wood,\nAzuma, Aldinger et al. (2000). In their system, an accurate 3D model is built of the object\nbeing represented. Then the Lumisphere of all rays emanating from each surface point is\nestimated or captured (Figure 13.9). Nearby Lumispheres will be highly correlated and hence\namenable to both compression and manipulation.\nTo estimate the diffuse component of each Lumisphere, a median ﬁltering over all visible\nexiting directions is ﬁrst performed for each channel. Once this has been subtracted from the\nLumisphere, the remaining values, which should consist mostly of the specular components,\nare reﬂected around the local surface normal (2.89), which turns each Lumisphere into a copy\nof the local environment around that point. Nearby Lumispheres can then be compressed\nusing predictive coding, vector quantization, or principal component analysis.",
  "image_path": "page_654.jpg",
  "pages": [
    653,
    654,
    655
  ]
}