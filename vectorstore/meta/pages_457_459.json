{
  "doc_id": "pages_457_459",
  "text": "9.1 Motion models\n435\nFigure 9.4\nFour images taken with a hand-held camera registered using a 3D rotation mo-\ntion model (Szeliski and Shum 1997) c⃝1997 ACM. Notice how the homographies, rather\nthan being arbitrary, have a well-deﬁned keystone shape whose width increases away from\nthe origin.\n9.1.4 Gap closing\nThe techniques presented in this section can be used to estimate a series of rotation matrices\nand focal lengths, which can be chained together to create large panoramas. Unfortunately,\nbecause of accumulated errors, this approach will rarely produce a closed 360◦panorama.\nInstead, there will invariably be either a gap or an overlap (Figure 9.5).\nWe can solve this problem by matching the ﬁrst image in the sequence with the last one.\nThe difference between the two rotation matrix estimates associated with the repeated ﬁrst\nindicates the amount of misregistration. This error can be distributed evenly across the whole\nsequence by taking the quotient of the two quaternions associated with these rotations and\ndividing this “error quaternion” by the number of images in the sequence (assuming relatively\nconstant inter-frame rotations). We can also update the estimated focal length based on the\namount of misregistration. To do this, we ﬁrst convert the error quaternion into a gap angle,\nθg and then update the focal length using the equation f ′ = f(1 −θg/360◦).\nFigure 9.5a shows the end of registered image sequence and the ﬁrst image. There is a\nbig gap between the last image and the ﬁrst which are in fact the same image. The gap is\n32◦because the wrong estimate of focal length (f = 510) was used. Figure 9.5b shows the\nregistration after closing the gap with the correct focal length (f = 468). Notice that both\nmosaics show very little visual misregistration (except at the gap), yet Figure 9.5a has been\ncomputed using a focal length that has 9% error. Related approaches have been developed by\nHartley (1994b), McMillan and Bishop (1995), Stein (1995), and Kang and Weiss (1997) to\nsolve the focal length estimation problem using pure panning motion and cylindrical images.\n436\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 9.5 Gap closing (Szeliski and Shum 1997) c⃝1997 ACM: (a) A gap is visible when\nthe focal length is wrong (f = 510). (b) No gap is visible for the correct focal length\n(f = 468).\nUnfortunately, this particular gap-closing heuristic only works for the kind of “one-dimensional”\npanorama where the camera is continuously turning in the same direction. In Section 9.2, we\ndescribe a different approach to removing gaps and overlaps that works for arbitrary camera\nmotions.\n9.1.5 Application: Video summarization and compression\nAn interesting application of image stitching is the ability to summarize and compress videos\ntaken with a panning camera. This application was ﬁrst suggested by Teodosio and Bender\n(1993), who called their mosaic-based summaries salient stills. These ideas were then ex-\ntended by Irani, Hsu, and Anandan (1995), Kumar, Anandan, Irani et al. (1995), and Irani and\nAnandan (1998) to additional applications, such as video compression and video indexing.\nWhile these early approaches used afﬁne motion models and were therefore restricted to long\nfocal lengths, the techniques were generalized by Lee, ge Chen, lung Bruce Lin et al. (1997)\nto full eight-parameter homographies and incorporated into the MPEG-4 video compression\nstandard, where the stitched background layers were called video sprites (Figure 9.6).\nWhile video stitching is in many ways a straightforward generalization of multiple-image\nstitching (Steedly, Pal, and Szeliski 2005; Baudisch, Tan, Steedly et al. 2006), the potential\npresence of large amounts of independent motion, camera zoom, and the desire to visualize\ndynamic events impose additional challenges. For example, moving foreground objects can\noften be removed using median ﬁltering. Alternatively, foreground objects can be extracted\ninto a separate layer (Sawhney and Ayer 1996) and later composited back into the stitched\npanoramas, sometimes as multiple instances to give the impressions of a “Chronophotograph”\n9.1 Motion models\n437\n+\n+\n+ · · · +\n=\nFigure 9.6 Video stitching the background scene to create a single sprite image that can be\ntransmitted and used to re-create the background in each frame (Lee, ge Chen, lung Bruce Lin\net al. 1997) c⃝1997 IEEE.\n(Massey and Bender 1996) and sometimes as video overlays (Irani and Anandan 1998).\nVideos can also be used to create animated panoramic video textures (Section 13.5.2), in\nwhich different portions of a panoramic scene are animated with independently moving video\nloops (Agarwala, Zheng, Pal et al. 2005; Rav-Acha, Pritch, Lischinski et al. 2005), or to shine\n“video ﬂashlights” onto a composite mosaic of a scene (Sawhney, Arpa, Kumar et al. 2002).\nVideo can also provide an interesting source of content for creating panoramas taken from\nmoving cameras. While this invalidates the usual assumption of a single point of view (opti-\ncal center), interesting results can still be obtained. For example, the VideoBrush system of\nSawhney, Kumar, Gendel et al. (1998) uses thin strips taken from the center of the image to\ncreate a panorama taken from a horizontally moving camera. This idea can be generalized\nto other camera motions and compositing surfaces using the concept of mosaics on adap-\ntive manifold (Peleg, Rousso, Rav-Acha et al. 2000), and also used to generate panoramic\nstereograms (Peleg, Ben-Ezra, and Pritch 2001). Related ideas have been used to create\npanoramic matte paintings for multi-plane cel animation (Wood, Finkelstein, Hughes et al.\n1997), for creating stitched images of scenes with parallax (Kumar, Anandan, Irani et al.\n1995), and as 3D representations of more complex scenes using multiple-center-of-projection\nimages (Rademacher and Bishop 1998) and multi-perspective panoramas (Rom´an, Garg, and\nLevoy 2004; Rom´an and Lensch 2006; Agarwala, Agrawala, Cohen et al. 2006).\nAnother interesting variant on video-based panoramas are concentric mosaics (Section 13.3.3)\n(Shum and He 1999). Here, rather than trying to produce a single panoramic image, the com-\nplete original video is kept and used to re-synthesize views (from different camera origins)\nusing ray remapping (light ﬁeld rendering), thus endowing the panorama with a sense of 3D",
  "image_path": "page_458.jpg",
  "pages": [
    457,
    458,
    459
  ]
}