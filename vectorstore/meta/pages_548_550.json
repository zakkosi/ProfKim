{
  "doc_id": "pages_548_550",
  "text": "526\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTelea 2004). More recent techniques use data-driven texture synthesis approaches (Drori,\nCohen-Or, and Yeshurun 2003; Kwatra, Sch¨odl, Essa et al. 2003; Criminisi, P´erez, and\nToyama 2004; Sun, Yuan, Jia et al. 2004; Kwatra, Essa, Bobick et al. 2005; Wilczkowiak,\nBrostow, Tordoff et al. 2005; Komodakis and Tziritas 2007b; Wexler, Shechtman, and Irani\n2007).\n10.7 Exercises\nEx 10.1: Radiometric calibration\nImplement one of the multi-exposure radiometric cali-\nbration algorithms described in Section 10.2 (Debevec and Malik 1997; Mitsunaga and Nayar\n1999; Reinhard, Ward, Pattanaik et al. 2005). This calibration will be useful in a number of\ndifferent applications, such as stitching images or stereo matching with different exposures\nand shape from shading.\n1. Take a series of bracketed images with your camera on a tripod. If your camera has\nan automatic exposure bracketing (AEB) mode, taking three images may be sufﬁcient\nto calibrate most of your camera’s dynamic range, especially if your scene has a lot of\nbright and dark regions. (Shooting outdoors or through a window on a sunny day is\nbest.)\n2. If your images are not taken on a tripod, ﬁrst perform a global alignment (similarity\ntransform).\n3. Estimate the radiometric response function using one of the techniques cited above.\n4. Estimate the high dynamic range radiance image by selecting or blending pixels from\ndifferent exposures (Debevec and Malik 1997; Mitsunaga and Nayar 1999; Eden, Uyt-\ntendaele, and Szeliski 2006).\n5. Repeat your calibration experiments under different conditions, e.g., indoors under in-\ncandescent light, to get a sense for the range of color balancing effects that your camera\nimposes.\n6. If your camera supports RAW and JPEG mode, calibrate both sets of images simulta-\nneously and to each other (the radiance at each pixel will correspond). See if you can\ncome up with a model for what your camera does, e.g., whether it treats color balance\nas a diagonal or full 3 × 3 matrix multiply, whether it uses non-linearities in addition\nto gamma, whether it sharpens the image while “developing” the JPEG image, etc.\n7. Develop an interactive viewer to change the exposure of an image based on the average\nexposure of a region around the mouse. (One variant is to show the adjusted image\n10.7 Exercises\n527\ninside a window around the mouse. Another is to adjust the complete image based on\nthe mouse position.)\n8. Implement a tone mapping operator (Exercise 10.5) and use this to map your radiance\nimage to a displayable gamut.\nEx 10.2: Noise level function\nDetermine your camera’s noise level function using either\nmultiple shots or by analyzing smooth regions.\n1. Set up your camera on a tripod looking at a calibration target or a static scene with a\ngood variation in input levels and colors. (Check your camera’s histogram to ensure\nthat all values are being sampled.)\n2. Take repeated images of the same scene (ideally with a remote shutter release) and\naverage them to compute the variance at each pixel. Discarding pixels near high gra-\ndients (which are affected by camera motion), plot for each color channel the standard\ndeviation at each pixel as a function of its output value.\n3. Fit a lower envelope to these measurements and use this as your noise level function.\nHow much variation do you see in the noise as a function of input level? How much of\nthis is signiﬁcant, i.e., away from ﬂat regions in your camera response function where\nyou do not want to be sampling anyway?\n4. (Optional) Using the same images, develop a technique that segments the image into\nnear-constant regions (Liu, Szeliski, Kang et al. 2008). (This is easier if you are pho-\ntographing a calibration chart.) Compute the deviations for each region from a single\nimage and use them to estimate the NLF. How does this compare to the multi-image\ntechnique, and how stable are your estimates from image to image?\nEx 10.3: Vignetting\nEstimate the amount of vignetting in some of your lenses using one of\nthe following three techniques (or devise one of your choosing):\n1. Take an image of a large uniform intensity region (well-illuminated wall or blue sky—\nbut be careful of brightness gradients) and ﬁt a radial polynomial curve to estimate the\nvignetting.\n2. Construct a center-weighted panorama and compare these pixel values to the input im-\nage values to estimate the vignetting function. Weight pixels in slowly varying regions\nmore highly, as small misalignments will give large errors at high gradients. Option-\nally estimate the radiometric response function as well (Litvinov and Schechner 2005;\nGoldman 2011).\n528\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Analyze the radial gradients (especially in low-gradient regions) and ﬁt the robust\nmeans of these gradients to the derivative of the vignetting function, as described by\nZheng, Yu, Kang et al. (2008).\nFor the parametric form of your vignetting function, you can either use a simple radial func-\ntion, e.g.,\nf(r) = 1 + α1r + α2r2 + · · ·\n(10.42)\nor one of the specialized equations developed by Kang and Weiss (2000) and Zheng, Lin, and\nKang (2006).\nIn all of these cases, be sure that you are using linearized intensity measurements, by\nusing either RAW images or images linearized through a radiometric response function, or at\nleast images where the gamma curve has been removed.\n(Optional) What happens if you forget to undo the gamma before ﬁtting a (multiplicative)\nvignetting function?\nEx 10.4: Optical blur (PSF) estimation\nCompute the optical PSF either using a known\ntarget (Figure 10.7) or by detecting and ﬁtting step edges (Section 10.1.4) (Joshi, Szeliski,\nand Kriegman 2008).\n1. Detect strong edges to sub-pixel precision.\n2. Fit a local proﬁle to each oriented edge and ﬁll these pixels into an ideal target image,\neither at image resolution or at a higher resolution (Figure 10.9c–d).\n3. Use least squares (10.1) at valid pixels to estimate the PSF kernel K, either globally or\nin locally overlapping sub-regions of the image.\n4. Visualize the recovered PSFs and use them to remove chromatic aberration or de-blur\nthe image.\nEx 10.5: Tone mapping\nImplement one of the tone mapping algorithms discussed in Sec-\ntion 10.2.1 (Durand and Dorsey 2002; Fattal, Lischinski, and Werman 2002; Reinhard, Stark,\nShirley et al. 2002; Lischinski, Farbman, Uyttendaele et al. 2006b) or any of the numer-\nous additional algorithms discussed by Reinhard, Ward, Pattanaik et al. (2005) and http:\n//stellar.mit.edu/S/course/6/sp08/6.815/materials.html.\n(Optional) Compare your algorithm to local histogram equalization (Section 3.1.4).\nEx 10.6: Flash enhancement\nDevelop an algorithm to combine ﬂash and non-ﬂash pho-\ntographs to best effect. You can use ideas from Eisemann and Durand (2004) and Petschnigg,\nAgrawala, Hoppe et al. (2004) or anything else you think might work well.",
  "image_path": "page_549.jpg",
  "pages": [
    548,
    549,
    550
  ]
}