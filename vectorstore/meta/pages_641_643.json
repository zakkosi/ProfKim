{
  "doc_id": "pages_641_643",
  "text": "Chapter 13\nImage-based rendering\n13.1 View interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621\n13.1.1 View-dependent texture maps\n. . . . . . . . . . . . . . . . . . . . . 623\n13.1.2 Application: Photo Tourism\n. . . . . . . . . . . . . . . . . . . . . . 624\n13.2 Layered depth images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626\n13.2.1 Impostors, sprites, and layers . . . . . . . . . . . . . . . . . . . . . . 626\n13.3 Light ﬁelds and Lumigraphs\n. . . . . . . . . . . . . . . . . . . . . . . . . . 628\n13.3.1 Unstructured Lumigraph . . . . . . . . . . . . . . . . . . . . . . . . 632\n13.3.2 Surface light ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . 632\n13.3.3 Application: Concentric mosaics . . . . . . . . . . . . . . . . . . . . 634\n13.4 Environment mattes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634\n13.4.1 Higher-dimensional light ﬁelds . . . . . . . . . . . . . . . . . . . . . 636\n13.4.2 The modeling to rendering continuum . . . . . . . . . . . . . . . . . 637\n13.5 Video-based rendering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638\n13.5.1 Video-based animation . . . . . . . . . . . . . . . . . . . . . . . . . 639\n13.5.2 Video textures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640\n13.5.3 Application: Animating pictures . . . . . . . . . . . . . . . . . . . . 643\n13.5.4 3D Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\n13.5.5 Application: Video-based walkthroughs . . . . . . . . . . . . . . . . 645\n13.6 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648\n13.7 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650\n620\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 13.1 Image-based and video-based rendering: (a) a 3D view of a Photo Tourism re-\nconstruction (Snavely, Seitz, and Szeliski 2006) c⃝2006 ACM; (b) a slice through a 4D light\nﬁeld (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996 ACM; (c) sprites with depth (Shade,\nGortler, He et al. 1998) c⃝1998 ACM; (d) surface light ﬁeld (Wood, Azuma, Aldinger et al.\n2000) c⃝2000 ACM; (e) environment matte in front of a novel background (Zongker, Werner,\nCurless et al. 1999) c⃝1999 ACM; (f) real-time video environment matte (Chuang, Zongker,\nHindorff et al. 2000) c⃝2000 ACM; (g) Video Rewrite used to re-animate old video (Bregler,\nCovell, and Slaney 1997) c⃝1997 ACM; (h) video texture of a candle ﬂame (Sch¨odl, Szeliski,\nSalesin et al. 2000) c⃝2000 ACM; (i) video view interpolation (Zitnick, Kang, Uyttendaele\net al. 2004) c⃝2004 ACM.\n13.1 View interpolation\n621\nOver the last two decades, image-based rendering has emerged as one of the most exciting\napplications of computer vision (Kang, Li, Tong et al. 2006; Shum, Chan, and Kang 2007).\nIn image-based rendering, 3D reconstruction techniques from computer vision are combined\nwith computer graphics rendering techniques that use multiple views of a scene to create inter-\nactive photo-realistic experiences, such as the Photo Tourism system shown in Figure 13.1a.\nCommercial versions of such systems include immersive street-level navigation in on-line\nmapping systems1 and the creation of 3D Photosynths2 from large collections of casually\nacquired photographs.\nIn this chapter, we explore a variety of image-based rendering techniques, such as those\nillustrated in Figure 13.1. We begin with view interpolation (Section 13.1), which creates a\nseamless transition between a pair of reference images using one or more pre-computed depth\nmaps. Closely related to this idea are view-dependent texture maps (Section 13.1.1), which\nblend multiple texture maps on a 3D model’s surface. The representations used for both the\ncolor imagery and the 3D geometry in view interpolation include a number of clever variants\nsuch as layered depth images (Section 13.2) and sprites with depth (Section 13.2.1).\nWe continue our exploration of image-based rendering with the light ﬁeld and Lumigraph\nfour-dimensional representations of a scene’s appearance (Section 13.3), which can be used\nto render the scene from any arbitrary viewpoint. Variants on these representations include\nthe unstructured Lumigraph (Section 13.3.1), surface light ﬁelds (Section 13.3.2), concentric\nmosaics (Section 13.3.3), and environment mattes (Section 13.4).\nThe last part of this chapter explores the topic of video-based rendering, which uses one\nor more videos in order to create novel video-based experiences (Section 13.5). The topics\nwe cover include video-based facial animation (Section 13.5.1), as well as video textures\n(Section 13.5.2), in which short video clips can be seamlessly looped to create dynamic real-\ntime video-based renderings of a scene. We close with a discussion of 3D videos created from\nmultiple video streams (Section 13.5.4), as well as video-based walkthroughs of environments\n(Section 13.5.5), which have found widespread application in immersive outdoor mapping\nand driving direction systems.\n13.1 View interpolation\nWhile the term image-based rendering ﬁrst appeared in the papers by Chen (1995) and\nMcMillan and Bishop (1995), the work on view interpolation by Chen and Williams (1993)\nis considered as the seminal paper in the ﬁeld. In view interpolation, pairs of rendered color\nimages are combined with their pre-computed depth maps to generate interpolated views that\n1 http://maps.bing.com and http://maps.google.com.\n2 http://photosynth.net.",
  "image_path": "page_642.jpg",
  "pages": [
    641,
    642,
    643
  ]
}