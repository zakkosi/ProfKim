{
  "doc_id": "pages_522_524",
  "text": "500\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 10.31 Super-resolution results using a variety of image priors (Capel 2001): (a) Low-\nres ROI (bicubic 3× zoom); (b) average image; (c) MLE @ 1.25× pixel-zoom; (d) simple\n∥x∥2 prior (λ = 0.004); (e) GMRF (λ = 0.003); (f) HMRF (λ = 0.01, α = 0.04). 10\nimages are used as input and a 3× super-resolved image is produced in each case, except for\nthe MLE result in (c).\n(a)\n(b)\n(c)\nFigure 10.32 Example-based super-resolution: (a) original 32 × 32 low-resolution image;\n(b) example-based super-resolved 256 × 256 image (Freeman, Jones, and Pasztor 2002) c⃝\n2002 IEEE; (c) upsampling via imposed edge statistics (Fattal 2007) c⃝2007 ACM.\n10.3 Super-resolution and blur removal\n501\ning set of sample images can be used to ﬁnd plausible mappings between low-frequency\noriginals and the missing higher frequencies. Inspired by some of the example-based texture\nsynthesis algorithms we discuss in Section 10.5, the example-based super-resolution algo-\nrithm developed by Freeman, Jones, and Pasztor (2002) uses training images to learn the\nmapping between local texture patches and missing higher-frequency details. To ensure that\noverlapping patches are similar in appearance, a Markov random ﬁeld is used and optimized\nusing either belief propagation (Freeman, Pasztor, and Carmichael 2000) or a raster-scan de-\nterministic variant (Freeman, Jones, and Pasztor 2002). Figure 10.32 shows the results of\nhallucinating missing details using this approach and compares these results to a more recent\nalgorithm by Fattal (2007). This latter algorithm learns to predict oriented gradient magni-\ntudes in the ﬁner resolution image based on a pixel’s location relative to the nearest detected\nedge along with the corresponding edge statistics (magnitude and width). It is also possible\nto combine sparse (robust) derivative priors with example-based super-resolution, as shown\nby Tappen, Russell, and Freeman (2003).\nAn alternative (but closely related) form of hallucination is to recognize the parts of a\ntraining database of images to which a low-resolution pixel might correspond. In their work,\nBaker and Kanade (2002) use local derivative-of-Gaussian ﬁlter responses as features and\nthen match parent structure vectors in a manner similar to De Bonet (1997).19 The high-\nfrequency gradient at each recognized training image location is then used as a constraint on\nthe super-resolved image, along with the usual reconstruction (prediction) equation (10.27).\nFigure 10.33 shows the result of hallucinating higher-resolution faces from lower-resolution\ninputs; Baker and Kanade (2002) also show examples of super-resolving known-font text.\nExercise 10.7 gives more details on how to implement and test one or more of these super-\nresolution techniques.\nUnder favorable conditions, super-resolution and related upsampling techniques can in-\ncrease the resolution of a well-photographed image or image collection. When the input\nimages are blurry to start with, the best one can often hope for is to reduce the amount of blur.\nThis problem is closely related super-resolution, with the biggest differences being that the\nblur kernel b is usually much larger and the downsampling factor D is unity. A large literature\non image deblurring exists; some of the more recent publications with nice literature reviews\ninclude those by Fergus, Singh, Hertzmann et al. (2006), Yuan, Sun, Quan et al. (2008), and\nJoshi, Zitnick, Szeliski et al. (2009). It is also possible to reduce blur by combining sharp (but\nnoisy) images with blurrier (but cleaner) images (Yuan, Sun, Quan et al. 2007), take lots of\nquick exposures20 (Hasinoff and Kutulakos 2008; Hasinoff, Kutulakos, Durand et al. 2009;\nHasinoff, Durand, and Freeman 2010), or use coded aperture techniques to simultaneously\n19 For face super-resolution, where all the images are pre-aligned, only corresponding pixels in different images\nare examined.\n20 The SONY DSC-WX1 takes multiple shots to produce better low-light photos.\n502\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.33 Recognition-based super-resolution (Baker and Kanade 2002) c⃝2002 IEEE.\nThe Hallucinated column shows the results of the recognition-based algorithm compared to\nthe regularization-based approach of Hardie, Barnard, and Armstrong (1997).\nestimate depth and reduce blur (Levin, Fergus, Durand et al. 2007; Zhou, Lin, and Nayar\n2009).\n10.3.1 Color image demosaicing\nA special case of super-resolution, which is used daily in most digital still cameras, is the\nprocess of demosaicing samples from a color ﬁlter array (CFA) into a full-color RGB image.\nFigure 10.34 shows the most commonly used CFA known as the Bayer pattern, which has\ntwice as many green (G) sensors as red and blue sensors.\nThe process of going from the known CFA pixels values to the full RGB image is quite\nchallenging. Unlike regular super-resolution, where small errors in guessing unknown values\nusually show up as blur or aliasing, demosaicing artifacts often produce spurious colors or\nhigh-frequency patterned zippering, which are quite visible to the eye (Figure 10.35b).\nOver the years, a variety of techniques have been developed for image demosaicing (Kim-\nmel 1999). Bennett, Uyttendaele, Zitnick et al. (2006) present a recently developed algorithm\nalong with some good references, while Longere, Delahunt, Zhang et al. (2002) and Tappen,\nRussell, and Freeman (2003) compare some previously developed techniques using percep-\ntually motivated metrics. To reduce the zippering effect, most techniques use the edge or",
  "image_path": "page_523.jpg",
  "pages": [
    522,
    523,
    524
  ]
}