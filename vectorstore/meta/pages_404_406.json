{
  "doc_id": "pages_404_406",
  "text": "382\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nﬂow\ninitial layers ﬁnal layers\nlayers with pixel assignments and ﬂow\n(c)\n(d)\n(e)\n(f)\nFigure 8.1\nMotion estimation: (a–b) regularization-based optical ﬂow (Nagel and Enkel-\nmann 1986) c⃝1986 IEEE; (c–d) layered motion estimation (Wang and Adelson 1994) c⃝\n1994 IEEE; (e–f) sample image and ground truth ﬂow from evaluation database (Baker,\nBlack, Lewis et al. 2007) c⃝2007 IEEE.\n8 Dense motion estimation\n383\nAlgorithms for aligning images and estimating motion in video sequences are among the most\nwidely used in computer vision. For example, frame-rate image alignment is widely used in\ncamcorders and digital cameras to implement their image stabilization (IS) feature.\nAn early example of a widely used image registration algorithm is the patch-based trans-\nlational alignment (optical ﬂow) technique developed by Lucas and Kanade (1981). Variants\nof this algorithm are used in almost all motion-compensated video compression schemes\nsuch as MPEG and H.263 (Le Gall 1991). Similar parametric motion estimation algorithms\nhave found a wide variety of applications, including video summarization (Teodosio and\nBender 1993; Irani and Anandan 1998), video stabilization (Hansen, Anandan, Dana et al.\n1994; Srinivasan, Chellappa, Veeraraghavan et al. 2005; Matsushita, Ofek, Ge et al. 2006),\nand video compression (Irani, Hsu, and Anandan 1995; Lee, ge Chen, lung Bruce Lin et\nal. 1997). More sophisticated image registration algorithms have also been developed for\nmedical imaging and remote sensing. Image registration techniques are surveyed by Brown\n(1992), Zitov’aa and Flusser (2003), Goshtasby (2005), and Szeliski (2006a).\nTo estimate the motion between two or more images, a suitable error metric must ﬁrst\nbe chosen to compare the images (Section 8.1). Once this has been established, a suitable\nsearch technique must be devised. The simplest technique is to exhaustively try all possible\nalignments, i.e., to do a full search. In practice, this may be too slow, so hierarchical coarse-\nto-ﬁne techniques (Section 8.1.1) based on image pyramids are normally used. Alternatively,\nFourier transforms (Section 8.1.2) can be used to speed up the computation.\nTo get sub-pixel precision in the alignment, incremental methods (Section 8.1.3) based\non a Taylor series expansion of the image function are often used. These can also be applied\nto parametric motion models (Section 8.2), which model global image transformations such\nas rotation or shearing. Motion estimation can be made more reliable by learning the typi-\ncal dynamics or motion statistics of the scenes or objects being tracked, e.g., the natural gait\nof walking people (Section 8.2.2). For more complex motions, piecewise parametric spline\nmotion models (Section 8.3) can be used. In the presence of multiple independent (and per-\nhaps non-rigid) motions, general-purpose optical ﬂow (or optic ﬂow) techniques need to be\nused (Section 8.4). For even more complex motions that include a lot of occlusions, layered\nmotion models (Section 8.5), which decompose the scene into coherently moving layers, can\nwork well.\nIn this chapter, we describe each of these techniques in more detail. Additional details\ncan be found in review and comparative evaluation papers on motion estimation (Barron,\nFleet, and Beauchemin 1994; Mitiche and Bouthemy 1996; Stiller and Konrad 1999; Szeliski\n2006a; Baker, Black, Lewis et al. 2007).\n384\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n8.1 Translational alignment\nThe simplest way to establish an alignment between two images or image patches is to shift\none image relative to the other. Given a template image I0(x) sampled at discrete pixel\nlocations {xi = (xi, yi)}, we wish to ﬁnd where it is located in image I1(x). A least squares\nsolution to this problem is to ﬁnd the minimum of the sum of squared differences (SSD)\nfunction\nESSD(u) =\nX\ni\n[I1(xi + u) −I0(xi)]2 =\nX\ni\ne2\ni ,\n(8.1)\nwhere u = (u, v) is the displacement and ei = I1(xi + u) −I0(xi) is called the residual\nerror (or the displaced frame difference in the video coding literature).1 (We ignore for the\nmoment the possibility that parts of I0 may lie outside the boundaries of I1 or be otherwise\nnot visible.) The assumption that corresponding pixel values remain the same in the two\nimages is often called the brightness constancy constraint.2\nIn general, the displacement u can be fractional, so a suitable interpolation function must\nbe applied to image I1(x). In practice, a bilinear interpolant is often used but bicubic inter-\npolation can yield slightly better results (Szeliski and Scharstein 2004). Color images can be\nprocessed by summing differences across all three color channels, although it is also possible\nto ﬁrst transform the images into a different color space or to only use the luminance (which\nis often done in video encoders).\nRobust error metrics.\nWe can make the above error metric more robust to outliers by re-\nplacing the squared error terms with a robust function ρ(ei) (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Black and Anandan 1996; Stewart 1999) to obtain\nESRD(u) =\nX\ni\nρ(I1(xi + u) −I0(xi)) =\nX\ni\nρ(ei).\n(8.2)\nThe robust norm ρ(e) is a function that grows less quickly than the quadratic penalty associ-\nated with least squares. One such function, sometimes used in motion estimation for video\ncoding because of its speed, is the sum of absolute differences (SAD) metric3 or L1 norm,\ni.e.,\nESAD(u) =\nX\ni\n|I1(xi + u) −I0(xi)| =\nX\ni\n|ei|.\n(8.3)\n1 The usual justiﬁcation for using least squares is that it is the optimal estimate with respect to Gaussian noise.\nSee the discussion below on robust error metrics as well as Appendix B.3.\n2 Brightness constancy (Horn 1974) is the tendency for objects to maintain their perceived brightness under\nvarying illumination conditions.\n3 In video compression, e.g., the H.264 standard (http://www.itu.int/rec/T-REC-H.264), the sum of absolute trans-\nformed differences (SATD), which measures the differences in a frequency transform space, e.g., using a Hadamard\ntransform, is often used since it more accurately predicts quality (Richardson 2003).",
  "image_path": "page_405.jpg",
  "pages": [
    404,
    405,
    406
  ]
}