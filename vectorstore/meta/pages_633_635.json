{
  "doc_id": "pages_633_635",
  "text": "12.7 Recovering texture maps and albedos\n611\none or more maps, e.g., using a subdivision mesh (Section 12.3.2) (Eck, DeRose, Duchamp\net al. 1995) or a geometry image (Section 12.3.3) (Gu, Gortler, and Hoppe 2002).\nOnce the (u, v) coordinates for each triangle have been ﬁxed, the perspective projec-\ntion equations mapping from texture (u, v) to an image j’s pixel (uj, vj) coordinates can be\nobtained by concatenating the afﬁne (u, v) →(X, Y, Z) mapping with the perspective ho-\nmography (X, Y, Z) →(uj, vj) (Szeliski and Shum 1997). The color values for the (u, v)\ntexture map can then be re-sampled and stored, or the original image can itself be used as the\ntexture source using projective texture mapping (OpenGL-ARB 1997).\nThe situation becomes more involved when more than one source image is available for\nappearance recovery, which is the usual case. One possibility is to use a view-dependent\ntexture map (Section 13.1.1), in which a different source image (or combination of source\nimages) is used for each polygonal face based on the angles between the virtual camera, the\nsurface normals, and the source images (Debevec, Taylor, and Malik 1996; Pighin, Hecker,\nLischinski et al. 1998). An alternative approach is to estimate a complete Surface Light Field\nfor each surface point (Wood, Azuma, Aldinger et al. 2000), as described in Section 13.3.2.\nIn some situations, e.g., when using models in traditional 3D games, it is preferable to\nmerge all of the source images into a single coherent texture map during pre-processing.\nIdeally, each surface triangle should select the source image where it is seen most directly\n(perpendicular to its normal) and at the resolution best matching the texture map resolution.14\nThis can be posed as a graph cut optimization problem, where the smoothness term encour-\nages adjacent triangles to use similar source images, followed by blending to compensate\nfor exposure differences (Lempitsky and Ivanov 2007; Sinha, Steedly, Szeliski et al. 2008).\nEven better results can be obtained by explicitly modeling geometric and photometric mis-\nalignments between the source images (Shum and Szeliski 2000; Gal, Wexler, Ofek et al.\n2010).\nThese kinds of approaches produce good results when the lighting stays ﬁxed with respect\nto the object, i.e., when the camera moves around the object or space. When the lighting is\nstrongly directional, however, and the object is being moved relative to this lighting, strong\nshading effects or specularities may be present, which will interfere with the reliable recov-\nery of a texture (albedo) map. In this case, it is preferable to explicitly undo the shading\neffects (Section 12.1) by modeling the light source directions and estimating the surface re-\nﬂectance properties while recovering the texture map (Sato and Ikeuchi 1996; Sato, Wheeler,\nand Ikeuchi 1997; Yu and Malik 1998; Yu, Debevec, Malik et al. 1999). Figure 12.22 shows\nthe results of one such approach, where the specularities are ﬁrst removed while estimat-\ning the matte reﬂectance component (albedo) and then later re-introduced by estimating the\nspecular component ks in a Torrance–Sparrow reﬂection model (2.91).\n14 When surfaces are seen at oblique viewing angles, it may be necessary to blend different images together to\nobtain the best resolution (Wang, Kang, Szeliski et al. 2001).\n612\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.22\nEstimating the diffuse albedo and reﬂectance parameters for a scanned 3D\nmodel (Sato, Wheeler, and Ikeuchi 1997) c⃝1997 ACM: (a) set of input images projected\nonto the model; (b) the complete diffuse reﬂection (albedo) model; (c) rendering from the\nreﬂectance model including the specular component.\n12.7.1 Estimating BRDFs\nA more ambitious approach to the problem of view-dependent appearance modeling is to\nestimate a general bidirectional reﬂectance distribution function (BRDF) for each point on an\nobject’s surface. Dana, van Ginneken, Nayar et al. (1999), Jensen, Marschner, Levoy et al.\n(2001), and Lensch, Kautz, Goesele et al. (2003) present different techniques for estimating\nsuch functions, while Dorsey, Rushmeier, and Sillion (2007) and Weyrich, Lawrence, Lensch\net al. (2008) present more recent surveys of the topics of BRDF modeling, recovery, and\nrendering.\nAs we saw in Section 2.2.2 (2.81), the BRDF can be written as\nfr(θi, φi, θr, φr; λ),\n(12.9)\nwhere (θi, φi) and (θr, φr) are the angles the incident ˆvi and reﬂected ˆvr light ray directions\nmake with the local surface coordinate frame ( ˆdx, ˆdy, ˆn) shown in Figure 2.15. When mod-\neling the appearance of an object, as opposed to the appearance of a patch of material, we\nneed to estimate this function at every point (x, y) on the object’s surface, which gives us the\nspatially varying BRDF, or SVBRDF (Weyrich, Lawrence, Lensch et al. 2008),\nfv(x, y, θi, φi, θr, φr; λ).\n(12.10)\nIf sub-surface scattering effects are being modeled, such as the long-range transmission\nof light through materials such as alabaster, the eight-dimensional bidirectional scattering-\nsurface reﬂectance-distribution function (BSSRDF) is used instead,\nfe(xi, yi, θi, φi, xe, ye, θe, φe; λ),\n(12.11)\nwhere the e subscript now represents the emitted rather than the reﬂected light directions.\n12.7 Recovering texture maps and albedos\n613\n(a)\n(b)\nFigure 12.23\nImage-based reconstruction of appearance and detailed geometry (Lensch,\nKautz, Goesele et al. 2003) c⃝2003 ACM. (a) Appearance models (BRDFs) are re-estimated\nusing divisive clustering. (b) In order to model detailed spatially varying appearance, each\nlumitexel is projected onto the basis formed by the clustered materials.\nWeyrich, Lawrence, Lensch et al. (2008) provide a nice survey of these and related topics,\nincluding basic photometry, BRDF models, traditional BRDF acquisition using gonio reﬂec-\ntometry (the precise measurement of visual angles and reﬂectances), multiplexed illumination\n(Schechner, Nayar, and Belhumeur 2009), skin modeling (Debevec, Hawkins, Tchou et al.\n2000; Weyrich, Matusik, Pﬁster et al. 2006), and image-based acquisition techniques, which\nsimultaneously recover an object’s 3D shape and reﬂectometry from multiple photographs.\nA nice example of this latter approach is the system developed by Lensch, Kautz, Goesele\net al. (2003), who estimate locally varying BRDFs and reﬁne their shape models using local\nestimates of surface normals. To build up their models, they ﬁrst associate a lumitexels, which\ncontains a 3D position, a surface normal, and a set of sparse radiance samples, with each\nsurface point. Next, they cluster such lumitexels into materials that share common properties,\nusing a Lafortune reﬂectance model (Lafortune, Foo, Torrance et al. 1997) and a divisive\nclustering approach (Figure 12.23a). Finally, in order to model detailed spatially varying\nappearance, each lumitexel (surface point) is projected onto the basis of clustered appearance\nmodels (Figure 12.23b).\nWhile most of the techniques discussed in this section require large numbers of views\nto estimate surface properties, a challenging future direction will be to take these techniques\nout of the lab and into the real world, and to combine them with regular and Internet photo\nimage-based modeling approaches.\n12.7.2 Application: 3D photography\nThe techniques described in this chapter for building complete 3D models from multiple im-\nages and then recovering their surface appearance have opened up a whole new range of\napplications that often go under the name 3D photography. Pollefeys and Van Gool (2002)\nprovide a nice introduction to this ﬁeld, including the processing steps of feature matching,",
  "image_path": "page_634.jpg",
  "pages": [
    633,
    634,
    635
  ]
}