{
  "doc_id": "pages_309_311",
  "text": "5.2 Split and merge\n287\n(a)\n(b)\n(c)\nFigure 5.14\nGraph-based merging segmentation (Felzenszwalb and Huttenlocher 2004b)\nc⃝2004 Springer: (a) input grayscale image that is successfully segmented into three regions\neven though the variation inside the smaller rectangle is larger than the variation across the\nmiddle edge; (b) input grayscale image; (c) resulting segmentation using an N8 pixel neigh-\nborhood.\nFor any region R, its internal difference is deﬁned as the largest edge weight in the re-\ngion’s minimum spanning tree,\nInt(R) =\nmin\ne∈MST (R) w(e).\n(5.20)\nFor any two adjacent regions with at least one edge connecting their vertices, the difference\nbetween these regions is deﬁned as the minimum weight edge connecting the two regions,\nDif (R1, R2) =\nmin\ne=(v1,v2)|v1∈R1,v2∈R2 w(e).\n(5.21)\nTheir algorithm merges any two adjacent regions whose difference is smaller than the mini-\nmum internal difference of these two regions,\nMInt(R1, R2) = min(Int(R1) + τ(R1), Int(R2) + τ(R2)),\n(5.22)\nwhere τ(R) is a heuristic region penalty that Felzenszwalb and Huttenlocher (2004b) set to\nk/|R|, but which can be set to any application-speciﬁc measure of region goodness.\nBy merging regions in decreasing order of the edges separating them (which can be efﬁ-\nciently evaluated using a variant of Kruskal’s minimum spanning tree algorithm), they prov-\nably produce segmentations that are neither too ﬁne (there exist regions that could have been\nmerged) nor too coarse (there are regions that could be split without being mergeable). For\nﬁxed-size pixel neighborhoods, the running time for this algorithm is O(N log N), where N\nis the number of image pixels, which makes it one of the fastest segmentation algorithms\n(Paris and Durand 2007). Figure 5.14 shows two examples of images segmented using their\ntechnique.\n288\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 5.15\nCoarse to ﬁne node aggregation in segmentation by weighted aggregation\n(SWA) (Sharon, Galun, Sharon et al. 2006) c⃝2006 Macmillan Publishers Ltd [Nature]: (a)\noriginal gray-level pixel grid; (b) inter-pixel couplings, where thicker lines indicate stronger\ncouplings; (c) after one level of coarsening, where each original pixel is strongly coupled to\none of the coarse-level nodes; (d) after two levels of coarsening.\n5.2.5 Probabilistic aggregation\nAlpert, Galun, Basri et al. (2007) develop a probabilistic merging algorithm based on two\ncues, namely gray-level similarity and texture similarity. The gray-level similarity between\nregions Ri and Rj is based on the minimal external difference from other neighboring regions,\nσ+\nlocal = min(∆+\ni , ∆+\nj ),\n(5.23)\nwhere ∆+\ni = mink |∆ik| and ∆ik is the difference in average intensities between regions Ri\nand Rk. This is compared to the average intensity difference,\nσ−\nlocal =\n∆−\ni + ∆−\nj\n2\n,\n(5.24)\nwhere ∆−\ni = P\nk(τik∆ik)/ P\nk(τik) and τik is the boundary length between regions Ri and\nRk. The texture similarity is deﬁned using relative differences between histogram bins of\nsimple oriented Sobel ﬁlter responses. The pairwise statistics σ+\nlocal and σ−\nlocal are used to\ncompute the likelihoods pij that two regions should be merged. (See the paper by Alpert,\nGalun, Basri et al. (2007) for more details.)\nMerging proceeds in a hierarchical fashion inspired by algebraic multigrid techniques\n(Brandt 1986; Briggs, Henson, and McCormick 2000) and previously used by Alpert, Galun,\nBasri et al. (2007) in their segmentation by weighted aggregation (SWA) algorithm (Sharon,\nGalun, Sharon et al. 2006), which we discuss in Section 5.4. A subset of the nodes C ⊂V\nthat are (collectively) strongly coupled to all of the original nodes (regions) are used to deﬁne\nthe problem at a coarser scale (Figure 5.15), where strong coupling is deﬁned as\nP\nj∈C pij\nP\nj∈V pij\n> φ,\n(5.25)\n5.3 Mean shift and mode ﬁnding\n289\nwith φ usually set to 0.2. The intensity and texture similarity statistics for the coarser nodes\nare recursively computed using weighted averaging, where the relative strengths (couplings)\nbetween coarse- and ﬁne-level nodes are based on their merge probabilities pij. This allows\nthe algorithm to run in essentially O(N) time, using the same kind of hierarchical aggrega-\ntion operations that are used in pyramid-based ﬁltering or preconditioning algorithms. After\na segmentation has been identiﬁed at a coarser level, the exact memberships of each pixel are\ncomputed by propagating coarse-level assignments to their ﬁner-level “children” (Sharon,\nGalun, Sharon et al. 2006; Alpert, Galun, Basri et al. 2007). Figure 5.22 shows the segmen-\ntations produced by this algorithm compared to other popular segmentation algorithms.\n5.3 Mean shift and mode ﬁnding\nMean-shift and mode ﬁnding techniques, such as k-means and mixtures of Gaussians, model\nthe feature vectors associated with each pixel (e.g., color and position) as samples from an\nunknown probability density function and then try to ﬁnd clusters (modes) in this distribution.\nConsider the color image shown in Figure 5.16a. How would you segment this image\nbased on color alone? Figure 5.16b shows the distribution of pixels in L*u*v* space, which\nis equivalent to what a vision algorithm that ignores spatial location would see. To make the\nvisualization simpler, let us only consider the L*u* coordinates, as shown in Figure 5.16c.\nHow many obvious (elongated) clusters do you see? How would you go about ﬁnding these\nclusters?\nThe k-means and mixtures of Gaussians techniques use a parametric model of the den-\nsity function to answer this question, i.e., they assume the density is the superposition of a\nsmall number of simpler distributions (e.g., Gaussians) whose locations (centers) and shape\n(covariance) can be estimated. Mean shift, on the other hand, smoothes the distribution and\nﬁnds its peaks as well as the regions of feature space that correspond to each peak. Since\na complete density is being modeled, this approach is called non-parametric (Bishop 2006).\nLet us look at these techniques in more detail.\n5.3.1 K-means and mixtures of Gaussians\nWhile k-means implicitly models the probability density as a superposition of spherically\nsymmetric distributions, it does not require any probabilistic reasoning or modeling (Bishop\n2006). Instead, the algorithm is given the number of clusters k it is supposed to ﬁnd; it\nthen iteratively updates the cluster center location based on the samples that are closest to\neach center. The algorithm can be initialized by randomly sampling k centers from the input\nfeature vectors. Techniques have also been developed for splitting or merging cluster centers",
  "image_path": "page_310.jpg",
  "pages": [
    309,
    310,
    311
  ]
}