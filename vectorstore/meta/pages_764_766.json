{
  "doc_id": "pages_764_766",
  "text": "742\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n=\nRT\n0 C1R0,\n(A.21)\nwhich, through recursion, can be turned into\nC = RT\n0 . . . RT\nn−1Rn−1 . . . R0 = RT R.\n(A.22)\nAlgorithm A.1 provides a more procedural deﬁnition, which can store the upper-triangular\nmatrix R in the same space as C, if desired. The total operation count for Cholesky factor-\nization is O(N 3) for a dense matrix but can be signiﬁcantly lower for sparse matrices with\nlow ﬁll-in (Appendix A.4).\nNote that Cholesky decomposition can also be applied to block-structured matrices, where\nthe term γ in (A.19) is now a square block sub-matrix and c is a rectangular matrix (Golub\nand Van Loan 1996). The computation of square roots can be avoided by leaving the γ on\nthe diagonal of the middle factor in (A.20), which results in the C = LDLT factorization,\nwhere D is a diagonal matrix. However, since square roots are relatively fast on modern\ncomputers, this is not worth the bother and Cholesky factorization is usually preferred.\nA.2 Linear least squares\nLeast squares ﬁtting problems are pervasive in computer vision. For example, the alignment\nof images based on matching feature points involves the minimization of a squared distance\nobjective function (6.2),\nELS =\nX\ni\n∥ri∥2 =\nX\ni\n∥f(xi; p) −x′\ni∥2,\n(A.23)\nwhere\nri = f(xi; p) −x′\ni = ˆx′\ni −˜x′\ni\n(A.24)\nis the residual between the measured location ˆx′\ni and its corresponding current predicted lo-\ncation ˜x′\ni = f(xi; p). More complex versions of least squares problems, such as large-scale\nstructure from motion (Section 7.4), may involve the minimization of functions of thousands\nof variables. Even problems such as image ﬁltering (Section 3.4.3) and regularization (Sec-\ntion 3.7.1) may involve the minimization of sums of squared errors.\nFigure A.2a shows an example of a simple least squares line ﬁtting problem, where the\nquantities being estimated are the line equation parameters (m, b). When the sampled vertical\nvalues yi are assumed to be noisy versions of points on the line y = mx + b, the optimal\nestimates for (m, b) can be found by minimizing the squared vertical residuals\nEVLS =\nX\ni\n|yi −(mxi + b)|2.\n(A.25)\nA.2 Linear least squares\n743\nNote that the function being ﬁtted need not itself be linear to use linear least squares. All that\nis required is that the function be linear in the unknown parameters. For example, polynomial\nﬁtting can be written as\nEPLS =\nX\ni\n|yi −(\np\nX\nj=0\najxj\ni)|2,\n(A.26)\nwhile sinusoid ﬁtting with unknown amplitude A and phase φ (but known frequency f) can\nbe written as\nESLS =\nX\ni\n|yi −A sin(2πfxi +φ)|2 =\nX\ni\n|yi −(B sin 2πfxi +C cos 2πfxi)|2, (A.27)\nwhich is linear in (B, C).\nIn general, it is more common to denote the unknown parameters using x and to write the\ngeneral form of linear least squares as5\nELLS =\nX\ni\n|aix −bi|2 = ∥Ax −b∥2.\n(A.28)\nExpanding the above equation gives us\nELLS = xT (AT A)x −2xT (AT b) + ∥b∥2,\n(A.29)\nwhose minimum value for x can be found by solving the associated normal equations (Bj¨orck\n1996; Golub and Van Loan 1996)\n(AT A)x = AT b.\n(A.30)\nThe preferred way to solve the normal equations is to use Cholesky factorization. Let\nC = AT A = RT R,\n(A.31)\nwhere R is the upper-triangular Cholesky factor of the Hessian C, and\nd = AT b.\n(A.32)\nAfter factorization, the solution for x can be obtained as\nRT z = d,\nRx = z,\n(A.33)\nwhich involves the solution of two triangular systems, i.e., forward and backward substitution\n(Bj¨orck 1996).\n5 Be extra careful in interpreting the variable names here. In the 2D line-ﬁtting example, x is used to denote the\nhorizontal axis, but in the general least squares problem, x = (m, b) denotes the unknown parameter vector.\n744\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx\ny\nb\nm\ny=mx+b\n×\n×\n×\n×\nx\ny\nax+by+c=0\n×\n×\n×\n×\n(a)\n(b)\nFigure A.2 Least squares regression. (a) The line y = mx + b is ﬁt to the four noisy data\npoints, {(xi, yi)}, denoted by × by minimizing the squared vertical residuals between the\ndata points and the line, P\ni ∥yi −(mxi + b)∥2. (b) When the measurements {(xi, yi)} are\nassumed to have noise in all directions, the sum of orthogonal squared distances to the line\nP\ni ∥axi + byi + c∥2 is minimized using total least squares.\nIn cases where the least squares problem is numerically poorly conditioned (which should\ngenerally be avoided by adding sufﬁcient regularization or prior knowledge about the param-\neters, (Appendix A.3)), it is possible to use QR factorization or SVD directly on the matrix\nA (Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Nocedal and Wright\n2006; Bj¨orck and Dahlquist 2010), e.g.,\nAx = QRx = b\n−→\nRx = QT b.\n(A.34)\nNote that the upper triangular matrices R produced by the Cholesky factorization of C =\nAT A and the QR factorization of A are the same, but that solving (A.34) is generally more\nstable (less sensitive to roundoff error) but slower (by a constant factor).\nA.2.1 Total least squares\nIn some problems, e.g., when performing geometric line ﬁtting in 2D images or 3D plane\nﬁtting to point cloud data, instead of having measurement error along one particular axis, the\nmeasured points have uncertainty in all directions, which is known as the errors-in-variables\nmodel (Van Huffel and Lemmerling 2002; Matei and Meer 2006). In this case, it makes more\nsense to minimize a set of homogeneous squared errors of the form\nETLS =\nX\ni\n(aix)2 = ∥Ax∥2,\n(A.35)\nwhich is known as total least squares (TLS) (Van Huffel and Vandewalle 1991; Bj¨orck 1996;\nGolub and Van Loan 1996; Van Huffel and Lemmerling 2002).",
  "image_path": "page_765.jpg",
  "pages": [
    764,
    765,
    766
  ]
}