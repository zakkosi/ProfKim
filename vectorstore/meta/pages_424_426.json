{
  "doc_id": "pages_424_426",
  "text": "402\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.5\nA schematic overview of the inverse compositional algorithm (copied, with\npermission, from (Baker, Gross, Ishikawa et al. 2003)). Steps 3–6 (light-colored arrows) are\nperformed once as a pre-computation. The main algorithm simply consists of iterating: image\nwarping (Step 1), image differencing (Step 2), image dot products (Step 7), multiplication\nwith the inverse of the Hessian (Step 8), and the update to the warp (Step 9). All of these\nsteps can be performed efﬁciently.\n8.2 Parametric motion\n403\nChellappa 1997; Srinivasan, Chellappa, Veeraraghavan et al. 2005). Algorithms for stabiliza-\ntion run inside both hardware devices, such as camcorders and still cameras, and software\npackages for improving the visual quality of shaky videos.\nIn their paper on full-frame video stabilization, Matsushita, Ofek, Ge et al. (2006) give\na nice overview of the three major stages of stabilization, namely motion estimation, motion\nsmoothing, and image warping. Motion estimation algorithms often use a similarity trans-\nform to handle camera translations, rotations, and zooming. The tricky part is getting these\nalgorithms to lock onto the background motion, which is a result of the camera movement,\nwithout getting distracted by independent moving foreground objects. Motion smoothing al-\ngorithms recover the low-frequency (slowly varying) part of the motion and then estimate\nthe high-frequency shake component that needs to be removed. Finally, image warping algo-\nrithms apply the high-frequency correction to render the original frames as if the camera had\nundergone only the smooth motion.\nThe resulting stabilization algorithms can greatly improve the appearance of shaky videos\nbut they often still contain visual artifacts. For example, image warping can result in missing\nborders around the image, which must be cropped, ﬁlled using information from other frames,\nor hallucinated using inpainting techniques (Section 10.5.1). Furthermore, video frames cap-\ntured during fast motion are often blurry. Their appearance can be improved either using\ndeblurring techniques (Section 10.3) or stealing sharper pixels from other frames with less\nmotion or better focus (Matsushita, Ofek, Ge et al. 2006). Exercise 8.3 has you implement\nand test some of these ideas.\nIn situations where the camera is translating a lot in 3D, e.g., when the videographer is\nwalking, an even better approach is to compute a full structure from motion reconstruction\nof the camera motion and 3D scene. A smooth 3D camera path can then be computed and\nthe original video re-rendered using view interpolation with the interpolated 3D point cloud\nserving as the proxy geometry while preserving salient features (Liu, Gleicher, Jin et al.\n2009). If you have access to a camera array instead of a single video camera, you can do even\nbetter using a light ﬁeld rendering approach (Section 13.3) (Smith, Zhang, Jin et al. 2009).\n8.2.2 Learned motion models\nAn alternative to parameterizing the motion ﬁeld with a geometric deformation such as an\nafﬁne transform is to learn a set of basis functions tailored to a particular application (Black,\nYacoob, Jepson et al. 1997). First, a set of dense motion ﬁelds (Section 8.4) is computed from\na set of training videos. Next, singular value decomposition (SVD) is applied to the stack of\nmotion ﬁelds ut(x) to compute the ﬁrst few singular vectors vk(x). Finally, for a new test\nsequence, a novel ﬂow ﬁeld is computed using a coarse-to-ﬁne algorithm that estimates the\n404\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 8.6\nLearned parameterized motion ﬁelds for a walking sequence (Black, Yacoob,\nJepson et al. 1997) c⃝1997 IEEE: (a) learned basis ﬂow ﬁelds; (b) plots of motion coefﬁcients\nover time and corresponding estimated motion ﬁelds.\nunknown coefﬁcient ak in the parameterized ﬂow ﬁeld\nu(x) =\nX\nk\nakvk(x).\n(8.66)\nFigure 8.6a shows a set of basis ﬁelds learned by observing videos of walking motions.\nFigure 8.6b shows the temporal evolution of the basis coefﬁcients as well as a few of the\nrecovered parametric motion ﬁelds. Note that similar ideas can also be applied to feature\ntracks (Torresani, Hertzmann, and Bregler 2008), which is a topic we discuss in more detail\nin Sections 4.1.4 and 12.6.4.\n8.3 Spline-based motion\nWhile parametric motion models are useful in a wide variety of applications (such as video\nstabilization and mapping onto planar surfaces), most image motion is too complicated to be\ncaptured by such low-dimensional models.\nTraditionally, optical ﬂow algorithms (Section 8.4) compute an independent motion esti-\nmate for each pixel, i.e., the number of ﬂow vectors computed is equal to the number of input\npixels. The general optical ﬂow analog to Equation (8.1) can thus be written as\nESSD−OF({ui}) =\nX\ni\n[I1(xi + ui) −I0(xi)]2.\n(8.67)",
  "image_path": "page_425.jpg",
  "pages": [
    424,
    425,
    426
  ]
}