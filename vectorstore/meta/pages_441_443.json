{
  "doc_id": "pages_441_443",
  "text": "8.5 Layered motion\n419\nto a monitor’s actual refresh rate. As with de-interlacing, information from novel in-between\nframes needs to be interpolated from preceding and subsequent frames. The best results can\nbe obtained if an accurate motion estimate can be computed at each unknown pixel’s lo-\ncation. However, in addition to computing the motion, occlusion information is critical to\nprevent colors from being contaminated by moving foreground objects that might obscure a\nparticular pixel in a preceding or subsequent frame.\nIn a little more detail, consider Figure 8.13c and assume that the arrows denote keyframes\nbetween which we wish to interpolate additional images. The orientations of the streaks\nin this ﬁgure encode the velocities of individual pixels. If the same motion estimate u0 is\nobtained at location x0 in image I0 as is obtained at location x0 + u0 in image I1, the ﬂow\nvectors are said to be consistent. This motion estimate can be transferred to location x0 +tu0\nin the image It being generated, where t ∈(0, 1) is the time of interpolation. The ﬁnal color\nvalue at pixel x0 + tu0 can be computed as a linear blend,\nIt(x0 + tu0) = (1 −t)I0(x0) + tI1(x0 + u0).\n(8.72)\nIf, however, the motion vectors are different at corresponding locations, some method must\nbe used to determine which is correct and which image contains colors that are occluded.\nThe actual reasoning is even more subtle than this. One example of such an interpolation\nalgorithm, based on earlier work in depth map interpolation (Shade, Gortler, He et al. 1998;\nZitnick, Kang, Uyttendaele et al. 2004) which is the one used in the ﬂow evaluation paper of\nBaker, Black, Lewis et al. (2007); Baker, Scharstein, Lewis et al. (2009). An even higher-\nquality frame interpolation algorithm, which uses gradient-based reconstruction, is presented\nby Mahajan, Huang, Matusik et al. (2009).\n8.5.2 Transparent layers and reﬂections\nA special case of layered motion that occurs quite often is transparent motion, which is usu-\nally caused by reﬂections seen in windows and picture frames (Figures 8.17 and 8.18).\nSome of the early work in this area handles transparent motion by either just estimating\nthe component motions (Shizawa and Mase 1991; Bergen, Burt, Hingorani et al. 1992; Darrell\nand Simoncelli 1993; Irani, Rousso, and Peleg 1994) or by assigning individual pixels to\ncompeting motion layers (Darrell and Pentland 1995; Black and Anandan 1996; Ju, Black,\nand Jepson 1996), which is appropriate for scenes partially seen through a ﬁne occluder\n(e.g., foliage). However, to accurately separate truly transparent layers, a better model for\nmotion due to reﬂections is required. Because of the way that light is both reﬂected from\nand transmitted through a glass surface, the correct model for reﬂections is an additive one,\nwhere each moving layer contributes some intensity to the ﬁnal image (Szeliski, Avidan, and\nAnandan 2000).\n420\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.17 Light reﬂecting off the transparent glass of a picture frame: (a) ﬁrst image from\nthe input sequence; (b) dominant motion layer min-composite; (c) secondary motion residual\nlayer max-composite; (d–e) ﬁnal estimated picture and reﬂection layers The original images\nare from Black and Anandan (1996), while the separated layers are from Szeliski, Avidan,\nand Anandan (2000) c⃝2000 IEEE.\nIf the motions of the individual layers are known, the recovery of the individual layers is\na simple constrained least squares problem, with the individual layer images are constrained\nto be positive. However, this problem can suffer from extended low-frequency ambiguities,\nespecially if either of the layers lacks dark (black) pixels or the motion is uni-directional. In\ntheir paper, Szeliski, Avidan, and Anandan (2000) show that the simultaneous estimation of\nthe motions and layer values can be obtained by alternating between robustly computing the\nmotion layers and then making conservative (upper- or lower-bound) estimates of the layer\nintensities. The ﬁnal motion and layer estimates can then be polished using gradient descent\non a joint constrained least squares formulation similar to (Baker, Szeliski, and Anandan\n1998), where the over compositing operator is replaced with addition.\nFigures 8.17 and 8.18 show the results of applying these techniques to two different pic-\nture frames with reﬂections. Notice how, in the second sequence, the amount of reﬂected light\nis quite low compared to the transmitted light (the picture of the girl) and yet the algorithm is\nstill able to recover both layers.\nUnfortunately, the simple parametric motion models used in (Szeliski, Avidan, and Anan-\ndan 2000) are only valid for planar reﬂectors and scenes with shallow depth. The extension of\nthese techniques to curved reﬂectors and scenes with signiﬁcant depth has also been studied\n8.6 Additional reading\n421\nFigure 8.18 Transparent motion separation (Szeliski, Avidan, and Anandan 2000) c⃝2000\nIEEE: (a) ﬁrst image from input sequence; (b) dominant motion layer min-composite; (c) sec-\nondary motion residual layer max-composite; (d–e) ﬁnal estimated picture and reﬂection lay-\ners. Note that the reﬂected layers in (c) and (e) are doubled in intensity to better show their\nstructure.\n(Swaminathan, Kang, Szeliski et al. 2002; Criminisi, Kang, Swaminathan et al. 2005), as has\nthe extension to scenes with more complex 3D depth (Tsin, Kang, and Szeliski 2006).\n8.6 Additional reading\nSome of the earliest algorithms for motion estimation were developed for motion-compen-\nsated video coding (Netravali and Robbins 1979) and such techniques continue to be used\nin modern coding standards such as MPEG, H.263, and H.264 (Le Gall 1991; Richardson\n2003).14 In computer vision, this ﬁeld was originally called image sequence analysis (Huang\n1981). Some of the early seminal papers include the variational approaches developed by\nHorn and Schunck (1981) and Nagel and Enkelmann (1986), and the patch-based translational\nalignment technique developed by Lucas and Kanade (1981). Hierarchical (coarse-to-ﬁne)\nversions of such algorithms were developed by Quam (1984), Anandan (1989), and Bergen,\nAnandan, Hanna et al. (1992), although they have also long been used in motion estimation\nfor video coding.\nTranslational motion models were generalized to afﬁne motion by Rehg and Witkin (1991),\nFuh and Maragos (1991), and Bergen, Anandan, Hanna et al. (1992) and to quadric refer-\nence surfaces by Shashua and Toelg (1997) and Shashua and Wexler (2001)—see Baker and\nMatthews (2004) for a nice review. Such parametric motion estimation algorithms have found\nwidespread application in video summarization (Teodosio and Bender 1993; Irani and Anan-\ndan 1998), video stabilization (Hansen, Anandan, Dana et al. 1994; Srinivasan, Chellappa,\n14 http://www.itu.int/rec/T-REC-H.264.",
  "image_path": "page_442.jpg",
  "pages": [
    441,
    442,
    443
  ]
}