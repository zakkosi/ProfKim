{
  "doc_id": "pages_253_255",
  "text": "4.1 Points and patches\n231\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 3708\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(b)\n(c)\nFigure 4.25\nPerformance of the feature descriptors evaluated by Mikolajczyk and Schmid\n(2005) c⃝2005 IEEE, shown for three matching strategies: (a) ﬁxed threshold; (b) nearest\nneighbor; (c) nearest neighbor distance ratio (NNDR). Note how the ordering of the algo-\nrithms does not change that much, but the overall performance varies signiﬁcantly between\nthe different matching strategies.\n232\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.26 The three Haar wavelet coefﬁcients used for hashing the MOPS descriptor de-\nvised by Brown, Szeliski, and Winder (2005) are computed by summing each 8×8 normalized\npatch over the light and dark gray regions and taking their difference.\nEfﬁcient matching\nOnce we have decided on a matching strategy, we still need to search efﬁciently for poten-\ntial candidates. The simplest way to ﬁnd all corresponding feature points is to compare all\nfeatures against all other features in each pair of potentially matching images. Unfortunately,\nthis is quadratic in the number of extracted features, which makes it impractical for most\napplications.\nA better approach is to devise an indexing structure, such as a multi-dimensional search\ntree or a hash table, to rapidly search for features near a given feature. Such indexing struc-\ntures can either be built for each image independently (which is useful if we want to only\nconsider certain potential matches, e.g., searching for a particular object) or globally for all\nthe images in a given database, which can potentially be faster, since it removes the need to it-\nerate over each image. For extremely large databases (millions of images or more), even more\nefﬁcient structures based on ideas from document retrieval (e.g., vocabulary trees, (Nist´er and\nStew´enius 2006)) can be used (Section 14.3.2).\nOne of the simpler techniques to implement is multi-dimensional hashing, which maps\ndescriptors into ﬁxed size buckets based on some function applied to each descriptor vector.\nAt matching time, each new feature is hashed into a bucket, and a search of nearby buckets\nis used to return potential candidates, which can then be sorted or graded to determine which\nare valid matches.\nA simple example of hashing is the Haar wavelets used by Brown, Szeliski, and Winder\n(2005) in their MOPS paper. During the matching structure construction, each 8 × 8 scaled,\noriented, and normalized MOPS patch is converted into a three-element index by perform-\ning sums over different quadrants of the patch (Figure 4.26). The resulting three values are\nnormalized by their expected standard deviations and then mapped to the two (of b = 10)\nnearest 1D bins. The three-dimensional indices formed by concatenating the three quantized\nvalues are used to index the 23 = 8 bins where the feature is stored (added). At query time,\nonly the primary (closest) indices are used, so only a single three-dimensional bin needs to\n4.1 Points and patches\n233\n(a)\n(b)\nFigure 4.27 K-d tree and best bin ﬁrst (BBF) search (Beis and Lowe 1999) c⃝1999 IEEE:\n(a) The spatial arrangement of the axis-aligned cutting planes is shown using dashed lines.\nIndividual data points are shown as small diamonds. (b) The same subdivision can be repre-\nsented as a tree, where each interior node represents an axis-aligned cutting plane (e.g., the\ntop node cuts along dimension d1 at value .34) and each leaf node is a data point. During a\nBBF search, a query point (denoted by “+”) ﬁrst looks in its containing bin (D) and then in\nits nearest adjacent bin (B), rather than its closest neighbor in the tree (C).\nbe examined. The coefﬁcients in the bin can then be used to select k approximate nearest\nneighbors for further processing (such as computing the NNDR).\nA more complex, but more widely applicable, version of hashing is called locality sen-\nsitive hashing, which uses unions of independently computed hashing functions to index\nthe features (Gionis, Indyk, and Motwani 1999; Shakhnarovich, Darrell, and Indyk 2006).\nShakhnarovich, Viola, and Darrell (2003) extend this technique to be more sensitive to the\ndistribution of points in parameter space, which they call parameter-sensitive hashing. Even\nmore recent work converts high-dimensional descriptor vectors into binary codes that can be\ncompared using Hamming distances (Torralba, Weiss, and Fergus 2008; Weiss, Torralba, and\nFergus 2008) or that can accommodate arbitrary kernel functions (Kulis and Grauman 2009;\nRaginsky and Lazebnik 2009).\nAnother widely used class of indexing structures are multi-dimensional search trees. The\nbest known of these are k-d trees, also often written as kd-trees, which divide the multi-\ndimensional feature space along alternating axis-aligned hyperplanes, choosing the threshold\nalong each axis so as to maximize some criterion, such as the search tree balance (Samet\n1989). Figure 4.27 shows an example of a two-dimensional k-d tree. Here, eight different data\npoints A–H are shown as small diamonds arranged on a two-dimensional plane. The k-d tree",
  "image_path": "page_254.jpg",
  "pages": [
    253,
    254,
    255
  ]
}