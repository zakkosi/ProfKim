{
  "doc_id": "pages_069_071",
  "text": "2.1 Geometric primitives and transformations\n47\n(a) 3D view\n(b) orthography\n(c) scaled orthography\n(d) para-perspective\n(e) perspective\n(f) object-centered\nFigure 2.7 Commonly used projection models: (a) 3D view of world, (b) orthography, (c)\nscaled orthography, (d) para-perspective, (e) perspective, (f) object-centered. Each diagram\nshows a top-down view of the projection. Note how parallel lines on the ground plane and\nbox sides remain parallel in the non-perspective projections.\n48\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ni.e., we drop the z component but keep the w component. Orthography is an approximate\nmodel for long focal length (telephoto) lenses and objects whose depth is shallow relative\nto their distance to the camera (Sawhney and Hanson 1991). It is exact only for telecentric\nlenses (Baker and Nayar 1999, 2001).\nIn practice, world coordinates (which may measure dimensions in meters) need to be\nscaled to ﬁt onto an image sensor (physically measured in millimeters, but ultimately mea-\nsured in pixels). For this reason, scaled orthography is actually more commonly used,\nx = [sI2×2|0] p.\n(2.48)\nThis model is equivalent to ﬁrst projecting the world points onto a local fronto-parallel image\nplane and then scaling this image using regular perspective projection. The scaling can be the\nsame for all parts of the scene (Figure 2.7b) or it can be different for objects that are being\nmodeled independently (Figure 2.7c). More importantly, the scaling can vary from frame to\nframe when estimating structure from motion, which can better model the scale change that\noccurs as an object approaches the camera.\nScaled orthography is a popular model for reconstructing the 3D shape of objects far away\nfrom the camera, since it greatly simpliﬁes certain computations. For example, pose (camera\norientation) can be estimated using simple least squares (Section 6.2.1). Under orthography,\nstructure and motion can simultaneously be estimated using factorization (singular value de-\ncomposition), as discussed in Section 7.3 (Tomasi and Kanade 1992).\nA closely related projection model is para-perspective (Aloimonos 1990; Poelman and\nKanade 1997). In this model, object points are again ﬁrst projected onto a local reference\nparallel to the image plane. However, rather than being projected orthogonally to this plane,\nthey are projected parallel to the line of sight to the object center (Figure 2.7d). This is\nfollowed by the usual projection onto the ﬁnal image plane, which again amounts to a scaling.\nThe combination of these two projections is therefore afﬁne and can be written as\n˜x =\n\n\na00\na01\na02\na03\na10\na11\na12\na13\n0\n0\n0\n1\n\n˜p.\n(2.49)\nNote how parallel lines in 3D remain parallel after projection in Figure 2.7b–d. Para-perspective\nprovides a more accurate projection model than scaled orthography, without incurring the\nadded complexity of per-pixel perspective division, which invalidates traditional factoriza-\ntion methods (Poelman and Kanade 1997).\nPerspective\nThe most commonly used projection in computer graphics and computer vision is true 3D\nperspective (Figure 2.7e). Here, points are projected onto the image plane by dividing them\n2.1 Geometric primitives and transformations\n49\nby their z component. Using inhomogeneous coordinates, this can be written as\n¯x = Pz(p) =\n\n\nx/z\ny/z\n1\n\n.\n(2.50)\nIn homogeneous coordinates, the projection has a simple linear form,\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n˜p,\n(2.51)\ni.e., we drop the w component of p. Thus, after projection, it is not possible to recover the\ndistance of the 3D point from the image, which makes sense for a 2D imaging sensor.\nA form often seen in computer graphics systems is a two-step projection that ﬁrst projects\n3D coordinates into normalized device coordinates in the range (x, y, z) ∈[−1, −1] ×\n[−1, 1] × [0, 1], and then rescales these coordinates to integer pixel coordinates using a view-\nport transformation (Watt 1995; OpenGL-ARB 1997).\nThe (initial) perspective projection\nis then represented using a 4 × 4 matrix\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n−zfar/zrange\nznearzfar/zrange\n0\n0\n1\n0\n\n˜p,\n(2.52)\nwhere znear and zfar are the near and far z clipping planes and zrange = zfar −znear. Note\nthat the ﬁrst two rows are actually scaled by the focal length and the aspect ratio so that\nvisible rays are mapped to (x, y, z) ∈[−1, −1]2. The reason for keeping the third row, rather\nthan dropping it, is that visibility operations, such as z-buffering, require a depth for every\ngraphical element that is being rendered.\nIf we set znear = 1, zfar →∞, and switch the sign of the third row, the third element\nof the normalized screen vector becomes the inverse depth, i.e., the disparity (Okutomi and\nKanade 1993). This can be quite convenient in many cases since, for cameras moving around\noutdoors, the inverse depth to the camera is often a more well-conditioned parameterization\nthan direct 3D distance.\nWhile a regular 2D image sensor has no way of measuring distance to a surface point,\nrange sensors (Section 12.2) and stereo matching algorithms (Chapter 11) can compute such\nvalues. It is then convenient to be able to map from a sensor-based depth or disparity value d\ndirectly back to a 3D location using the inverse of a 4 × 4 matrix (Section 2.1.5). We can do\nthis if we represent perspective projection using a full-rank 4 × 4 matrix, as in (2.64).",
  "image_path": "page_070.jpg",
  "pages": [
    69,
    70,
    71
  ]
}