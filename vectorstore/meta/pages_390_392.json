{
  "doc_id": "pages_390_392",
  "text": "368\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7.4.2 Application: Match move and augmented reality\nOne of the neatest applications of structure from motion is to estimate the 3D motion of a\nvideo or ﬁlm camera, along with the geometry of a 3D scene, in order to superimpose 3D\ngraphics or computer-generated images (CGI) on the scene. In the visual effects industry,\nthis is known as the match move problem (Roble 1999), since the motion of the synthetic 3D\ncamera used to render the graphics must be matched to that of the real-world camera. For\nvery small motions, or motions involving pure camera rotations, one or two tracked points can\nsufﬁce to compute the necessary visual motion. For planar surfaces moving in 3D, four points\nare needed to compute the homography, which can then be used to insert planar overlays, e.g.,\nto replace the contents of advertising billboards during sporting events.\nThe general version of this problem requires the estimation of the full 3D camera pose\nalong with the focal length (zoom) of the lens and potentially its radial distortion parameters\n(Roble 1999). When the 3D structure of the scene is known ahead of time, pose estima-\ntion techniques such as view correlation (Bogart 1991) or through-the-lens camera control\n(Gleicher and Witkin 1992) can be used, as described in Section 6.2.3.\nFor more complex scenes, it is usually preferable to recover the 3D structure simultane-\nously with the camera motion using structure-from-motion techniques. The trick with using\nsuch techniques is that in order to prevent any visible jitter between the synthetic graph-\nics and the actual scene, features must be tracked to very high accuracy and ample feature\ntracks must be available in the vicinity of the insertion location. Some of today’s best known\nmatch move software packages, such as the boujou package from 2d3,15 which won an Emmy\naward in 2002, originated in structure-from-motion research in the computer vision commu-\nnity (Fitzgibbon and Zisserman 1998).\nClosely related to the match move problem is robotics navigation, where a robot must es-\ntimate its location relative to its environment, while simultaneously avoiding any dangerous\nobstacles. This problem is often known as simultaneous localization and mapping (SLAM)\n(Thrun, Burgard, and Fox 2005) or visual odometry (Levin and Szeliski 2004; Nist´er, Nar-\noditsky, and Bergen 2006; Maimone, Cheng, and Matthies 2007). Early versions of such\nalgorithms used range-sensing techniques, such as ultrasound, laser range ﬁnders, or stereo\nmatching, to estimate local 3D geometry, which could then be fused into a 3D model. Newer\ntechniques can perform the same task based purely on visual feature tracking, sometimes not\neven requiring a stereo camera rig (Davison, Reid, Molton et al. 2007).\nAnother closely related application is augmented reality, where 3D objects are inserted\ninto a video feed in real time, often to annotate or help users understand a scene (Azuma,\nBaillot, Behringer et al. 2001). While traditional systems require prior knowledge about the\nscene or object being visually tracked (Rosten and Drummond 2005), newer systems can\n15 http://www.2d3.com/.\n7.4 Bundle adjustment\n369\n(a)\n(b)\nFigure 7.10\n3D augmented reality: (a) Darth Vader and a horde of Ewoks battle it out\non a table-top recovered using real-time, keyframe-based structure from motion (Klein and\nMurray 2007) c⃝2007 IEEE; (b) a virtual teapot is ﬁxed to the top of a real-world coffee cup,\nwhose pose is re-recognized at each time frame (Gordon and Lowe 2006) c⃝2007 Springer.\nsimultaneously build up a model of the 3D environment and then track it, so that graphics can\nbe superimposed.\nKlein and Murray (2007) describe a parallel tracking and mapping (PTAM) system,\nwhich simultaneously applies full bundle adjustment to keyframes selected from a video\nstream, while performing robust real-time pose estimation on intermediate frames.\nFig-\nure 7.10a shows an example of their system in use. Once an initial 3D scene has been\nreconstructed, a dominant plane is estimated (in this case, the table-top) and 3D animated\ncharacters are virtually inserted. Klein and Murray (2008) extend their previous system to\nhandle even faster camera motion by adding edge features, which can still be detected even\nwhen interest points become too blurred. They also use a direct (intensity-based) rotation\nestimation algorithm for even faster motions.\nInstead of modeling the whole scene as one rigid reference frame, Gordon and Lowe\n(2006) ﬁrst build a 3D model of an individual object using feature matching and structure\nfrom motion. Once the system has been initialized, for every new frame, they ﬁnd the object\nand its pose using a 3D instance recognition algorithm, and then superimpose a graphical\nobject onto that model, as shown in Figure 7.10b.\nWhile reliably tracking such objects and environments is now a well-solved problem,\ndetermining which pixels should be occluded by foreground scene elements still remains an\nopen problem (Chuang, Agarwala, Curless et al. 2002; Wang and Cohen 2007a).\n370\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7.4.3 Uncertainty and ambiguities\nBecause structure from motion involves the estimation of so many highly coupled parameters,\noften with no known “ground truth” components, the estimates produced by structure from\nmotion algorithms can often exhibit large amounts of uncertainty (Szeliski and Kang 1997).\nAn example of this is the classic bas-relief ambiguity, which makes it hard to simultaneously\nestimate the 3D depth of a scene and the amount of camera motion (Oliensis 2005).16\nAs mentioned before, a unique coordinate frame and scale for a reconstructed scene can-\nnot be recovered from monocular visual measurements alone. (When a stereo rig is used,\nthe scale can be recovered if we know the distance (baseline) between the cameras.) This\nseven-degree-of-freedom gauge ambiguity makes it tricky to compute the covariance matrix\nassociated with a 3D reconstruction (Triggs, McLauchlan, Hartley et al. 1999; Kanatani and\nMorris 2001). A simple way to compute a covariance matrix that ignores the gauge freedom\n(indeterminacy) is to throw away the seven smallest eigenvalues of the information matrix (in-\nverse covariance), whose values are equivalent to the problem Hessian A up to noise scaling\n(see Section 6.1.4 and Appendix B.6). After we do this, the resulting matrix can be inverted\nto obtain an estimate of the parameter covariance.\nSzeliski and Kang (1997) use this approach to visualize the largest directions of variation\nin typical structure from motion problems. Not surprisingly, they ﬁnd that (ignoring the gauge\nfreedoms), the greatest uncertainties for problems such as observing an object from a small\nnumber of nearby viewpoints are in the depths of the 3D structure relative to the extent of the\ncamera motion.17\nIt is also possible to estimate local or marginal uncertainties for individual parameters,\nwhich corresponds simply to taking block sub-matrices from the full covariance matrix. Un-\nder certain conditions, such as when the camera poses are relatively certain compared to 3D\npoint locations, such uncertainty estimates can be meaningful. However, in many cases, indi-\nvidual uncertainty measures can mask the extent to which reconstruction errors are correlated,\nwhich is why looking at the ﬁrst few modes of greatest joint variation can be helpful.\nThe other way in which gauge ambiguities affect structure from motion and, in particular,\nbundle adjustment is that they make the system Hessian matrix A rank-deﬁcient and hence\nimpossible to invert. A number of techniques have been proposed to mitigate this problem\n(Triggs, McLauchlan, Hartley et al. 1999; Bartoli 2003). In practice, however, it appears that\nsimply adding a small amount of the Hessian diagonal λdiag(A) to the Hessian A itself, as is\ndone in the Levenberg–Marquardt non-linear least squares algorithm (Appendix A.3), usually\n16 Bas-relief refers to a kind of sculpture in which objects, often on ornamental friezes, are sculpted with less\ndepth than they actually occupy. When lit from above by sunlight, they appear to have true 3D depth because of the\nambiguity between relative depth and the angle of the illuminant (Section 12.1.1).\n17 A good way to minimize the amount of such ambiguities is to use wide ﬁeld of view cameras (Antone and\nTeller 2002; Levin and Szeliski 2006).",
  "image_path": "page_391.jpg",
  "pages": [
    390,
    391,
    392
  ]
}