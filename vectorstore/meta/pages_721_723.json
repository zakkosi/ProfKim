{
  "doc_id": "pages_721_723",
  "text": "14.4 Category recognition\n699\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nlevel 2\nlevel 1\nlevel 0\n\u0001 1/4\n\u0001 1/4\n\u0001 1/2\n+\n+\n+\n(a)\n(b)\nFigure 14.37\nComparing collections of feature vectors using pyramid matching. (a) The\nfeature-space pyramid match kernel (Grauman and Darrell 2007b) constructs a pyramid in\nhigh-dimensional feature space and uses it to compute distances (and implicit correspon-\ndences) between sets of feature vectors. (b) Spatial pyramid matching (Lazebnik, Schmid,\nand Ponce 2006) c⃝2006 IEEE divides the image into a pyramid of pooling regions and\ncomputes separate visual word histograms (distributions) inside each spatial bin.\nInstead of quantizing feature vectors to visual words, Grauman and Darrell (2007b) de-\nvelop a technique for directly computing an approximate distance between two variably sized\ncollections of feature vectors. Their approach is to bin the feature vectors into a multi-\nresolution pyramid deﬁned in feature space (Figure 14.37a) and count the number of features\nthat land in corresponding bins Bil and B′\nil (Figure 14.38a–c). The distance between the two\nsets of feature vectors (which can be thought of as points in a high-dimensional space) is\ncomputed using histogram intersection between corresponding bins\nCl =\nX\ni\nmin(Bil, B′\nil)\n(14.40)\n(Figure 14.38d). These per-level counts are then summed up in a weighted fashion\nD∆=\nX\nl\nwlNl\nwith\nNl = Cl −Cl−1\nand\nwl =\n1\nd2l\n(14.41)\n(Figure 14.38e), which discounts matches already found at ﬁner levels while weighting ﬁner\nmatches more heavily. (d is the dimension of the embedding space, i.e., the length of the\nfeature vectors.) In follow-on work, Grauman and Darrell (2007a) show how an explicit\nconstruction of the pyramid can be avoided using hashing techniques.\nInspired by this work, Lazebnik, Schmid, and Ponce (2006) show how a similar idea\ncan be employed to augment bags of keypoints with loose notions of 2D spatial location\n700\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.38\nA one-dimensional illustration of comparing collections of feature vectors\nusing the pyramid match kernel (Grauman and Darrell 2007b): (a) distribution of feature\nvectors (point sets) into the pyramidal bins; (b–c) histogram of point counts in bins Bil and\nB′\nil for the two images; (d) histogram intersections (minimum values); (e) per-level similarity\nscores, which are weighted and summed to form the ﬁnal distance/similarity metric.\nanalogous to the pooling performed by SIFT (Lowe 2004) and “gist” (Torralba, Murphy,\nFreeman et al. 2003). In their work, they extract afﬁne region descriptors (Lazebnik, Schmid,\nand Ponce 2005) and quantize them into visual words. (Based on previous results by Fei-Fei\nand Perona (2005), the feature descriptors are extracted densely (on a regular grid) over the\nimage, which can be helpful in describing textureless regions such as the sky.) They then form\na spatial pyramid of bins containing word counts (histograms), as shown in Figure 14.37b, and\nuse a similar pyramid match kernel to combine histogram intersection counts in a hierarchical\nfashion.\nThe debate about whether to use quantized feature descriptors or continuous descriptors\nand also whether to use sparse or dense features continues to this day. Boiman, Shechtman,\nand Irani (2008) show that if query images are compared to all the features representing a\ngiven class, rather than just each class image individually, nearest-neighbor matching fol-\nlowed by a na¨ıve Bayes classiﬁer outperforms quantized visual words (Figure 14.39). In-\nstead of using generic feature detectors and descriptors, some authors have been investigat-\ning learning class-speciﬁc features (Ferencz, Learned-Miller, and Malik 2008), often using\nrandomized forests (Philbin, Chum, Isard et al. 2007; Moosmann, Nowak, and Jurie 2008;\nShotton, Johnson, and Cipolla 2008) or combining the feature generation and image classi-\n14.4 Category recognition\n701\nFigure 14.39\n“Image-to-Image” vs. “Image-to-Class” distance comparison (Boiman,\nShechtman, and Irani 2008) c⃝2008 IEEE. The query image on the upper left may not match\nthe feature distribution of any of the database images in the bottom row. However, if each\nfeature in the query is matched to its closest analog in all the class images, a good match can\nbe found.\nﬁcation stages (Yang, Jin, Sukthankar et al. 2008). Others, such as Serre, Wolf, and Poggio\n(2005) and Mutch and Lowe (2008) use hierarchies of dense feature transforms inspired by\nbiological (visual cortical) processing combined with SVMs for ﬁnal classiﬁcation.\n14.4.2 Part-based models\nRecognizing an object by ﬁnding its constituent parts and measuring their geometric rela-\ntionships is one of the oldest approaches to object recognition (Fischler and Elschlager 1973;\nKanade 1977; Yuille 1991). We have already seen examples of part-based approaches being\nused for face recognition (Figure 14.18) (Moghaddam and Pentland 1997; Heisele, Ho, Wu\net al. 2003; Heisele, Serre, and Poggio 2007) and pedestrian detection (Figure 14.9) (Felzen-\nszwalb, McAllester, and Ramanan 2008).\nIn this section, we look more closely at some of the central issues in part-based recog-\nnition, namely, the representation of geometric relationships, the representation of individ-\nual parts, and algorithms for learning such descriptions and recognizing them at run time.\nMore details on part-based models for recognition can be found in the course notes of Fergus\n(2007b, 2009).\nThe earliest approaches to representing geometric relationships were dubbed pictorial\nstructures by Fischler and Elschlager (1973) and consisted of spring-like connections between\ndifferent feature locations (Figure 14.1a). To ﬁt a pictorial structure to an image, an energy",
  "image_path": "page_722.jpg",
  "pages": [
    721,
    722,
    723
  ]
}