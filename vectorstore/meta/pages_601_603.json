{
  "doc_id": "pages_601_603",
  "text": "12 3D reconstruction\n579\nAs we saw in the previous chapter, a variety of stereo matching techniques have been de-\nveloped to reconstruct high quality 3D models from two or more images. However, stereo\nis just one of the many potential cues that can be used to infer shape from images. In this\nchapter, we investigate a number of such techniques, which include not only visual cues such\nas shading and focus, but also techniques for merging multiple range or depth images into 3D\nmodels, as well as techniques for reconstructing specialized models, such as heads, bodies,\nor architecture.\nAmong the various cues that can be used to infer shape, the shading on a surface (Fig-\nure 12.1a) can provide a lot of information about local surface orientations and hence overall\nsurface shape (Section 12.1.1). This approach becomes even more powerful when lights\nshining from different directions can be turned on and off separately (photometric stereo).\nTexture gradients (Figure 12.1b), i.e., the foreshortening of regular patterns as the surface\nslants or bends away from the camera, can provide similar cues on local surface orientation\n(Section 12.1.2). Focus is another powerful cue to scene depth, especially when two or more\nimages with different focus settings are used (Section 12.1.3).\n3D shape can also be estimated using active illumination techniques such as light stripes\n(Figure 12.1d) or time of ﬂight range ﬁnders (Section 12.2). The partial surface models\nobtained using such techniques (or passive image-based stereo) can then be merged into more\ncoherent 3D surface models (Figure 12.1e), as discussed in Section 12.2.1. Such techniques\nhave been used to construct highly detailed and accurate models of cultural heritage such as\nhistoric sites (Section 12.2.2). The resulting surface models can then be simpliﬁed to support\nviewing at different resolutions and streaming across the Web (Section 12.3.2). An alternative\nto working with continuous surfaces is to represent 3D surfaces as dense collections of 3D\noriented points (Section 12.4) or as volumetric primitives (Section 12.5).\n3D modeling can be more efﬁcient and effective if we know something about the objects\nwe are trying to reconstruct. In Section 12.6, we look at three specialized but commonly\noccurring examples, namely architecture (Figure 12.1g), heads and faces (Figure 12.1h), and\nwhole bodies (Figure 12.1i). In addition to modeling people, we also discuss techniques for\ntracking them.\nThe last stage of shape and appearance modeling is to extract some textures to paint onto\nour 3D models (Section 12.7). Some techniques go beyond this and actually estimate full\nBRDFs (Section 12.7.1).\nBecause there exists such a large variety of techniques to perform 3D modeling, this\nchapter does not go into detail on any one of these. Readers are encouraged to ﬁnd more\ninformation in the cited references or more specialized publications and conferences de-\nvoted to these topics, e.g., the International Symposium on 3D Data Processing, Visualiza-\ntion, and Transmission (3DPVT), the International Conference on 3D Digital Imaging and\nModeling (3DIM), the International Conference on Automatic Face and Gesture Recognition\n580\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(FG), the IEEE Workshop on Analysis and Modeling of Faces and Gestures, and the Interna-\ntional Workshop on Tracking Humans for the Evaluation of their Motion in Image Sequences\n(THEMIS).\n12.1 Shape from X\nIn addition to binocular disparity, shading, texture, and focus all play a role in how we per-\nceive shape. The study of how shape can be inferred from such cues is sometimes called\nshape from X, since the individual instances are called shape from shading, shape from tex-\nture, and shape from focus.1 In this section, we look at these three cues and how they can\nbe used to reconstruct 3D geometry. A good overview of all these topics can be found in the\ncollection of papers on physics-based shape inference edited by Wolff, Shafer, and Healey\n(1992b).\n12.1.1 Shape from shading and photometric stereo\nWhen you look at images of smooth shaded objects, such as the ones shown in Figure 12.2,\nyou can clearly see the shape of the object from just the shading variation. How is this\npossible? The answer is that as the surface normal changes across the object, the apparent\nbrightness changes as a function of the angle between the local surface orientation and the\nincident illumination, as shown in Figure 2.15 (Section 2.2.2).\nThe problem of recovering the shape of a surface from this intensity variation is known as\nshape from shading and is one of the classic problems in computer vision (Horn 1975). The\ncollection of papers edited by Horn and Brooks (1989) is a great source of information on\nthis topic, especially the chapter on variational approaches. The survey by Zhang, Tsai, Cryer\net al. (1999) not only reviews more recent techniques, but also provides some comparative\nresults.\nMost shape from shading algorithms assume that the surface under consideration is of a\nuniform albedo and reﬂectance, and that the light source directions are either known or can\nbe calibrated by the use of a reference object. Under the assumptions of distant light sources\nand observer, the variation in intensity (irradiance equation) become purely a function of the\nlocal surface orientation,\nI(x, y) = R(p(x, y), q(x, y)),\n(12.1)\nwhere (p, q) = (zx, zy) are the depth map derivatives and R(p, q) is called the reﬂectance\nmap. For example, a diffuse (Lambertian) surface has a reﬂectance map that is the (non-\n1 We have already seen examples of shape from stereo, shape from proﬁles, and shape from silhouettes in Chap-\nter 11.\n12.1 Shape from X\n581\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 12.2 Synthetic shape from shading (Zhang, Tsai, Cryer et al. 1999) c⃝1999 IEEE:\nshaded images, (a–b) with light from in front (0, 0, 1) and (c–d) with light the front right\n(1, 0, 1); (e–f) corresponding shape from shading reconstructions using the technique of Tsai\nand Shah (1994).\nnegative) dot product (2.88) between the surface normal ˆn = (p, q, 1)/\np\n1 + p2 + q2 and\nthe light source direction v = (vx, vy, vz),\nR(p, q) = max\n \n0, ρpvx + qvy + vz\np\n1 + p2 + q2\n!\n,\n(12.2)\nwhere ρ is the surface reﬂectance factor (albedo).\nIn principle, Equations (12.1–12.2) can be used to estimate (p, q) using non-linear least\nsquares or some other method. Unfortunately, unless additional constraints are imposed, there\nare more unknowns per pixel (p, q) than there are measurements (I). One commonly used\nconstraint is the smoothness constraint,\nEs =\nZ\np2\nx + p2\ny + q2\nx + q2\ny dx dy =\nZ\n∥∇p∥2 + ∥∇q∥2 dx dy,\n(12.3)\nwhich we already saw in Section 3.7.1 (3.94). The other is the integrability constraint,\nEi =\nZ\n(py −qx)2 dx dy,\n(12.4)",
  "image_path": "page_602.jpg",
  "pages": [
    601,
    602,
    603
  ]
}