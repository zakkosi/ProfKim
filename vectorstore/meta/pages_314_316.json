{
  "doc_id": "pages_314_316",
  "text": "292\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhere\nNk =\nX\ni\nzik.\n(5.33)\nis an estimate of the number of sample points assigned to each cluster.\nBishop (2006) has a wonderful exposition of both mixture of Gaussians estimation and the\nmore general topic of expectation maximization.\nIn the context of image segmentation, Ma, Derksen, Hong et al. (2007) present a nice\nreview of segmentation using mixtures of Gaussians and develop their own extension based\non Minimum Description Length (MDL) coding, which they show produces good results on\nthe Berkeley segmentation database.\n5.3.2 Mean shift\nWhile k-means and mixtures of Gaussians use a parametric form to model the probability den-\nsity function being segmented, mean shift implicitly models this distribution using a smooth\ncontinuous non-parametric model. The key to mean shift is a technique for efﬁciently ﬁnd-\ning peaks in this high-dimensional data distribution without ever computing the complete\nfunction explicitly (Fukunaga and Hostetler 1975; Cheng 1995; Comaniciu and Meer 2002).\nConsider once again the data points shown in Figure 5.16c, which can be thought of as\nhaving been drawn from some probability density function. If we could compute this density\nfunction, as visualized in Figure 5.16e, we could ﬁnd its major peaks (modes) and identify\nregions of the input space that climb to the same peak as being part of the same region. This\nis the inverse of the watershed algorithm described in Section 5.2.1, which climbs downhill\nto ﬁnd basins of attraction.\nThe ﬁrst question, then, is how to estimate the density function given a sparse set of\nsamples. One of the simplest approaches is to just smooth the data, e.g., by convolving it\nwith a ﬁxed kernel of width h,\nf(x) =\nX\ni\nK(x −xi) =\nX\ni\nk\n\u0012∥x −xi∥2\nh2\n\u0013\n,\n(5.34)\nwhere xi are the input samples and k(r) is the kernel function (or Parzen window).9 This\napproach is known as kernel density estimation or the Parzen window technique (Duda, Hart,\nand Stork 2001, Section 4.3; Bishop 2006, Section 2.5.1). Once we have computed f(x), as\nshown in Figures 5.16e and 5.17, we can ﬁnd its local maxima using gradient ascent or some\nother optimization technique.\n9 In this simpliﬁed formula, a Euclidean metric is used. We discuss a little later (5.42) how to generalize this\nto non-uniform (scaled or oriented) metrics. Note also that this distribution may not be proper, i.e., integrate to 1.\nSince we are looking for maxima in the density, this does not matter.\n5.3 Mean shift and mode ﬁnding\n293\nx\nf (x)\nxi\nK(x)\nG(x)\nf '(xk)\nxk\nm(xk)\nFigure 5.17 One-dimensional visualization of the kernel density estimate, its derivative, and\na mean shift. The kernel density estimate f(x) is obtained by convolving the sparse set of\ninput samples xi with the kernel function K(x). The derivative of this function, f ′(x), can\nbe obtained by convolving the inputs with the derivative kernel G(x). Estimating the local\ndisplacement vectors around a current estimate xk results in the mean-shift vector m(xk),\nwhich, in a multi-dimensional setting, point in the same direction as the function gradient\n∇f(xk). The red dots indicate local maxima in f(x) to which the mean shifts converge.\nThe problem with this “brute force” approach is that, for higher dimensions, it becomes\ncomputationally prohibitive to evaluate f(x) over the complete search space.10 Instead, mean\nshift uses a variant of what is known in the optimization literature as multiple restart gradient\ndescent. Starting at some guess for a local maximum, yk, which can be a random input data\npoint xi, mean shift computes the gradient of the density estimate f(x) at yk and takes an\nuphill step in that direction (Figure 5.17). The gradient of f(x) is given by\n∇f(x) =\nX\ni\n(xi −x)G(x −xi) =\nX\ni\n(xi −x)g\n\u0012∥x −xi∥2\nh2\n\u0013\n,\n(5.35)\nwhere\ng(r) = −k′(r),\n(5.36)\nand k′(r) is the ﬁrst derivative of k(r). We can re-write the gradient of the density function\nas\n∇f(x) =\n\"X\ni\nG(x −xi)\n#\nm(x),\n(5.37)\nwhere the vector\nm(x) =\nP\ni xiG(x −xi)\nP\ni G(x −xi)\n−x\n(5.38)\nis called the mean shift, since it is the difference between the weighted mean of the neighbors\nxi around x and the current value of x.\n10 Even for one dimension, if the space is extremely sparse, it may be inefﬁcient.\n294\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn the mean-shift procedure, the current estimate of the mode yk at iteration k is replaced\nby its locally weighted mean,\nyk+1 = yk + m(yk) =\nP\ni xiG(yk −xi)\nP\ni G(yk −xi) .\n(5.39)\nComaniciu and Meer (2002) prove that this algorithm converges to a local maximum of f(x)\nunder reasonably weak conditions on the kernel k(r), i.e., that it is monotonically decreasing.\nThis convergence is not guaranteed for regular gradient descent unless appropriate step size\ncontrol is used.\nThe two kernels that Comaniciu and Meer (2002) studied are the Epanechnikov kernel,\nkE(r) = max(0, 1 −r),\n(5.40)\nwhich is a radial generalization of a bilinear kernel, and the Gaussian (normal) kernel,\nkN(r) = exp\n\u0012\n−1\n2r\n\u0013\n.\n(5.41)\nThe corresponding derivative kernels g(r) are a unit ball and another Gaussian, respectively.\nUsing the Epanechnikov kernel converges in a ﬁnite number of steps, while the Gaussian\nkernel has a smoother trajectory (and produces better results), but converges very slowly near\na mode (Exercise 5.5).\nThe simplest way to apply mean shift is to start a separate mean-shift mode estimate\ny at every input point xi and to iterate for a ﬁxed number of steps or until the mean-shift\nmagnitude is below a threshold. A faster approach is to randomly subsample the input points\nxi and to keep track of each point’s temporal evolution. The remaining points can then be\nclassiﬁed based on the nearest evolution path (Comaniciu and Meer 2002). Paris and Durand\n(2007) review a number of other more efﬁcient implementations of mean shift, including their\nown approach, which is based on using an efﬁcient low-resolution estimate of the complete\nmulti-dimensional space of f(x) along with its stationary points.\nThe color-based segmentation shown in Figure 5.16 only looks at pixel colors when deter-\nmining the best clustering. It may therefore cluster together small isolated pixels that happen\nto have the same color, which may not correspond to a semantically meaningful segmentation\nof the image.\nBetter results can usually be obtained by clustering in the joint domain of color and lo-\ncation. In this approach, the spatial coordinates of the image xs = (x, y), which are called\nthe spatial domain, are concatenated with the color values xr, which are known as the range\ndomain, and mean-shift clustering is applied in this ﬁve-dimensional space xj. Since location\nand color may have different scales, the kernels are adjusted accordingly, i.e., we use a kernel\nof the form\nK(xj) = k\n\u0012∥xr∥2\nh2r\n\u0013\nk\n\u0012∥xs∥2\nh2s\n\u0013\n,\n(5.42)",
  "image_path": "page_315.jpg",
  "pages": [
    314,
    315,
    316
  ]
}