{
  "doc_id": "pages_498_500",
  "text": "476\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nestimate the vignetting. Figure 10.5 shows the results of applying each of these algorithms\nto an image with a large amount of vignetting. Exercise 10.3 has you implement some of the\nabove techniques.\n10.1.4 Optical blur (spatial response) estimation\nOne ﬁnal characteristic of imaging systems that you should calibrate is the spatial response\nfunction, which encodes the optical blur that gets convolved with the incoming image to pro-\nduce the point-sampled image. The shape of the convolution kernel, which is also known as\npoint spread function (PSF) or optical transfer function, depends on several factors, including\nlens blur and radial distortion (Section 2.2.3), anti-aliasing ﬁlters in front of the sensor, and\nthe shape and extent of each active pixel area (Section 2.3) (Figure 10.2). A good estimate of\nthis function is required for applications such as multi-image super-resolution and de-blurring\n(Section 10.3).\nIn theory, one could estimate the PSF by simply observing an inﬁnitely small point light\nsource everywhere in the image. Creating an array of samples by drilling through a dark plate\nand backlighting with a very bright light source is difﬁcult in practice.\nA more practical approach is to observe an image composed of long straight lines or\nbars, since these can be ﬁtted to arbitrary precision. Because the location of a horizontal\nor vertical edge can be aliased during acquisition, slightly slanted edges are preferred. The\nproﬁle and locations of such edges can be estimated to sub-pixel precision, which makes it\npossible to estimate the PSF at sub-pixel resolutions (Reichenbach, Park, and Narayanswamy\n1991; Burns and Williams 1999; Williams and Burns 2001; Goesele, Fuchs, and Seidel 2003).\nThe thesis by Murphy (2005) contains a nice survey of all aspects of camera calibration,\nincluding the spatial frequency response (SFR), spatial uniformity, tone reproduction, color\nreproduction, noise, dynamic range, color channel registration, and depth of ﬁeld. It also\nincludes a description of a slant-edge calibration algorithm called sfrmat2.\nThe slant-edge technique can be used to recover a 1D projection of the 2D PSF, e.g.,\nslightly vertical edges are used to recover the horizontal line spread function (LSF) (Williams\n1999). The LSF is then often converted into the Fourier domain and its magnitude plotted as a\none-dimensional modulation transfer function (MTF), which indicates which image frequen-\ncies are lost (blurred) and aliased during the acquisition process (Section 2.3.1). For most\ncomputational photography applications, it is preferable to directly estimate the full 2D PSF,\nsince it can be hard to recover from its projections (Williams 1999).\nFigure 10.7 shows a pattern containing edges at all orientations, which can be used to\ndirectly recover a two-dimensional PSF. First, corners in the pattern are located by extracting\nedges in the sensed image, linking them, and ﬁnding the intersections of the circular arcs.\nNext, the ideal pattern, whose analytic form is known, is warped (using a homography) to\n10.1 Photometric calibration\n477\nFigure 10.7 Calibration pattern with edges equally distributed at all orientations that can be\nused for PSF and radial distortion estimation (Joshi, Szeliski, and Kriegman 2008) c⃝2008\nIEEE. A portion of an actual sensed image is shown in the middle and a close-up of the ideal\npattern is on the right.\nﬁt the central portion of the input image and its intensities are adjusted to ﬁt the ones in\nthe sensed image. If desired, the pattern can be rendered at a higher resolution than the input\nimage, which enables the estimation of the PSF to sub-pixel resolution (Figure 10.8a). Finally\na large linear least squares system is solved to recover the unknown PSF kernel K,\nK = arg min\nK ∥B −D(I ∗K)∥2,\n(10.1)\nwhere B is the sensed (blurred) image, I is the predicted (sharp) image, and D is an optional\ndownsampling operator that matches the resolution of the ideal and sensed images (Joshi,\nSzeliski, and Kriegman 2008). In terms of the notation (3.75) introduced in Section 3.4.3,\nthis could also be written as\nb = arg min\nb\n∥o −D(s ∗b)∥2,\n(10.2)\nwhere o is the observed image, s is the sharp image, and b is the blur kernel.\nIf the process of estimating the PSF is done locally in overlapping patches of the image,\nit can also be used to estimate the radial distortion and chromatic aberration induced by the\nlens (Figure 10.8b). Because the homography mapping the ideal target to the sensed image\nis estimated in the central (undistorted) part of the image, any (per-channel) shifts induced\nby the optics manifest themselves as a displacement in the PSF centers.10 Compensating\nfor these shifts eliminates both the achromatic radial distortion and the inter-channel shifts\nthat result in visible chromatic aberration. The color-dependent blurring caused by chromatic\naberration (Figure 2.21) can also be removed using the de-blurring techniques discussed in\n10 This process confounds the distinction between geometric and photometric calibration. In principle, any ge-\nometric distortion could be modeled by spatially varying displaced PSFs. In practice, it is easier to fold any large\nshifts into the geometric correction component.\n478\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n(a)\n(b)\n(c)\nFigure 10.8 Point spread function estimation using a calibration target (Joshi, Szeliski, and\nKriegman 2008) c⃝2008 IEEE. (a) Sub-pixel PSFs at successively higher resolutions (note\nthe interaction between the square sensing area and the circular lens blur). (b) The radial\ndistortion and chromatic aberration can also be estimated and removed. (c) PSF for a mis-\nfocused (blurred) lens showing some diffraction and vignetting effects in the corners.\nSection 10.3. Figure 10.8b shows how the radial distortion and chromatic aberration manifest\nthemselves as elongated and displaced PSFs, along with the result of removing these effects\nin a region of the calibration target.\nThe local 2D PSF estimation technique can also be used to estimate vignetting. Fig-\nure 10.8c shows how the mechanical vignetting manifests itself as clipping of the PSF in the\ncorners of the image. In order for the overall dimming associated with vignetting to be prop-\nerly captured, the modiﬁed intensities of the ideal pattern need to be extrapolated from the\ncenter, which is best done with a uniformly illuminated target.\nWhen working with RAW Bayer-pattern images, the correct way to estimate the PSF is\nto only evaluate the least squares terms in (10.1) at sensed pixel values, while interpolating\nthe ideal image to all values. For JPEG images, you should linearize your intensities ﬁrst,\ne.g., remove the gamma and any other non-linearities in your estimated radiometric response\nfunction.\nWhat if you have an image that was taken with an uncalibrated camera? Can you still\nrecover the PSF an use it to correct the image? In fact, with a slight modiﬁcation, the previous\nalgorithms still work.\nInstead of assuming a known calibration image, you can detect strong elongated edges\nand ﬁt ideal step edges in such regions (Figure 10.9b), resulting in the sharp image shown",
  "image_path": "page_499.jpg",
  "pages": [
    498,
    499,
    500
  ]
}