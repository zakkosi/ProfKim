{
  "doc_id": "pages_759_761",
  "text": "A.1 Matrix decompositions\n737\n=\n\nu0\n· · ·\nup−1\n\n\n\n\nσ0\n...\nσp−1\n\n\n\n\nvT\n0\n· · ·\nvT\np−1\n\n,\nwhere P = min(M, N). The matrices U and V are orthonormal, i.e., U T U = I and\nV T V = I, and so are their column vectors,\nui · uj = vi · vj = δij.\n(A.2)\nThe singular values are all non-negative and can be ordered in decreasing order\nσ0 ≥σ1 ≥· · · ≥σp−1 ≥0.\n(A.3)\nA geometric intuition for the SVD of a matrix A can be obtained by re-writing A =\nUΣV T in (A.2) as\nAV = UΣ\nor\nAvj = σjuj.\n(A.4)\nThis formula says that the matrix A takes any basis vector vj and maps it to a direction uj\nwith length σj, as shown in Figure A.1\nIf only the ﬁrst r singular values are positive, the matrix A is of rank r and the index p\nin the SVD decomposition (A.2) can be replaced by r. (In other words, we can drop the last\np −r columns of U and V .)\nAn important property of the singular value decomposition of a matrix (also true for\nthe eigenvalue decomposition of a real symmetric non-negative deﬁnite matrix) is that if we\ntruncate the expansion\nA =\nt\nX\nj=0\nσjujvT\nj ,\n(A.5)\nwe obtain the best possible least squares approximation to the original matrix A. This is\nused both in eigenface-based face recognition systems (Section 14.2.1) and in the separable\napproximation of convolution kernels (3.21).\nA.1.2 Eigenvalue decomposition\nIf the matrix C is symmetric (m = n),1 it can be written as an eigenvalue decomposition,\nC\n=\nUΛU T =\n\nu0\n· · ·\nun−1\n\n\n\n\nλ0\n...\nλn−1\n\n\n\n\nuT\n0\n· · ·\nuT\nn−1\n\n\n=\nn−1\nX\ni=0\nλiuiuT\ni .\n(A.6)\n1 In this appendix, we denote symmetric matrices using C and general rectangular matrices using A.\n738\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nv0\nv1\nu0\nu1\nσ0\nσ1\nA\nFigure A.1 The action of a matrix A can be visualized by thinking of the domain as being\nspanned by a set of orthonormal vectors vj, each of which is transformed to a new orthogonal\nvector uj with a length σj. When A is interpreted as a covariance matrix and its eigenvalue\ndecomposition is performed, each of the uj axes denote a principal direction (component)\nand each σj denotes one standard deviation along that direction.\n(The eigenvector matrix U is sometimes written as Φ and the eigenvectors u as φ.) In this\ncase, the eigenvalues\nλ0 ≥λ1 ≥· · · ≥λn−1\n(A.7)\ncan be both positive and negative.2\nA special case of the symmetric matrix C occurs when it is constructed as the sum of a\nnumber of outer products\nC =\nX\ni\naiaT\ni = AAT ,\n(A.8)\nwhich often occurs when solving least squares problems (Appendix A.2), where the matrix A\nconsists of all the ai column vectors stacked side-by-side. In this case, we are guaranteed that\nall of the eigenvalues λi are non-negative. The associated matrix C is positive semi-deﬁnite\nxT Cx ≥0, ∀x.\n(A.9)\nIf the matrix C is of full rank, the eigenvalues are all positive and the matrix is called sym-\nmetric positive deﬁnite (SPD).\nSymmetric positive semi-deﬁnite matrices also arise in the statistical analysis of data,\nsince they represent the covariance of a set of {xi} points around their mean ¯x,\nC = 1\nn\nX\ni\n(xi −¯x)(xi −¯x)T .\n(A.10)\nIn this case, performing the eigenvalue decomposition is known as principal component anal-\nysis (PCA), since it models the principal directions (and magnitudes) of variation of the point\n2 Eigenvalue decompositions can be computed for non-symmetric matrices but the eigenvalues and eigenvectors\ncan have complex entries in that case.\nA.1 Matrix decompositions\n739\ndistribution around their mean, as shown in Section 5.1.1 (5.13–5.15), Section 14.2.1 (14.9),\nand Appendix B.1.1 (B.10). Figure A.1 shows how the principal components of the covari-\nance matrix C denote the principal axes uj of the uncertainty ellipsoid corresponding to this\npoint distribution and how the σj =\np\nλj denote the standard deviations along each axis.\nThe eigenvalues and eigenvectors of C and the singular values and singular vectors of A\nare closely related. Given\nA = UΣV T ,\n(A.11)\nwe get\nC = AAT = UΣV T V ΣU T = UΛU T .\n(A.12)\nFrom this, we see that λi = σ2\ni and that the left singular vectors of A are the eigenvectors of\nC.\nThis relationship gives us an efﬁcient method for computing the eigenvalue decomposi-\ntion of large matrices that are rank deﬁcient, such as the scatter matrices observed in comput-\ning eigenfaces (Section 14.2.1). Observe that the covariance matrix C in (14.9) is exactly the\nsame as C in (A.8). Note also that the individual difference-from-mean images ai = xi −¯x\nare long vectors of length P (the number of pixels in the image), while the total number of ex-\nemplars N (the number of faces in the training database) is much smaller. Instead of forming\nC = AAT , which is P × P, we form the matrix\nˆ\nC = AT A,\n(A.13)\nwhich is N × N. (This involves taking the dot product between every pair of difference\nimages ai and aj.) The eigenvalues of ˆ\nC are the squared singular values of A, namely Σ2,\nand are hence also the eigenvalues of C. The eigenvectors of ˆ\nC are the right singular vectors\nV of A, from which the desired eigenfaces U, which are the left singular vectors of A, can\nbe computed as\nU = AV Σ−1.\n(A.14)\nThis ﬁnal step is essentially computing the eigenfaces as linear combinations of the difference\nimages (Turk and Pentland 1991a). If you have access to a high-quality linear algebra pack-\nage such as LAPACK, routines for efﬁciently computing a small number of the left singular\nvectors and singular values of rectangular matrices such as A are usually provided (Ap-\npendix C.2). However, if storing all of the images in memory is prohibitive, the construction\nof ˆ\nC in (A.13) can be used instead.\nHow can eigenvalue and singular value decompositions actually be computed? Notice\nthat an eigenvector is deﬁned by the equation\nλiui = Cui\nor\n(λiI −C)ui = 0.\n(A.15)",
  "image_path": "page_760.jpg",
  "pages": [
    759,
    760,
    761
  ]
}