{
  "doc_id": "pages_723_725",
  "text": "14.4 Category recognition\n701\nFigure 14.39\n“Image-to-Image” vs. “Image-to-Class” distance comparison (Boiman,\nShechtman, and Irani 2008) c⃝2008 IEEE. The query image on the upper left may not match\nthe feature distribution of any of the database images in the bottom row. However, if each\nfeature in the query is matched to its closest analog in all the class images, a good match can\nbe found.\nﬁcation stages (Yang, Jin, Sukthankar et al. 2008). Others, such as Serre, Wolf, and Poggio\n(2005) and Mutch and Lowe (2008) use hierarchies of dense feature transforms inspired by\nbiological (visual cortical) processing combined with SVMs for ﬁnal classiﬁcation.\n14.4.2 Part-based models\nRecognizing an object by ﬁnding its constituent parts and measuring their geometric rela-\ntionships is one of the oldest approaches to object recognition (Fischler and Elschlager 1973;\nKanade 1977; Yuille 1991). We have already seen examples of part-based approaches being\nused for face recognition (Figure 14.18) (Moghaddam and Pentland 1997; Heisele, Ho, Wu\net al. 2003; Heisele, Serre, and Poggio 2007) and pedestrian detection (Figure 14.9) (Felzen-\nszwalb, McAllester, and Ramanan 2008).\nIn this section, we look more closely at some of the central issues in part-based recog-\nnition, namely, the representation of geometric relationships, the representation of individ-\nual parts, and algorithms for learning such descriptions and recognizing them at run time.\nMore details on part-based models for recognition can be found in the course notes of Fergus\n(2007b, 2009).\nThe earliest approaches to representing geometric relationships were dubbed pictorial\nstructures by Fischler and Elschlager (1973) and consisted of spring-like connections between\ndifferent feature locations (Figure 14.1a). To ﬁt a pictorial structure to an image, an energy\n702\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.40 Using pictorial structures to locate and track a person (Felzenszwalb and Hut-\ntenlocher 2005) c⃝2005 Springer. The structure consists of articulated rectangular body parts\n(torso, head, and limbs) connected in a tree topology that encodes relative part positions and\norientations. To ﬁt a pictorial structure model, a binary silhouette image is ﬁrst computed\nusing background subtraction.\nfunction of the form\nE =\nX\ni\nVi(li) +\nX\nij∈E\nVij(li, lj)\n(14.42)\nis minimized over all potential part locations or poses {li} and pairs of parts (i, j) for which\nan edge (geometric relationship) exists in E. Note how this energy is closely related to\nthat used with Markov random ﬁelds (3.108–3.109), which can be used to embed pictorial\nstructures in a probabilistic framework that makes parameter learning easier (Felzenszwalb\nand Huttenlocher 2005).\nPart-based models can have different topologies for the geometric connections between\nthe parts (Figure 14.41). For example, Felzenszwalb and Huttenlocher (2005) restrict the\nconnections to a tree (Figure 14.41d), which makes learning and inference more tractable. A\ntree topology enables the use of a recursive Viterbi (dynamic programming) algorithm (Pearl\n1988; Bishop 2006), in which leaf nodes are ﬁrst optimized as a function of their parents, and\nthe resulting values are then plugged in and eliminated from the energy function—see Ap-\npendix B.5.2. The Viterbi algorithm computes an optimal match in O(N 2|E| + NP) time,\nwhere N is the number of potential locations or poses for each part, |E| is the number of\nedges (pairwise constraints), and P = |V | is the number of parts (vertices in the graphical\nmodel, which is equal to |E| + 1 in a tree). To further increase the efﬁciency of the infer-\nence algorithm, Felzenszwalb and Huttenlocher (2005) restrict the pairwise energy functions\nVij(li, lj) to be Mahalanobis distances on functions of location variables and then use fast\ndistance transform algorithms to minimize each pairwise interaction in time that is closer to\nlinear in N.\nFigure 14.40 shows the results of using their pictorial structures algorithm to ﬁt an articu-\n14.4 Category recognition\n703\nX1\nX2\nX3\nX4\nX5\nX6\nX1\nX2\nX3\nX4\nX5\nX6\nX4\nX5\nX3\nX6\nX2\nX1\nX1\nX2\nX3\nX4\nX5\nX6\n(a)\n(b)\n(c)\n(d)\nX2\nX3\nX4\nX5\nX6\nX1\ng\nh1\nhg\nl1\nl2\nlK\nX1\nX3\nX2\nX5\nX6\nX7\n. . .\n. . .\nCenter\nPart\nSubpart\n. . .\nX1\nX2\nX3\nX4\nX5\nX6\nX1\nX2\nX3\nX4\nX5\nX6\nk=1\nk=2\n(e)\n(f)\n(g)\nFigure 14.41\nGraphical models for geometric spatial priors (Carneiro and Lowe 2006) c⃝\n2006 Springer: (a) constellation (Fergus, Perona, and Zisserman 2007); (b) star (Crandall,\nFelzenszwalb, and Huttenlocher 2005; Fergus, Perona, and Zisserman 2005); (c) k-fan (k =\n2) (Crandall, Felzenszwalb, and Huttenlocher 2005); (d) tree (Felzenszwalb and Huttenlocher\n2005); (e) bag of features (Csurka, Dance, Fan et al. 2004); (f) hierarchy (Bouchard and\nTriggs 2005); (g) sparse ﬂexible model (Carneiro and Lowe 2006).\nlated body model to a binary image obtained by background segmentation. In this application\nof pictorial structures, parts are parameterized by the locations, sizes, and orientations of their\napproximating rectangles. Unary matching potentials Vi(li) are determined by counting the\npercentage of foreground and background pixels inside and just outside the tilted rectangle\nrepresenting each part.\nOver the last decade, a large number of different graphical models have been proposed\nfor part-based recognition, as shown in Figure 14.41. Carneiro and Lowe (2006) discuss\na number of these models and propose one of their own, which they call a sparse ﬂexible\nmodel; it involves ordering the parts and having each part’s location depend on at most k of\nits ancestor locations.\nThe simplest models, which we saw in Section 14.4.1, are bags of words, where there are\nno geometric relationships between different parts or features. While such models can be very\nefﬁcient, they have a very limited capacity to express the spatial arrangement of parts. Trees\nand stars (a special case of trees where all leaf nodes are directly connected to a common root)\nare the most efﬁcient in terms of inference and hence also learning (Felzenszwalb and Hutten-\nlocher 2005; Fergus, Perona, and Zisserman 2005; Felzenszwalb, McAllester, and Ramanan\n2008). Directed acyclic graphs (Figure 14.41f–g) come next in terms of complexity and can\nstill support efﬁcient inference, although at the cost of imposing a causal structure on the",
  "image_path": "page_724.jpg",
  "pages": [
    723,
    724,
    725
  ]
}