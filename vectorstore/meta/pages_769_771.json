{
  "doc_id": "pages_769_771",
  "text": "A.4 Direct sparse matrix techniques\n747\nwhich is called a damped Gauss–Newton method. The damping parameter λ is increased if\nthe squared residual is not decreasing as fast as expected, i.e., as predicted by (A.46), and\nis decreased if the expected decrease is obtained (Madsen, Nielsen, and Tingleff 2004). The\ncombination of the Newton (ﬁrst-order Taylor series) approximation (A.46) and the adaptive\ndamping parameter λ is commonly known as the Levenberg–Marquardt algorithm (Leven-\nberg 1944; Marquardt 1963) and is an example of more general trust region methods, which\nare discussed in more detail in (Bj¨orck 1996; Conn, Gould, and Toint 2000; Madsen, Nielsen,\nand Tingleff 2004; Nocedal and Wright 2006).\nWhen the initial solution is far away from its quadratic region of convergence around a\nlocal minimum, large residual methods, e.g., Newton-type methods, which add a second-order\nterm to the Taylor series expansion in (A.46), may converge faster. Quasi-Newton methods\nsuch as BFGS, which require only gradient evaluations, can also be useful if memory size is\nan issue. Such techniques are discussed in textbooks and papers on numerical optimization\n(Toint 1987; Bj¨orck 1996; Conn, Gould, and Toint 2000; Nocedal and Wright 2006).\nA.4 Direct sparse matrix techniques\nMany optimization problems in computer vision, such as bundle adjustment (Szeliski and\nKang 1994; Triggs, McLauchlan, Hartley et al. 1999; Hartley and Zisserman 2004; Snavely,\nSeitz, and Szeliski 2008b; Agarwal, Snavely, Simon et al. 2009) have Jacobian and (approx-\nimate) Hessian matrices that are extremely sparse (Section 7.4.1). For example, Figure 7.9a\nshows the bipartite model typical of structure from motion problems, in which most points\nare only observed by a subset of the cameras, which results in the sparsity patterns for the\nJacobian and Hessian shown in Figure 7.9b–c.\nWhenever the Hessian matrix is sparse enough, it is more efﬁcient to use sparse Cholesky\nfactorization instead of regular Cholesky factorization. In such sparse direct techniques, the\nHessian matrix C and its associated Cholesky factor R are stored in compressed form, in\nwhich the amount of storage is proportional to the number of (potentially) non-zero entries\n(Bj¨orck 1996; Davis 2006).7 Algorithms for computing the non-zero elements in C and R\nfrom the sparsity pattern of the Jacobian matrix J are given by Bj¨orck (1996, Section 6.4),\nand algorithms for computing the numerical Cholesky and QR decompositions (once the\nsparsity pattern has been computed and storage allocated) are discussed by Bj¨orck (1996,\nSection 6.5).\n7 For example, you can store a list of (i, j, cij) triples. One example of such a scheme is compressed sparse\nrow (CSR) storage. An alternative storage method called skyline, which stores adjacent vertical spans of non-zero\nelements (Bathe 2007), is sometimes used in ﬁnite element analysis. Banded systems such as snakes (5.3) can store\njust the non-zero band elements (Bj¨orck 1996, Section 6.2) and can be solved in O(nb2), where n is the number of\nvariables and b is the bandwidth.\n748\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nA.4.1 Variable reordering\nThe key to efﬁciently solving sparse problems using direct (non-iterative) techniques is to\ndetermine an efﬁcient ordering for the variables, which reduces the amount of ﬁll-in, i.e., the\nnumber of non-zero entries in R that were zero in the original C matrix. We already saw in\nSection 7.4.1 how storing the more numerous 3D point parameters before the camera param-\neters and using the Schur complement (7.56) results in a more efﬁcient algorithm. Similarly,\nsorting parameters by time in video-based reconstruction problems usually results in lower\nﬁll-in. Furthermore, any problem whose adjacency graph (the graph corresponding to the\nsparsity pattern) is a tree can be solved in linear time with an appropriate reordering of the\nvariables (putting all the children before their parents). All of these are examples of good\nreordering techniques.\nIn the general case of unstructured data, there are many heuristics available to ﬁnd good\nreorderings (Bj¨orck 1996; Davis 2006).8 For general adjacency (sparsity) graphs, minimum\ndegree orderings generally produce good results. For planar graphs, which often arise on im-\nage or spline grids (Section 8.3), nested dissection, which recursively splits the graph into two\nequal halves along a frontier (or boundary) of small size, generally works well. Such domain\ndecomposition (or multi-frontal) techniques also enable the use of parallel processing, since\nindependent sub-graphs can be processed in parallel on separate processors (Davis 2008).\nThe overall set of steps used to perform the direct solution of sparse least squares problems\nare summarized in Algorithm A.2, which is a modiﬁed version of Algorithm 6.6.1 by Bj¨orck\n(1996, Section 6.6)). If a series of related least squares problems is being solved, as is the\ncase in iterative non-linear least squares (Appendix A.3), steps 1–3 can be performed ahead of\ntime and reused for each new invocation with different C and d values. When the problem is\nblock-structured, as is the case in structure from motion where point (structure) variables have\ndense 3×3 sub-entries in C and cameras have 6×6 (or larger) entries, the cost of performing\nthe reordering computation is small compared to the actual numerical factorization, which\ncan beneﬁt from block-structured matrix operations (Golub and Van Loan 1996). It is also\npossible to apply sparse reordering and multifrontal techniques to QR factorization (Davis\n2008), which may be preferable when the least squares problems are poorly conditioned.\nA.5 Iterative techniques\nWhen problems become large, the amount of memory required to store the Hessian matrix\nC and its factor R, and the amount of time it takes to compute the factorization, can be-\ncome prohibitively large, especially when there are large amounts of ﬁll-in. This is often\n8Finding the optimal reordering with minimal ﬁll-in is provably NP-hard.\nA.5 Iterative techniques\n749\nprocedure SparseCholeskySolve(C, d):\n1. Determine symbolically the structure of C, i.e., the adjacency graph.\n2. (Optional) Compute a reordering for the variables, taking into ac-\ncount any block structure inherent in the problem.\n3. Determine the ﬁll-in pattern for R and allocate the compressed stor-\nage for R as well as storage for the permuted right hand side ˆd.\n4. Copy the elements of C and d into R and ˆd, permuting the values\naccording to the computed ordering.\n5. Perform the numerical factorization of R using Algorithm A.1.\n6. Solve the factored system (A.33), i.e.,\nRT z = ˆd,\nRx = z.\n7. Return the solution x, after undoing the permutation.\nAlgorithm A.2\nSparse least squares using a sparse Cholesky decomposition of the matrix\nC.\nthe case with image processing problems deﬁned on pixel grids, since, even with the optimal\nreordering (nested dissection) the amount of ﬁll can still be large.\nA preferable approach to solving such linear systems is to use iterative techniques, which\ncompute a series of estimates that converge to the ﬁnal solution, e.g., by taking a series of\ndownhill steps in an energy function such as (A.29).\nA large number of iterative techniques have been developed over the years, including such\nwell-known algorithms as successive overrelaxation and multi-grid. These are described in\nspecialized textbooks on iterative solution techniques (Axelsson 1996; Saad 2003) as well as\nin more general books on numerical linear algebra and least squares techniques (Bj¨orck 1996;\nGolub and Van Loan 1996; Trefethen and Bau 1997; Nocedal and Wright 2006; Bj¨orck and\nDahlquist 2010).\nA.5.1 Conjugate gradient\nThe iterative solution technique that often performs best is conjugate gradient descent, which\ntakes a series of downhill steps that are conjugate to each other with respect to the C matrix,",
  "image_path": "page_770.jpg",
  "pages": [
    769,
    770,
    771
  ]
}