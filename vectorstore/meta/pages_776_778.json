{
  "doc_id": "pages_776_778",
  "text": "754\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAppendix B\nBayesian modeling and inference\nB.1\nEstimation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757\nB.1.1\nLikelihood for multivariate Gaussian noise\n. . . . . . . . . . . . . . 757\nB.2\nMaximum likelihood estimation and least squares . . . . . . . . . . . . . . . 759\nB.3\nRobust statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 760\nB.4\nPrior models and Bayesian inference . . . . . . . . . . . . . . . . . . . . . . 762\nB.5\nMarkov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763\nB.5.1\nGradient descent and simulated annealing . . . . . . . . . . . . . . . 765\nB.5.2\nDynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 766\nB.5.3\nBelief propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\nB.5.4\nGraph cuts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770\nB.5.5\nLinear programming . . . . . . . . . . . . . . . . . . . . . . . . . . 773\nB.6\nUncertainty estimation (error analysis) . . . . . . . . . . . . . . . . . . . . . 775\n756\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe following problem commonly recurs in this book: Given a number of measurements\n(images, feature positions, etc.), estimate the values of some unknown structure or parameter\n(camera positions, object shape, etc.). These kinds of problems are in general called inverse\nproblems because they involve estimating unknown model parameters instead of simulating\nthe forward formation equations.1 Computer graphics is a classic forward modeling problem\n(given some objects, cameras, and lighting, simulate the images that would result), while\ncomputer vision problems are usually of the inverse kind (given one or more images, recover\nthe scene that gave rise to these images).\nGiven an instance of an inverse problem, there are, in general, several ways to proceed.\nFor instance, through clever (or sometimes straightforward) algebraic manipulation, a closed\nform solution for the unknowns can sometimes be derived. Consider, for example, the camera\nmatrix calibration problem (Section 6.2.1): given an image of a calibration pattern consisting\nof known 3D point positions, compute the 3×4 camera matrix P that maps these points onto\nthe image plane.\nIn more detail, we can write this problem as (6.33–6.34)\nxi\n=\np00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(B.1)\nyi\n=\np10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n,\n(B.2)\nwhere (xi, yi) is the feature position of the ith point measured in the image plane, (Xi, Yi, Zi)\nis the corresponding 3D point position, and the pij are the unknown entries of the camera\nmatrix P . Moving the denominator over to the left hand side, we end up with a set of\nsimultaneous linear equations,\nxi(p20Xi + p21Yi + p22Zi + p23)\n=\np00Xi + p01Yi + p02Zi + p03,\n(B.3)\nyi(p20Xi + p21Yi + p22Zi + p23)\n=\np10Xi + p11Yi + p12Zi + p13,\n(B.4)\nwhich we can solve using linear least squares (Appendix A.2) to obtain an estimate of P .\nThe question then arises: is this set of equations the right ones to be solving? If the\nmeasurements are totally noise-free or we do not care about getting the best possible answer,\nthen the answer is yes. However, in general, we cannot be sure that we have a reasonable\nalgorithm unless we make a model of the likely sources of error and devise an algorithm that\nperforms as well as possible given these potential errors.\n1 In machine learning, these problems are called regression problems, because we are trying to estimate a contin-\nuous quantity from noisy inputs, as opposed to a discrete classiﬁcation task (Bishop 2006).",
  "image_path": "page_777.jpg",
  "pages": [
    776,
    777,
    778
  ]
}