{
  "doc_id": "pages_075_077",
  "text": "2.1 Geometric primitives and transformations\n53\nW/2\nf\nθ/2\n(x,y,1)\n(X,Y,Z)\nZ\nFigure 2.10\nCentral projection, showing the relationship between the 3D and 2D coordi-\nnates, p and x, as well as the relationship between the focal length f, image width W, and\nthe ﬁeld of view θ.\nFigure 2.9 shows how these quantities can be visualized as part of a simpliﬁed imaging\nmodel. Note that now we have placed the image plane in front of the nodal point (projection\ncenter of the lens). The sense of the y axis has also been ﬂipped to get a coordinate system\ncompatible with the way that most imaging libraries treat the vertical (row) coordinate. Cer-\ntain graphics libraries, such as Direct3D, use a left-handed coordinate system, which can lead\nto some confusion.\nA note on focal lengths\nThe issue of how to express focal lengths is one that often causes confusion in implementing\ncomputer vision algorithms and discussing their results. This is because the focal length\ndepends on the units used to measure pixels.\nIf we number pixel coordinates using integer values, say [0, W)×[0, H), the focal length\nf and camera center (cx, cy) in (2.59) can be expressed as pixel values. How do these quan-\ntities relate to the more familiar focal lengths used by photographers?\nFigure 2.10 illustrates the relationship between the focal length f, the sensor width W,\nand the ﬁeld of view θ, which obey the formula\ntan θ\n2 = W\n2f\nor\nf = W\n2\n\u0014\ntan θ\n2\n\u0015−1\n.\n(2.60)\nFor conventional ﬁlm cameras, W = 35mm, and hence f is also expressed in millimeters.\nSince we work with digital images, it is more convenient to express W in pixels so that the\nfocal length f can be used directly in the calibration matrix K as in (2.59).\nAnother possibility is to scale the pixel coordinates so that they go from [−1, 1) along\nthe longer image dimension and [−a−1, a−1) along the shorter axis, where a ≥1 is the\nimage aspect ratio (as opposed to the sensor cell aspect ratio introduced earlier). This can be\n54\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\naccomplished using modiﬁed normalized device coordinates,\nx′\ns = (2xs −W)/S and y′\ns = (2ys −H)/S,\nwhere\nS = max(W, H).\n(2.61)\nThis has the advantage that the focal length f and optical center (cx, cy) become independent\nof the image resolution, which can be useful when using multi-resolution, image-processing\nalgorithms, such as image pyramids (Section 3.5).2 The use of S instead of W also makes the\nfocal length the same for landscape (horizontal) and portrait (vertical) pictures, as is the case\nin 35mm photography. (In some computer graphics textbooks and systems, normalized device\ncoordinates go from [−1, 1] × [−1, 1], which requires the use of two different focal lengths\nto describe the camera intrinsics (Watt 1995; OpenGL-ARB 1997).) Setting S = W = 2 in\n(2.60), we obtain the simpler (unitless) relationship\nf −1 = tan θ\n2.\n(2.62)\nThe conversion between the various focal length representations is straightforward, e.g.,\nto go from a unitless f to one expressed in pixels, multiply by W/2, while to convert from an\nf expressed in pixels to the equivalent 35mm focal length, multiply by 35/W.\nCamera matrix\nNow that we have shown how to parameterize the calibration matrix K, we can put the\ncamera intrinsics and extrinsics together to obtain a single 3 × 4 camera matrix\nP = K\nh\nR\nt\ni\n.\n(2.63)\nIt is sometimes preferable to use an invertible 4 × 4 matrix, which can be obtained by not\ndropping the last row in the P matrix,\n˜\nP =\n\"\nK\n0\n0T\n1\n# \"\nR\nt\n0T\n1\n#\n= ˜\nKE,\n(2.64)\nwhere E is a 3D rigid-body (Euclidean) transformation and ˜\nK is the full-rank calibration\nmatrix. The 4 × 4 camera matrix ˜\nP can be used to map directly from 3D world coordinates\n¯pw = (xw, yw, zw, 1) to screen coordinates (plus disparity), xs = (xs, ys, 1, d),\nxs ∼˜\nP ¯pw,\n(2.65)\nwhere ∼indicates equality up to scale. Note that after multiplication by ˜\nP , the vector is\ndivided by the third element of the vector to obtain the normalized form xs = (xs, ys, 1, d).\n2 To make the conversion truly accurate after a downsampling step in a pyramid, ﬂoating point values of W and\nH would have to be maintained since they can become non-integral if they are ever odd at a larger resolution in the\npyramid.\n2.1 Geometric primitives and transformations\n55\nC\n(xs,ys,d)\nZ\nimage plane\nd=1.0 d=0.67 d=0.5\nd = inverse depth\n(xw,yw,zw)\nd\nz\nC\n(xs,ys,d)\nZ\nimage plane\nd=0.5\nd=0\nd=-0.25\nd = projective depth\n(xw,yw,zw)\nz\nplane\nparallax\nFigure 2.11 Regular disparity (inverse depth) and projective depth (parallax from a reference\nplane).\nPlane plus parallax (projective depth)\nIn general, when using the 4 × 4 matrix ˜\nP , we have the freedom to remap the last row to\nwhatever suits our purpose (rather than just being the “standard” interpretation of disparity as\ninverse depth). Let us re-write the last row of ˜\nP as p3 = s3[ˆn0|c0], where ∥ˆn0∥= 1. We\nthen have the equation\nd = s3\nz (ˆn0 · pw + c0),\n(2.66)\nwhere z = p2 · ¯pw = rz · (pw −c) is the distance of pw from the camera center C (2.25)\nalong the optical axis Z (Figure 2.11). Thus, we can interpret d as the projective disparity\nor projective depth of a 3D scene point pw from the reference plane ˆn0 · pw + c0 = 0\n(Szeliski and Coughlan 1997; Szeliski and Golland 1999; Shade, Gortler, He et al. 1998;\nBaker, Szeliski, and Anandan 1998). (The projective depth is also sometimes called parallax\nin reconstruction algorithms that use the term plane plus parallax (Kumar, Anandan, and\nHanna 1994; Sawhney 1994).) Setting ˆn0 = 0 and c0 = 1, i.e., putting the reference plane\nat inﬁnity, results in the more standard d = 1/z version of disparity (Okutomi and Kanade\n1993).\nAnother way to see this is to invert the ˜\nP matrix so that we can map pixels plus disparity\ndirectly back to 3D points,\n˜pw = ˜\nP\n−1xs.\n(2.67)\nIn general, we can choose ˜\nP to have whatever form is convenient, i.e., to sample space us-\ning an arbitrary projection. This can come in particularly handy when setting up multi-view\nstereo reconstruction algorithms, since it allows us to sweep a series of planes (Section 11.1.2)\nthrough space with a variable (projective) sampling that best matches the sensed image mo-\ntions (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).",
  "image_path": "page_076.jpg",
  "pages": [
    75,
    76,
    77
  ]
}