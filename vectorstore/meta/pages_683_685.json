{
  "doc_id": "pages_683_685",
  "text": "14.1 Object detection\n661\nFigure 14.4\nLearning a mixture of Gaussians model for face detection (Sung and Poggio\n1998) c⃝1998 IEEE. The face and non-face images (192-long vectors) are ﬁrst clustered into\nsix separate clusters (each) using k-means and then analyzed using PCA. The cluster centers\nare shown in the right-hand columns.\nClustering and PCA.\nOnce the face and non-face patterns have been pre-processed, Sung\nand Poggio (1998) cluster each of these datasets into six separate clusters using k-means\nand then ﬁt PCA subspaces to each of the resulting 12 clusters (Figure 14.4). At detection\ntime, the DIFS and DFFS metrics ﬁrst developed by Moghaddam and Pentland (1997) (see\nFigure 14.14 and (14.14)) are used to produce 24 Mahalanobis distance measurements (two\nper cluster). The resulting 24 measurements are input to a multi-layer perceptron (MLP),\nwhich is a neural network with alternating layers of weighted summations and sigmoidal non-\nlinearities trained using the “backpropagation” algorithm (Rumelhart, Hinton, and Williams\n1986).\nNeural networks.\nInstead of ﬁrst clustering the data and computing Mahalanobis distances\nto the cluster centers, Rowley, Baluja, and Kanade (1998a) apply a neural network (MLP) di-\nrectly to the 20×20 pixel patches of gray-level intensities, using a variety of differently sized\nhand-crafted “receptive ﬁelds” to capture both large-scale and smaller scale structure (Fig-\nure 14.5). The resulting neural network directly outputs the likelihood of a face at the center\n662\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.5\nA neural network for face detection (Rowley, Baluja, and Kanade 1998a) c⃝\n1998 IEEE. Overlapping patches are extracted from different levels of a pyramid and then\npre-processed as shown in Figure 14.3b. A three-layer neural network is then used to detect\nlikely face locations.\nof every overlapping patch in a multi-resolution pyramid. Since several overlapping patches\n(in both space and resolution) may ﬁre near a face, an additional merging network is used\nto merge overlapping detections. The authors also experiment with training several networks\nand merging their outputs. Figure 14.2 shows a sample result from their face detector.\nTo make the detector run faster, a separate network operating on 30×30 patches is trained\nto detect both faces and faces shifted by ±5 pixels. This network is evaluated at every 10th\npixel in the image (horizontally and vertically) and the results of this “coarse” or “sloppy”\ndetector are used to select regions on which to run the slower single-pixel overlap technique.\nTo deal with in-plane rotations of faces, Rowley, Baluja, and Kanade (1998b) train a router\nnetwork to estimate likely rotation angles from input patches and then apply the estimated\nrotation to each patch before running the result through their upright face detector.\nSupport vector machines.\nInstead of using a neural network to classify patches, Osuna,\nFreund, and Girosi (1997) use a support vector machine (SVM) (Hastie, Tibshirani, and\nFriedman 2001; Sch¨olkopf and Smola 2002; Bishop 2006; Lampert 2008) to classify the same\npreprocessed patches as Sung and Poggio (1998). An SVM searches for a series of maximum\nmargin separating planes in feature space between different classes (in this case, face and\nnon-face patches). In those cases where linear classiﬁcation boundaries are insufﬁcient, the\nfeature space can be lifted into higher-dimensional features using kernels (Hastie, Tibshirani,\nand Friedman 2001; Sch¨olkopf and Smola 2002; Bishop 2006). SVMs have been used by\nother researchers for both face detection and face recognition (Heisele, Ho, Wu et al. 2003;\n14.1 Object detection\n663\n(a)\n(b)\nFigure 14.6\nSimple features used in boosting-based face detector (Viola and Jones 2004)\nc⃝2004 Springer: (a) difference of rectangle feature composed of 2–4 different rectangles\n(pixels inside the white rectangles are subtracted from the gray ones); (b) the ﬁrst and second\nfeatures selected by AdaBoost. The ﬁrst feature measures the differences in intensity between\nthe eyes and the cheeks, the second one between the eyes and the bridge of the nose.\nHeisele, Serre, and Poggio 2007) and are a widely used tool in object recognition in general.\nBoosting.\nOf all the face detectors currently in use, the one introduced by Viola and Jones\n(2004) is probably the best known and most widely used. Their technique was the ﬁrst to\nintroduce the concept of boosting to the computer vision community, which involves train-\ning a series of increasingly discriminating simple classiﬁers and then blending their outputs\n(Hastie, Tibshirani, and Friedman 2001; Bishop 2006).\nIn more detail, boosting involves constructing a classiﬁer h(x) as a sum of simple weak\nlearners,\nh(x) = sign\n\n\nm−1\nX\nj=0\nαjhj(x)\n\n,\n(14.1)\nwhere each of the weak learners hj(x) is an extremely simple function of the input, and hence\nis not expected to contribute much (in isolation) to the classiﬁcation performance.\nIn most variants of boosting, the weak learners are threshold functions,\nhj(x) = aj[fj < θj] + bj[fj ≥θj] =\n(\naj\nif fj < θj\nbj\notherwise,\n(14.2)\nwhich are also known as decision stumps (basically, the simplest possible version of decision\ntrees). In most cases, it is also traditional (and simpler) to set aj and bj to ±1, i.e., aj = −sj,\nbj = +sj, so that only the feature fj, the threshold value θj, and the polarity of the threshold\nsj ∈±1 need to be selected.4\n4Some variants, such as that of Viola and Jones (2004), use (aj, bj) ∈[0, 1] and adjust the learning algorithm",
  "image_path": "page_684.jpg",
  "pages": [
    683,
    684,
    685
  ]
}