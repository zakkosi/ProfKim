{
  "doc_id": "pages_094_096",
  "text": "72\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzi=102mm\nf = 100mm\nzo=5m\nδi\nd\nδo\nα\nα\nα\nP\nJ\nI\nO\nQ\nro\nFigure 2.22\nThe amount of light hitting a pixel of surface area δi depends on the square of\nthe ratio of the aperture diameter d to the focal length f, as well as the fourth power of the\noff-axis angle α cosine, cos4 α.\n1992; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm, Trudeau et\nal. 2009), as mentioned in Section 2.1.6.\nVignetting\nAnother property of real-world lenses is vignetting, which is the tendency for the brightness\nof the image to fall off towards the edge of the image.\nTwo kinds of phenomena usually contribute to this effect (Ray 2002). The ﬁrst is called\nnatural vignetting and is due to the foreshortening in the object surface, projected pixel, and\nlens aperture, as shown in Figure 2.22. Consider the light leaving the object surface patch\nof size δo located at an off-axis angle α. Because this patch is foreshortened with respect\nto the camera lens, the amount of light reaching the lens is reduced by a factor cos α. The\namount of light reaching the lens is also subject to the usual 1/r2 fall-off; in this case, the\ndistance ro = zo/ cos α. The actual area of the aperture through which the light passes\nis foreshortened by an additional factor cos α, i.e., the aperture as seen from point O is an\nellipse of dimensions d×d cos α. Putting all of these factors together, we see that the amount\nof light leaving O and passing through the aperture on its way to the image pixel located at I\nis proportional to\nδo cos α\nr2o\nπ\n\u0012d\n2\n\u00132\ncos α = δoπ\n4\nd2\nz2o\ncos4 α.\n(2.98)\nSince triangles ∆OPQ and ∆IPJ are similar, the projected areas of of the object surface δo\nand image pixel δi are in the same (squared) ratio as zo : zi,\nδo\nδi = z2\no\nz2\ni\n.\n(2.99)\nPutting these together, we obtain the ﬁnal relationship between the amount of light reaching\n2.3 The digital camera\n73\npixel i and the aperture diameter d, the focusing distance zi ≈f, and the off-axis angle α,\nδoπ\n4\nd2\nz2o\ncos4 α = δiπ\n4\nd2\nz2\ni\ncos4 α ≈δiπ\n4\n\u0012 d\nf\n\u00132\ncos4 α,\n(2.100)\nwhich is called the fundamental radiometric relation between the scene radiance L and the\nlight (irradiance) E reaching the pixel sensor,\nE = Lπ\n4\n\u0012 d\nf\n\u00132\ncos4 α,\n(2.101)\n(Horn 1986; Nalwa 1993; Hecht 2001; Ray 2002). Notice in this equation how the amount of\nlight depends on the pixel surface area (which is why the smaller sensors in point-and-shoot\ncameras are so much noisier than digital single lens reﬂex (SLR) cameras), the inverse square\nof the f-stop N = f/d (2.97), and the fourth power of the cos4 α off-axis fall-off, which is\nthe natural vignetting term.\nThe other major kind of vignetting, called mechanical vignetting, is caused by the internal\nocclusion of rays near the periphery of lens elements in a compound lens, and cannot easily\nbe described mathematically without performing a full ray-tracing of the actual lens design.9\nHowever, unlike natural vignetting, mechanical vignetting can be decreased by reducing the\ncamera aperture (increasing the f-number). It can also be calibrated (along with natural vi-\ngnetting) using special devices such as integrating spheres, uniformly illuminated targets, or\ncamera rotation, as discussed in Section 10.1.3.\n2.3 The digital camera\nAfter starting from one or more light sources, reﬂecting off one or more surfaces in the world,\nand passing through the camera’s optics (lenses), light ﬁnally reaches the imaging sensor.\nHow are the photons arriving at this sensor converted into the digital (R, G, B) values that\nwe observe when we look at a digital image? In this section, we develop a simple model\nthat accounts for the most important effects such as exposure (gain and shutter speed), non-\nlinear mappings, sampling and aliasing, and noise. Figure 2.23, which is based on camera\nmodels developed by Healey and Kondepudy (1994); Tsin, Ramesh, and Kanade (2001); Liu,\nSzeliski, Kang et al. (2008), shows a simple version of the processing stages that occur in\nmodern digital cameras. Chakrabarti, Scharstein, and Zickler (2009) developed a sophisti-\ncated 24-parameter model that is an even better match to the processing performed in today’s\ncameras.\n9 There are some empirical models that work well in practice (Kang and Weiss 2000; Zheng, Lin, and Kang\n2006).\n74\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 2.23 Image sensing pipeline, showing the various sources of noise as well as typical\ndigital post-processing steps.\nLight falling on an imaging sensor is usually picked up by an active sensing area, inte-\ngrated for the duration of the exposure (usually expressed as the shutter speed in a fraction of\na second, e.g.,\n1\n125,\n1\n60,\n1\n30), and then passed to a set of sense ampliﬁers . The two main kinds\nof sensor used in digital still and video cameras today are charge-coupled device (CCD) and\ncomplementary metal oxide on silicon (CMOS).\nIn a CCD, photons are accumulated in each active well during the exposure time. Then,\nin a transfer phase, the charges are transferred from well to well in a kind of “bucket brigade”\nuntil they are deposited at the sense ampliﬁers, which amplify the signal and pass it to\nan analog-to-digital converter (ADC).10 Older CCD sensors were prone to blooming, when\ncharges from one over-exposed pixel spilled into adjacent ones, but most newer CCDs have\nanti-blooming technology (“troughs” into which the excess charge can spill).\nIn CMOS, the photons hitting the sensor directly affect the conductivity (or gain) of a\nphotodetector, which can be selectively gated to control exposure duration, and locally am-\npliﬁed before being read out using a multiplexing scheme.\nTraditionally, CCD sensors\noutperformed CMOS in quality sensitive applications, such as digital SLRs, while CMOS\nwas better for low-power applications, but today CMOS is used in most digital cameras.\nThe main factors affecting the performance of a digital image sensor are the shutter speed,\nsampling pitch, ﬁll factor, chip size, analog gain, sensor noise, and the resolution (and quality)\n10 In digital still cameras, a complete frame is captured and then read out sequentially at once. However, if video\nis being captured, a rolling shutter, which exposes and transfers each line separately, is often used. In older video\ncameras, the even ﬁelds (lines) were scanned ﬁrst, followed by the odd ﬁelds, in a process that is called interlacing.",
  "image_path": "page_095.jpg",
  "pages": [
    94,
    95,
    96
  ]
}