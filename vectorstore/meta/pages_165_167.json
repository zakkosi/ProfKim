{
  "doc_id": "pages_165_167",
  "text": "3.4 Fourier transforms\n143\n-1.00\n-0.75\n-0.50\n-0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 3.26 Discrete cosine transform (DCT) basis functions: The ﬁrst DC (i.e., constant)\nbasis is the horizontal blue line, the second is the brown half-cycle waveform, etc. These\nbases are widely used in image and video compression standards such as JPEG.\ndifferent frequencies,\nF(k) =\nN−1\nX\ni=0\ncos\n\u0012 π\nN (i + 1\n2)k\n\u0013\nf(i),\n(3.76)\nwhere k is the coefﬁcient (frequency) index, and the 1/2-pixel offset is used to make the\nbasis coefﬁcients symmetric (Wallace 1991). Some of the discrete cosine basis functions are\nshown in Figure 3.26. As you can see, the ﬁrst basis function (the straight blue line) encodes\nthe average DC value in the block of pixels, while the second encodes a slightly curvy version\nof the slope.\nIn turns out that the DCT is a good approximation to the optimal Karhunen–Lo`eve decom-\nposition of natural image statistics over small patches, which can be obtained by performing\na principal component analysis (PCA) of images, as described in Section 14.2.1. The KL-\ntransform de-correlates the signal optimally (assuming the signal is described by its spectrum)\nand thus, theoretically, leads to optimal compression.\nThe two-dimensional version of the DCT is deﬁned similarly,\nF(k, l) =\nN−1\nX\ni=0\nN−1\nX\nj=0\ncos\n\u0012 π\nN (i + 1\n2)k\n\u0013\ncos\n\u0012 π\nN (j + 1\n2)l\n\u0013\nf(i, j).\n(3.77)\nLike the 2D Fast Fourier Transform, the 2D DCT can be implemented separably, i.e., ﬁrst\ncomputing the DCT of each line in the block and then computing the DCT of each resulting\ncolumn. Like the FFT, each of the DCTs can also be computed in O(N log N) time.\nAs we mentioned in Section 2.3.3, the DCT is widely used in today’s image and video\ncompression algorithms, although it is slowly being supplanted by wavelet algorithms (Si-\nmoncelli and Adelson 1990b), as discussed in Section 3.5.4, and overlapped variants of the\nDCT (Malvar 1990, 1998, 2000), which are used in the new JPEG XR standard.12 These\n12 http://www.itu.int/rec/T-REC-T.832-200903-I/en.\n144\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nnewer algorithms suffer less from the blocking artifacts (visible edge-aligned discontinuities)\nthat result from the pixels in each block (typically 8 × 8) being transformed and quantized\nindependently. See Exercise 3.30 for ideas on how to remove blocking artifacts from com-\npressed JPEG images.\n3.4.4 Application: Sharpening, blur, and noise removal\nAnother common application of image processing is the enhancement of images through the\nuse of sharpening and noise removal operations, which require some kind of neighborhood\nprocessing. Traditionally, these kinds of operation were performed using linear ﬁltering (see\nSections 3.2 and Section 3.4.3). Today, it is more common to use non-linear ﬁlters (Sec-\ntion 3.3.1), such as the weighted median or bilateral ﬁlter (3.34–3.37), anisotropic diffusion\n(3.39–3.40), or non-local means (Buades, Coll, and Morel 2008). Variational methods (Sec-\ntion 3.7.1), especially those using non-quadratic (robust) norms such as the L1 norm (which\nis called total variation), are also often used. Figure 3.19 shows some examples of linear and\nnon-linear ﬁlters being used to remove noise.\nWhen measuring the effectiveness of image denoising algorithms, it is common to report\nthe results as a peak signal-to-noise ratio (PSNR) measurement (2.119), where I(x) is the\noriginal (noise-free) image and ˆI(x) is the image after denoising; this is for the case where the\nnoisy image has been synthetically generated, so that the clean image is known. A better way\nto measure the quality is to use a perceptually based similarity metric, such as the structural\nsimilarity (SSIM) index (Wang, Bovik, Sheikh et al. 2004; Wang, Bovik, and Simoncelli\n2005).\nExercises 3.11, 3.16, 3.17, 3.21, and 3.28 have you implement some of these operations\nand compare their effectiveness. More sophisticated techniques for blur removal and the\nrelated task of super-resolution are discussed in Section 10.3.\n3.5 Pyramids and wavelets\nSo far in this chapter, all of the image transformations we have studied produce output images\nof the same size as the inputs. Often, however, we may wish to change the resolution of an\nimage before proceeding further. For example, we may need to interpolate a small image to\nmake its resolution match that of the output printer or computer screen. Alternatively, we\nmay want to reduce the size of an image to speed up the execution of an algorithm or to save\non storage space or transmission time.\nSometimes, we do not even know what the appropriate resolution for the image should\nbe. Consider, for example, the task of ﬁnding a face in an image (Section 14.1.1). Since we\ndo not know the scale at which the face will appear, we need to generate a whole pyramid\n3.5 Pyramids and wavelets\n145\nof differently sized images and scan each one for possible faces. (Biological visual systems\nalso operate on a hierarchy of scales (Marr 1982).) Such a pyramid can also be very helpful\nin accelerating the search for an object by ﬁrst ﬁnding a smaller instance of that object at a\ncoarser level of the pyramid and then looking for the full resolution object only in the vicinity\nof coarse-level detections (Section 8.1.1). Finally, image pyramids are extremely useful for\nperforming multi-scale editing operations such as blending images while maintaining details.\nIn this section, we ﬁrst discuss good ﬁlters for changing image resolution, i.e., upsampling\n(interpolation, Section 3.5.1) and downsampling (decimation, Section 3.5.2). We then present\nthe concept of multi-resolution pyramids, which can be used to create a complete hierarchy\nof differently sized images and to enable a variety of applications (Section 3.5.3). A closely\nrelated concept is that of wavelets, which are a special kind of pyramid with higher frequency\nselectivity and other useful properties (Section 3.5.4). Finally, we present a useful application\nof pyramids, namely the blending of different images in a way that hides the seams between\nthe image boundaries (Section 3.5.5).\n3.5.1 Interpolation\nIn order to interpolate (or upsample) an image to a higher resolution, we need to select some\ninterpolation kernel with which to convolve the image,\ng(i, j) =\nX\nk,l\nf(k, l)h(i −rk, j −rl).\n(3.78)\nThis formula is related to the discrete convolution formula (3.14), except that we replace k\nand l in h() with rk and rl, where r is the upsampling rate. Figure 3.27a shows how to think\nof this process as the superposition of sample weighted interpolation kernels, one centered\nat each input sample k. An alternative mental model is shown in Figure 3.27b, where the\nkernel is centered at the output pixel value i (the two forms are equivalent). The latter form\nis sometimes called the polyphase ﬁlter form, since the kernel values h(i) can be stored as r\nseparate kernels, each of which is selected for convolution with the input samples depending\non the phase of i relative to the upsampled grid.\nWhat kinds of kernel make good interpolators? The answer depends on the application\nand the computation time involved. Any of the smoothing kernels shown in Tables 3.2 and 3.3\ncan be used after appropriate re-scaling.13 The linear interpolator (corresponding to the tent\nkernel) produces interpolating piecewise linear curves, which result in unappealing creases\nwhen applied to images (Figure 3.28a). The cubic B-spline, whose discrete 1/2-pixel sam-\npling appears as the binomial kernel in Table 3.3, is an approximating kernel (the interpolated\n13 The smoothing kernels in Table 3.3 have a unit area. To turn them into interpolating kernels, we simply scale\nthem up by the interpolation rate r.",
  "image_path": "page_166.jpg",
  "pages": [
    165,
    166,
    167
  ]
}