{
  "doc_id": "pages_228_230",
  "text": "206\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 4.1 A variety of feature detectors and descriptors can be used to analyze, describe and\nmatch images: (a) point-like interest operators (Brown, Szeliski, and Winder 2005) c⃝2005\nIEEE; (b) region-like interest operators (Matas, Chum, Urban et al. 2004) c⃝2004 Elsevier;\n(c) edges (Elder and Goldberg 2001) c⃝2001 IEEE; (d) straight lines (Sinha, Steedly, Szeliski\net al. 2008) c⃝2008 ACM.\n4.1 Points and patches\n207\nFeature detection and matching are an essential component of many computer vision appli-\ncations. Consider the two pairs of images shown in Figure 4.2. For the ﬁrst pair, we may\nwish to align the two images so that they can be seamlessly stitched into a composite mosaic\n(Chapter 9). For the second pair, we may wish to establish a dense set of correspondences so\nthat a 3D model can be constructed or an in-between view can be generated (Chapter 11). In\neither case, what kinds of features should you detect and then match in order to establish such\nan alignment or set of correspondences? Think about this for a few moments before reading\non.\nThe ﬁrst kind of feature that you may notice are speciﬁc locations in the images, such as\nmountain peaks, building corners, doorways, or interestingly shaped patches of snow. These\nkinds of localized feature are often called keypoint features or interest points (or even corners)\nand are often described by the appearance of patches of pixels surrounding the point location\n(Section 4.1). Another class of important features are edges, e.g., the proﬁle of mountains\nagainst the sky, (Section 4.2). These kinds of features can be matched based on their orien-\ntation and local appearance (edge proﬁles) and can also be good indicators of object bound-\naries and occlusion events in image sequences. Edges can be grouped into longer curves and\nstraight line segments, which can be directly matched or analyzed to ﬁnd vanishing points\nand hence internal and external camera parameters (Section 4.3).\nIn this chapter, we describe some practical approaches to detecting such features and\nalso discuss how feature correspondences can be established across different images. Point\nfeatures are now used in such a wide variety of applications that it is good practice to read and\nimplement some of the algorithms from (Section 4.1). Edges and lines provide information\nthat is complementary to both keypoint and region-based descriptors and are well-suited to\ndescribing object boundaries and man-made objects. These alternative descriptors, while\nextremely useful, can be skipped in a short introductory course.\n4.1 Points and patches\nPoint features can be used to ﬁnd a sparse set of corresponding locations in different im-\nages, often as a pre-cursor to computing camera pose (Chapter 7), which is a prerequisite for\ncomputing a denser set of correspondences using stereo matching (Chapter 11). Such corre-\nspondences can also be used to align different images, e.g., when stitching image mosaics or\nperforming video stabilization (Chapter 9). They are also used extensively to perform object\ninstance and category recognition (Sections 14.3 and 14.4). A key advantage of keypoints\nis that they permit matching even in the presence of clutter (occlusion) and large scale and\norientation changes.\nFeature-based correspondence techniques have been used since the early days of stereo\n208\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.2\nTwo pairs of images to be matched. What kinds of feature might one use to\nestablish a set of correspondences between these images?\nmatching (Hannah 1974; Moravec 1983; Hannah 1988) and have more recently gained pop-\nularity for image-stitching applications (Zoghlami, Faugeras, and Deriche 1997; Brown and\nLowe 2007) as well as fully automated 3D modeling (Beardsley, Torr, and Zisserman 1996;\nSchaffalitzky and Zisserman 2002; Brown and Lowe 2003; Snavely, Seitz, and Szeliski 2006).\nThere are two main approaches to ﬁnding feature points and their correspondences. The\nﬁrst is to ﬁnd features in one image that can be accurately tracked using a local search tech-\nnique, such as correlation or least squares (Section 4.1.4). The second is to independently\ndetect features in all the images under consideration and then match features based on their\nlocal appearance (Section 4.1.3). The former approach is more suitable when images are\ntaken from nearby viewpoints or in rapid succession (e.g., video sequences), while the lat-\nter is more suitable when a large amount of motion or appearance change is expected, e.g.,\nin stitching together panoramas (Brown and Lowe 2007), establishing correspondences in\nwide baseline stereo (Schaffalitzky and Zisserman 2002), or performing object recognition\n(Fergus, Perona, and Zisserman 2007).\nIn this section, we split the keypoint detection and matching pipeline into four separate\nstages. During the feature detection (extraction) stage (Section 4.1.1), each image is searched\nfor locations that are likely to match well in other images. At the feature description stage\n(Section 4.1.2), each region around detected keypoint locations is converted into a more com-\npact and stable (invariant) descriptor that can be matched against other descriptors. The",
  "image_path": "page_229.jpg",
  "pages": [
    228,
    229,
    230
  ]
}