{
  "doc_id": "pages_349_351",
  "text": "6.3 Geometric intrinsic calibration\n327\nis mounted on the TV monitor. Pose estimation is then used to infer the remote control’s\nlocation and orientation at very high frame rates. The Wii system can be extended to a variety\nof other user interaction applications by mounting the bar on a hand-held device, as described\nby Johnny Lee.11\nExercises 6.4 and 6.5 have you implement two different tracking and pose estimation sys-\ntems for augmented-reality applications. The ﬁrst system tracks the outline of a rectangular\nobject, such as a book cover or magazine page, and the second has you track the pose of a\nhand-held Rubik’s cube.\n6.3 Geometric intrinsic calibration\nAs described above in Equations (6.42–6.43), the computation of the internal (intrinsic) cam-\nera calibration parameters can occur simultaneously with the estimation of the (extrinsic)\npose of the camera with respect to a known calibration target. This, indeed, is the “classic”\napproach to camera calibration used in both the photogrammetry (Slama 1980) and the com-\nputer vision (Tsai 1987) communities. In this section, we look at alternative formulations\n(which may not involve the full solution of a non-linear regression problem), the use of alter-\nnative calibration targets, and the estimation of the non-linear part of camera optics such as\nradial distortion.12\n6.3.1 Calibration patterns\nThe use of a calibration pattern or set of markers is one of the more reliable ways to estimate\na camera’s intrinsic parameters. In photogrammetry, it is common to set up a camera in a\nlarge ﬁeld looking at distant calibration targets whose exact location has been precomputed\nusing surveying equipment (Slama 1980; Atkinson 1996; Kraus 1997). In this case, the trans-\nlational component of the pose becomes irrelevant and only the camera rotation and intrinsic\nparameters need to be recovered.\nIf a smaller calibration rig needs to be used, e.g., for indoor robotics applications or for\nmobile robots that carry their own calibration target, it is best if the calibration object can span\nas much of the workspace as possible (Figure 6.8a), as planar targets often fail to accurately\npredict the components of the pose that lie far away from the plane. A good way to determine\nif the calibration has been successfully performed is to estimate the covariance in the param-\neters (Section 6.1.4) and then project 3D points from various points in the workspace into the\nimage in order to estimate their 2D positional uncertainty.\n11 http://johnnylee.net/projects/wii/.\n12 In some applications, you can use the EXIF tags associated with a JPEG image to obtain a rough estimate of a\ncamera’s focal length but this technique should be used with caution as the results are often inaccurate.\n328\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 6.7\nCalibrating a lens by drawing straight lines on cardboard (Debevec, Wenger,\nTchou et al. 2002) c⃝2002 ACM: (a) an image taken by the video camera showing a hand\nholding a metal ruler whose right edge appears vertical in the image; (b) the set of lines drawn\non the cardboard converging on the front nodal point (center of projection) of the lens and\nindicating the horizontal ﬁeld of view.\nAn alternative method for estimating the focal length and center of projection of a lens\nis to place the camera on a large ﬂat piece of cardboard and use a long metal ruler to draw\nlines on the cardboard that appear vertical in the image, as shown in Figure 6.7a (Debevec,\nWenger, Tchou et al. 2002). Such lines lie on planes that are parallel to the vertical axis of\nthe camera sensor and also pass through the lens’ front nodal point. The location of the nodal\npoint (projected vertically onto the cardboard plane) and the horizontal ﬁeld of view (deter-\nmined from lines that graze the left and right edges of the visible image) can be recovered by\nintersecting these lines and measuring their angular extent (Figure 6.7b).\nIf no calibration pattern is available, it is also possible to perform calibration simulta-\nneously with structure and pose recovery (Sections 6.3.4 and 7.4), which is known as self-\ncalibration (Faugeras, Luong, and Maybank 1992; Hartley and Zisserman 2004; Moons, Van\nGool, and Vergauwen 2010). However, such an approach requires a large amount of imagery\nto be accurate.\nPlanar calibration patterns\nWhen a ﬁnite workspace is being used and accurate machining and motion control platforms\nare available, a good way to perform calibration is to move a planar calibration target in a\ncontrolled fashion through the workspace volume. This approach is sometimes called the N-\nplanes calibration approach (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee,\nSzeliski et al. 1992; Grossberg and Nayar 2001) and has the advantage that each camera pixel\ncan be mapped to a unique 3D ray in space, which takes care of both linear effects modeled\n6.3 Geometric intrinsic calibration\n329\n(a)\n(b)\nFigure 6.8 Calibration patterns: (a) a three-dimensional target (Quan and Lan 1999) c⃝1999\nIEEE; (b) a two-dimensional target (Zhang 2000) c⃝2000 IEEE. Note that radial distortion\nneeds to be removed from such images before the feature points can be used for calibration.\nby the calibration matrix K and non-linear effects such as radial distortion (Section 6.3.5).\nA less cumbersome but also less accurate calibration can be obtained by waving a pla-\nnar calibration pattern in front of a camera (Figure 6.8b). In this case, the pattern’s pose\nhas (in principle) to be recovered in conjunction with the intrinsics. In this technique, each\ninput image is used to compute a separate homography (6.19–6.23) ˜\nH mapping the plane’s\ncalibration points (Xi, Yi, 0) into image coordinates (xi, yi),\nxi =\n\n\nxi\nyi\n1\n\n∼K\nh\nr0\nr1\nt\ni\n\n\nXi\nYi\n1\n\n∼˜\nHpi,\n(6.49)\nwhere the ri are the ﬁrst two columns of R and ∼indicates equality up to scale. From\nthese, Zhang (2000) shows how to form linear constraints on the nine entries in the B =\nK−T K−1 matrix, from which the calibration matrix K can be recovered using a matrix\nsquare root and inversion. (The matrix B is known as the image of the absolute conic (IAC)\nin projective geometry and is commonly used for camera calibration (Hartley and Zisserman\n2004, Section 7.5).) If only the focal length is being recovered, the even simpler approach of\nusing vanishing points can be used instead.\n6.3.2 Vanishing points\nA common case for calibration that occurs often in practice is when the camera is looking at\na man-made scene with strong extended rectahedral objects such as boxes or room walls. In\nthis case, we can intersect the 2D lines corresponding to 3D parallel lines to compute their\nvanishing points, as described in Section 4.3.3, and use these to determine the intrinsic and\nextrinsic calibration parameters (Caprile and Torre 1990; Becker and Bove 1995; Liebowitz",
  "image_path": "page_350.jpg",
  "pages": [
    349,
    350,
    351
  ]
}