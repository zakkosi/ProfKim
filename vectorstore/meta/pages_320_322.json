{
  "doc_id": "pages_320_322",
  "text": "298\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 5.20 Sample weight table and its second smallest eigenvector (Shi and Malik 2000)\nc⃝2000 IEEE: (a) sample 32 × 32 weight matrix W ; (b) eigenvector corresponding to the\nsecond smallest eigenvalue of the generalized eigenvalue problem (D −W )y = λDy.\neigenvector values are associated with the two cut components. This process can be further\nrepeated to hierarchically subdivide an image, as shown in Figure 5.21.\nThe original algorithm proposed by Shi and Malik (2000) used spatial position and image\nfeature differences to compute the pixel-wise afﬁnities,\nwij = exp\n\u0012\n−∥F i −F j∥2\nσ2\nF\n−∥xi −xj∥2\nσ2s\n\u0013\n,\n(5.48)\nfor pixels within a radius ∥xi −xj∥< r, where F is a feature vector that consists of intensi-\nties, colors, or oriented ﬁlter histograms. (Note how (5.48) is the negative exponential of the\njoint feature space distance (5.42).)\nIn subsequent work, Malik, Belongie, Leung et al. (2001) look for intervening contours\nbetween pixels i and j and deﬁne an intervening contour weight\nwIC\nij = 1 −max\nx∈lij pcon(x),\n(5.49)\nwhere lij is the image line joining pixels i and j and pcon(x) is the probability of an inter-\nvening contour perpendicular to this line, which is deﬁned as the negative exponential of the\noriented energy in the perpendicular direction. They multiply these weights with a texton-\nbased texture similarity metric and use an initial over-segmentation based purely on local\npixel-wise features to re-estimate intervening contours and texture statistics in a region-based\nmanner. Figure 5.22 shows the results of running this improved algorithm on a number of\ntest images.\nBecause it requires the solution of large sparse eigenvalue problems, normalized cuts can\nbe quite slow. Sharon, Galun, Sharon et al. (2006) present a way to accelerate the com-\nputation of the normalized cuts using an approach inspired by algebraic multigrid (Brandt\n5.4 Normalized cuts\n299\nFigure 5.21 Normalized cuts segmentation (Shi and Malik 2000) c⃝2000 IEEE: The input\nimage and the components returned by the normalized cuts algorithm.\nFigure 5.22\nComparative segmentation results (Alpert, Galun, Basri et al. 2007) c⃝2007\nIEEE. “Our method” refers to the probabilistic bottom-up merging algorithm developed by\nAlpert et al.\n300\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1986; Briggs, Henson, and McCormick 2000). To coarsen the original problem, they select\na smaller number of variables such that the remaining ﬁne-level variables are strongly cou-\npled to at least one coarse-level variable. Figure 5.15 shows this process schematically, while\n(5.25) gives the deﬁnition for strong coupling except that, in this case, the original weights\nwij in the normalized cut are used instead of merge probabilities pij.\nOnce a set of coarse variables has been selected, an inter-level interpolation matrix with\nelements similar to the left hand side of (5.25) is used to deﬁne a reduced version of the nor-\nmalized cuts problem. In addition to computing the weight matrix using interpolation-based\ncoarsening, additional region statistics are used to modulate the weights. After a normalized\ncut has been computed at the coarsest level of analysis, the membership values of ﬁner-level\nnodes are computed by interpolating parent values and mapping values within ϵ = 0.1 of 0\nand 1 to pure Boolean values.\nAn example of the segmentation produced by weighted aggregation (SWA) is shown in\nFigure 5.22, along with the most recent probabilistic bottom-up merging algorithm by Alpert,\nGalun, Basri et al. (2007), which was described in Section 5.2. In even more recent work,\nWang and Oliensis (2010) show how to estimate statistics over segmentations (e.g., mean\nregion size) directly from the afﬁnity graph. They use this to produce segmentations that are\nmore central with respect to other possible segmentations.\n5.5 Graph cuts and energy-based methods\nA common theme in image segmentation algorithms is the desire to group pixels that have\nsimilar appearance (statistics) and to have the boundaries between pixels in different regions\nbe of short length and across visible discontinuities. If we restrict the boundary measurements\nto be between immediate neighbors and compute region membership statistics by summing\nover pixels, we can formulate this as a classic pixel-based energy function using either a\nvariational formulation (regularization, see Section 3.7.1) or as a binary Markov random\nﬁeld (Section 3.7.2).\nExamples of the continuous approach include (Mumford and Shah 1989; Chan and Vese\n1992; Zhu and Yuille 1996; Tabb and Ahuja 1997) along with the level set approaches dis-\ncussed in Section 5.1.4.\nAn early example of a discrete labeling problem that combines\nboth region-based and boundary-based energy terms is the work of Leclerc (1989), who used\nminimum description length (MDL) coding to derive the energy function being minimized.\nBoykov and Funka-Lea (2006) present a wonderful survey of various energy-based tech-\nniques for binary object segmentation, some of which we discuss below.\nAs we saw in Section 3.7.2, the energy corresponding to a segmentation problem can be",
  "image_path": "page_321.jpg",
  "pages": [
    320,
    321,
    322
  ]
}