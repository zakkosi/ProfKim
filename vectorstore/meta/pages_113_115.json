{
  "doc_id": "pages_113_115",
  "text": "2.3 The digital camera\n91\n(a) RGB\n(b) R\n(c) G\n(d) B\n(e) rgb\n(f) r\n(g) g\n(h) b\n(i) L*\n(j) a*\n(k) b*\n(l) H\n(m) S\n(n) V\nFigure 2.32 Color space transformations: (a–d) RGB; (e–h) rgb. (i–k) L*a*b*; (l–n) HSV.\nNote that the rgb, L*a*b*, and HSV values are all re-scaled to ﬁt the dynamic range of the\nprinted page.\nfrequency response to color than to luminance changes.) In video, it is common to subsam-\nple Cb and Cr by a factor of two horizontally; with still images (JPEG), the subsampling\n(averaging) occurs both horizontally and vertically.\nOnce the luminance and chrominance images have been appropriately subsampled and\nseparated into individual images, they are then passed to a block transform stage. The most\ncommon technique used here is the discrete cosine transform (DCT), which is a real-valued\nvariant of the discrete Fourier transform (DFT) (see Section 3.4.3). The DCT is a reasonable\napproximation to the Karhunen–Lo`eve or eigenvalue decomposition of natural image patches,\ni.e., the decomposition that simultaneously packs the most energy into the ﬁrst coefﬁcients\nand diagonalizes the joint covariance matrix among the pixels (makes transform coefﬁcients\n92\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 2.33 Image compressed with JPEG at three quality settings. Note how the amount\nof block artifact and high-frequency aliasing (“mosquito noise”) increases from left to right.\nstatistically independent). Both MPEG and JPEG use 8 × 8 DCT transforms (Wallace 1991;\nLe Gall 1991), although newer variants use smaller 4×4 blocks or alternative transformations,\nsuch as wavelets (Taubman and Marcellin 2002) and lapped transforms (Malvar 1990, 1998,\n2000) are now used.\nAfter transform coding, the coefﬁcient values are quantized into a set of small integer\nvalues that can be coded using a variable bit length scheme such as a Huffman code or an\narithmetic code (Wallace 1991). (The DC (lowest frequency) coefﬁcients are also adaptively\npredicted from the previous block’s DC values. The term “DC” comes from “direct current”,\ni.e., the non-sinusoidal or non-alternating part of a signal.) The step size in the quantization\nis the main variable controlled by the quality setting on the JPEG ﬁle (Figure 2.33).\nWith video, it is also usual to perform block-based motion compensation, i.e., to encode\nthe difference between each block and a predicted set of pixel values obtained from a shifted\nblock in the previous frame. (The exception is the motion-JPEG scheme used in older DV\ncamcorders, which is nothing more than a series of individually JPEG compressed image\nframes.) While basic MPEG uses 16 × 16 motion compensation blocks with integer motion\nvalues (Le Gall 1991), newer standards use adaptively sized block, sub-pixel motions, and\nthe ability to reference blocks from older frames. In order to recover more gracefully from\nfailures and to allow for random access to the video stream, predicted P frames are interleaved\namong independently coded I frames. (Bi-directional B frames are also sometimes used.)\nThe quality of a compression algorithm is usually reported using its peak signal-to-noise\nratio (PSNR), which is derived from the average mean square error,\nMSE = 1\nn\nX\nx\nh\nI(x) −ˆI(x)\ni2\n,\n(2.117)\nwhere I(x) is the original uncompressed image and ˆI(x) is its compressed counterpart, or\nequivalently, the root mean square error (RMS error), which is deﬁned as\nRMS =\n√\nMSE.\n(2.118)\n2.4 Additional reading\n93\nThe PSNR is deﬁned as\nPSNR = 10 log10\nI2\nmax\nMSE = 20 log10\nImax\nRMS ,\n(2.119)\nwhere Imax is the maximum signal extent, e.g., 255 for eight-bit images.\nWhile this is just a high-level sketch of how image compression works, it is useful to\nunderstand so that the artifacts introduced by such techniques can be compensated for in\nvarious computer vision applications.\n2.4 Additional reading\nAs we mentioned at the beginning of this chapter, it provides but a brief summary of a very\nrich and deep set of topics, traditionally covered in a number of separate ﬁelds.\nA more thorough introduction to the geometry of points, lines, planes, and projections\ncan be found in textbooks on multi-view geometry (Hartley and Zisserman 2004; Faugeras\nand Luong 2001) and computer graphics (Foley, van Dam, Feiner et al. 1995; Watt 1995;\nOpenGL-ARB 1997). Topics covered in more depth include higher-order primitives such as\nquadrics, conics, and cubics, as well as three-view and multi-view geometry.\nThe image formation (synthesis) process is traditionally taught as part of a computer\ngraphics curriculum (Foley, van Dam, Feiner et al. 1995; Glassner 1995; Watt 1995; Shirley\n2005) but it is also studied in physics-based computer vision (Wolff, Shafer, and Healey\n1992a).\nThe behavior of camera lens systems is studied in optics (M¨oller 1988; Hecht 2001; Ray\n2002).\nSome good books on color theory have been written by Healey and Shafer (1992); Wyszecki\nand Stiles (2000); Fairchild (2005), with Livingstone (2008) providing a more fun and infor-\nmal introduction to the topic of color perception. Mark Fairchild’s page of color books and\nlinks25 lists many other sources.\nTopics relating to sampling and aliasing are covered in textbooks on signal and image\nprocessing (Crane 1997; J¨ahne 1997; Oppenheim and Schafer 1996; Oppenheim, Schafer,\nand Buck 1999; Pratt 2007; Russ 2007; Burger and Burge 2008; Gonzales and Woods 2008).\n2.5 Exercises\nA note to students: This chapter is relatively light on exercises since it contains mostly\nbackground material and not that many usable techniques. If you really want to understand\n25 http://www.cis.rit.edu/fairchild/WhyIsColor/books links.html.",
  "image_path": "page_114.jpg",
  "pages": [
    113,
    114,
    115
  ]
}