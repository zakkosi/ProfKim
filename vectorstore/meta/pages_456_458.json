{
  "doc_id": "pages_456_458",
  "text": "434\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nintrinsically more stable than estimating a homography with a full eight degrees of freedom,\nwhich makes this the method of choice for large-scale image stitching algorithms (Szeliski\nand Shum 1997; Shum and Szeliski 2000; Brown and Lowe 2007).\nGiven this representation, how do we update the rotation matrices to best align two over-\nlapping images? Given a current estimate for the homography ˜\nH10 in (9.5), the best way to\nupdate R10 is to prepend an incremental rotation matrix R(ω) to the current estimate R10\n(Szeliski and Shum 1997; Shum and Szeliski 2000),\n˜\nH(ω) = K1R(ω)R10K−1\n0\n= [K1R(ω)K−1\n1 ][K1R10K−1\n0 ] = D ˜\nH10.\n(9.8)\nNote that here we have written the update rule in the compositional form, where the in-\ncremental update D is prepended to the current homography ˜\nH10. Using the small-angle\napproximation to R(ω) given in (2.35), we can write the incremental update matrix as\nD = K1R(ω)K−1\n1\n≈K1(I + [ω]×)K−1\n1\n=\n\n\n1\n−ωz\nf1ωy\nωz\n1\n−f1ωx\n−ωy/f1\nωx/f1\n1\n\n.\n(9.9)\nNotice how there is now a nice one-to-one correspondence between the entries in the D\nmatrix and the h00, . . . , h21 parameters used in Table 6.1 and Equation (6.19), i.e.,\n(h00, h01, h02, h00, h11, h12, h20, h21) = (0, −ωz, f1ωy, ωz, 0, −f1ωx, −ωy/f1, ωx/f1).\n(9.10)\nWe can therefore apply the chain rule to Equations (6.24 and 9.10) to obtain\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\n−xy/f1\nf1 + x2/f1\n−y\n−(f1 + y2/f1)\nxy/f1\nx\n# \n\nωx\nωy\nωz\n\n,\n(9.11)\nwhich give us the linearized update equations needed to estimate ω = (ωx, ωy, ωz).4 Notice\nthat this update rule depends on the focal length f1 of the target view and is independent\nof the focal length f0 of the template view. This is because the compositional algorithm\nessentially makes small perturbations to the target. Once the incremental rotation vector ω\nhas been computed, the R1 rotation matrix can be updated using R1 ←R(ω)R1.\nThe formulas for updating the focal length estimates are a little more involved and are\ngiven in (Shum and Szeliski 2000). We will not repeat them here, since an alternative up-\ndate rule, based on minimizing the difference between back-projected 3D rays, is given in\nSection 9.2.1. Figure 9.4 shows the alignment of four images under the 3D rotation motion\nmodel.\n4 This is the same as the rotational component of instantaneous rigid ﬂow (Bergen, Anandan, Hanna et al. 1992)\nand the update equations given by Szeliski and Shum (1997) and Shum and Szeliski (2000).\n9.1 Motion models\n435\nFigure 9.4\nFour images taken with a hand-held camera registered using a 3D rotation mo-\ntion model (Szeliski and Shum 1997) c⃝1997 ACM. Notice how the homographies, rather\nthan being arbitrary, have a well-deﬁned keystone shape whose width increases away from\nthe origin.\n9.1.4 Gap closing\nThe techniques presented in this section can be used to estimate a series of rotation matrices\nand focal lengths, which can be chained together to create large panoramas. Unfortunately,\nbecause of accumulated errors, this approach will rarely produce a closed 360◦panorama.\nInstead, there will invariably be either a gap or an overlap (Figure 9.5).\nWe can solve this problem by matching the ﬁrst image in the sequence with the last one.\nThe difference between the two rotation matrix estimates associated with the repeated ﬁrst\nindicates the amount of misregistration. This error can be distributed evenly across the whole\nsequence by taking the quotient of the two quaternions associated with these rotations and\ndividing this “error quaternion” by the number of images in the sequence (assuming relatively\nconstant inter-frame rotations). We can also update the estimated focal length based on the\namount of misregistration. To do this, we ﬁrst convert the error quaternion into a gap angle,\nθg and then update the focal length using the equation f ′ = f(1 −θg/360◦).\nFigure 9.5a shows the end of registered image sequence and the ﬁrst image. There is a\nbig gap between the last image and the ﬁrst which are in fact the same image. The gap is\n32◦because the wrong estimate of focal length (f = 510) was used. Figure 9.5b shows the\nregistration after closing the gap with the correct focal length (f = 468). Notice that both\nmosaics show very little visual misregistration (except at the gap), yet Figure 9.5a has been\ncomputed using a focal length that has 9% error. Related approaches have been developed by\nHartley (1994b), McMillan and Bishop (1995), Stein (1995), and Kang and Weiss (1997) to\nsolve the focal length estimation problem using pure panning motion and cylindrical images.\n436\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 9.5 Gap closing (Szeliski and Shum 1997) c⃝1997 ACM: (a) A gap is visible when\nthe focal length is wrong (f = 510). (b) No gap is visible for the correct focal length\n(f = 468).\nUnfortunately, this particular gap-closing heuristic only works for the kind of “one-dimensional”\npanorama where the camera is continuously turning in the same direction. In Section 9.2, we\ndescribe a different approach to removing gaps and overlaps that works for arbitrary camera\nmotions.\n9.1.5 Application: Video summarization and compression\nAn interesting application of image stitching is the ability to summarize and compress videos\ntaken with a panning camera. This application was ﬁrst suggested by Teodosio and Bender\n(1993), who called their mosaic-based summaries salient stills. These ideas were then ex-\ntended by Irani, Hsu, and Anandan (1995), Kumar, Anandan, Irani et al. (1995), and Irani and\nAnandan (1998) to additional applications, such as video compression and video indexing.\nWhile these early approaches used afﬁne motion models and were therefore restricted to long\nfocal lengths, the techniques were generalized by Lee, ge Chen, lung Bruce Lin et al. (1997)\nto full eight-parameter homographies and incorporated into the MPEG-4 video compression\nstandard, where the stitched background layers were called video sprites (Figure 9.6).\nWhile video stitching is in many ways a straightforward generalization of multiple-image\nstitching (Steedly, Pal, and Szeliski 2005; Baudisch, Tan, Steedly et al. 2006), the potential\npresence of large amounts of independent motion, camera zoom, and the desire to visualize\ndynamic events impose additional challenges. For example, moving foreground objects can\noften be removed using median ﬁltering. Alternatively, foreground objects can be extracted\ninto a separate layer (Sawhney and Ayer 1996) and later composited back into the stitched\npanoramas, sometimes as multiple instances to give the impressions of a “Chronophotograph”",
  "image_path": "page_457.jpg",
  "pages": [
    456,
    457,
    458
  ]
}