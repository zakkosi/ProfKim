{
  "doc_id": "pages_116_118",
  "text": "94\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmulti-view geometry in a thorough way, I encourage you to read and do the exercises provided\nby Hartley and Zisserman (2004). Similarly, if you want some exercises related to the image\nformation process, Glassner’s (1995) book is full of challenging problems.\nEx 2.1: Least squares intersection point and line ﬁtting—advanced\nEquation (2.4) shows\nhow the intersection of two 2D lines can be expressed as their cross product, assuming the\nlines are expressed as homogeneous coordinates.\n1. If you are given more than two lines and want to ﬁnd a point ˜x that minimizes the sum\nof squared distances to each line,\nD =\nX\ni\n(˜x · ˜li)2,\n(2.120)\nhow can you compute this quantity? (Hint: Write the dot product as ˜xT˜li and turn the\nsquared quantity into a quadratic form, ˜xT A˜x.)\n2. To ﬁt a line to a bunch of points, you can compute the centroid (mean) of the points\nas well as the covariance matrix of the points around this mean. Show that the line\npassing through the centroid along the major axis of the covariance ellipsoid (largest\neigenvector) minimizes the sum of squared distances to the points.\n3. These two approaches are fundamentally different, even though projective duality tells\nus that points and lines are interchangeable. Why are these two algorithms so appar-\nently different? Are they actually minimizing different objectives?\nEx 2.2: 2D transform editor\nWrite a program that lets you interactively create a set of\nrectangles and then modify their “pose” (2D transform). You should implement the following\nsteps:\n1. Open an empty window (“canvas”).\n2. Shift drag (rubber-band) to create a new rectangle.\n3. Select the deformation mode (motion model): translation, rigid, similarity, afﬁne, or\nperspective.\n4. Drag any corner of the outline to change its transformation.\nThis exercise should be built on a set of pixel coordinate and transformation classes, either\nimplemented by yourself or from a software library. Persistence of the created representation\n(save and load) should also be supported (for each rectangle, save its transformation).\n2.5 Exercises\n95\nEx 2.3: 3D viewer\nWrite a simple viewer for 3D points, lines, and polygons. Import a set\nof point and line commands (primitives) as well as a viewing transform. Interactively modify\nthe object or camera transform. This viewer can be an extension of the one you created in\n(Exercise 2.2). Simply replace the viewing transformations with their 3D equivalents.\n(Optional) Add a z-buffer to do hidden surface removal for polygons.\n(Optional) Use a 3D drawing package and just write the viewer control.\nEx 2.4: Focus distance and depth of ﬁeld\nFigure out how the focus distance and depth of\nﬁeld indicators on a lens are determined.\n1. Compute and plot the focus distance zo as a function of the distance traveled from the\nfocal length ∆zi = f −zi for a lens of focal length f (say, 100mm). Does this explain\nthe hyperbolic progression of focus distances you see on a typical lens (Figure 2.20)?\n2. Compute the depth of ﬁeld (minimum and maximum focus distances) for a given focus\nsetting zo as a function of the circle of confusion diameter c (make it a fraction of\nthe sensor width), the focal length f, and the f-stop number N (which relates to the\naperture diameter d). Does this explain the usual depth of ﬁeld markings on a lens that\nbracket the in-focus marker, as in Figure 2.20a?\n3. Now consider a zoom lens with a varying focal length f. Assume that as you zoom,\nthe lens stays in focus, i.e., the distance from the rear nodal point to the sensor plane\nzi adjusts itself automatically for a ﬁxed focus distance zo. How do the depth of ﬁeld\nindicators vary as a function of focal length? Can you reproduce a two-dimensional\nplot that mimics the curved depth of ﬁeld lines seen on the lens in Figure 2.20b?\nEx 2.5: F-numbers and shutter speeds\nList the common f-numbers and shutter speeds\nthat your camera provides. On older model SLRs, they are visible on the lens and shut-\nter speed dials. On newer cameras, you have to look at the electronic viewﬁnder (or LCD\nscreen/indicator) as you manually adjust exposures.\n1. Do these form geometric progressions; if so, what are the ratios? How do these relate\nto exposure values (EVs)?\n2. If your camera has shutter speeds of 1\n60 and\n1\n125, do you think that these two speeds are\nexactly a factor of two apart or a factor of 125/60 = 2.083 apart?\n3. How accurate do you think these numbers are? Can you devise some way to measure\nexactly how the aperture affects how much light reaches the sensor and what the exact\nexposure times actually are?\n96\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 2.6: Noise level calibration\nEstimate the amount of noise in your camera by taking re-\npeated shots of a scene with the camera mounted on a tripod. (Purchasing a remote shutter\nrelease is a good investment if you own a DSLR.) Alternatively, take a scene with constant\ncolor regions (such as a color checker chart) and estimate the variance by ﬁtting a smooth\nfunction to each color region and then taking differences from the predicted function.\n1. Plot your estimated variance as a function of level for each of your color channels\nseparately.\n2. Change the ISO setting on your camera; if you cannot do that, reduce the overall light\nin your scene (turn off lights, draw the curtains, wait until dusk). Does the amount of\nnoise vary a lot with ISO/gain?\n3. Compare your camera to another one at a different price point or year of make. Is\nthere evidence to suggest that “you get what you pay for”? Does the quality of digital\ncameras seem to be improving over time?\nEx 2.7: Gamma correction in image stitching\nHere’s a relatively simple puzzle. Assume\nyou are given two images that are part of a panorama that you want to stitch (see Chapter 9).\nThe two images were taken with different exposures, so you want to adjust the RGB values\nso that they match along the seam line. Is it necessary to undo the gamma in the color values\nin order to achieve this?\nEx 2.8: Skin color detection\nDevise a simple skin color detector (Forsyth and Fleck 1999;\nJones and Rehg 2001; Vezhnevets, Sazonov, and Andreeva 2003; Kakumanu, Makrogiannis,\nand Bourbakis 2007) based on chromaticity or other color properties.\n1. Take a variety of photographs of people and calculate the xy chromaticity values for\neach pixel.\n2. Crop the photos or otherwise indicate with a painting tool which pixels are likely to be\nskin (e.g. face and arms).\n3. Calculate a color (chromaticity) distribution for these pixels. You can use something as\nsimple as a mean and covariance measure or as complicated as a mean-shift segmenta-\ntion algorithm (see Section 5.3.2). You can optionally use non-skin pixels to model the\nbackground distribution.\n4. Use your computed distribution to ﬁnd the skin regions in an image. One easy way to\nvisualize this is to paint all non-skin pixels a given color, such as white or black.\n5. How sensitive is your algorithm to color balance (scene lighting)?",
  "image_path": "page_117.jpg",
  "pages": [
    116,
    117,
    118
  ]
}