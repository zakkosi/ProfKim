{
  "doc_id": "pages_252_254",
  "text": "230\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nDB\nDA\nd1\nDD\nDC\nd2\nDE\nd1\n \n’\nd2\n \n’\nFigure 4.24 Fixed threshold, nearest neighbor, and nearest neighbor distance ratio matching.\nAt a ﬁxed distance threshold (dashed circles), descriptor DA fails to match DB and DD\nincorrectly matches DC and DE. If we pick the nearest neighbor, DA correctly matches DB\nbut DD incorrectly matches DC. Using nearest neighbor distance ratio (NNDR) matching,\nthe small NNDR d1/d2 correctly matches DA with DB, and the large NNDR d′\n1/d′\n2 correctly\nrejects matches for DD.\nof thresholds can vary a lot as we move to different parts of the feature space (Lowe 2004;\nMikolajczyk and Schmid 2005). A better strategy in such cases is to simply match the nearest\nneighbor in feature space. Since some features may have no matches (e.g., they may be part\nof background clutter in object recognition or they may be occluded in the other image), a\nthreshold is still used to reduce the number of false positives.\nIdeally, this threshold itself will adapt to different regions of the feature space. If sufﬁcient\ntraining data is available (Hua, Brown, and Winder 2007), it is sometimes possible to learn\ndifferent thresholds for different features. Often, however, we are simply given a collection\nof images to match, e.g., when stitching images or constructing 3D models from unordered\nphoto collections (Brown and Lowe 2007, 2003; Snavely, Seitz, and Szeliski 2006). In this\ncase, a useful heuristic can be to compare the nearest neighbor distance to that of the second\nnearest neighbor, preferably taken from an image that is known not to match the target (e.g.,\na different object in the database) (Brown and Lowe 2002; Lowe 2004). We can deﬁne this\nnearest neighbor distance ratio (Mikolajczyk and Schmid 2005) as\nNNDR = d1\nd2\n= ∥DA −DB|\n∥DA −DC|,\n(4.18)\nwhere d1 and d2 are the nearest and second nearest neighbor distances, DA is the target\ndescriptor, and DB and DC are its closest two neighbors (Figure 4.24).\nThe effects of using these three different matching strategies for the feature descriptors\nevaluated by Mikolajczyk and Schmid (2005) are shown in Figure 4.25. As you can see, the\nnearest neighbor and NNDR strategies produce improved ROC curves.\n4.1 Points and patches\n231\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 3708\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(b)\n(c)\nFigure 4.25\nPerformance of the feature descriptors evaluated by Mikolajczyk and Schmid\n(2005) c⃝2005 IEEE, shown for three matching strategies: (a) ﬁxed threshold; (b) nearest\nneighbor; (c) nearest neighbor distance ratio (NNDR). Note how the ordering of the algo-\nrithms does not change that much, but the overall performance varies signiﬁcantly between\nthe different matching strategies.\n232\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.26 The three Haar wavelet coefﬁcients used for hashing the MOPS descriptor de-\nvised by Brown, Szeliski, and Winder (2005) are computed by summing each 8×8 normalized\npatch over the light and dark gray regions and taking their difference.\nEfﬁcient matching\nOnce we have decided on a matching strategy, we still need to search efﬁciently for poten-\ntial candidates. The simplest way to ﬁnd all corresponding feature points is to compare all\nfeatures against all other features in each pair of potentially matching images. Unfortunately,\nthis is quadratic in the number of extracted features, which makes it impractical for most\napplications.\nA better approach is to devise an indexing structure, such as a multi-dimensional search\ntree or a hash table, to rapidly search for features near a given feature. Such indexing struc-\ntures can either be built for each image independently (which is useful if we want to only\nconsider certain potential matches, e.g., searching for a particular object) or globally for all\nthe images in a given database, which can potentially be faster, since it removes the need to it-\nerate over each image. For extremely large databases (millions of images or more), even more\nefﬁcient structures based on ideas from document retrieval (e.g., vocabulary trees, (Nist´er and\nStew´enius 2006)) can be used (Section 14.3.2).\nOne of the simpler techniques to implement is multi-dimensional hashing, which maps\ndescriptors into ﬁxed size buckets based on some function applied to each descriptor vector.\nAt matching time, each new feature is hashed into a bucket, and a search of nearby buckets\nis used to return potential candidates, which can then be sorted or graded to determine which\nare valid matches.\nA simple example of hashing is the Haar wavelets used by Brown, Szeliski, and Winder\n(2005) in their MOPS paper. During the matching structure construction, each 8 × 8 scaled,\noriented, and normalized MOPS patch is converted into a three-element index by perform-\ning sums over different quadrants of the patch (Figure 4.26). The resulting three values are\nnormalized by their expected standard deviations and then mapped to the two (of b = 10)\nnearest 1D bins. The three-dimensional indices formed by concatenating the three quantized\nvalues are used to index the 23 = 8 bins where the feature is stored (added). At query time,\nonly the primary (closest) indices are used, so only a single three-dimensional bin needs to",
  "image_path": "page_253.jpg",
  "pages": [
    252,
    253,
    254
  ]
}