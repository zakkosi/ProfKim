{
  "doc_id": "pages_384_386",
  "text": "362\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 7.6\n3D teacup model reconstructed from a 240-frame video sequence (Tomasi and\nKanade 1992) c⃝1992 Springer: (a) ﬁrst frame of video; (b) last frame of video; (c) side\nview of 3D model; (d) top view of 3D model.\ncommon plane seen by all the cameras (Rother and Carlsson 2002). In a calibrated camera\nsetting, this can correspond to estimating consistent rotations for all of the cameras, for ex-\nample, using matched vanishing points (Antone and Teller 2002). Once these have been\nrecovered, the camera positions can then be obtained by solving a linear system (Antone and\nTeller 2002; Rother and Carlsson 2002; Rother 2003).\n7.3.2 Application: Sparse 3D model extraction\nOnce a multi-view 3D reconstruction of the scene has been estimated, it then becomes possi-\nble to create a texture-mapped 3D model of the object and to look at it from new directions.\nThe ﬁrst step is to create a denser 3D model than the sparse point cloud that structure\nfrom motion produces. One alternative is to run dense multi-view stereo (Sections 11.3–\n11.6). Alternatively, a simpler technique such as 3D triangulation can be used, as shown in\nFigure 7.6, in which 207 reconstructed 3D points are triangulated to produce a surface mesh.\nIn order to create a more realistic model, a texture map can be extracted for each trian-\ngle face. The equations to map points on the surface of a 3D triangle to a 2D image are\nstraightforward: just pass the local 2D coordinates on the triangle through the 3 × 4 camera\nprojection matrix to obtain a 3 × 3 homography (planar perspective projection). When mul-\ntiple source images are available, as is usually the case in multi-view reconstruction, either\nthe closest and most fronto-parallel image can be used or multiple images can be blended in\nto deal with view-dependent foreshortening (Wang, Kang, Szeliski et al. 2001) or to obtain\nsuper-resolved results (Goldluecke and Cremers 2009) Another alternative is to create a sep-\narate texture map from each reference camera and to blend between them during rendering,\nwhich is known as view-dependent texture mapping (Section 13.1.1) (Debevec, Taylor, and\nMalik 1996; Debevec, Yu, and Borshukov 1998).\n7.4 Bundle adjustment\n363\nfC(x)\n= Kx\nfj\nfP(x)\n= p/z\nfR(x)\n= Rjx\nqj\nfT(x)\n= x-cj\ncj\npi\ny(1)\ny(2)\ny(4)\nfRD(x)\n= ...\ny(3)\nκj\nρ(||x-xij||Σ)\neij\nΣij\nxij~\nxij\n^\n^\nFigure 7.7\nA set of chained transforms for projecting a 3D point pi into a 2D measure-\nment xij through a series of transformations f (k), each of which is controlled by its own\nset of parameters. The dashed lines indicate the ﬂow of information as partial derivatives\nare computed during a backward pass. The formula for the radial distortion function is\nf RD(x) = (1 + κ1r2 + κ2r4)x.\n7.4 Bundle adjustment\nAs we have mentioned several times before, the most accurate way to recover structure and\nmotion is to perform robust non-linear minimization of the measurement (re-projection) er-\nrors, which is commonly known in the photogrammetry (and now computer vision) commu-\nnities as bundle adjustment.13 Triggs, McLauchlan, Hartley et al. (1999) provide an excellent\noverview of this topic, including its historical development, pointers to the photogrammetry\nliterature (Slama 1980; Atkinson 1996; Kraus 1997), and subtle issues with gauge ambigu-\nities. The topic is also treated in depth in textbooks and surveys on multi-view geometry\n(Faugeras and Luong 2001; Hartley and Zisserman 2004; Moons, Van Gool, and Vergauwen\n2010).\nWe have already introduced the elements of bundle adjustment in our discussion on iter-\native pose estimation (Section 6.2.2), i.e., Equations (6.42–6.48) and Figure 6.5. The biggest\ndifference between these formulas and full bundle adjustment is that our feature location mea-\nsurements xij now depend not only on the point (track index) i but also on the camera pose\nindex j,\nxij = f(pi, Rj, cj, Kj),\n(7.49)\nand that the 3D point positions pi are also being simultaneously updated. In addition, it is\ncommon to add a stage for radial distortion parameter estimation (2.78),\nf RD(x) = (1 + κ1r2 + κ2r4)x,\n(7.50)\nif the cameras being used have not been pre-calibrated, as shown in Figure 7.7.\n13 The term ”bundle” refers to the bundles of rays connecting camera centers to 3D points and the term ”adjust-\nment” refers to the iterative minimization of re-projection error. Alternative terms for this in the vision community\ninclude optimal motion estimation (Weng, Ahuja, and Huang 1993) and non-linear least squares (Appendix A.3)\n(Taylor, Kriegman, and Anandan 1991; Szeliski and Kang 1994).\n364\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhile most of the boxes (transforms) in Figure 7.7 have previously been explained (6.47),\nthe leftmost box has not. This box performs a robust comparison of the predicted and mea-\nsured 2D locations ˆxij and ˜xij after re-scaling by the measurement noise covariance Σij. In\nmore detail, this operation can be written as\nrij\n=\n˜xij −ˆxij,\n(7.51)\ns2\nij\n=\nrT\nijΣ−1\nij rij,\n(7.52)\neij\n=\nˆρ(s2\nij),\n(7.53)\nwhere ˆρ(r2) = ρ(r). The corresponding Jacobians (partial derivatives) can be written as\n∂eij\n∂s2\nij\n=\nˆρ′(s2\nij),\n(7.54)\n∂s2\nij\n∂˜xij\n=\nΣ−1\nij rij.\n(7.55)\nThe advantage of the chained representation introduced above is that it not only makes\nthe computations of the partial derivatives and Jacobians simpler but it can also be adapted\nto any camera conﬁguration. Consider for example a pair of cameras mounted on a robot\nthat is moving around in the world, as shown in Figure 7.8a. By replacing the rightmost\ntwo transformations in Figure 7.7 with the transformations shown in Figure 7.8b, we can\nsimultaneously recover the position of the robot at each time and the calibration of each\ncamera with respect to the rig, in addition to the 3D structure of the world.\n7.4.1 Exploiting sparsity\nLarge bundle adjustment problems, such as those involving reconstructing 3D scenes from\nthousands of Internet photographs (Snavely, Seitz, and Szeliski 2008b; Agarwal, Snavely,\nSimon et al. 2009; Agarwal, Furukawa, Snavely et al. 2010; Snavely, Simon, Goesele et al.\n2010), can require solving non-linear least squares problems with millions of measurements\n(feature matches) and tens of thousands of unknown parameters (3D point positions and cam-\nera poses). Unless some care is taken, these kinds of problem can become intractable, since\nthe (direct) solution of dense least squares problems is cubic in the number of unknowns.\nFortunately, structure from motion is a bipartite problem in structure and motion. Each\nfeature point xij in a given image depends on one 3D point position pi and one 3D camera\npose (Rj, cj). This is illustrated in Figure 7.9a, where each circle (1–9) indicates a 3D point,\neach square (A–D) indicates a camera, and lines (edges) indicate which points are visible in\nwhich cameras (2D features). If the values for all the points are known or ﬁxed, the equations\nfor all the cameras become independent, and vice versa.",
  "image_path": "page_385.jpg",
  "pages": [
    384,
    385,
    386
  ]
}