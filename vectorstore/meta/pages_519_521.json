{
  "doc_id": "pages_519_521",
  "text": "10.3 Super-resolution and blur removal\n497\nFigure 10.30\nFlash/no-ﬂash photography algorithm (Petschnigg, Agrawala, Hoppe et al.\n2004) c⃝2004 ACM. The ambient (no-ﬂash) image A is ﬁltered with a regular bilateral ﬁlter\nto produce ABase, which is used in shadow and specularity regions, and a joint bilaterally\nﬁltered noise reduced image ANR. The ﬂash image F is bilaterally ﬁltered to produce a\nbase image F Base and a detail (ratio) image F Detail, which is used to modulate the de-\nnoised ambient image. The shadow/specularity mask M is computed by comparing linearized\nversions of the ﬂash and no-ﬂash images.\n10.3 Super-resolution and blur removal\nWhile high dynamic range imaging enables us to obtain an image with a larger dynamic\nrange than a single regular image, super-resolution enables us to create images with higher\nspatial resolution and less noise than regular camera images (Chaudhuri 2001; Park, Park,\nand Kang 2003; Capel and Zisserman 2003; Capel 2004; van Ouwerkerk 2006). Most com-\nmonly, super-resolution refers to the process of aligning and combining several input images\nto produce such high-resolution composites (Irani and Peleg 1991; Cheeseman, Kanefsky,\nHanson et al. 1993; Pickup, Capel, Roberts et al. 2009). However, some newer techniques\ncan super-resolve a single image (Freeman, Jones, and Pasztor 2002; Baker and Kanade 2002;\nFattal 2007) and are hence closely related to techniques for removing blur (Sections 3.4.3 and\n3.4.4).\nThe most principled way to formulate the super-resolution problem is to write down the\nstochastic image formation equations and image priors and to then use Bayesian inference to\nrecover the super-resolved (original) sharp image. We can do this by generalizing the image\n498\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nformation equations (3.75) used for image deblurring (Section 3.4.3), which we also used\nin (10.2) for blur kernel (PSF) estimation (Section 10.1.4). In this case, we have several ob-\nserved images {ok(x)}, as well as an image warping function ˆhk(x) for each observed image\n(Figure 3.47). Combining all of these elements, we get the (noisy) observation equations17\nok(x) = D{b(x) ∗s(ˆhk(x))} + nk(x),\n(10.26)\nwhere D is the downsampling operator, which operates after the super-resolved (sharp)\nwarped image s(ˆhk(x)) has been convolved with the blur kernel b(x). The above image\nformation equations lead to the following least squares problem,\nX\nk\n∥ok(x) −D{bk(x) ∗s(ˆhk(x))}∥2.\n(10.27)\nIn most super-resolution algorithms, the alignment (warping) ˆhk is estimated using one of\nthe input frames as the reference frame; either feature-based (Section 6.1.3) or direct (image-\nbased) (Section 8.2) parametric alignment techniques can be used. (A few algorithms, such\nas those described by Schultz and Stevenson (1996) or Capel (2004) use dense (per-pixel\nﬂow) estimates.) A better approach is to re-compute the alignment by directly minimizing\n(10.27) once an initial estimate of s(x) has been computed (Hardie, Barnard, and Armstrong\n1997) or to marginalize out the motion parameters altogether (Pickup, Capel, Roberts et al.\n2007)—see also the work of Protter and Elad (2009) for some related video super-resolution\nwork.\nThe point spread function (blur kernel) bk is either inferred from knowledge of the image\nformation process (e.g., the amount of motion or defocus blur and the camera sensor optics)\nor calibrated from a test image or the observed images {ok} using one of the techniques\ndescribed in Section 10.1.4. The problem of simultaneously inferring the blur kernel and the\nsharp image is known as blind image deconvolution (Kundur and Hatzinakos 1996; Levin\n2006).18\nGiven an estimate of ˆhk and bk(x), (10.27) can be re-written using matrix/vector notation\nas a large sparse least squares problem in the unknown values of the super-resolved pixels s,\nX\nk\n∥ok −DBkW ks∥2.\n(10.28)\n17 It is also possible to add an unknown bias–gain term to each observation (Capel 2004), as was done for motion\nestimation in (8.8).\n18 Notice that there is a chicken-and-egg problem if both the blur kernel and the super-resolved image are un-\nknown. This can be “broken” either using structural assumptions about the sharp image, e.g., the presence of edges\n(Joshi, Szeliski, and Kriegman 2008) or prior models for the image, such as edge sparsity (Fergus, Singh, Hertzmann\net al. 2006).\n10.3 Super-resolution and blur removal\n499\n(Recall from (3.89) that once the warping function ˆhk is known, values of s(ˆhk(x)) depend\nlinearly on those in s(x).) An efﬁcient way to solve this least squares problem is to use\npreconditioned conjugate gradient descent (Capel 2004), although some earlier algorithms,\nsuch as the one developed by Irani and Peleg (1991), used regular gradient descent (also\nknown as iterative back projection (IBP), in the computed tomography literature).\nThe above formulation assumes that warping can be expressed as a simple (sinc or bicu-\nbic) interpolated resampling of the super-resolved sharp image, followed by a stationary\n(spatially invariant) blurring (PSF) and area integration process. However, if the surface is\nseverely foreshortened, we have to take into account the spatially varying ﬁltering that occurs\nduring the image warping (Section 3.6.1), before we can then model the PSF induced by the\noptics and camera sensor (Wang, Kang, Szeliski et al. 2001; Capel 2004).\nHow well does this least squares (MLE) approach to super-resolution work? In practice,\nthis depends a lot on the amount of blur and aliasing in the camera optics, as well as the accu-\nracy in the motion and PSF estimates (Baker and Kanade 2002; Jiang, Wong, and Bao 2003;\nCapel 2004). Less blurring and more aliasing means that there is more (aliased) high fre-\nquency information available to be recovered. However, because the least squares (maximum\nlikelihood) formulation uses no image prior, a lot of high-frequency noise can be introduced\ninto the solution (Figure 10.31c).\nFor this reason, most super-resolution algorithms assume some form of image prior. The\nsimplest of these is to place a penalty on the image derivatives similar to Equations (3.105\nand 3.113), e.g.,\nX\n(i,j)\nρp(s(i, j) −s(i + 1, j)) + ρp(s(i, j) −s(i, j + 1)).\n(10.29)\nAs discussed in Section 3.7.2, when ρp is quadratic, this is a form of Tikhonov regulariza-\ntion (Section 3.7.1), and the overall problem is still linear least squares. The resulting prior\nimage model is a Gaussian Markov random ﬁeld (GMRF), which can be extended to other\n(e.g., diagonal) differences, as in (Capel 2004) (Figure 10.31).\nUnfortunately, GMRFs tend to produce solutions with visible ripples, which can also\nbe interpreted as increased noise sensitivity in middle frequencies (Exercise 3.17). A bet-\nter image prior is a robust prior that encourages piecewise continuous solutions (Black and\nRangarajan 1996), see Appendix B.3. Examples of such priors include the Huber potential\n(Schultz and Stevenson 1996; Capel and Zisserman 2003), which is a blend of a Gaussian\nwith a longer-tailed Laplacian, and the even sparser (heavier-tailed) hyper-Laplacians used\nby Levin, Fergus, Durand et al. (2007) and Krishnan and Fergus (2009). It is also possible to\nlearn the parameters for such priors using cross-validation (Capel 2004; Pickup 2007).\nWhile sparse (robust) derivative priors can reduce rippling effects and increase edge\nsharpness, they cannot hallucinate higher-frequency texture or details. To do this, a train-",
  "image_path": "page_520.jpg",
  "pages": [
    519,
    520,
    521
  ]
}