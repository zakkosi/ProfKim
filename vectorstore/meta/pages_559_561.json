{
  "doc_id": "pages_559_561",
  "text": "11.1 Epipolar geometry\n537\nthe general taxonomy proposed by Scharstein and Szeliski (2002). We begin in Section 11.1\nwith a review of the geometry of stereo image matching, i.e., how to compute for a given\npixel in one image the range of possible locations the pixel might appear at in the other\nimage, i.e., its epipolar line. We describe how to pre-warp images so that corresponding\nepipolar lines are coincident (rectiﬁcation). We also describe a general resampling algorithm\ncalled plane sweep that can be used to perform multi-image stereo matching with arbitrary\ncamera conﬁgurations.\nNext, we brieﬂy survey techniques for the sparse stereo matching of interest points and\nedge-like features (Section 11.2). We then turn to the main topic of this chapter, namely the\nestimation of a dense set of pixel-wise correspondences in the form of a disparity map (Fig-\nure 11.1c). This involves ﬁrst selecting a pixel matching criterion (Section 11.3) and then\nusing either local area-based aggregation (Section 11.4) or global optimization (Section 11.5)\nto help disambiguate potential matches. In Section 11.6, we discuss multi-view stereo meth-\nods that aim to reconstruct a complete 3D model instead of just a single disparity image\n(Figure 11.1d–f).\n11.1 Epipolar geometry\nGiven a pixel in one image, how can we compute its correspondence in the other image? In\nChapter 8, we saw that a variety of search techniques can be used to match pixels based on\ntheir local appearance as well as the motions of neighboring pixels. In the case of stereo\nmatching, however, we have some additional information available, namely the positions and\ncalibration data for the cameras that took the pictures of the same static scene (Section 7.2).\nHow can we exploit this information to reduce the number of potential correspondences,\nand hence both speed up the matching and increase its reliability? Figure 11.3a shows how a\npixel in one image x0 projects to an epipolar line segment in the other image. The segment\nis bounded at one end by the projection of the original viewing ray at inﬁnity p∞and at the\nother end by the projection of the original camera center c0 into the second camera, which\nis known as the epipole e1. If we project the epipolar line in the second image back into the\nﬁrst, we get another line (segment), this time bounded by the other corresponding epipole\ne0. Extending both line segments to inﬁnity, we get a pair of corresponding epipolar lines\n(Figure 11.3b), which are the intersection of the two image planes with the epipolar plane\nthat passes through both camera centers c0 and c1 as well as the point of interest p (Faugeras\nand Luong 2001; Hartley and Zisserman 2004).\n538\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np\nx1\nx0\n(R,t)\np∞\ne1\ne0\nc0\nc1\n       epipolar plane\np∞\np\n(R,t)\nc0\nc1\nepipolar\nlines\nx\n0\ne0\ne1\nx1\nl1\nl0\n(a)\n(b)\nFigure 11.3\nEpipolar geometry: (a) epipolar line segment corresponding to one ray; (b)\ncorresponding set of epipolar lines and their epipolar plane.\n11.1.1 Rectiﬁcation\nAs we saw in Section 7.2, the epipolar geometry for a pair of cameras is implicit in the\nrelative pose and calibrations of the cameras, and can easily be computed from seven or more\npoint matches using the fundamental matrix (or ﬁve or more points for the calibrated essential\nmatrix) (Zhang 1998a,b; Faugeras and Luong 2001; Hartley and Zisserman 2004). Once this\ngeometry has been computed, we can use the epipolar line corresponding to a pixel in one\nimage to constrain the search for corresponding pixels in the other image. One way to do this\nis to use a general correspondence algorithm, such as optical ﬂow (Section 8.4), but to only\nconsider locations along the epipolar line (or to project any ﬂow vectors that fall off back onto\nthe line).\nA more efﬁcient algorithm can be obtained by ﬁrst rectifying (i.e, warping) the input\nimages so that corresponding horizontal scanlines are epipolar lines (Loop and Zhang 1999;\nFaugeras and Luong 2001; Hartley and Zisserman 2004).2 Afterwards, it is possible to match\nhorizontal scanlines independently or to shift images horizontally while computing matching\nscores (Figure 11.4).\nA simple way to rectify the two images is to ﬁrst rotate both cameras so that they are\nlooking perpendicular to the line joining the camera centers c0 and c1. Since there is a de-\ngree of freedom in the tilt, the smallest rotations that achieve this should be used. Next, to\ndetermine the desired twist around the optical axes, make the up vector (the camera y axis)\n2 This makes most sense if the cameras are next to each other, although by rotating the cameras, rectiﬁcation can\nbe performed on any pair that is not verged too much or has too much of a scale change. In those latter cases, using\nplane sweep (below) or hypothesizing small planar patch locations in 3D (Goesele, Snavely, Curless et al. 2007) may\nbe preferable.\n11.1 Epipolar geometry\n539\n(a)\n(b)\n(c)\n(d)\nFigure 11.4\nThe multi-stage stereo rectiﬁcation algorithm of Loop and Zhang (1999) c⃝\n1999 IEEE. (a) Original image pair overlaid with several epipolar lines; (b) images trans-\nformed so that epipolar lines are parallel; (c) images rectiﬁed so that epipolar lines are hori-\nzontal and in vertial correspondence; (d) ﬁnal rectiﬁcation that minimizes horizontal distor-\ntions.\nperpendicular to the camera center line. This ensures that corresponding epipolar lines are\nhorizontal and that the disparity for points at inﬁnity is 0. Finally, re-scale the images, if nec-\nessary, to account for different focal lengths, magnifying the smaller image to avoid aliasing.\n(The full details of this procedure can be found in Fusiello, Trucco, and Verri (2000) and Ex-\nercise 11.1.) Note that in general, it is not possible to rectify an arbitrary collection of images\nsimultaneously unless their optical centers are collinear, although rotating the cameras so that\nthey all point in the same direction reduces the inter-camera pixel movements to scalings and\ntranslations.\nThe resulting standard rectiﬁed geometry is employed in a lot of stereo camera setups and\nstereo algorithms, and leads to a very simple inverse relationship between 3D depths Z and\ndisparities d,\nd = f B\nZ ,\n(11.1)\nwhere f is the focal length (measured in pixels), B is the baseline, and\nx′ = x + d(x, y), y′ = y\n(11.2)\ndescribes the relationship between corresponding pixel coordinates in the left and right im-\nages (Bolles, Baker, and Marimont 1987; Okutomi and Kanade 1993; Scharstein and Szeliski",
  "image_path": "page_560.jpg",
  "pages": [
    559,
    560,
    561
  ]
}