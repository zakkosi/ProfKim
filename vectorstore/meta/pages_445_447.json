{
  "doc_id": "pages_445_447",
  "text": "8.7 Exercises\n423\n• windowed versions of the above (8.22–8.23)\n• Fourier-based implementations of the above measures (8.18–8.20)\n• phase correlation (8.24)\n• gradient cross-correlation (Argyriou and Vlachos 2003).\nCompare a few of your algorithms on different motion sequences with different amounts of\nnoise, exposure variation, occlusion, and frequency variations (e.g., high-frequency textures,\nsuch as sand or cloth, and low-frequency images, such as clouds or motion-blurred video).\nSome datasets with illumination variation and ground truth correspondences (horizontal mo-\ntion) can be found at http://vision.middlebury.edu/stereo/data/ (the 2005 and 2006 datasets).\nSome additional ideas, variants, and questions:\n1. When do you think that phase correlation will outperform regular correlation or SSD?\nCan you show this experimentally or justify it analytically?\n2. For the Fourier-based masked or windowed correlation and sum of squared differences,\nthe results should be the same as the direct implementations. Note that you will have\nto expand (8.5) into a sum of pairwise correlations, just as in (8.22). (This is part of the\nexercise.)\n3. For the bias–gain corrected variant of squared differences (8.9), you will also have\nto expand the terms to end up with a 3 × 3 (least squares) system of equations. If\nimplementing the Fast Fourier Transform version, you will need to ﬁgure out how all\nof these entries can be evaluated in the Fourier domain.\n4. (Optional) Implement some of the additional techniques studied by Hirschm¨uller and\nScharstein (2009) and see if your results agree with theirs.\nEx 8.2: Afﬁne registration\nImplement a coarse-to-ﬁne direct method for afﬁne and pro-\njective image alignment.\n1. Does it help to use lower-order (simpler) models at coarser levels of the pyramid\n(Bergen, Anandan, Hanna et al. 1992)?\n2. (Optional) Implement patch-based acceleration (Shum and Szeliski 2000; Baker and\nMatthews 2004).\n3. See the Baker and Matthews (2004) survey for more comparisons and ideas.\nEx 8.3: Stabilization\nWrite a program to stabilize an input video sequence. You should\nimplement the following steps, as described in Section 8.2.1:\n424\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Compute the translation (and, optionally, rotation) between successive frames with ro-\nbust outlier rejection.\n2. Perform temporal high-pass ﬁltering on the motion parameters to remove the low-\nfrequency component (smooth the motion).\n3. Compensate for the high-frequency motion, zooming in slightly (a user-speciﬁed amount)\nto avoid missing edge pixels.\n4. (Optional) Do not zoom in, but instead borrow pixels from previous or subsequent\nframes to ﬁll in.\n5. (Optional) Compensate for images that are blurry because of fast motion by “stealing”\nhigher frequencies from adjacent frames.\nEx 8.4: Optical ﬂow\nCompute optical ﬂow (spline-based or per-pixel) between two im-\nages, using one or more of the techniques described in this chapter.\n1. Test your algorithms on the motion sequences available at http://vision.middlebury.\nedu/ﬂow/ or http://people.csail.mit.edu/celiu/motionAnnotation/ and compare your re-\nsults (visually) to those available on these Web sites. If you think your algorithm is\ncompetitive with the best, consider submitting it for formal evaluation.\n2. Visualize the quality of your results by generating in-between images using frame in-\nterpolation (Exercise 8.5).\n3. What can you say about the relative efﬁciency (speed) of your approach?\nEx 8.5: Automated morphing / frame interpolation\nWrite a program to automatically morph\nbetween pairs of images. Implement the following steps, as sketched out in Section 8.5.1 and\nby Baker, Scharstein, Lewis et al. (2009):\n1. Compute the ﬂow both ways (previous exercise). Consider using a multi-frame (n > 2)\ntechnique to better deal with occluded regions.\n2. For each intermediate (morphed) image, compute a set of ﬂow vectors and which im-\nages should be used in the ﬁnal composition.\n3. Blend (cross-dissolve) the images and view with a sequence viewer.\nTry this out on images of your friends and colleagues and see what kinds of morphs you get.\nAlternatively, take a video sequence and do a high-quality slow-motion effect. Compare your\nalgorithm with simple cross-fading.\n8.7 Exercises\n425\nEx 8.6: Motion-based user interaction\nWrite a program to compute a low-resolution mo-\ntion ﬁeld in order to interactively control a simple application (Cutler and Turk 1998). For\nexample:\n1. Downsample each image using a pyramid and compute the optical ﬂow (spline-based\nor pixel-based) from the previous frame.\n2. Segment each training video sequence into different “actions” (e.g., hand moving in-\nwards, moving up, no motion) and “learn” the velocity ﬁelds associated with each one.\n(You can simply ﬁnd the mean and variance for each motion ﬁeld or use something\nmore sophisticated, such as a support vector machine (SVM).)\n3. Write a recognizer that ﬁnds successive actions of approximately the right duration and\nhook it up to an interactive application (e.g., a sound generator or a computer game).\n4. Ask your friends to test it out.\nEx 8.7: Video denoising\nImplement the algorithm sketched in Application 8.4.2. Your al-\ngorithm should contain the following steps:\n1. Compute accurate per-pixel ﬂow.\n2. Determine which pixels in the reference image have good matches with other frames.\n3. Either average all of the matched pixels or choose the sharpest image, if trying to\ncompensate for blur. Don’t forget to use regular single-frame denoising techniques as\npart of your solution, (see Section 3.4.4, Section 3.7.3, and Exercise 3.11).\n4. Devise a fall-back strategy for areas where you don’t think the ﬂow estimates are accu-\nrate enough.\nEx 8.8: Motion segmentation\nWrite a program to segment an image into separately mov-\ning regions or to reliably ﬁnd motion boundaries.\nUse the human-assisted motion segmentation database at http://people.csail.mit.edu/celiu/\nmotionAnnotation/ as some of your test data.\nEx 8.9: Layered motion estimation\nDecompose into separate layers (Section 8.5) a video\nsequence of a scene taken with a moving camera:\n1. Find the set of dominant (afﬁne or planar perspective) motions, either by computing\nthem in blocks or ﬁnding a robust estimate and then iteratively re-ﬁtting outliers.\n2. Determine which pixels go with each motion.",
  "image_path": "page_446.jpg",
  "pages": [
    445,
    446,
    447
  ]
}