{
  "doc_id": "pages_667_669",
  "text": "13.5 Video-based rendering\n645\nother hand, associates a two-layer depth map with each input image, which allows them to\naccurately model occlusion effects such as the mixed pixels that occur at object boundaries.\nTheir system, which consists of eight synchronized video cameras connected to a disk array\n(Figure 13.15a), ﬁrst uses segmentation-based stereo to extract a depth map for each input\nimage (Figure 13.15e). Near object boundaries (depth discontinuities), the background layer\nis extended along a strip behind the foreground object (Figure 13.15c) and its color is es-\ntimated from the neighboring images where it is not occluded (Figure 13.15d). Automated\nmatting techniques (Section 10.4) are then used to estimate the fractional opacity and color\nof boundary pixels in the foreground layer (Figure 13.15f).\nAt render time, given a new virtual camera that lies between two of the original cameras,\nthe layers in the neighboring cameras are rendered as texture-mapped triangles and the fore-\nground layer (which may have fractional opacities) is then composited over the background\nlayer (Figure 13.15b). The resulting two images are merged and blended by comparing their\nrespective z-buffer values. (Whenever the two z-values are sufﬁciently close, a linear blend of\nthe two colors is computed.) The interactive rendering system runs in real time using regular\ngraphics hardware. It can therefore be used to change the observer’s viewpoint while playing\nthe video or to freeze the scene and explore it in 3D. More recently, Rogmans, Lu, Bekaert\net al. (2009) have developed GPU implementations of both real-time stereo matching and\nreal-time rendering algorithms, which enable them to explore algorithmic alternatives in a\nreal-time setting.\nAt present, the depth maps computed from the eight stereo cameras using off-line stereo\nmatching have produced the highest quality depth maps associated with live video.10 They\nare therefore often used in studies of 3D video compression, which is an active area of re-\nsearch (Smolic and Kauff 2005; Gotchev and Rosenhahn 2009). Active video-rate depth\nsensing cameras, such as the 3DV Zcam (Iddan and Yahav 2001), which we discussed in\nSection 12.2.1, are another potential source of such data.\nWhen large numbers of closely spaced cameras are available, as in the Stanford Light\nField Camera (Wilburn, Joshi, Vaish et al. 2005), it may not always be necessary to compute\nexplicit depth maps to create video-based rendering effects, although the results are usually\nof higher quality if you do (Vaish, Szeliski, Zitnick et al. 2006).\n13.5.5 Application: Video-based walkthroughs\nVideo camera arrays enable the simultaneous capture of 3D dynamic scenes from multiple\nviewpoints, which can then enable the viewer to explore the scene from viewpoints near the\noriginal capture locations. What if instead we wish to capture an extended area, such as a\nhome, a movie set, or even an entire city?\n10 http://research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/.\n646\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn this case, it makes more sense to move the camera through the environment and play\nback the video as an interactive video-based walkthrough. In order to allow the viewer to\nlook around in all directions, it is preferable to use a panoramic video camera (Uyttendaele,\nCriminisi, Kang et al. 2004).11\nOne way to structure the acquisition process is to capture these images in a 2D horizontal\nplane, e.g., over a grid superimposed inside a room. The resulting sea of images (Aliaga,\nFunkhouser, Yanovsky et al. 2003) can be used to enable continuous motion between the\ncaptured locations.12 However, extending this idea to larger settings, e.g., beyond a single\nroom, can become tedious and data-intensive.\nInstead, a natural way to explore a space is often to just walk through it along some pre-\nspeciﬁed paths, just as museums or home tours guide users along a particular path, say down\nthe middle of each room.13 Similarly, city-level exploration can be achieved by driving down\nthe middle of each street and allowing the user to branch at each intersection. This idea dates\nback to the Aspen MovieMap project (Lippman 1980), which recorded analog video taken\nfrom moving cars onto videodiscs for later interactive playback.\nRecent improvements in video technology now enable the capture of panoramic (spheri-\ncal) video using a small co-located array of cameras, such as the Point Grey Ladybug cam-\nera14 (Figure 13.16b) developed by Uyttendaele, Criminisi, Kang et al. (2004) for their inter-\nactive video-based walkthrough project. In their system, the synchronized video streams from\nthe six cameras (Figure 13.16a) are stitched together into 360◦panoramas using a variety of\ntechniques developed speciﬁcally for this project.\nBecause the cameras do not share the same center of projection, parallax between the\ncameras can lead to ghosting in the overlapping ﬁelds of view (Figure 13.16c). To remove\nthis, a multi-perspective plane sweep stereo algorithm is used to estimate per-pixel depths at\neach column in the overlap area. To calibrate the cameras relative to each other, the camera\nis spun in place and a constrained structure from motion algorithm (Figure 7.8) is used to\nestimate the relative camera poses and intrinsics. Feature tracking is then run on the walk-\nthrough video in order to stabilize the video sequence—Liu, Gleicher, Jin et al. (2009) have\ncarried out more recent work along these lines.\nIndoor environments with windows, as well as sunny outdoor environments with strong\nshadows, often have a dynamic range that exceeds the capabilities of video sensors. For\nthis reason, the Ladybug camera has a programmable exposure capability that enables the\nbracketing of exposures at subsequent video frames. In order to merge the resulting video\n11 See http://www.cis.upenn.edu/∼kostas/omni.html for descriptions of panoramic (omnidirectional) vision sys-\ntems and associated workshops.\n12 (The Photo Tourism system of Snavely, Seitz, and Szeliski (2006) applies this idea to less structured collections.\n13 In computer games, restricting a player to forward and backward motion along predetermined paths is called\nrail-based gaming.\n14 http://www.ptgrey.com/.\n13.5 Video-based rendering\n647\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 13.16 Video-based walkthroughs (Uyttendaele, Criminisi, Kang et al. 2004) c⃝2004\nIEEE: (a) system diagram of video pre-processing; (b) the Point Grey Ladybug camera; (c)\nghost removal using multi-perspective plane sweep; (d) point tracking, used both for calibra-\ntion and stabilization; (e) interactive garden walkthrough with map below; (f) overhead map\nauthoring and sound placement; (g) interactive home walkthrough with navigation bar (top)\nand icons of interest (bottom).",
  "image_path": "page_668.jpg",
  "pages": [
    667,
    668,
    669
  ]
}