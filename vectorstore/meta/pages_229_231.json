{
  "doc_id": "pages_229_231",
  "text": "4.1 Points and patches\n207\nFeature detection and matching are an essential component of many computer vision appli-\ncations. Consider the two pairs of images shown in Figure 4.2. For the ﬁrst pair, we may\nwish to align the two images so that they can be seamlessly stitched into a composite mosaic\n(Chapter 9). For the second pair, we may wish to establish a dense set of correspondences so\nthat a 3D model can be constructed or an in-between view can be generated (Chapter 11). In\neither case, what kinds of features should you detect and then match in order to establish such\nan alignment or set of correspondences? Think about this for a few moments before reading\non.\nThe ﬁrst kind of feature that you may notice are speciﬁc locations in the images, such as\nmountain peaks, building corners, doorways, or interestingly shaped patches of snow. These\nkinds of localized feature are often called keypoint features or interest points (or even corners)\nand are often described by the appearance of patches of pixels surrounding the point location\n(Section 4.1). Another class of important features are edges, e.g., the proﬁle of mountains\nagainst the sky, (Section 4.2). These kinds of features can be matched based on their orien-\ntation and local appearance (edge proﬁles) and can also be good indicators of object bound-\naries and occlusion events in image sequences. Edges can be grouped into longer curves and\nstraight line segments, which can be directly matched or analyzed to ﬁnd vanishing points\nand hence internal and external camera parameters (Section 4.3).\nIn this chapter, we describe some practical approaches to detecting such features and\nalso discuss how feature correspondences can be established across different images. Point\nfeatures are now used in such a wide variety of applications that it is good practice to read and\nimplement some of the algorithms from (Section 4.1). Edges and lines provide information\nthat is complementary to both keypoint and region-based descriptors and are well-suited to\ndescribing object boundaries and man-made objects. These alternative descriptors, while\nextremely useful, can be skipped in a short introductory course.\n4.1 Points and patches\nPoint features can be used to ﬁnd a sparse set of corresponding locations in different im-\nages, often as a pre-cursor to computing camera pose (Chapter 7), which is a prerequisite for\ncomputing a denser set of correspondences using stereo matching (Chapter 11). Such corre-\nspondences can also be used to align different images, e.g., when stitching image mosaics or\nperforming video stabilization (Chapter 9). They are also used extensively to perform object\ninstance and category recognition (Sections 14.3 and 14.4). A key advantage of keypoints\nis that they permit matching even in the presence of clutter (occlusion) and large scale and\norientation changes.\nFeature-based correspondence techniques have been used since the early days of stereo\n208\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.2\nTwo pairs of images to be matched. What kinds of feature might one use to\nestablish a set of correspondences between these images?\nmatching (Hannah 1974; Moravec 1983; Hannah 1988) and have more recently gained pop-\nularity for image-stitching applications (Zoghlami, Faugeras, and Deriche 1997; Brown and\nLowe 2007) as well as fully automated 3D modeling (Beardsley, Torr, and Zisserman 1996;\nSchaffalitzky and Zisserman 2002; Brown and Lowe 2003; Snavely, Seitz, and Szeliski 2006).\nThere are two main approaches to ﬁnding feature points and their correspondences. The\nﬁrst is to ﬁnd features in one image that can be accurately tracked using a local search tech-\nnique, such as correlation or least squares (Section 4.1.4). The second is to independently\ndetect features in all the images under consideration and then match features based on their\nlocal appearance (Section 4.1.3). The former approach is more suitable when images are\ntaken from nearby viewpoints or in rapid succession (e.g., video sequences), while the lat-\nter is more suitable when a large amount of motion or appearance change is expected, e.g.,\nin stitching together panoramas (Brown and Lowe 2007), establishing correspondences in\nwide baseline stereo (Schaffalitzky and Zisserman 2002), or performing object recognition\n(Fergus, Perona, and Zisserman 2007).\nIn this section, we split the keypoint detection and matching pipeline into four separate\nstages. During the feature detection (extraction) stage (Section 4.1.1), each image is searched\nfor locations that are likely to match well in other images. At the feature description stage\n(Section 4.1.2), each region around detected keypoint locations is converted into a more com-\npact and stable (invariant) descriptor that can be matched against other descriptors. The\n4.1 Points and patches\n209\nFigure 4.3\nImage pairs with extracted patches below. Notice how some patches can be\nlocalized or matched with higher accuracy than others.\nfeature matching stage (Section 4.1.3) efﬁciently searches for likely matching candidates in\nother images. The feature tracking stage (Section 4.1.4) is an alternative to the third stage\nthat only searches a small neighborhood around each detected feature and is therefore more\nsuitable for video processing.\nA wonderful example of all of these stages can be found in David Lowe’s (2004) paper,\nwhich describes the development and reﬁnement of his Scale Invariant Feature Transform\n(SIFT). Comprehensive descriptions of alternative techniques can be found in a series of\nsurvey and evaluation papers covering both feature detection (Schmid, Mohr, and Bauck-\nhage 2000; Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuytelaars and Mikolajczyk 2007)\nand feature descriptors (Mikolajczyk and Schmid 2005). Shi and Tomasi (1994) and Triggs\n(2004) also provide nice reviews of feature detection techniques.\n4.1.1 Feature detectors\nHow can we ﬁnd image locations where we can reliably ﬁnd correspondences with other\nimages, i.e., what are good features to track (Shi and Tomasi 1994; Triggs 2004)? Look again\nat the image pair shown in Figure 4.3 and at the three sample patches to see how well they\nmight be matched or tracked. As you may notice, textureless patches are nearly impossible\nto localize. Patches with large contrast changes (gradients) are easier to localize, although\nstraight line segments at a single orientation suffer from the aperture problem (Horn and\nSchunck 1981; Lucas and Kanade 1981; Anandan 1989), i.e., it is only possible to align\nthe patches along the direction normal to the edge direction (Figure 4.4b). Patches with",
  "image_path": "page_230.jpg",
  "pages": [
    229,
    230,
    231
  ]
}