{
  "doc_id": "pages_235_237",
  "text": "4.1 Points and patches\n213\nFigure 4.6\nUncertainty ellipse corresponding to an eigenvalue analysis of the auto-\ncorrelation matrix A.\n(1988) were the ﬁrst to propose using local maxima in rotationally invariant scalar measures\nderived from the auto-correlation matrix to locate keypoints for the purpose of sparse feature\nmatching. (Schmid, Mohr, and Bauckhage (2000); Triggs (2004) give more detailed histori-\ncal reviews of feature detection algorithms.) Both of these techniques also proposed using a\nGaussian weighting window instead of the previously used square patches, which makes the\ndetector response insensitive to in-plane image rotations.\nThe minimum eigenvalue λ0 (Shi and Tomasi 1994) is not the only quantity that can be\nused to ﬁnd keypoints. A simpler quantity, proposed by Harris and Stephens (1988), is\ndet(A) −α trace(A)2 = λ0λ1 −α(λ0 + λ1)2\n(4.9)\nwith α = 0.06. Unlike eigenvalue analysis, this quantity does not require the use of square\nroots and yet is still rotationally invariant and also downweights edge-like features where\nλ1 ≫λ0. Triggs (2004) suggests using the quantity\nλ0 −αλ1\n(4.10)\n(say, with α = 0.05), which also reduces the response at 1D edges, where aliasing errors\nsometimes inﬂate the smaller eigenvalue. He also shows how the basic 2 × 2 Hessian can be\nextended to parametric motions to detect points that are also accurately localizable in scale\nand rotation. Brown, Szeliski, and Winder (2005), on the other hand, use the harmonic mean,\ndet A\ntr A\n=\nλ0λ1\nλ0 + λ1\n,\n(4.11)\nwhich is a smoother function in the region where λ0 ≈λ1. Figure 4.7 shows isocontours\nof the various interest point operators, from which we can see how the two eigenvalues are\nblended to determine the ﬁnal interest value.\n214\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.7\nIsocontours of popular keypoint detection functions (Brown, Szeliski, and\nWinder 2004).\nEach detector looks for points where the eigenvalues λ0, λ1 of A =\nw ∗∇I∇IT are both large.\n1. Compute the horizontal and vertical derivatives of the image Ix and Iy by con-\nvolving the original image with derivatives of Gaussians (Section 3.2.3).\n2. Compute the three images corresponding to the outer products of these gradients.\n(The matrix A is symmetric, so only three entries are needed.)\n3. Convolve each of these images with a larger Gaussian.\n4. Compute a scalar interest measure using one of the formulas discussed above.\n5. Find local maxima above a certain threshold and report them as detected feature\npoint locations.\nAlgorithm 4.1 Outline of a basic feature detection algorithm.\n4.1 Points and patches\n215\n(a)\n(b)\n(c)\nFigure 4.8 Interest operator responses: (a) Sample image, (b) Harris response, and (c) DoG\nresponse. The circle sizes and colors indicate the scale at which each interest point was\ndetected. Notice how the two detectors tend to respond at complementary locations.\nThe steps in the basic auto-correlation-based keypoint detector are summarized in Algo-\nrithm 4.1. Figure 4.8 shows the resulting interest operator responses for the classic Harris\ndetector as well as the difference of Gaussian (DoG) detector discussed below.\nAdaptive non-maximal suppression (ANMS).\nWhile most feature detectors simply look\nfor local maxima in the interest function, this can lead to an uneven distribution of feature\npoints across the image, e.g., points will be denser in regions of higher contrast. To mitigate\nthis problem, Brown, Szeliski, and Winder (2005) only detect features that are both local\nmaxima and whose response value is signiﬁcantly (10%) greater than that of all of its neigh-\nbors within a radius r (Figure 4.9c–d). They devise an efﬁcient way to associate suppression\nradii with all local maxima by ﬁrst sorting them by their response strength and then creating\na second list sorted by decreasing suppression radius (Brown, Szeliski, and Winder 2005).\nFigure 4.9 shows a qualitative comparison of selecting the top n features and using ANMS.\nMeasuring repeatability.\nGiven the large number of feature detectors that have been de-\nveloped in computer vision, how can we decide which ones to use? Schmid, Mohr, and\nBauckhage (2000) were the ﬁrst to propose measuring the repeatability of feature detectors,\nwhich they deﬁne as the frequency with which keypoints detected in one image are found\nwithin ϵ (say, ϵ = 1.5) pixels of the corresponding location in a transformed image. In their\npaper, they transform their planar images by applying rotations, scale changes, illumination\nchanges, viewpoint changes, and adding noise. They also measure the information content\navailable at each detected feature point, which they deﬁne as the entropy of a set of rotation-\nally invariant local grayscale descriptors. Among the techniques they survey, they ﬁnd that\nthe improved (Gaussian derivative) version of the Harris operator with σd = 1 (scale of the\nderivative Gaussian) and σi = 2 (scale of the integration Gaussian) works best.",
  "image_path": "page_236.jpg",
  "pages": [
    235,
    236,
    237
  ]
}