{
  "doc_id": "pages_101_103",
  "text": "2.3 The digital camera\n79\n(a)\n(b)\n(c)\n(d)\nFigure 2.25\nAliasing of a two-dimensional signal: (a) original full-resolution image; (b)\ndownsampled 4× with a 25% ﬁll factor box ﬁlter; (c) downsampled 4× with a 100% ﬁll\nfactor box ﬁlter; (d) downsampled 4× with a high-quality 9-tap ﬁlter. Notice how the higher\nfrequencies are aliased into visible frequencies with the lower quality ﬁlters, while the 9-tap\nﬁlter completely removes these higher frequencies.\nIf we know the blur function of the lens and the ﬁll factor (sensor area shape and spacing)\nfor the imaging chip (plus, optionally, the response of the anti-aliasing ﬁlter), we can convolve\nthese (as described in Section 3.2) to obtain the PSF. Figure 2.26a shows the one-dimensional\ncross-section of a PSF for a lens whose blur function is assumed to be a disc of a radius\nequal to the pixel spacing s plus a sensing chip whose horizontal ﬁll factor is 80%. Taking\nthe Fourier transform of this PSF (Section 3.4), we obtain the modulation transfer function\n(MTF), from which we can estimate the amount of aliasing as the area of the Fourier magni-\ntude outside the f ≤fs Nyquist frequency.17 If we de-focus the lens so that the blur function\nhas a radius of 2s (Figure 2.26c), we see that the amount of aliasing decreases signiﬁcantly,\nbut so does the amount of image detail (frequencies closer to f = fs).\nUnder laboratory conditions, the PSF can be estimated (to pixel precision) by looking at a\npoint light source such as a pin hole in a black piece of cardboard lit from behind. However,\nthis PSF (the actual image of the pin hole) is only accurate to a pixel resolution and, while\nit can model larger blur (such as blur caused by defocus), it cannot model the sub-pixel\nshape of the PSF and predict the amount of aliasing. An alternative technique, described in\nSection 10.1.4, is to look at a calibration pattern (e.g., one consisting of slanted step edges\n(Reichenbach, Park, and Narayanswamy 1991; Williams and Burns 2001; Joshi, Szeliski, and\nKriegman 2008)) whose ideal appearance can be re-synthesized to sub-pixel precision.\nIn addition to occurring during image acquisition, aliasing can also be introduced in var-\nious image processing operations, such as resampling, upsampling, and downsampling. Sec-\ntions 3.4 and 3.5.2 discuss these issues and show how careful selection of ﬁlters can reduce\n17 The complex Fourier transform of the PSF is actually called the optical transfer function (OTF) (Williams\n1999). Its magnitude is called the modulation transfer function (MTF) and its phase is called the phase transfer\nfunction (PTF).\n80\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n(a)\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n(c)\n(d)\nFigure 2.26\nSample point spread functions (PSF): The diameter of the blur disc (blue) in\n(a) is equal to half the pixel spacing, while the diameter in (c) is twice the pixel spacing. The\nhorizontal ﬁll factor of the sensing chip is 80% and is shown in brown. The convolution of\nthese two kernels gives the point spread function, shown in green. The Fourier response of\nthe PSF (the MTF) is plotted in (b) and (d). The area above the Nyquist frequency where\naliasing occurs is shown in red.\nthe amount of aliasing that operations inject.\n2.3.2 Color\nIn Section 2.2, we saw how lighting and surface reﬂections are functions of wavelength.\nWhen the incoming light hits the imaging sensor, light from different parts of the spectrum is\nsomehow integrated into the discrete red, green, and blue (RGB) color values that we see in\na digital image. How does this process work and how can we analyze and manipulate color\nvalues?\nYou probably recall from your childhood days the magical process of mixing paint colors\nto obtain new ones. You may recall that blue+yellow makes green, red+blue makes purple,\nand red+green makes brown. If you revisited this topic at a later age, you may have learned\nthat the proper subtractive primaries are actually cyan (a light blue-green), magenta (pink),\nand yellow (Figure 2.27b), although black is also often used in four-color printing (CMYK).\n(If you ever subsequently took any painting classes, you learned that colors can have even\n2.3 The digital camera\n81\n(a)\n(b)\nFigure 2.27 Primary and secondary colors: (a) additive colors red, green, and blue can be\nmixed to produce cyan, magenta, yellow, and white; (b) subtractive colors cyan, magenta,\nand yellow can be mixed to produce red, green, blue, and black.\nmore fanciful names, such as alizarin crimson, cerulean blue, and chartreuse.) The subtractive\ncolors are called subtractive because pigments in the paint absorb certain wavelengths in the\ncolor spectrum.\nLater on, you may have learned about the additive primary colors (red, green, and blue)\nand how they can be added (with a slide projector or on a computer monitor) to produce cyan,\nmagenta, yellow, white, and all the other colors we typically see on our TV sets and monitors\n(Figure 2.27a).\nThrough what process is it possible for two different colors, such as red and green, to\ninteract to produce a third color like yellow? Are the wavelengths somehow mixed up to\nproduce a new wavelength?\nYou probably know that the correct answer has nothing to do with physically mixing\nwavelengths. Instead, the existence of three primaries is a result of the tri-stimulus (or tri-\nchromatic) nature of the human visual system, since we have three different kinds of cone,\neach of which responds selectively to a different portion of the color spectrum (Glassner 1995;\nWyszecki and Stiles 2000; Fairchild 2005; Reinhard, Ward, Pattanaik et al. 2005; Livingstone\n2008).18 Note that for machine vision applications, such as remote sensing and terrain clas-\nsiﬁcation, it is preferable to use many more wavelengths. Similarly, surveillance applications\ncan often beneﬁt from sensing in the near-infrared (NIR) range.\nCIE RGB and XYZ\nTo test and quantify the tri-chromatic theory of perception, we can attempt to reproduce all\nmonochromatic (single wavelength) colors as a mixture of three suitably chosen primaries.\n18 See also Mark Fairchild’s Web page, http://www.cis.rit.edu/fairchild/WhyIsColor/books links.html.",
  "image_path": "page_102.jpg",
  "pages": [
    101,
    102,
    103
  ]
}