{
  "doc_id": "pages_779_781",
  "text": "B.1 Estimation theory\n757\nB.1 Estimation theory\nThe study of such inference problems from noisy data is often called estimation theory (Gelb\n1974), and its extension to problems where we explicitly choose a loss function is called sta-\ntistical decision theory (Berger 1993; Hastie, Tibshirani, and Friedman 2001; Bishop 2006;\nRobert 2007). We ﬁrst start by writing down the forward process that leads from our un-\nknowns (and knowns) to a set of noise-corrupted measurements. We then devise an algorithm\nthat will give us an estimate (or set of estimates) that are both insensitive to the noise (as best\nthey can be) and also quantify the reliability of these estimates.\nThe speciﬁc equations above (B.1) are just a particular instance of a more general set of\nmeasurement equations,\nyi = f i(x) + ni.\n(B.5)\nHere, the yi are the noise-corrupted measurements, e.g., (xi, yi) in Equation (B.1), and x is\nthe unknown state vector.2\nEach measurement comes with its associated measurement model f i(x), which maps the\nunknown into that particular measurement. An alternative formulation would be to have one\ngeneral function f(x, pi) and to use a per-measurement parameter vector pi to distinguish\nbetween different measurements, e.g., (Xi, Yi, Zi) in Equation (B.1). Note that the use of the\nf i(x) form makes it straightforward to have measurements of different dimensions, which\nbecomes useful when we start adding in prior information (Appendix B.4).\nEach measurement is also contaminated with some noise ni. In Equation (B.5), we have\nindicated that ni is a zero-mean normal (Gaussian) random variable with a covariance matrix\nΣi. In general, the noise need not be Gaussian and, in fact, it is usually prudent to assume\nthat some measurements may be outliers. However, we defer this discussion to Appendix B.3,\nafter we have explored the simpler Gaussian noise case more fully. We also assume that the\nnoise vectors ni are independent. In the case where they are not (e.g., when some constant\ngain or offset contaminates all of the pixels in a given image), we can add this effect as a\nnuisance parameter to our state vector x and later estimate its value (and discard it, if so\ndesired).\nB.1.1 Likelihood for multivariate Gaussian noise\nGiven all of the noisy measurements y = {yi}, we would like to infer a probability distribu-\ntion on the unknown x vector. We can write the likelihood of having observed the {yi} given\na particular value of x as\nL = p(y|x) =\nY\ni\np(yi|x) =\nY\ni\np(yi|f i(x)) =\nY\ni\np(ni).\n(B.6)\n2 In the Kalman ﬁltering literature (Gelb 1974), it is more common to use z instead of y to denote measurements.\n758\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen each noise vector ni is a multivariate Gaussian with covariance Σi,\nni ∼N(0, Σi),\n(B.7)\nwe can write this likelihood as\nL\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2(yi −f i(x))T Σ−1\ni (yi −f i(x))\n\u0013\n(B.8)\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2∥yi −f i(x)∥2\nΣ\n−1\ni\n\u0013\n,\nwhere the matrix norm ∥x∥2\nA is a shorthand notation for xT Ax.\nThe norm ∥yi −yi∥Σ\n−1\ni\nis often called the Mahalanobis distance (5.26 and 14.14) and is\nused to measure the distance between a measurement and the mean of a multivariate Gaussian\ndistribution. Contours of equal Mahalanobis distance are equi-probability contours. Note\nthat when the measurement covariance is isotropic (the same in all directions), i.e., when\nΣi = σ2\ni I, the likelihood can be written as\nL =\nY\ni\n(2πσ2\ni )−Ni/2 exp\n\u0012\n−1\n2σ2\ni\n∥yi −f i(x)∥2\n\u0013\n,\n(B.9)\nwhere Ni is the length of the ith measurement vector yi.\nWe can more easily visualize the structure of the covariance matrix and the correspond-\ning Mahalanobis distance if we ﬁrst perform an eigenvalue or principal component analysis\n(PCA) of the covariance matrix (A.6),\nΣ = Φ diag(λ0 . . . λN−1) ΦT .\n(B.10)\nEqual-probability contours of the corresponding multi-variate Gaussian, which are also equi-\ndistance contours in the Mahalanobis distance (Figure 14.14), are multi-dimensional ellip-\nsoids whose axis directions are given by the columns of Φ (the eigenvectors) and whose\nlengths are given by the σj =\np\nλj (Figure A.1).\nIt is usually more convenient to work with the negative log likelihood, which we can think\nof as a cost or energy\nE = −log L\n=\n1\n2\nX\ni\n(yi −f i(x))T Σ−1\ni (yi −f i(x)) + k\n(B.11)\n=\n1\n2\nX\ni\n∥yi −f i(x)∥2\nΣ\n−1\ni\n+ k,\n(B.12)\nwhere k = P\ni log |2πΣi| is a constant that depends on the measurement variances, but is\nindependent of x.\nB.2 Maximum likelihood estimation and least squares\n759\nNotice that the inverse covariance Ci = Σ−1\ni\nplays the role of a weight on each of the\nmeasurement error residuals, i.e., the difference between the contaminated measurement yi\nand its uncontaminated (predicted) value f i(x). In fact, the inverse covariance is often called\nthe (Fisher) information matrix (Bishop 2006), since it tells us how much information is\ncontained in a given measurement, i.e., how well it constrains the ﬁnal estimate. We can also\nthink of this matrix as denoting the amount of conﬁdence to associate with each measurement\n(hence the letter C).\nIn this formulation, it is quite acceptable for some information matrices to be singular\n(of degenerate rank) or even zero (if the measurement is missing altogether). Rank-deﬁcient\nmeasurements often occur, for example, when using a line feature or edge to measure a 3D\nedge-like feature, since its exact position along the edge is unknown (of inﬁnite or extremely\nlarge variance) §8.1.3.\nIn order to make the distinction between the noise contaminated measurement and its\nexpected value for a particular setting of x more explicit, we adopt the notation ˜y for the\nformer (think of the tilde as the approximate or noisy value) and ˆy = f i(x) for the latter\n(think of the hat as the predicted or expected value). We can then write the negative log\nlikelihood as\nE = −log L =\nX\ni\n∥˜yi −ˆyi∥Σ\n−1\ni\n+ k.\n(B.13)\nB.2 Maximum likelihood estimation and least squares\nNow that we have presented the likelihood and log likelihood functions, how can we ﬁnd the\noptimal value for our state estimate x? One plausible choice might be to select the value of x\nthat maximizes L = p(y|x). In fact, in the absence of any prior model for x (Appendix B.4),\nwe have\nL = p(y|x) = p(y, x) = p(x|y).\nTherefore, choosing the value of x that maximizes the likelihood is equivalent to choosing\nthe maximum of our probability density estimate for x.\nWhen might this be a good idea? If the data (measurements) constrain the possible values\nof x so that they all cluster tightly around one value (e.g., if the distribution p(x|y) is a\nunimodal Gaussian), the maximum likelihood estimate is the optimal one in that it is both\nunbiased and has the least possible variance. In many other cases, e.g., if a single estimate\nis all that is required, it is still often the best estimate.3 However, if the probability is multi-\nmodal, i.e., it has several local minima in the log likelihood (Figure 5.7), much more care\n3 According to the Gauss-Markov theorem, least squares produces the best linear unbiased estimator (BLUE) for\na linear measurement model regardless of the actual noise distribution, assuming that the noise is zero mean and\nuncorrelated.",
  "image_path": "page_780.jpg",
  "pages": [
    779,
    780,
    781
  ]
}