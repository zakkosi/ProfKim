{
  "doc_id": "pages_367_369",
  "text": "7.1 Triangulation\n345\nIn the previous chapter, we saw how 2D and 3D point sets could be aligned and how such\nalignments could be used to estimate both a camera’s pose and its internal calibration parame-\nters. In this chapter, we look at the converse problem of estimating the locations of 3D points\nfrom multiple images given only a sparse set of correspondences between image features.\nWhile this process often involves simultaneously estimating both 3D geometry (structure)\nand camera pose (motion), it is commonly known as structure from motion (Ullman 1979).\nThe topics of projective geometry and structure from motion are extremely rich and\nsome excellent textbooks and surveys have been written on them (Faugeras and Luong 2001;\nHartley and Zisserman 2004; Moons, Van Gool, and Vergauwen 2010). This chapter skips\nover a lot of the richer material available in these books, such as the trifocal tensor and al-\ngebraic techniques for full self-calibration, and concentrates instead on the basics that we\nhave found useful in large-scale, image-based reconstruction problems (Snavely, Seitz, and\nSzeliski 2006).\nWe begin with a brief discussion of triangulation (Section 7.1), which is the problem of\nestimating a point’s 3D location when it is seen from multiple cameras. Next, we look at the\ntwo-frame structure from motion problem (Section 7.2), which involves the determination of\nthe epipolar geometry between two cameras and which can also be used to recover certain\ninformation about the camera intrinsics using self-calibration (Section 7.2.2). Section 7.3\nlooks at factorization approaches to simultaneously estimating structure and motion from\nlarge numbers of point tracks using orthographic approximations to the projection model.\nWe then develop a more general and useful approach to structure from motion, namely the\nsimultaneous bundle adjustment of all the camera and 3D structure parameters (Section 7.4).\nWe also look at special cases that arise when there are higher-level structures, such as lines\nand planes, in the scene (Section 7.5).\n7.1 Triangulation\nThe problem of determining a point’s 3D position from a set of corresponding image locations\nand known camera positions is known as triangulation. This problem is the converse of the\npose estimation problem we studied in Section 6.2.\nOne of the simplest ways to solve this problem is to ﬁnd the 3D point p that lies closest to\nall of the 3D rays corresponding to the 2D matching feature locations {xj} observed by cam-\neras {P j = Kj[Rj|tj]}, where tj = −Rjcj and cj is the jth camera center (2.55–2.56).\nAs you can see in Figure 7.2, these rays originate at cj in a direction ˆvj = N(R−1\nj K−1\nj xj).\nThe nearest point to p on this ray, which we denote as qj, minimizes the distance\n∥cj + djˆvj −p∥2,\n(7.1)\n346\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np\nx1\nx0\nR0\nc0\nc1\nR1\nv0\nv1\nd0\nd1\nq0\n^\n^\nq1\nFigure 7.2\n3D point triangulation by ﬁnding the point p that lies nearest to all of the optical\nrays cj + djˆvj.\nwhich has a minimum at dj = ˆvj · (p −cj). Hence,\nqj = cj + (ˆvjˆvT\nj )(p −cj) = cj + (p −cj)∥,\n(7.2)\nin the notation of Equation (2.29), and the squared distance between p and qj is\nr2\nj = ∥(I −ˆvjˆvT\nj )(p −cj)∥2 = ∥(p −cj)⊥∥2.\n(7.3)\nThe optimal value for p, which lies closest to all of the rays, can be computed as a regular\nleast squares problem by summing over all the r2\nj and ﬁnding the optimal value of p,\np =\n\nX\nj\n(I −ˆvjˆvT\nj )\n\n\n−1 \nX\nj\n(I −ˆvjˆvT\nj )cj\n\n.\n(7.4)\nAn alternative formulation, which is more statistically optimal and which can produce\nsigniﬁcantly better estimates if some of the cameras are closer to the 3D point than others, is\nto minimize the residual in the measurement equations\nxj\n=\np(j)\n00 X + p(j)\n01 Y + p(j)\n02 Z + p(j)\n03 W\np(j)\n20 X + p(j)\n21 Y + p(j)\n22 Z + p(j)\n23 W\n(7.5)\nyj\n=\np(j)\n10 X + p(j)\n11 Y + p(j)\n12 Z + p(j)\n13 W\np(j)\n20 X + p(j)\n21 Y + p(j)\n22 Z + p(j)\n23 W\n,\n(7.6)\nwhere (xj, yj) are the measured 2D feature locations and {p(j)\n00 . . . p(j)\n23 } are the known entries\nin camera matrix P j (Sutherland 1974).\nAs with Equations (6.21, 6.33, and 6.34), this set of non-linear equations can be converted\ninto a linear least squares problem by multiplying both sides of the denominator. Note that if\n7.2 Two-frame structure from motion\n347\nwe use homogeneous coordinates p = (X, Y, Z, W), the resulting set of equations is homo-\ngeneous and is best solved as a singular value decomposition (SVD) or eigenvalue problem\n(looking for the smallest singular vector or eigenvector). If we set W = 1, we can use regular\nlinear least squares, but the resulting system may be singular or poorly conditioned, i.e., if all\nof the viewing rays are parallel, as occurs for points far away from the camera.\nFor this reason, it is generally preferable to parameterize 3D points using homogeneous\ncoordinates, especially if we know that there are likely to be points at greatly varying dis-\ntances from the cameras. Of course, minimizing the set of observations (7.5–7.6) using non-\nlinear least squares, as described in (6.14 and 6.23), is preferable to using linear least squares,\nregardless of the representation chosen.\nFor the case of two observations, it turns out that the location of the point p that exactly\nminimizes the true reprojection error (7.5–7.6) can be computed using the solution of degree\nsix equations (Hartley and Sturm 1997). Another problem to watch out for with triangulation\nis the issue of chirality, i.e., ensuring that the reconstructed points lie in front of all the\ncameras (Hartley 1998). While this cannot always be guaranteed, a useful heuristic is to take\nthe points that lie behind the cameras because their rays are diverging (imagine Figure 7.2\nwhere the rays were pointing away from each other) and to place them on the plane at inﬁnity\nby setting their W values to 0.\n7.2 Two-frame structure from motion\nSo far in our study of 3D reconstruction, we have always assumed that either the 3D point\npositions or the 3D camera poses are known in advance. In this section, we take our ﬁrst look\nat structure from motion, which is the simultaneous recovery of 3D structure and pose from\nimage correspondences.\nConsider Figure 7.3, which shows a 3D point p being viewed from two cameras whose\nrelative position can be encoded by a rotation R and a translation t. Since we do not know\nanything about the camera positions, without loss of generality, we can set the ﬁrst camera at\nthe origin c0 = 0 and at a canonical orientation R0 = I.\nNow notice that the observed location of point p in the ﬁrst image, p0 = d0ˆx0 is mapped\ninto the second image by the transformation\nd1ˆx1 = p1 = Rp0 + t = R(d0ˆx0) + t,\n(7.7)\nwhere ˆxj = K−1\nj xj are the (local) ray direction vectors. Taking the cross product of both\nsides with t in order to annihilate it on the right hand side yields1\nd1[t]×ˆx1 = d0[t]×Rˆx0.\n(7.8)\n1 The cross-product operator [ ]× was introduced in (2.32).",
  "image_path": "page_368.jpg",
  "pages": [
    367,
    368,
    369
  ]
}