{
  "doc_id": "pages_245_247",
  "text": "4.1 Points and patches\n223\nBias and gain normalization (MOPS).\nFor tasks that do not exhibit large amounts of fore-\nshortening, such as image stitching, simple normalized intensity patches perform reasonably\nwell and are simple to implement (Brown, Szeliski, and Winder 2005) (Figure 4.17). In or-\nder to compensate for slight inaccuracies in the feature point detector (location, orientation,\nand scale), these multi-scale oriented patches (MOPS) are sampled at a spacing of ﬁve pixels\nrelative to the detection scale, using a coarser level of the image pyramid to avoid aliasing.\nTo compensate for afﬁne photometric variations (linear exposure changes or bias and gain,\n(3.3)), patch intensities are re-scaled so that their mean is zero and their variance is one.\nScale invariant feature transform (SIFT).\nSIFT features are formed by computing the\ngradient at each pixel in a 16×16 window around the detected keypoint, using the appropriate\nlevel of the Gaussian pyramid at which the keypoint was detected. The gradient magnitudes\nare downweighted by a Gaussian fall-off function (shown as a blue circle in (Figure 4.18a) in\norder to reduce the inﬂuence of gradients far from the center, as these are more affected by\nsmall misregistrations.\nIn each 4 × 4 quadrant, a gradient orientation histogram is formed by (conceptually)\nadding the weighted gradient value to one of eight orientation histogram bins. To reduce the\neffects of location and dominant orientation misestimation, each of the original 256 weighted\ngradient magnitudes is softly added to 2 × 2 × 2 histogram bins using trilinear interpolation.\nSoftly distributing values to adjacent histogram bins is generally a good idea in any appli-\ncation where histograms are being computed, e.g., for Hough transforms (Section 4.3.2) or\nlocal histogram equalization (Section 3.1.4).\nThe resulting 128 non-negative values form a raw version of the SIFT descriptor vector.\nTo reduce the effects of contrast or gain (additive variations are already removed by the gra-\ndient), the 128-D vector is normalized to unit length. To further make the descriptor robust to\nother photometric variations, values are clipped to 0.2 and the resulting vector is once again\nrenormalized to unit length.\nPCA-SIFT.\nKe and Sukthankar (2004) propose a simpler way to compute descriptors in-\nspired by SIFT; it computes the x and y (gradient) derivatives over a 39 × 39 patch and\nthen reduces the resulting 3042-dimensional vector to 36 using principal component analysis\n(PCA) (Section 14.2.1 and Appendix A.1.2). Another popular variant of SIFT is SURF (Bay,\nTuytelaars, and Van Gool 2006), which uses box ﬁlters to approximate the derivatives and\nintegrals used in SIFT.\nGradient location-orientation histogram (GLOH).\nThis descriptor, developed by Miko-\nlajczyk and Schmid (2005), is a variant on SIFT that uses a log-polar binning structure instead\nof the four quadrants used by Lowe (2004) (Figure 4.19). The spatial bins are of radius 6,\n224\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.18 A schematic representation of Lowe’s (2004) scale invariant feature transform\n(SIFT): (a) Gradient orientations and magnitudes are computed at each pixel and weighted\nby a Gaussian fall-off function (blue circle). (b) A weighted gradient orientation histogram\nis then computed in each subregion, using trilinear interpolation. While this ﬁgure shows an\n8 × 8 pixel patch and a 2 × 2 descriptor array, Lowe’s actual implementation uses 16 × 16\npatches and a 4 × 4 array of eight-bin histograms.\n11, and 15, with eight angular bins (except for the central region), for a total of 17 spa-\ntial bins and 16 orientation bins. The 272-dimensional histogram is then projected onto\na 128-dimensional descriptor using PCA trained on a large database. In their evaluation,\nMikolajczyk and Schmid (2005) found that GLOH, which has the best performance overall,\noutperforms SIFT by a small margin.\nSteerable ﬁlters.\nSteerable ﬁlters (Section 3.2.3) are combinations of derivative of Gaus-\nsian ﬁlters that permit the rapid computation of even and odd (symmetric and anti-symmetric)\nedge-like and corner-like features at all possible orientations (Freeman and Adelson 1991).\nBecause they use reasonably broad Gaussians, they too are somewhat insensitive to localiza-\ntion and orientation errors.\nPerformance of local descriptors.\nAmong the local descriptors that Mikolajczyk and Schmid\n(2005) compared, they found that GLOH performed best, followed closely by SIFT (see Fig-\nure 4.25). They also present results for many other descriptors not covered in this book.\nThe ﬁeld of feature descriptors continues to evolve rapidly, with some of the newer tech-\nniques looking at local color information (van de Weijer and Schmid 2006; Abdel-Hakim\nand Farag 2006). Winder and Brown (2007) develop a multi-stage framework for feature\ndescriptor computation that subsumes both SIFT and GLOH (Figure 4.20a) and also allows\nthem to learn optimal parameters for newer descriptors that outperform previous hand-tuned\n4.1 Points and patches\n225\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.19 The gradient location-orientation histogram (GLOH) descriptor uses log-polar\nbins instead of square bins to compute orientation histograms (Mikolajczyk and Schmid\n2005).\ndescriptors. Hua, Brown, and Winder (2007) extend this work by learning lower-dimensional\nprojections of higher-dimensional descriptors that have the best discriminative power. Both\nof these papers use a database of real-world image patches (Figure 4.20b) obtained by sam-\npling images at locations that were reliably matched using a robust structure-from-motion\nalgorithm applied to Internet photo collections (Snavely, Seitz, and Szeliski 2006; Goesele,\nSnavely, Curless et al. 2007). In concurrent work, Tola, Lepetit, and Fua (2010) developed a\nsimilar DAISY descriptor for dense stereo matching and optimized its parameters based on\nground truth stereo data.\nWhile these techniques construct feature detectors that optimize for repeatability across\nall object classes, it is also possible to develop class- or instance-speciﬁc feature detectors that\nmaximize discriminability from other classes (Ferencz, Learned-Miller, and Malik 2008).\n4.1.3 Feature matching\nOnce we have extracted features and their descriptors from two or more images, the next step\nis to establish some preliminary feature matches between these images. In this section, we\ndivide this problem into two separate components. The ﬁrst is to select a matching strategy,\nwhich determines which correspondences are passed on to the next stage for further process-\ning. The second is to devise efﬁcient data structures and algorithms to perform this matching\nas quickly as possible. (See the discussion of related techniques in Section 14.3.2.)",
  "image_path": "page_246.jpg",
  "pages": [
    245,
    246,
    247
  ]
}