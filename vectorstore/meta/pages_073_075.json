{
  "doc_id": "pages_073_075",
  "text": "2.1 Geometric primitives and transformations\n51\nThe matrix M s is parameterized by eight unknowns: the three parameters describing\nthe rotation Rs, the three parameters describing the translation cs, and the two scale factors\n(sx, sy). Note that we ignore here the possibility of skew between the two axes on the image\nplane, since solid-state manufacturing techniques render this negligible. In practice, unless\nwe have accurate external knowledge of the sensor spacing or sensor orientation, there are\nonly seven degrees of freedom, since the distance of the sensor from the origin cannot be\nteased apart from the sensor spacing, based on external image measurement alone.\nHowever, estimating a camera model M s with the required seven degrees of freedom\n(i.e., where the ﬁrst two columns are orthogonal after an appropriate re-scaling) is impractical,\nso most practitioners assume a general 3 × 3 homogeneous matrix form.\nThe relationship between the 3D pixel center p and the 3D camera-centered point pc is\ngiven by an unknown scaling s, p = spc. We can therefore write the complete projection\nbetween pc and a homogeneous version of the pixel address ˜xs as\n˜xs = αM −1\ns pc = Kpc.\n(2.54)\nThe 3 × 3 matrix K is called the calibration matrix and describes the camera intrinsics (as\nopposed to the camera’s orientation in space, which are called the extrinsics).\nFrom the above discussion, we see that K has seven degrees of freedom in theory and\neight degrees of freedom (the full dimensionality of a 3×3 homogeneous matrix) in practice.\nWhy, then, do most textbooks on 3D computer vision and multi-view geometry (Faugeras\n1993; Hartley and Zisserman 2004; Faugeras and Luong 2001) treat K as an upper-triangular\nmatrix with ﬁve degrees of freedom?\nWhile this is usually not made explicit in these books, it is because we cannot recover\nthe full K matrix based on external measurement alone. When calibrating a camera (Chap-\nter 6) based on external 3D points or other measurements (Tsai 1987), we end up estimating\nthe intrinsic (K) and extrinsic (R, t) camera parameters simultaneously using a series of\nmeasurements,\n˜xs = K\nh\nR\nt\ni\npw = P pw,\n(2.55)\nwhere pw are known 3D world coordinates and\nP = K[R|t]\n(2.56)\nis known as the camera matrix. Inspecting this equation, we see that we can post-multiply\nK by R1 and pre-multiply [R|t] by RT\n1 , and still end up with a valid calibration. Thus, it\nis impossible based on image measurements alone to know the true orientation of the sensor\nand the true camera intrinsics.\nThe choice of an upper-triangular form for K seems to be conventional. Given a full\n3 × 4 camera matrix P = K[R|t], we can compute an upper-triangular K matrix using QR\n52\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\n0\nyc\nxs\nys\nW-1\nH-1\n(cx,cy)\n0\nf\nFigure 2.9\nSimpliﬁed camera intrinsics showing the focal length f and the optical center\n(cx, cy). The image width and height are W and H.\nfactorization (Golub and Van Loan 1996). (Note the unfortunate clash of terminologies: In\nmatrix algebra textbooks, R represents an upper-triangular (right of the diagonal) matrix; in\ncomputer vision, R is an orthogonal rotation.)\nThere are several ways to write the upper-triangular form of K. One possibility is\nK =\n\n\nfx\ns\ncx\n0\nfy\ncy\n0\n0\n1\n\n,\n(2.57)\nwhich uses independent focal lengths fx and fy for the sensor x and y dimensions. The entry\ns encodes any possible skew between the sensor axes due to the sensor not being mounted\nperpendicular to the optical axis and (cx, cy) denotes the optical center expressed in pixel\ncoordinates. Another possibility is\nK =\n\n\nf\ns\ncx\n0\naf\ncy\n0\n0\n1\n\n,\n(2.58)\nwhere the aspect ratio a has been made explicit and a common focal length f is used.\nIn practice, for many applications an even simpler form can be obtained by setting a = 1\nand s = 0,\nK =\n\n\nf\n0\ncx\n0\nf\ncy\n0\n0\n1\n\n.\n(2.59)\nOften, setting the origin at roughly the center of the image, e.g., (cx, cy) = (W/2, H/2),\nwhere W and H are the image height and width, can result in a perfectly usable camera\nmodel with a single unknown, i.e., the focal length f.\n2.1 Geometric primitives and transformations\n53\nW/2\nf\nθ/2\n(x,y,1)\n(X,Y,Z)\nZ\nFigure 2.10\nCentral projection, showing the relationship between the 3D and 2D coordi-\nnates, p and x, as well as the relationship between the focal length f, image width W, and\nthe ﬁeld of view θ.\nFigure 2.9 shows how these quantities can be visualized as part of a simpliﬁed imaging\nmodel. Note that now we have placed the image plane in front of the nodal point (projection\ncenter of the lens). The sense of the y axis has also been ﬂipped to get a coordinate system\ncompatible with the way that most imaging libraries treat the vertical (row) coordinate. Cer-\ntain graphics libraries, such as Direct3D, use a left-handed coordinate system, which can lead\nto some confusion.\nA note on focal lengths\nThe issue of how to express focal lengths is one that often causes confusion in implementing\ncomputer vision algorithms and discussing their results. This is because the focal length\ndepends on the units used to measure pixels.\nIf we number pixel coordinates using integer values, say [0, W)×[0, H), the focal length\nf and camera center (cx, cy) in (2.59) can be expressed as pixel values. How do these quan-\ntities relate to the more familiar focal lengths used by photographers?\nFigure 2.10 illustrates the relationship between the focal length f, the sensor width W,\nand the ﬁeld of view θ, which obey the formula\ntan θ\n2 = W\n2f\nor\nf = W\n2\n\u0014\ntan θ\n2\n\u0015−1\n.\n(2.60)\nFor conventional ﬁlm cameras, W = 35mm, and hence f is also expressed in millimeters.\nSince we work with digital images, it is more convenient to express W in pixels so that the\nfocal length f can be used directly in the calibration matrix K as in (2.59).\nAnother possibility is to scale the pixel coordinates so that they go from [−1, 1) along\nthe longer image dimension and [−a−1, a−1) along the shorter axis, where a ≥1 is the\nimage aspect ratio (as opposed to the sensor cell aspect ratio introduced earlier). This can be",
  "image_path": "page_074.jpg",
  "pages": [
    73,
    74,
    75
  ]
}