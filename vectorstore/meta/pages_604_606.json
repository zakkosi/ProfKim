{
  "doc_id": "pages_604_606",
  "text": "582\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhich arises naturally, since for a valid depth map z(x, y) with (p, q) = (zx, zy), we have\npy = zxy = zyx = qx.\nInstead of ﬁrst recovering the orientation ﬁelds (p, q) and integrating them to obtain a\nsurface, it is also possible to directly minimize the discrepancy in the image formation equa-\ntion (12.1) while ﬁnding the optimal depth map z(x, y) (Horn 1990). Unfortunately, shape\nfrom shading is susceptible to local minima in the search space and, like other variational\nproblems that involve the simultaneous estimation of many variables, can also suffer from\nslow convergence. Using multi-resolution techniques (Szeliski 1991a) can help accelerate\nthe convergence, while using more sophisticated optimization techniques (Dupuis and Olien-\nsis 1994) can help avoid local minima.\nIn practice, surfaces other than plaster casts are rarely of a single uniform albedo. Shape\nfrom shading therefore needs to be combined with some other technique or extended in some\nway to make it useful. One way to do this is to combine it with stereo matching (Fua and\nLeclerc 1995) or known texture (surface patterns) (White and Forsyth 2006). The stereo and\ntexture components provide information in textured regions, while shape from shading helps\nﬁll in the information across uniformly colored regions and also provides ﬁner information\nabout surface shape.\nPhotometric stereo.\nAnother way to make shape from shading more reliable is to use mul-\ntiple light sources that can be selectively turned on and off. This technique is called photo-\nmetric stereo, since the light sources play a role analogous to the cameras located at different\nlocations in traditional stereo (Woodham 1981).2 For each light source, we have a differ-\nent reﬂectance map, R1(p, q), R2(p, q), etc. Given the corresponding intensities I1, I2, etc.\nat a pixel, we can in principle recover both an unknown albedo ρ and a surface orientation\nestimate (p, q).\nFor diffuse surfaces (12.2), if we parameterize the local orientation by ˆn, we get (for\nnon-shadowed pixels) a set of linear equations of the form\nIk = ρˆn · vk,\n(12.5)\nfrom which we can recover ρˆn using linear least squares. These equations are well condi-\ntioned as long as the (three or more) vectors vk are linearly independent, i.e., they are not\nalong the same azimuth (direction away from the viewer).\nOnce the surface normals or gradients have been recovered at each pixel, they can be\nintegrated into a depth map using a variant of regularized surface ﬁtting (3.100). (Nehab,\nRusinkiewicz, Davis et al. (2005) and Harker and O’Leary (2008) have produced some recent\nwork in this area.)\n2 An alternative to turning lights on-and-off is to use three colored lights (Woodham 1994; Hernandez, Vogiatzis,\nBrostow et al. 2007; Hernandez and Vogiatzis 2010).\n12.1 Shape from X\n583\nCamera\nScene\nMirror surface \nc\nEstimate tangent planes\nScene Pattern\n(a)\n(b)\n(c)\n(d)\nFigure 12.3\nSynthetic shape from texture (Garding 1992) c⃝1992 Springer:\n(a) regular\ntexture wrapped onto a curved surface and (b) the corresponding surface normal estimates.\nShape from mirror reﬂections (Savarese, Chen, and Perona 2005) c⃝2005 Springer: (c) a\nregular pattern reﬂecting off a curved mirror gives rise to (d) curved lines, from which 3D\npoint locations and normals can be inferred.\nWhen surfaces are specular, more than three light directions may be required. In fact,\nthe irradiance equation given in (12.1) not only requires that the light sources and camera be\ndistant from the surface, it also neglects inter-reﬂections, which can be a signiﬁcant source\nof the shading observed on object surfaces, e.g., the darkening seen inside concave structures\nsuch as grooves and crevasses (Nayar, Ikeuchi, and Kanade 1991).\n12.1.2 Shape from texture\nThe variation in foreshortening observed in regular textures can also provide useful informa-\ntion about local surface orientation. Figure 12.3 shows an example of such a pattern, along\nwith the estimated local surface orientations. Shape from texture algorithms require a number\nof processing steps, including the extraction of repeated patterns or the measurement of local\nfrequencies in order to compute local afﬁne deformations, and a subsequent stage to infer lo-\ncal surface orientation. Details on these various stages can be found in the research literature\n(Witkin 1981; Ikeuchi 1981; Blostein and Ahuja 1987; Garding 1992; Malik and Rosenholtz\n1997; Lobay and Forsyth 2006).\nWhen the original pattern is regular, it is possible to ﬁt a regular but slightly deformed\ngrid to the image and use this grid for a variety of image replacement or analysis tasks (Liu,\nCollins, and Tsin 2004; Liu, Lin, and Hays 2004; Hays, Leordeanu, Efros et al. 2006; Lin,\nHays, Wu et al. 2006; Park, Brocklehurst, Collins et al. 2009). This process becomes even\neasier if specially printed textured cloth patterns are used (White and Forsyth 2006; White,\nCrane, and Forsyth 2007).\nThe deformations induced in a regular pattern when it is viewed in the reﬂection of a\ncurved mirror, as shown in Figure 12.3c–d, can be used to recover the shape of the surface\n584\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 12.4 Real time depth from defocus (Nayar, Watanabe, and Noguchi 1996) c⃝1996\nIEEE: (a) the real-time focus range sensor, which includes a half-silvered mirror between the\ntwo telecentric lenses (lower right), a prism that splits the image into two CCD sensors (lower\nleft), and an edged checkerboard pattern illuminated by a Xenon lamp (top); (b–c) input video\nframes from the two cameras along with (d) the corresponding depth map; (e–f) two frames\n(you can see the texture if you zoom in) and (g) the corresponding 3D mesh model.\n(Savarese, Chen, and Perona 2005; Rozenfeld, Shimshoni, and Lindenbaum 2007). It is is\nalso possible to infer local shape information from specular ﬂow, i.e., the motion of specu-\nlarities when viewed from a moving camera (Oren and Nayar 1997; Zisserman, Giblin, and\nBlake 1989; Swaminathan, Kang, Szeliski et al. 2002).\n12.1.3 Shape from focus\nA strong cue for object depth is the amount of blur, which increases as the object’s surface\nmoves away from the camera’s focusing distance. As shown in Figure 2.19, moving the object\nsurface away from the focus plane increases the circle of confusion, according to a formula\nthat is easy to establish using similar triangles (Exercise 2.4).\nA number of techniques have been developed to estimate depth from the amount of de-\nfocus (depth from defocus) (Pentland 1987; Nayar and Nakagawa 1994; Nayar, Watanabe,\nand Noguchi 1996; Watanabe and Nayar 1998; Chaudhuri and Rajagopalan 1999; Favaro\nand Soatto 2006). In order to make such a technique practical, a number issues need to be\naddressed:\n• The amount of blur increase in both directions as you move away from the focus plane.\nTherefore, it is necessary to use two or more images captured with different focus",
  "image_path": "page_605.jpg",
  "pages": [
    604,
    605,
    606
  ]
}