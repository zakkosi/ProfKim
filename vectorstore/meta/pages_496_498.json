{
  "doc_id": "pages_496_498",
  "text": "474\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.4\nNoise level function estimates obtained from a single color photograph (Liu,\nSzeliski, Kang et al. 2008) c⃝2008 IEEE. The colored curves are the estimated NLF ﬁt as the\nprobabilistic lower envelope of the measured deviations between the noisy piecewise-smooth\nimages. The ground truth NLFs obtained by averaging 29 images are shown in gray.\naway from large gradients and region boundaries. Plot these as a function of output level for\neach color channel, as shown in Figure 10.4. Finally, ﬁt a lower envelope to this distribution\nin order to ignore pixels or deviations that are outliers. A fully Bayesian approach to this\nproblem that models the statistical distribution of each quantity is presented by (Liu, Szeliski,\nKang et al. 2008). A simpler approach, which should produce useful results in most cases,\nis to ﬁt a low-dimensional function (e.g., positive valued B-spline) to the lower envelope (see\nExercise 10.2).\nIn more recent work, Matsushita and Lin (2007) present a technique for simultaneously\nestimating a camera’s response and noise level functions based on skew (asymmetries) in\nlevel-dependent noise distributions. Their paper also contains extensive references to previ-\nous work in these areas.\n10.1.3 Vignetting\nA common problem with using wide-angle and wide-aperture lenses is that the image tends\nto darken in the corners (Figure 10.5a). This problem is generally known as vignetting and\ncomes in several different forms, including natural, optical, and mechanical vignetting (Sec-\ntion 2.2.3) (Ray 2002). As with radiometric response function calibration, the most accurate\nway to calibrate vignetting is to use an integrating sphere or a picture of a uniformly colored\nand illuminated blank wall.\nAn alternative approach is to stitch a panoramic scene and to assume that the true radiance\nat each pixel comes from the central portion of each input image. This is easier to do if\nthe radiometric response function is already known (e.g., by shooting in RAW mode) and\nif the exposure is kept constant. If the response function, image exposures, and vignetting\nfunction are unknown, they can still be recovered by optimizing a large least squares ﬁtting\n10.1 Photometric calibration\n475\nFigure 10.5\nSingle image vignetting correction (Zheng, Yu, Kang et al. 2008) c⃝2008\nIEEE: (a) original image with strong visible vignetting; (b) vignetting compensation as de-\nscribed by Zheng, Zhou, Georgescu et al. (2006); (c–d) vignetting compensation as described\nby Zheng, Yu, Kang et al. (2008).\n(a)\n(b)\n(c)\n(d)\nFigure 10.6\nSimultaneous estimation of vignetting, exposure, and radiometric response\n(Goldman 2011) c⃝2011 IEEE: (a) original average of the input images; (b) after compen-\nsating for vignetting; (c) using gradient domain blending only (note the remaining mottled\nlook); (d) after both vignetting compensation and blending.\nproblem (Litvinov and Schechner 2005; Goldman 2011). Figure 10.6 shows an example of\nsimultaneously estimating the vignetting, exposure, and radiometric response function from\na set of overlapping photographs (Goldman 2011). Note that unless vignetting is modeled\nand compensated, regular gradient-domain image blending (Section 9.3.4) will not create an\nattractive image.\nIf only a single image is available, vignetting can be estimated by looking for slow con-\nsistent intensity variations in the radial direction. The original algorithm proposed by Zheng,\nLin, and Kang (2006) ﬁrst pre-segmented the image into smoothly varying regions and then\nperformed an analysis inside each region. Instead of pre-segmenting the image, Zheng, Yu,\nKang et al. (2008) compute the radial gradients at all the pixels and use the asymmetry in\nthis distribution (since gradients away from the center are, on average, slightly negative) to\n476\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nestimate the vignetting. Figure 10.5 shows the results of applying each of these algorithms\nto an image with a large amount of vignetting. Exercise 10.3 has you implement some of the\nabove techniques.\n10.1.4 Optical blur (spatial response) estimation\nOne ﬁnal characteristic of imaging systems that you should calibrate is the spatial response\nfunction, which encodes the optical blur that gets convolved with the incoming image to pro-\nduce the point-sampled image. The shape of the convolution kernel, which is also known as\npoint spread function (PSF) or optical transfer function, depends on several factors, including\nlens blur and radial distortion (Section 2.2.3), anti-aliasing ﬁlters in front of the sensor, and\nthe shape and extent of each active pixel area (Section 2.3) (Figure 10.2). A good estimate of\nthis function is required for applications such as multi-image super-resolution and de-blurring\n(Section 10.3).\nIn theory, one could estimate the PSF by simply observing an inﬁnitely small point light\nsource everywhere in the image. Creating an array of samples by drilling through a dark plate\nand backlighting with a very bright light source is difﬁcult in practice.\nA more practical approach is to observe an image composed of long straight lines or\nbars, since these can be ﬁtted to arbitrary precision. Because the location of a horizontal\nor vertical edge can be aliased during acquisition, slightly slanted edges are preferred. The\nproﬁle and locations of such edges can be estimated to sub-pixel precision, which makes it\npossible to estimate the PSF at sub-pixel resolutions (Reichenbach, Park, and Narayanswamy\n1991; Burns and Williams 1999; Williams and Burns 2001; Goesele, Fuchs, and Seidel 2003).\nThe thesis by Murphy (2005) contains a nice survey of all aspects of camera calibration,\nincluding the spatial frequency response (SFR), spatial uniformity, tone reproduction, color\nreproduction, noise, dynamic range, color channel registration, and depth of ﬁeld. It also\nincludes a description of a slant-edge calibration algorithm called sfrmat2.\nThe slant-edge technique can be used to recover a 1D projection of the 2D PSF, e.g.,\nslightly vertical edges are used to recover the horizontal line spread function (LSF) (Williams\n1999). The LSF is then often converted into the Fourier domain and its magnitude plotted as a\none-dimensional modulation transfer function (MTF), which indicates which image frequen-\ncies are lost (blurred) and aliased during the acquisition process (Section 2.3.1). For most\ncomputational photography applications, it is preferable to directly estimate the full 2D PSF,\nsince it can be hard to recover from its projections (Williams 1999).\nFigure 10.7 shows a pattern containing edges at all orientations, which can be used to\ndirectly recover a two-dimensional PSF. First, corners in the pattern are located by extracting\nedges in the sensed image, linking them, and ﬁnding the intersections of the circular arcs.\nNext, the ideal pattern, whose analytic form is known, is warped (using a homography) to",
  "image_path": "page_497.jpg",
  "pages": [
    496,
    497,
    498
  ]
}