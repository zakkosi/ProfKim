{
  "doc_id": "pages_583_585",
  "text": "11.6 Multi-view stereo\n561\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\n(a)\n(b)\nFigure 11.16 Spatio-temporally shiftable windows (Kang, Szeliski, and Chai 2001) c⃝2001\nIEEE: A simple three-image sequence (the middle image is the reference image), which has\na moving frontal gray square (marked F) and a stationary background. Regions B, C, D, and\nE are partially occluded. (a) A regular SSD algorithm will make mistakes when matching\npixels in these regions (e.g. the window centered on the black pixel in region B) and in\nwindows straddling depth discontinuities (the window centered on the white pixel in region\nF). (b) Shiftable windows help mitigate the problems in partially occluded regions and near\ndepth discontinuities. The shifted window centered on the white pixel in region F matches\ncorrectly in all frames. The shifted window centered on the black pixel in region B matches\ncorrectly in the left image, but requires temporal selection to disable matching the right image.\nFigure 11.15b shows an EPI corresponding to this sequence and describes in more detail how\ntemporal selection works.\nFigure11.15b shows how such spatio-temporal selection or shifting of windows corresponds\nto selecting the most likely un-occluded volumetric region in the epipolar plane image vol-\nume.\nThe results of applying these techniques to the multi-frame ﬂower garden image sequence\nare shown in Figure 11.17, which compares the results of using regular (non-shifted) SSSD\nwith spatially shifted windows and full spatio-temporal window selection.\n(The task of\napplying stereo to a rigid scene ﬁlmed with a moving camera is sometimes called motion\nstereo). Similar improvements from using spatio-temporal selection are reported by (Kang\nand Szeliski 2004) and are evident even when local measurements are combined with global\noptimization.\nWhile computing a depth map from multiple inputs outperforms pairwise stereo match-\ning, even more dramatic improvements can be obtained by estimating multiple depth maps\nsimultaneously (Szeliski 1999; Kang and Szeliski 2004). The existence of multiple depth\nmaps enables more accurate reasoning about occlusions, as regions which are occluded in\none image may be visible (and matchable) in others. The multi-view reconstruction problem\ncan be formulated as the simultaneous estimation of depth maps at key frames (Figure 8.13c)\nwhile maximizing not only photoconsistency and piecewise disparity smoothness but also the\nconsistency between disparity estimates at different frames. While Szeliski (1999) and Kang\n562\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 11.17 Local (5 × 5 window-based) matching results (Kang, Szeliski, and Chai 2001)\nc⃝2001 IEEE: (a) window that is not spatially perturbed (centered); (b) spatially perturbed\nwindow; (c) using the best ﬁve of 10 neighboring frames; (d) using the better half sequence.\nNotice how the results near the tree trunk are improved using temporal selection.\nand Szeliski (2004) use soft (penalty-based) constraints to encourage multiple disparity maps\nto be consistent, Kolmogorov and Zabih (2002) show how such consistency measures can\nbe encoded as hard constraints, which guarantee that the multiple depth maps are not only\nsimilar but actually identical in overlapping regions. Newer algorithms that simultaneously\nestimate multiple disparity maps include papers by Maitre, Shinagawa, and Do (2008) and\nZhang, Jia, Wong et al. (2008).\nA closely related topic to multi-frame stereo estimation is scene ﬂow, in which multiple\ncameras are used to capture a dynamic scene. The task is then to simultaneously recover the\n3D shape of the object at every instant in time and to estimate the full 3D motion of every\nsurface point between frames. Representative papers in this area include those by Vedula,\nBaker, Rander et al. (2005), Zhang and Kambhamettu (2003), Pons, Keriven, and Faugeras\n(2007), Huguet and Devernay (2007), and Wedel, Rabe, Vaudrey et al. (2008). Figure 11.18a\nshows an image of the 3D scene ﬂow for the tango dancer shown in Figure 11.2h–j, while\nFigure 11.18b shows 3D scene ﬂows captured from a moving vehicle for the purpose of\nobstacle avoidance. In addition to supporting mensuration and safety applications, scene\nﬂow can be used to support both spatial and temporal view interpolation (Section 13.5.4), as\ndemonstrated by Vedula, Baker, and Kanade (2005).\n11.6.1 Volumetric and 3D surface reconstruction\nAccording to Seitz, Curless, Diebel et al. (2006):\nThe goal of multi-view stereo is to reconstruct a complete 3D object model from\na collection of images taken from known camera viewpoints.\nThe most challenging but potentially most useful variant of multi-view stereo reconstruc-\ntion is to create globally consistent 3D models. This topic has a long history in computer\nvision, starting with surface mesh reconstruction techniques such as the one developed by\n11.6 Multi-view stereo\n563\n(a)\n(b)\nFigure 11.18 Three-dimensional scene ﬂow: (a) computed from a multi-camera dome sur-\nrounding the dancer shown in Figure 11.2h–j (Vedula, Baker, Rander et al. 2005) c⃝2005\nIEEE; (b) computed from stereo cameras mounted on a moving vehicle (Wedel, Rabe, Vau-\ndrey et al. 2008) c⃝2008 Springer.\nFua and Leclerc (1995) (Figure 11.19a). A variety of approaches and representations have\nbeen used to solve this problem, including 3D voxel representations (Seitz and Dyer 1999;\nSzeliski and Golland 1999; De Bonet and Viola 1999; Kutulakos and Seitz 2000; Eisert, Stein-\nbach, and Girod 2000; Slabaugh, Culbertson, Slabaugh et al. 2004; Sinha and Pollefeys 2005;\nVogiatzis, Hernandez, Torr et al. 2007; Hiep, Keriven, Pons et al. 2009), level sets (Faugeras\nand Keriven 1998; Pons, Keriven, and Faugeras 2007), polygonal meshes (Fua and Leclerc\n1995; Narayanan, Rander, and Kanade 1998; Hernandez and Schmitt 2004; Furukawa and\nPonce 2009), and multiple depth maps (Kolmogorov and Zabih 2002). Figure 11.19 shows\nrepresentative examples of 3D object models reconstructed using some of these techniques.\nIn order to organize and compare all these techniques, Seitz, Curless, Diebel et al. (2006)\ndeveloped a six-point taxonomy that can help classify algorithms according to the scene rep-\nresentation, photoconsistency measure, visibility model, shape priors, reconstruction algo-\nrithm, and initialization requirements they use. Below, we summarize some of these choices\nand list a few representative papers. For more details, please consult the full survey paper\n(Seitz, Curless, Diebel et al. 2006) and the evaluation Web site, http://vision.middlebury.edu/\nmview/, which contains pointers to even more recent papers and results.\nScene representation.\nOne of the more popular 3D representations is a uniform grid of 3D\nvoxels,7 which can be reconstructed using a variety of carving (Seitz and Dyer 1999; Kutu-\nlakos and Seitz 2000) or optimization (Sinha and Pollefeys 2005; Vogiatzis, Hernandez, Torr\net al. 2007; Hiep, Keriven, Pons et al. 2009) techniques. Level set techniques (Section 5.1.4)\nalso operate on a uniform grid but, instead of representing a binary occupancy map, they\nrepresent the signed distance to the surface (Faugeras and Keriven 1998; Pons, Keriven, and\nFaugeras 2007), which can encode a ﬁner level of detail. Polygonal meshes are another pop-\n7 For outdoor scenes that go to inﬁnity, a non-uniform gridding of space may be preferable (Slabaugh, Culbertson,\nSlabaugh et al. 2004).",
  "image_path": "page_584.jpg",
  "pages": [
    583,
    584,
    585
  ]
}