{
  "doc_id": "pages_773_775",
  "text": "A.5 Iterative techniques\n751\nConjugateGradient(C, d, x0)\n1. r0 = d −Cx0\n2. p0 = r0\n3. for k = 0 . . .\n4.\nwk = Cpk\n5.\nαk = ∥rk∥2/(pk · wk)\n6.\nxk+1 = xk + αkpk\n7.\nrk+1 = rk −αkwk\n8.\n9.\nβk+1 = ∥rk+1∥2/∥rk∥2\n10.\npk+1 = rk+1 + βkpk\nConjugateGradientLS(A, b, x0)\n1. q0 = b −Ax0, r0 = AT q0\n2. p0 = r0\n3. for k = 0 . . .\n4.\nvk = Apk\n5.\nαk = ∥rk∥2/∥vk∥2\n6.\nxk+1 = xk + αkpk\n7.\nqk+1 = qk −αkvk\n8.\nrk+1 = AT qk+1\n9.\nβk+1 = ∥rk+1∥2/∥rk∥2\n10.\npk+1 = rk+1 + βkpk\nAlgorithm A.3\nConjugate gradient and conjugate gradient least squares algorithms. The\nalgorithm is described in more detail in the text, but in brief, they choose descent directions\npk that are conjugate to each other with respect to C by computing a factor β by which to\ndiscount the previous search direction pk−1. They then ﬁnd the optimal step size α and take\na downhill step by an amount αkpk.\nA.5.2 Preconditioning\nAs we mentioned previously, the rate of convergence of the conjugate gradient algorithm\nis governed in large part by the condition number κ(C). Its effectiveness can therefore be\nincreased dramatically by reducing this number, e.g., by rescaling elements in x, which cor-\nresponds to rescaling rows and columns in C.\nIn general, preconditioning is usually thought of as a change of basis from the vector x to\na new vector\nˆx = Sx.\n(A.52)\nThe corresponding linear system being solved then becomes\nAS−1ˆx = S−1b\nor\nˆ\nAˆx = ˆb,\n(A.53)\n752\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwith a corresponding least squares energy (A.29) of the form\nEPLS = ˆxT (S−T CS−1)ˆx −2ˆxT (S−T d) + ∥ˆb∥2.\n(A.54)\nThe actual preconditioned matrix ˆC = S−T CS−1 is usually not explicitly computed. In-\nstead, Algorithm A.3 is extended to insert S−T and ST operations at the appropriate places\n(Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Saad 2003; Nocedal and\nWright 2006).\nA good preconditioner S is easy and cheap to compute, but is also a decent approximation\nto a square root of C, so that κ(S−T CS−1) is closer to 1. The simplest such choice is the\nsquare root of the diagonal matrix S = D1/2, with D = diag(C). This has the advantage\nthat any scalar change in variables (e.g., using radians instead of degrees for angular measure-\nments) has no effect on the range of convergence of the iterative technique. For problems that\nare naturally block-structured, e.g., for structure from motion, where 3D point positions or\n6D camera poses are being estimated, a block diagonal preconditioner is often a good choice.\nA wide variety of more sophisticated preconditioners have been developed over the years\n(Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Saad 2003; Nocedal and\nWright 2006), many of which can be directly applied to problems in computer vision (Byr¨od\nand øAstr¨om 2009; Jeong, Nist´er, Steedly et al. 2010; Agarwal, Snavely, Seitz et al. 2010).\nSome of these are based on an incomplete Cholesky factorization of C, i.e., one in which the\namount of ﬁll-in in R is strictly limited, e.g., to just the original non-zero elements in C.10\nOther preconditioners are based on a sparsiﬁed, e.g., tree-based or clustered, approximation\nto C (Koutis 2007; Koutis and Miller 2008; Grady 2008; Koutis, Miller, and Tolliver 2009),\nsince these are known to have efﬁcient inversion properties.\nFor grid-based image-processing applications, parallel or hierarchical preconditioners\noften perform extremely well (Yserentant 1986; Szeliski 1990b; Pentland 1994; Saad 2003;\nSzeliski 2006b). These approaches use a change of basis transformation S that resembles\nthe pyramidal or wavelet representations discussed in Section 3.5, and are hence amenable\nto parallel and GPU-based implementations. Coarser elements in the new representation\nquickly converge to the low-frequency components in the solution, while ﬁner-level elements\nencode the higher-frequency components. Some of the relationships between hierarchical\npreconditioners, incomplete Cholesky factorization, and multigrid techniques are explored\nby Saad (2003) and Szeliski (2006b).\n10 If a complete Cholesky factorization C = RT R is used, we get ˆC = R−T CR−1 = I and all iterative\nalgorithms converge in a single step, thereby obviating the need to use them, but the complete factorization is often\ntoo expensive. Note that incomplete factorization can also beneﬁt from reordering.\nA.5 Iterative techniques\n753\nA.5.3 Multigrid\nOne other class of iterative techniques widely used in computer vision is multigrid techniques\n(Briggs, Henson, and McCormick 2000; Trottenberg, Oosterlee, and Schuller 2000), which\nhave been applied to problems such as surface interpolation (Terzopoulos 1986a), optical\nﬂow (Terzopoulos 1986a; Bruhn, Weickert, Kohlberger et al. 2006), high dynamic range tone\nmapping (Fattal, Lischinski, and Werman 2002), colorization (Levin, Lischinski, and Weiss\n2004), natural image matting (Levin, Lischinski, and Weiss 2008), and segmentation (Grady\n2008).\nThe main idea behind multigrid is to form coarser (lower-resolution) versions of the prob-\nlems and use them to compute the low-frequency components of the solution. However,\nunlike simple coarse-to-ﬁne techniques, which use the coarse solutions to initialize the ﬁne\nsolution, multigrid techniques only correct the low-frequency component of the current solu-\ntion and use multiple rounds of coarsening and reﬁnement (in what are often called “V” and\n“W” patterns of motion across the pyramid) to obtain rapid convergence.\nOn certain simple homogeneous problems (such as solving Poisson equations), multigrid\ntechniques can achieve optimal performance, i.e., computation times linear in the number\nof variables. However, for more inhomogeneous problems or problems on irregular grids,\nvariants on these techniques, such as algebraic multigrid (AMG) approaches, which look at\nthe structure of C to derive coarse level problems, may be preferable. Saad (2003) has a\nnice discussion of the relationship between multigrid and parallel preconditioners and on the\nrelative merits of using multigrid or conjugate gradient approaches.",
  "image_path": "page_774.jpg",
  "pages": [
    773,
    774,
    775
  ]
}