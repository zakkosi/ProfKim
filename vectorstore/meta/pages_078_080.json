{
  "doc_id": "pages_078_080",
  "text": "56\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z,1)\nx1 = (x1,y1,1,d1)\nx0 = (x0,y0,1,d0)\n~\n~\nM10\nn0·p+c0= 0\nx1 = (x1,y1,1)\nx0 = (x0,y0,1)\n~\n~\nH10\n^\n.\n(a)\n(b)\nFigure 2.12 A point is projected into two images: (a) relationship between the 3D point co-\nordinate (X, Y, Z, 1) and the 2D projected point (x, y, 1, d); (b) planar homography induced\nby points all lying on a common plane ˆn0 · p + c0 = 0.\nMapping from one camera to another\nWhat happens when we take two images of a 3D scene from different camera positions or\norientations (Figure 2.12a)? Using the full rank 4 × 4 camera matrix ˜\nP = ˜\nKE from (2.64),\nwe can write the projection from world to screen coordinates as\n˜x0 ∼˜\nK0E0p = ˜\nP 0p.\n(2.68)\nAssuming that we know the z-buffer or disparity value d0 for a pixel in one image, we can\ncompute the 3D point location p using\np ∼E−1\n0\n˜\nK\n−1\n0 ˜x0\n(2.69)\nand then project it into another image yielding\n˜x1 ∼˜\nK1E1p = ˜\nK1E1E−1\n0\n˜\nK\n−1\n0 ˜x0 = ˜\nP 1 ˜\nP\n−1\n0 ˜x0 = M 10˜x0.\n(2.70)\nUnfortunately, we do not usually have access to the depth coordinates of pixels in a regular\nphotographic image. However, for a planar scene, as discussed above in (2.66), we can\nreplace the last row of P 0 in (2.64) with a general plane equation, ˆn0 · p + c0 that maps\npoints on the plane to d0 = 0 values (Figure 2.12b). Thus, if we set d0 = 0, we can ignore\nthe last column of M 10 in (2.70) and also its last row, since we do not care about the ﬁnal\nz-buffer depth. The mapping equation (2.70) thus reduces to\n˜x1 ∼˜\nH10˜x0,\n(2.71)\nwhere ˜\nH10 is a general 3 × 3 homography matrix and ˜x1 and ˜x0 are now 2D homogeneous\ncoordinates (i.e., 3-vectors) (Szeliski 1996).This justiﬁes the use of the 8-parameter homog-\nraphy as a general alignment model for mosaics of planar scenes (Mann and Picard 1994;\nSzeliski 1996).\n2.1 Geometric primitives and transformations\n57\nThe other special case where we do not need to know depth to perform inter-camera\nmapping is when the camera is undergoing pure rotation (Section 9.1.3), i.e., when t0 = t1.\nIn this case, we can write\n˜x1 ∼K1R1R−1\n0 K−1\n0 ˜x0 = K1R10K−1\n0 ˜x0,\n(2.72)\nwhich again can be represented with a 3 × 3 homography. If we assume that the calibration\nmatrices have known aspect ratios and centers of projection (2.59), this homography can be\nparameterized by the rotation amount and the two unknown focal lengths. This particular\nformulation is commonly used in image-stitching applications (Section 9.1.3).\nObject-centered projection\nWhen working with long focal length lenses, it often becomes difﬁcult to reliably estimate\nthe focal length from image measurements alone. This is because the focal length and the\ndistance to the object are highly correlated and it becomes difﬁcult to tease these two effects\napart. For example, the change in scale of an object viewed through a zoom telephoto lens\ncan either be due to a zoom change or a motion towards the user. (This effect was put to\ndramatic use in some of Alfred Hitchcock’s ﬁlm Vertigo, where the simultaneous change of\nzoom and camera motion produces a disquieting effect.)\nThis ambiguity becomes clearer if we write out the projection equation corresponding to\nthe simple calibration matrix K (2.59),\nxs\n=\nf rx · p + tx\nrz · p + tz\n+ cx\n(2.73)\nys\n=\nf ry · p + ty\nrz · p + tz\n+ cy,\n(2.74)\nwhere rx, ry, and rz are the three rows of R. If the distance to the object center tz ≫∥p∥\n(the size of the object), the denominator is approximately tz and the overall scale of the\nprojected object depends on the ratio of f to tz. It therefore becomes difﬁcult to disentangle\nthese two quantities.\nTo see this more clearly, let ηz = t−1\nz\nand s = ηzf. We can then re-write the above\nequations as\nxs\n=\ns rx · p + tx\n1 + ηzrz · p + cx\n(2.75)\nys\n=\ns ry · p + ty\n1 + ηzrz · p + cy\n(2.76)\n(Szeliski and Kang 1994; Pighin, Hecker, Lischinski et al. 1998). The scale of the projection\ns can be reliably estimated if we are looking at a known object (i.e., the 3D coordinates p\n58\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nare known). The inverse distance ηz is now mostly decoupled from the estimates of s and\ncan be estimated from the amount of foreshortening as the object rotates. Furthermore, as\nthe lens becomes longer, i.e., the projection model becomes orthographic, there is no need to\nreplace a perspective imaging model with an orthographic one, since the same equation can\nbe used, with ηz →0 (as opposed to f and tz both going to inﬁnity). This allows us to form\na natural link between orthographic reconstruction techniques such as factorization and their\nprojective/perspective counterparts (Section 7.3).\n2.1.6 Lens distortions\nThe above imaging models all assume that cameras obey a linear projection model where\nstraight lines in the world result in straight lines in the image. (This follows as a natural\nconsequence of linear matrix operations being applied to homogeneous coordinates.) Unfor-\ntunately, many wide-angle lenses have noticeable radial distortion, which manifests itself as\na visible curvature in the projection of straight lines. (See Section 2.2.3 for a more detailed\ndiscussion of lens optics, including chromatic aberration.) Unless this distortion is taken into\naccount, it becomes impossible to create highly accurate photorealistic reconstructions. For\nexample, image mosaics constructed without taking radial distortion into account will often\nexhibit blurring due to the mis-registration of corresponding features before pixel blending\n(Chapter 9).\nFortunately, compensating for radial distortion is not that difﬁcult in practice. For most\nlenses, a simple quartic model of distortion can produce good results. Let (xc, yc) be the\npixel coordinates obtained after perspective division but before scaling by focal length f and\nshifting by the optical center (cx, cy), i.e.,\nxc\n=\nrx · p + tx\nrz · p + tz\nyc\n=\nry · p + ty\nrz · p + tz\n.\n(2.77)\nThe radial distortion model says that coordinates in the observed images are displaced away\n(barrel distortion) or towards (pincushion distortion) the image center by an amount propor-\ntional to their radial distance (Figure 2.13a–b).3\nThe simplest radial distortion models use\nlow-order polynomials, e.g.,\nˆxc\n=\nxc(1 + κ1r2\nc + κ2r4\nc)\nˆyc\n=\nyc(1 + κ1r2\nc + κ2r4\nc),\n(2.78)\n3 Anamorphic lenses, which are widely used in feature ﬁlm production, do not follow this radial distortion model.\nInstead, they can be thought of, to a ﬁrst approximation, as inducing different vertical and horizontal scalings, i.e.,\nnon-square pixels.",
  "image_path": "page_079.jpg",
  "pages": [
    78,
    79,
    80
  ]
}