{
  "doc_id": "pages_688_690",
  "text": "666\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 14.8 Pedestrian detection using histograms of oriented gradients (Dalal and Triggs\n2005) c⃝2005 IEEE: (a) the average gradient image over the training examples; (b) each\n“pixel” shows the maximum positive SVM weight in the block centered on the pixel; (c) like-\nwise, for the negative SVM weights; (d) a test image; (e) the computed R-HOG (rectangular\nhistogram of gradients) descriptor; (f) the R-HOG descriptor weighted by the positive SVM\nweights; (g) the R-HOG descriptor weighted by the negative SVM weights.\n14.1.2 Pedestrian detection\nWhile a lot of the research on object detection has focused on faces, the detection of other\nobjects, such as pedestrians and cars, has also received widespread attention (Gavrila and\nPhilomin 1999; Gavrila 1999; Papageorgiou and Poggio 2000; Mohan, Papageorgiou, and\nPoggio 2001; Schneiderman and Kanade 2004). Some of these techniques maintain the same\nfocus as face detection on speed and efﬁciency. Others, however, focus instead on accuracy,\nviewing detection as a more challenging variant of generic class recognition (Section 14.4)\nin which the locations and extents of objects are to be determined as accurately as possible.\n(See, for example, the PASCAL VOC detection challenge, http://pascallin.ecs.soton.ac.uk/\nchallenges/VOC/.)\nAn example of a well-known pedestrian detector is the algorithm developed by Dalal\nand Triggs (2005), who use a set of overlapping histogram of oriented gradients (HOG) de-\nscriptors fed into a support vector machine (Figure 14.8). Each HOG has cells to accumulate\nmagnitude-weighted votes for gradients at particular orientations, just as in the scale invariant\nfeature transform (SIFT) developed by Lowe (2004), which we discussed in Section 4.1.2 and\nFigure 4.18. Unlike SIFT, however, which is only evaluated at interest point locations, HOGs\nare evaluated on a regular overlapping grid and their descriptor magnitudes are normalized\nusing an even coarser grid; they are only computed at a single scale and a ﬁxed orientation. In\norder to capture the subtle variations in orientation around a person’s outline, a large number\nof orientation bins is used and no smoothing is performed in the central difference gradi-\nent computation—see the work of Dalal and Triggs (2005) for more implementation details.\n14.1 Object detection\n667\nFigure 14.8d shows a sample input image, while Figure 14.8e shows the associated HOG\ndescriptors.\nOnce the descriptors have been computed, a support vector machine (SVM) is trained\non the resulting high-dimensional continuous descriptor vectors. Figures 14.8b–c show a\ndiagram of the (most) positive and negative SVM weights in each block, while Figures 14.8f–\ng show the corresponding weighted HOG responses for the central input image. As you can\nsee, there are a fair number of positive responses around the head, torso, and feet of the\nperson, and relatively few negative responses (mainly around the middle and the neck of the\nsweater).\nThe ﬁelds of pedestrian and general object detection have continued to evolve rapidly\nover the last decade (Belongie, Malik, and Puzicha 2002; Mikolajczyk, Schmid, and Zis-\nserman 2004; Leibe, Seemann, and Schiele 2005; Opelt, Pinz, and Zisserman 2006; Tor-\nralba 2007; Andriluka, Roth, and Schiele 2009, 2010; Doll`ar, Belongie, and Perona 2010).\nMunder and Gavrila (2006) compare a number of pedestrian detectors and conclude that\nthose based on local receptive ﬁelds and SVMs perform the best, with a boosting-based ap-\nproach coming close. Maji, Berg, and Malik (2008) improve on the best of these results using\nnon-overlapping multi-resolution HOG descriptors and a histogram intersection kernel SVM\nbased on a spatial pyramid match kernel from Lazebnik, Schmid, and Ponce (2006).\nWhen detectors for several different classes are being constructed simultaneously, Tor-\nralba, Murphy, and Freeman (2007) show that sharing features and weak learners between\ndetectors yields better performance, both in terms of faster computation times and fewer\ntraining examples. To ﬁnd the features and decision stumps that work best in a shared man-\nner, they introduce a novel joint boosting algorithm that optimizes, at each stage, a summed\nexpected exponential loss function using the “gentleboost” algorithm of Friedman, Hastie,\nand Tibshirani (2000).\nIn more recent work, Felzenszwalb, McAllester, and Ramanan (2008) extend the his-\ntogram of oriented gradients person detector to incorporate ﬂexible parts models (Section 14.4.2).\nEach part is trained and detected on HOGs evaluated at two pyramid levels below the overall\nobject model and the locations of the parts relative to the parent node (the overall bounding\nbox) are also learned and used during recognition (Figure 14.9b). To compensate for inac-\ncuracies or inconsistencies in the training example bounding boxes (dashed white lines in\nFigure 14.9c), the “true” location of the parent (blue) bounding box is considered a latent\n(hidden) variable and is inferred during both training and recognition. Since the locations\nof the parts are also latent, the system can be trained in a semi-supervised fashion, without\nneeding part labels in the training data. An extension to this system (Felzenszwalb, Girshick,\nMcAllester et al. 2010), which includes among its improvements a simple contextual model,\nwas among the two best object detection systems in the 2008 Visual Object Classes detection\nchallenge. Other recent improvements to part-based person detection and pose estimation in-\n668\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.9\nPart-based object detection (Felzenszwalb, McAllester, and Ramanan 2008)\nc⃝2008 IEEE: (a) An input photograph and its associated person (blue) and part (yellow)\ndetection results. (b) The detection model is deﬁned by a coarse template, several higher\nresolution part templates, and a spatial model for the location of each part. (c) True positive\ndetection of a skier and (d) false positive detection of a cow (labeled as a person).\nclude the work by Andriluka, Roth, and Schiele (2009) and Kumar, Zisserman, and H.S.Torr\n(2009).\nAn even more accurate estimate of a person’s pose and location is presented by Rogez,\nRihan, Ramalingam et al. (2008), who compute both the phase of a person in a walk cycle and\nthe locations of individual joints, using random forests built on top of HOGs (Figure 14.11).\nSince their system produces full 3D pose information, it is closer in its application domain to\n3D person trackers (Sidenbladh, Black, and Fleet 2000; Andriluka, Roth, and Schiele 2010),\nwhich we discussed in Section 12.6.4.\nOne ﬁnal note on person and object detection. When video sequences are available, the\nadditional information present in the optic ﬂow and motion discontinuities can greatly aid in\nthe detection task, as discussed by Efros, Berg, Mori et al. (2003), Viola, Jones, and Snow\n(2003), and Dalal, Triggs, and Schmid (2006).\n14.2 Face recognition\nAmong the various recognition tasks that computers might be asked to perform, face recog-\nnition is the one where they have arguably had the most success.5 While computers cannot\npick out suspects from thousands of people streaming in front of video cameras (even people\ncannot readily distinguish between similar people with whom they are not familiar (O’Toole,\nJiang, Roark et al. 2006; O’Toole, Phillips, Jiang et al. 2009)), their ability to distinguish\n5Instance recognition, i.e., the re-recognition of known objects such as locations or planar objects, is the other\nmost successful application of general image recognition. In the general domain of biometrics, i.e., identity recogni-\ntion, specialized images such as irises and ﬁngerprints perform even better (Jain, Bolle, and Pankanti 1999; Pankanti,\nBolle, and Jain 2000; Daugman 2004).",
  "image_path": "page_689.jpg",
  "pages": [
    688,
    689,
    690
  ]
}