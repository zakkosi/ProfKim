{
  "doc_id": "pages_118_120",
  "text": "96\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 2.6: Noise level calibration\nEstimate the amount of noise in your camera by taking re-\npeated shots of a scene with the camera mounted on a tripod. (Purchasing a remote shutter\nrelease is a good investment if you own a DSLR.) Alternatively, take a scene with constant\ncolor regions (such as a color checker chart) and estimate the variance by ﬁtting a smooth\nfunction to each color region and then taking differences from the predicted function.\n1. Plot your estimated variance as a function of level for each of your color channels\nseparately.\n2. Change the ISO setting on your camera; if you cannot do that, reduce the overall light\nin your scene (turn off lights, draw the curtains, wait until dusk). Does the amount of\nnoise vary a lot with ISO/gain?\n3. Compare your camera to another one at a different price point or year of make. Is\nthere evidence to suggest that “you get what you pay for”? Does the quality of digital\ncameras seem to be improving over time?\nEx 2.7: Gamma correction in image stitching\nHere’s a relatively simple puzzle. Assume\nyou are given two images that are part of a panorama that you want to stitch (see Chapter 9).\nThe two images were taken with different exposures, so you want to adjust the RGB values\nso that they match along the seam line. Is it necessary to undo the gamma in the color values\nin order to achieve this?\nEx 2.8: Skin color detection\nDevise a simple skin color detector (Forsyth and Fleck 1999;\nJones and Rehg 2001; Vezhnevets, Sazonov, and Andreeva 2003; Kakumanu, Makrogiannis,\nand Bourbakis 2007) based on chromaticity or other color properties.\n1. Take a variety of photographs of people and calculate the xy chromaticity values for\neach pixel.\n2. Crop the photos or otherwise indicate with a painting tool which pixels are likely to be\nskin (e.g. face and arms).\n3. Calculate a color (chromaticity) distribution for these pixels. You can use something as\nsimple as a mean and covariance measure or as complicated as a mean-shift segmenta-\ntion algorithm (see Section 5.3.2). You can optionally use non-skin pixels to model the\nbackground distribution.\n4. Use your computed distribution to ﬁnd the skin regions in an image. One easy way to\nvisualize this is to paint all non-skin pixels a given color, such as white or black.\n5. How sensitive is your algorithm to color balance (scene lighting)?\n2.5 Exercises\n97\n6. Does a simpler chromaticity measurement, such as a color ratio (2.116), work just as\nwell?\nEx 2.9: White point balancing—tricky\nA common (in-camera or post-processing) tech-\nnique for performing white point adjustment is to take a picture of a white piece of paper and\nto adjust the RGB values of an image to make this a neutral color.\n1. Describe how you would adjust the RGB values in an image given a sample “white\ncolor” of (Rw, Gw, Bw) to make this color neutral (without changing the exposure too\nmuch).\n2. Does your transformation involve a simple (per-channel) scaling of the RGB values or\ndo you need a full 3 × 3 color twist matrix (or something else)?\n3. Convert your RGB values to XYZ. Does the appropriate correction now only depend\non the XY (or xy) values? If so, when you convert back to RGB space, do you need a\nfull 3 × 3 color twist matrix to achieve the same effect?\n4. If you used pure diagonal scaling in the direct RGB mode but end up with a twist if you\nwork in XYZ space, how do you explain this apparent dichotomy? Which approach is\ncorrect? (Or is it possible that neither approach is actually correct?)\nIf you want to ﬁnd out what your camera actually does, continue on to the next exercise.\nEx 2.10: In-camera color processing—challenging\nIf your camera supports a RAW pixel\nmode, take a pair of RAW and JPEG images, and see if you can infer what the camera is doing\nwhen it converts the RAW pixel values to the ﬁnal color-corrected and gamma-compressed\neight-bit JPEG pixel values.\n1. Deduce the pattern in your color ﬁlter array from the correspondence between co-\nlocated RAW and color-mapped pixel values. Use a color checker chart at this stage\nif it makes your life easier. You may ﬁnd it helpful to split the RAW image into four\nseparate images (subsampling even and odd columns and rows) and to treat each of\nthese new images as a “virtual” sensor.\n2. Evaluate the quality of the demosaicing algorithm by taking pictures of challenging\nscenes which contain strong color edges (such as those shown in in Section 10.3.1).\n3. If you can take the same exact picture after changing the color balance values in your\ncamera, compare how these settings affect this processing.\n4. Compare your results against those presented by Chakrabarti, Scharstein, and Zickler\n(2009) or use the data available in their database of color images.26\n26 http://vision.middlebury.edu/color/.\n98\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "image_path": "page_119.jpg",
  "pages": [
    118,
    119,
    120
  ]
}