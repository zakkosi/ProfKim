{
  "doc_id": "pages_715_717",
  "text": "14.3 Instance recognition\n693\nFigure 14.31 Location or building recognition using randomized trees (Philbin, Chum, Isard\net al. 2007) c⃝2007 IEEE. The left image is the query, the other images are the highest-ranked\nresults.\nquery expansion, which involves re-submitting top-ranked images from the initial query as\nadditional queries to generate additional candidate results, to further improve recognition\nrates for difﬁcult (occluded or oblique) examples. Philbin, Chum, Sivic et al. (2008) show\nhow to mitigate quantization problems in visual words selection using soft assignment, where\neach feature descriptor is mapped to a number of visual words based on its distance from the\ncluster prototypes. The soft weights derived from these distances are used, in turn, to weight\nthe counts used in the tf-idf vectors and to retrieve additional images for later veriﬁcation.\nTaken together, these recent advances hold the promise of extending current instance recog-\nnition algorithms to performing Web-scale retrieval and matching tasks (Agarwal, Snavely,\nSimon et al. 2009; Agarwal, Furukawa, Snavely et al. 2010; Snavely, Simon, Goesele et al.\n2010).\n14.3.3 Application: Location recognition\nOne of the most exciting applications of instance recognition today is in the area of location\nrecognition, which can be used both in desktop applications (where did I take this holiday\nsnap?) and in mobile (cell-phone) applications. The latter case includes not only ﬁnding out\nyour current location based on a cell-phone image but also providing you with navigation\ndirections or annotating your images with useful information, such as building names and\nrestaurant reviews (i.e., a portable form of augmented reality).\nSome approaches to location recognition assume that the photos consist of architectural\nscenes for which vanishing directions can be used to pre-rectify the images for easier match-\ning (Robertson and Cipolla 2004). Other approaches use general afﬁne covariant interest\npoints to perform wide baseline matching (Schaffalitzky and Zisserman 2002). The Photo\nTourism system of Snavely, Seitz, and Szeliski (2006) (Section 13.1.2) was the ﬁrst to apply\nthese kinds of ideas to large-scale image matching and (implicit) location recognition from\n694\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 14.32 Feature-based location recognition (Schindler, Brown, and Szeliski 2007) c⃝\n2007 IEEE: (a) three typical series of overlapping street photos; (b) handheld camera shots\nand (c) their corresponding database photos.\nInternet photo collections taken under a wide variety of viewing conditions.\nThe main difﬁculty in location recognition is in dealing with the extremely large commu-\nnity (user-generated) photo collections on Web sites such as Flickr (Philbin, Chum, Isard et\nal. 2007; Chum, Philbin, Sivic et al. 2007; Philbin, Chum, Sivic et al. 2008; Turcot and Lowe\n2009) or commercially captured databases (Schindler, Brown, and Szeliski 2007). The preva-\nlence of commonly appearing elements such as foliage, signs, and common architectural ele-\nments further complicates the task. Figure 14.31 shows some results on location recognition\nfrom community photo collections, while Figure 14.32 shows sample results from denser\ncommercially acquired datasets. In the latter case, the overlap between adjacent database\nimages can be used to verify and prune potential matches using “temporal” ﬁltering, i.e., re-\nquiring the query image to match nearby overlapping database images before accepting the\nmatch.\nAnother variant on location recognition is the automatic discovery of landmarks, i.e.,\nfrequently photographed objects and locations. Simon, Snavely, and Seitz (2007) show how\nthese kinds of objects can be discovered simply by analyzing the matching graph constructed\nas part of the 3D modeling process in Photo Tourism. More recent work has extended this\napproach to larger data sets using efﬁcient clustering techniques (Philbin and Zisserman 2008;\nLi, Wu, Zach et al. 2008; Chum, Philbin, and Zisserman 2008; Chum and Matas 2010) as well\nas combining meta-data such as GPS and textual tags with visual search (Quack, Leibe, and\nVan Gool 2008; Crandall, Backstrom, Huttenlocher et al. 2009), as shown in Figure 14.33.\nIt is now even possible to automatically associate object tags with images based on their co-\noccurrence in multiple loosely tagged images (Simon and Seitz 2008; Gammeter, Bossard,\n14.4 Category recognition\n695\nFigure 14.33\nAutomatic mining, annotation, and localization of community photo collec-\ntions (Quack, Leibe, and Van Gool 2008) c⃝2008 ACM. This ﬁgure does not show the textual\nannotations or corresponding Wikipedia entries, which are also discovered.\nA\nB\nC\nD\n(a)\n(b)\nFigure 14.34 Locating star ﬁelds using astrometry, http://astrometry.net/. (a) Input star ﬁeld\nand some selected star quads. (b) The 2D coordinates of stars C and D are encoded relative\nto the unit square deﬁned by A and B.\nQuack et al. 2009).\nThe concept of organizing the world’s photo collections by location has even been re-\ncently extended to organizing all of the universe’s (astronomical) photos in an application\ncalled astrometry, http://astrometry.net/. The technique used to match any two star ﬁelds is\nto take quadruplets of nearby stars (a pair of stars and another pair inside their diameter) to\nform a 30-bit geometric hash by encoding the relative positions of the second pair of points\nusing the inscribed square as the reference frame, as shown in Figure 14.34. Traditional in-\nformation retrieval techniques (k-d trees built for different parts of a sky atlas) are then used\nto ﬁnd matching quads as potential star ﬁeld location hypotheses, which can then be veriﬁed\nusing a similarity transform.",
  "image_path": "page_716.jpg",
  "pages": [
    715,
    716,
    717
  ]
}