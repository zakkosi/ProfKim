{
  "doc_id": "pages_549_551",
  "text": "10.7 Exercises\n527\ninside a window around the mouse. Another is to adjust the complete image based on\nthe mouse position.)\n8. Implement a tone mapping operator (Exercise 10.5) and use this to map your radiance\nimage to a displayable gamut.\nEx 10.2: Noise level function\nDetermine your camera’s noise level function using either\nmultiple shots or by analyzing smooth regions.\n1. Set up your camera on a tripod looking at a calibration target or a static scene with a\ngood variation in input levels and colors. (Check your camera’s histogram to ensure\nthat all values are being sampled.)\n2. Take repeated images of the same scene (ideally with a remote shutter release) and\naverage them to compute the variance at each pixel. Discarding pixels near high gra-\ndients (which are affected by camera motion), plot for each color channel the standard\ndeviation at each pixel as a function of its output value.\n3. Fit a lower envelope to these measurements and use this as your noise level function.\nHow much variation do you see in the noise as a function of input level? How much of\nthis is signiﬁcant, i.e., away from ﬂat regions in your camera response function where\nyou do not want to be sampling anyway?\n4. (Optional) Using the same images, develop a technique that segments the image into\nnear-constant regions (Liu, Szeliski, Kang et al. 2008). (This is easier if you are pho-\ntographing a calibration chart.) Compute the deviations for each region from a single\nimage and use them to estimate the NLF. How does this compare to the multi-image\ntechnique, and how stable are your estimates from image to image?\nEx 10.3: Vignetting\nEstimate the amount of vignetting in some of your lenses using one of\nthe following three techniques (or devise one of your choosing):\n1. Take an image of a large uniform intensity region (well-illuminated wall or blue sky—\nbut be careful of brightness gradients) and ﬁt a radial polynomial curve to estimate the\nvignetting.\n2. Construct a center-weighted panorama and compare these pixel values to the input im-\nage values to estimate the vignetting function. Weight pixels in slowly varying regions\nmore highly, as small misalignments will give large errors at high gradients. Option-\nally estimate the radiometric response function as well (Litvinov and Schechner 2005;\nGoldman 2011).\n528\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Analyze the radial gradients (especially in low-gradient regions) and ﬁt the robust\nmeans of these gradients to the derivative of the vignetting function, as described by\nZheng, Yu, Kang et al. (2008).\nFor the parametric form of your vignetting function, you can either use a simple radial func-\ntion, e.g.,\nf(r) = 1 + α1r + α2r2 + · · ·\n(10.42)\nor one of the specialized equations developed by Kang and Weiss (2000) and Zheng, Lin, and\nKang (2006).\nIn all of these cases, be sure that you are using linearized intensity measurements, by\nusing either RAW images or images linearized through a radiometric response function, or at\nleast images where the gamma curve has been removed.\n(Optional) What happens if you forget to undo the gamma before ﬁtting a (multiplicative)\nvignetting function?\nEx 10.4: Optical blur (PSF) estimation\nCompute the optical PSF either using a known\ntarget (Figure 10.7) or by detecting and ﬁtting step edges (Section 10.1.4) (Joshi, Szeliski,\nand Kriegman 2008).\n1. Detect strong edges to sub-pixel precision.\n2. Fit a local proﬁle to each oriented edge and ﬁll these pixels into an ideal target image,\neither at image resolution or at a higher resolution (Figure 10.9c–d).\n3. Use least squares (10.1) at valid pixels to estimate the PSF kernel K, either globally or\nin locally overlapping sub-regions of the image.\n4. Visualize the recovered PSFs and use them to remove chromatic aberration or de-blur\nthe image.\nEx 10.5: Tone mapping\nImplement one of the tone mapping algorithms discussed in Sec-\ntion 10.2.1 (Durand and Dorsey 2002; Fattal, Lischinski, and Werman 2002; Reinhard, Stark,\nShirley et al. 2002; Lischinski, Farbman, Uyttendaele et al. 2006b) or any of the numer-\nous additional algorithms discussed by Reinhard, Ward, Pattanaik et al. (2005) and http:\n//stellar.mit.edu/S/course/6/sp08/6.815/materials.html.\n(Optional) Compare your algorithm to local histogram equalization (Section 3.1.4).\nEx 10.6: Flash enhancement\nDevelop an algorithm to combine ﬂash and non-ﬂash pho-\ntographs to best effect. You can use ideas from Eisemann and Durand (2004) and Petschnigg,\nAgrawala, Hoppe et al. (2004) or anything else you think might work well.\n10.7 Exercises\n529\nEx 10.7: Super-resolution\nImplement one or more super-resolution algorithms and com-\npare their performance.\n1. Take a set of photographs of the same scene using a hand-held camera (to ensure that\nthere is some jitter between the photographs).\n2. Determine the PSF for the images you are trying to super-resolve using one of the\ntechniques in Exercise 10.4.\n3. Alternatively, simulate a collection of lower-resolution images by taking a high-quality\nphotograph (avoid those with compression artifacts) and applying your own pre-ﬁlter\nkernel and downsampling.\n4. Estimate the relative motion between the images using a parametric translation and\nrotation motion estimation algorithm (Sections 6.1.3 or 8.2).\n5. Implement a basic least squares super-resolution algorithm by minimizing the differ-\nence between the observed and downsampled images (10.27–10.28).\n6. Add in a gradient image prior, either as another least squares term or as a robust term\nthat can be minimized using iteratively reweighted least squares (Appendix A.3).\n7. (Optional) Implement one of the example-based super-resolution techniques, where\nmatching against a set of exemplar images is used either to infer higher-frequency\ninformation to be added to the reconstruction (Freeman, Jones, and Pasztor 2002)\nor higher-frequency gradients to be matched in the super-resolved image (Baker and\nKanade 2002).\n8. (Optional) Use local edge statistic information to improve the quality of the super-\nresolved image (Fattal 2007).\nEx 10.8: Image matting\nDevelop an algorithm for pulling a foreground matte from natural\nimages, as described in Section 10.4.\n1. Make sure that the images you are taking are linearized (Exercise 10.1 and Section 10.1)\nand that your camera exposure is ﬁxed (full manual mode), at least when taking multi-\nple shots of the same scene.\n2. To acquire ground truth data, place your object in front of a computer monitor and\ndisplay a variety of solid background colors as well as some natural imagery.\n3. Remove your object and re-display the same images to acquire known background\ncolors.",
  "image_path": "page_550.jpg",
  "pages": [
    549,
    550,
    551
  ]
}