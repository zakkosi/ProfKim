{
  "doc_id": "pages_115_117",
  "text": "2.4 Additional reading\n93\nThe PSNR is deﬁned as\nPSNR = 10 log10\nI2\nmax\nMSE = 20 log10\nImax\nRMS ,\n(2.119)\nwhere Imax is the maximum signal extent, e.g., 255 for eight-bit images.\nWhile this is just a high-level sketch of how image compression works, it is useful to\nunderstand so that the artifacts introduced by such techniques can be compensated for in\nvarious computer vision applications.\n2.4 Additional reading\nAs we mentioned at the beginning of this chapter, it provides but a brief summary of a very\nrich and deep set of topics, traditionally covered in a number of separate ﬁelds.\nA more thorough introduction to the geometry of points, lines, planes, and projections\ncan be found in textbooks on multi-view geometry (Hartley and Zisserman 2004; Faugeras\nand Luong 2001) and computer graphics (Foley, van Dam, Feiner et al. 1995; Watt 1995;\nOpenGL-ARB 1997). Topics covered in more depth include higher-order primitives such as\nquadrics, conics, and cubics, as well as three-view and multi-view geometry.\nThe image formation (synthesis) process is traditionally taught as part of a computer\ngraphics curriculum (Foley, van Dam, Feiner et al. 1995; Glassner 1995; Watt 1995; Shirley\n2005) but it is also studied in physics-based computer vision (Wolff, Shafer, and Healey\n1992a).\nThe behavior of camera lens systems is studied in optics (M¨oller 1988; Hecht 2001; Ray\n2002).\nSome good books on color theory have been written by Healey and Shafer (1992); Wyszecki\nand Stiles (2000); Fairchild (2005), with Livingstone (2008) providing a more fun and infor-\nmal introduction to the topic of color perception. Mark Fairchild’s page of color books and\nlinks25 lists many other sources.\nTopics relating to sampling and aliasing are covered in textbooks on signal and image\nprocessing (Crane 1997; J¨ahne 1997; Oppenheim and Schafer 1996; Oppenheim, Schafer,\nand Buck 1999; Pratt 2007; Russ 2007; Burger and Burge 2008; Gonzales and Woods 2008).\n2.5 Exercises\nA note to students: This chapter is relatively light on exercises since it contains mostly\nbackground material and not that many usable techniques. If you really want to understand\n25 http://www.cis.rit.edu/fairchild/WhyIsColor/books links.html.\n94\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmulti-view geometry in a thorough way, I encourage you to read and do the exercises provided\nby Hartley and Zisserman (2004). Similarly, if you want some exercises related to the image\nformation process, Glassner’s (1995) book is full of challenging problems.\nEx 2.1: Least squares intersection point and line ﬁtting—advanced\nEquation (2.4) shows\nhow the intersection of two 2D lines can be expressed as their cross product, assuming the\nlines are expressed as homogeneous coordinates.\n1. If you are given more than two lines and want to ﬁnd a point ˜x that minimizes the sum\nof squared distances to each line,\nD =\nX\ni\n(˜x · ˜li)2,\n(2.120)\nhow can you compute this quantity? (Hint: Write the dot product as ˜xT˜li and turn the\nsquared quantity into a quadratic form, ˜xT A˜x.)\n2. To ﬁt a line to a bunch of points, you can compute the centroid (mean) of the points\nas well as the covariance matrix of the points around this mean. Show that the line\npassing through the centroid along the major axis of the covariance ellipsoid (largest\neigenvector) minimizes the sum of squared distances to the points.\n3. These two approaches are fundamentally different, even though projective duality tells\nus that points and lines are interchangeable. Why are these two algorithms so appar-\nently different? Are they actually minimizing different objectives?\nEx 2.2: 2D transform editor\nWrite a program that lets you interactively create a set of\nrectangles and then modify their “pose” (2D transform). You should implement the following\nsteps:\n1. Open an empty window (“canvas”).\n2. Shift drag (rubber-band) to create a new rectangle.\n3. Select the deformation mode (motion model): translation, rigid, similarity, afﬁne, or\nperspective.\n4. Drag any corner of the outline to change its transformation.\nThis exercise should be built on a set of pixel coordinate and transformation classes, either\nimplemented by yourself or from a software library. Persistence of the created representation\n(save and load) should also be supported (for each rectangle, save its transformation).\n2.5 Exercises\n95\nEx 2.3: 3D viewer\nWrite a simple viewer for 3D points, lines, and polygons. Import a set\nof point and line commands (primitives) as well as a viewing transform. Interactively modify\nthe object or camera transform. This viewer can be an extension of the one you created in\n(Exercise 2.2). Simply replace the viewing transformations with their 3D equivalents.\n(Optional) Add a z-buffer to do hidden surface removal for polygons.\n(Optional) Use a 3D drawing package and just write the viewer control.\nEx 2.4: Focus distance and depth of ﬁeld\nFigure out how the focus distance and depth of\nﬁeld indicators on a lens are determined.\n1. Compute and plot the focus distance zo as a function of the distance traveled from the\nfocal length ∆zi = f −zi for a lens of focal length f (say, 100mm). Does this explain\nthe hyperbolic progression of focus distances you see on a typical lens (Figure 2.20)?\n2. Compute the depth of ﬁeld (minimum and maximum focus distances) for a given focus\nsetting zo as a function of the circle of confusion diameter c (make it a fraction of\nthe sensor width), the focal length f, and the f-stop number N (which relates to the\naperture diameter d). Does this explain the usual depth of ﬁeld markings on a lens that\nbracket the in-focus marker, as in Figure 2.20a?\n3. Now consider a zoom lens with a varying focal length f. Assume that as you zoom,\nthe lens stays in focus, i.e., the distance from the rear nodal point to the sensor plane\nzi adjusts itself automatically for a ﬁxed focus distance zo. How do the depth of ﬁeld\nindicators vary as a function of focal length? Can you reproduce a two-dimensional\nplot that mimics the curved depth of ﬁeld lines seen on the lens in Figure 2.20b?\nEx 2.5: F-numbers and shutter speeds\nList the common f-numbers and shutter speeds\nthat your camera provides. On older model SLRs, they are visible on the lens and shut-\nter speed dials. On newer cameras, you have to look at the electronic viewﬁnder (or LCD\nscreen/indicator) as you manually adjust exposures.\n1. Do these form geometric progressions; if so, what are the ratios? How do these relate\nto exposure values (EVs)?\n2. If your camera has shutter speeds of 1\n60 and\n1\n125, do you think that these two speeds are\nexactly a factor of two apart or a factor of 125/60 = 2.083 apart?\n3. How accurate do you think these numbers are? Can you devise some way to measure\nexactly how the aperture affects how much light reaches the sensor and what the exact\nexposure times actually are?",
  "image_path": "page_116.jpg",
  "pages": [
    115,
    116,
    117
  ]
}