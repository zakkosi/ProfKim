{
  "doc_id": "pages_338_340",
  "text": "316\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nis now a Jacobian-weighted sum of residual vectors. This makes intuitive sense, as the pa-\nrameters are pulled in the direction of the prediction error with a strength proportional to the\nJacobian.\nOnce A and b have been computed, we solve for ∆p using\n(A + λdiag(A))∆p = b,\n(6.18)\nand update the parameter vector p ←p + ∆p accordingly. The parameter λ is an addi-\ntional damping parameter used to ensure that the system takes a “downhill” step in energy\n(squared error) and is an essential component of the Levenberg–Marquardt algorithm (de-\nscribed in more detail in Appendix A.3). In many applications, it can be set to 0 if the system\nis successfully converging.\nFor the case of our 2D translation+rotation, we end up with a 3×3 set of normal equations\nin the unknowns (δtx, δty, δθ). An initial guess for (tx, ty, θ) can be obtained by ﬁtting a\nfour-parameter similarity transform in (tx, ty, c, s) and then setting θ = tan−1(s/c). An\nalternative approach is to estimate the translation parameters using the centroids of the 2D\npoints and to then estimate the rotation angle using polar coordinates (Exercise 6.3).\nFor the other 2D motion models, the derivatives in Table 6.1 are all fairly straightforward,\nexcept for the projective 2D motion (homography), which arises in image-stitching applica-\ntions (Chapter 9). These equations can be re-written from (2.21) in their new parametric form\nas\nx′ = (1 + h00)x + h01y + h02\nh20x + h21y + 1\nand y′ = h10x + (1 + h11)y + h12\nh20x + h21y + 1\n.\n(6.19)\nThe Jacobian is therefore\nJ = ∂f\n∂p = 1\nD\n\"\nx\ny\n1\n0\n0\n0\n−x′x\n−x′y\n0\n0\n0\nx\ny\n1\n−y′x\n−y′y\n#\n,\n(6.20)\nwhere D = h20x + h21y + 1 is the denominator in (6.19), which depends on the current\nparameter settings (as do x′ and y′).\nAn initial guess for the eight unknowns {h00, h01, . . . , h21} can be obtained by multiply-\ning both sides of the equations in (6.19) through by the denominator, which yields the linear\nset of equations,\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\nx\ny\n1\n0\n0\n0\n−ˆx′x\n−ˆx′y\n0\n0\n0\nx\ny\n1\n−ˆy′x\n−ˆy′y\n#\n\n\nh00\n...\nh21\n\n.\n(6.21)\nHowever, this is not optimal from a statistical point of view, since the denominator D, which\nwas used to multiply each equation, can vary quite a bit from point to point.6\n6 Hartley and Zisserman (2004) call this strategy of forming linear equations from rational equations the direct\n6.1 2D and 3D feature-based alignment\n317\nOne way to compensate for this is to reweight each equation by the inverse of the current\nestimate of the denominator, D,\n1\nD\n\"\nˆx′ −x\nˆy′ −y\n#\n= 1\nD\n\"\nx\ny\n1\n0\n0\n0\n−ˆx′x\n−ˆx′y\n0\n0\n0\nx\ny\n1\n−ˆy′x\n−ˆy′y\n#\n\n\nh00\n...\nh21\n\n.\n(6.22)\nWhile this may at ﬁrst seem to be the exact same set of equations as (6.21), because least\nsquares is being used to solve the over-determined set of equations, the weightings do matter\nand produce a different set of normal equations that performs better in practice.\nThe most principled way to do the estimation, however, is to directly minimize the squared\nresidual equations (6.13) using the Gauss–Newton approximation, i.e., performing a ﬁrst-\norder Taylor series expansion in p, as shown in (6.14), which yields the set of equations\n\"\nˆx′ −˜x′\nˆy′ −˜y′\n#\n= 1\nD\n\"\nx\ny\n1\n0\n0\n0\n−˜x′x\n−˜x′y\n0\n0\n0\nx\ny\n1\n−˜y′x\n−˜y′y\n#\n\n\n∆h00\n...\n∆h21\n\n.\n(6.23)\nWhile these look similar to (6.22), they differ in two important respects. First, the left hand\nside consists of unweighted prediction errors rather than point displacements and the solution\nvector is a perturbation to the parameter vector p. Second, the quantities inside J involve\npredicted feature locations (˜x′, ˜y′) instead of sensed feature locations (ˆx′, ˆy′). Both of these\ndifferences are subtle and yet they lead to an algorithm that, when combined with proper\nchecking for downhill steps (as in the Levenberg–Marquardt algorithm), will converge to a\nlocal minimum. Note that iterating Equations (6.22) is not guaranteed to converge, since it is\nnot minimizing a well-deﬁned energy function.\nEquation (6.23) is analogous to the additive algorithm for direct intensity-based regis-\ntration (Section 8.2), since the change to the full transformation is being computed. If we\nprepend an incremental homography to the current homography instead, i.e., we use a com-\npositional algorithm (described in Section 8.2), we get D = 1 (since p = 0) and the above\nformula simpliﬁes to\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\nx\ny\n1\n0\n0\n0\n−x2\n−xy\n0\n0\n0\nx\ny\n1\n−xy\n−y2\n#\n\n\n∆h00\n...\n∆h21\n\n,\n(6.24)\nwhere we have replaced (˜x′, ˜y′) with (x, y) for conciseness. (Notice how this results in the\nsame Jacobian as (8.63).)\nlinear transform, but that term is more commonly associated with pose estimation (Section 6.2). Note also that our\ndeﬁnition of the hij parameters differs from that used in their book, since we deﬁne hii to be the difference from\nunity and we do not leave h22 as a free parameter, which means that we cannot handle certain extreme homographies.\n318\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n6.1.4 Robust least squares and RANSAC\nWhile regular least squares is the method of choice for measurements where the noise follows\na normal (Gaussian) distribution, more robust versions of least squares are required when\nthere are outliers among the correspondences (as there almost always are). In this case, it is\npreferable to use an M-estimator (Huber 1981; Hampel, Ronchetti, Rousseeuw et al. 1986;\nBlack and Rangarajan 1996; Stewart 1999), which involves applying a robust penalty function\nρ(r) to the residuals\nERLS(∆p) =\nX\ni\nρ(∥ri∥)\n(6.25)\ninstead of squaring them.\nWe can take the derivative of this function with respect to p and set it to 0,\nX\ni\nψ(∥ri∥)∂∥ri∥\n∂p\n=\nX\ni\nψ(∥ri∥)\n∥ri∥\nrT\ni\n∂ri\n∂p = 0,\n(6.26)\nwhere ψ(r) = ρ′(r) is the derivative of ρ and is called the inﬂuence function. If we introduce\na weight function, w(r) = Ψ(r)/r, we observe that ﬁnding the stationary point of (6.25) using\n(6.26) is equivalent to minimizing the iteratively reweighted least squares (IRLS) problem\nEIRLS =\nX\ni\nw(∥ri∥)∥ri∥2,\n(6.27)\nwhere the w(∥ri∥) play the same local weighting role as σ−2\ni\nin (6.10). The IRLS algo-\nrithm alternates between computing the inﬂuence functions w(∥ri∥) and solving the result-\ning weighted least squares problem (with ﬁxed w values).\nOther incremental robust least\nsquares algorithms can be found in the work of Sawhney and Ayer (1996); Black and Anan-\ndan (1996); Black and Rangarajan (1996); Baker, Gross, Ishikawa et al. (2003) and textbooks\nand tutorials on robust statistics (Huber 1981; Hampel, Ronchetti, Rousseeuw et al. 1986;\nRousseeuw and Leroy 1987; Stewart 1999).\nWhile M-estimators can deﬁnitely help reduce the inﬂuence of outliers, in some cases,\nstarting with too many outliers will prevent IRLS (or other gradient descent algorithms) from\nconverging to the global optimum. A better approach is often to ﬁnd a starting set of inlier\ncorrespondences, i.e., points that are consistent with a dominant motion estimate.7\nTwo widely used approaches to this problem are called RANdom SAmple Consensus, or\nRANSAC for short (Fischler and Bolles 1981), and least median of squares (LMS) (Rousseeuw\n1984). Both techniques start by selecting (at random) a subset of k correspondences, which is\n7 For pixel-based alignment methods (Section 8.1.1), hierarchical (coarse-to-ﬁne) techniques are often used to\nlock onto the dominant motion in a scene.",
  "image_path": "page_339.jpg",
  "pages": [
    338,
    339,
    340
  ]
}