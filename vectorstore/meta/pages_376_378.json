{
  "doc_id": "pages_376_378",
  "text": "354\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIts smallest left singular vector indicates the epipole e1 in the image 1 and its smallest right\nsingular vector is e0 (Figure 7.3). The homography ˜\nH in (7.29), which in principle should\nequal\n˜\nH = K−T\n1\nRK−1\n0 ,\n(7.31)\ncannot be uniquely recovered from F , since any homography of the form ˜\nH\n′ = ˜\nH + evT\nresults in the same F matrix. (Note that [e]× annihilates any multiple of e.)\nAny one of these valid homographies ˜\nH maps some plane in the scene from one image\nto the other. It is not possible to tell in advance which one it is without either selecting four\nor more co-planar correspondences to compute ˜\nH as part of the F estimation process (in a\nmanner analogous to guessing a rotation for E) or mapping all points in one image through\n˜\nH and seeing which ones line up with their corresponding locations in the other.7\nIn order to create a projective reconstruction of the scene, we can pick any valid homog-\nraphy ˜\nH that satisﬁes Equation (7.29). For example, following a technique analogous to\nEquations (7.18–7.24), we get\nF = [e]× ˜\nH = SZR90◦ST ˜\nH = UΣV T\n(7.32)\nand hence\n˜\nH = URT\n90◦ˆΣV T ,\n(7.33)\nwhere ˆΣ is the singular value matrix with the smallest value replaced by a reasonable alter-\nnative (say, the middle value).8 We can then form a pair of camera matrices\nP 0 = [I|0]\nand\nP 0 = [ ˜\nH|e],\n(7.34)\nfrom which a projective reconstruction of the scene can be computed using triangulation\n(Section 7.1).\nWhile the projective reconstruction may not be useful in practice, it can often be upgraded\nto an afﬁne or metric reconstruction, as detailed below. Even without this step, however,\nthe fundamental matrix F can be very useful in ﬁnding additional correspondences, as they\nmust all lie on corresponding epipolar lines, i.e., any feature x0 in image 0 must have its\ncorrespondence lying on the associated epipolar line l1 = F x0 in image 1, assuming that the\npoint motions are due to a rigid transformation.\n7 This process is sometimes referred to as plane plus parallax (Section 2.1.5) (Kumar, Anandan, and Hanna 1994;\nSawhney 1994).\n8 Hartley and Zisserman (2004, p. 237) recommend using ˜\nH = [e]×F (Luong and Vi´eville 1996), which places\nthe camera on the plane at inﬁnity.\n7.2 Two-frame structure from motion\n355\n7.2.2 Self-calibration\nThe results of structure from motion computation are much more useful (and intelligible) if\na metric reconstruction is obtained, i.e., one in which parallel lines are parallel, orthogonal\nwalls are at right angles, and the reconstructed model is a scaled version of reality. Over\nthe years, a large number of self-calibration (or auto-calibration) techniques have been de-\nveloped for converting a projective reconstruction into a metric one, which is equivalent to\nrecovering the unknown calibration matrices Kj associated with each image (Hartley and\nZisserman 2004; Moons, Van Gool, and Vergauwen 2010).\nIn situations where certain additional information is known about the scene, different\nmethods may be employed. For example, if there are parallel lines in the scene (usually,\nhaving several lines converge on the same vanishing point is good evidence), three or more\nvanishing points, which are the images of points at inﬁnity, can be used to establish the ho-\nmography for the plane at inﬁnity, from which focal lengths and rotations can be recovered.\nIf two or more ﬁnite orthogonal vanishing points have been observed, the single-image cali-\nbration method based on vanishing points (Section 6.3.2) can be used instead.\nIn the absence of such external information, it is not possible to recover a fully parameter-\nized independent calibration matrix Kj for each image from correspondences alone. To see\nthis, consider the set of all camera matrices P j = Kj[Rj|tj] projecting world coordinates\npi = (Xi, Yi, Zi, Wi) into screen coordinates xij ∼P jpi. Now consider transforming the\n3D scene {pi} through an arbitrary 4 × 4 projective transformation ˜\nH, yielding a new model\nconsisting of points p′\ni = ˜\nHpi. Post-multiplying each P j matrix by ˜\nH\n−1 still produces the\nsame screen coordinates and a new set calibration matrices can be computed by applying RQ\ndecomposition to the new camera matrix P ′\nj = P j ˜\nH\n−1.\nFor this reason, all self-calibration methods assume some restricted form of the calibration\nmatrix, either by setting or equating some of their elements or by assuming that they do not\nvary over time. While most of the techniques discussed by Hartley and Zisserman (2004);\nMoons, Van Gool, and Vergauwen (2010) require three or more frames, in this section we\npresent a simple technique that can recover the focal lengths (f0, f1) of both images from the\nfundamental matrix F in a two-frame reconstruction (Hartley and Zisserman 2004, p. 456).\nTo accomplish this, we assume that the camera has zero skew, a known aspect ratio (usu-\nally set to 1), and a known optical center, as in Equation (2.59). How reasonable is this\nassumption in practice? The answer, as with many questions, is “it depends”.\nIf absolute metric accuracy is required, as in photogrammetry applications, it is imperative\nto pre-calibrate the cameras using one of the techniques from Section 6.3 and to use ground\ncontrol points to pin down the reconstruction. If instead, we simply wish to reconstruct the\nworld for visualization or image-based rendering applications, as in the Photo Tourism system\nof Snavely, Seitz, and Szeliski (2006), this assumption is quite reasonable in practice.\n356\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMost cameras today have square pixels and an optical center near the middle of the image,\nand are much more likely to deviate from a simple camera model due to radial distortion\n(Section 6.3.5), which should be compensated for whenever possible. The biggest problems\noccur when images have been cropped off-center, in which case the optical center will no\nlonger be in the middle, or when perspective pictures have been taken of a different picture,\nin which case a general camera matrix becomes necessary.9\nGiven these caveats, the two-frame focal length estimation algorithm based on the Kruppa\nequations developed by Hartley and Zisserman (2004, p. 456) proceeds as follows. Take the\nleft and right singular vectors {u0, u1, v0, v1} of the fundamental matrix F (7.30) and their\nassociated singular values {σ0, σ1) and form the following set of equations:\nuT\n1 D0u1\nσ2\n0vT\n0 D1v0\n= −\nuT\n0 D0u1\nσ0σ1vT\n0 D1v1\n=\nuT\n0 D0u0\nσ2\n1vT\n1 D1v1\n,\n(7.35)\nwhere the two matrices\nDj = KjKT\nj = diag(f 2\nj , f 2\nj , 1) =\n\n\nf 2\nj\nf 2\nj\n1\n\n\n(7.36)\nencode the unknown focal lengths. For simplicity, let us rewrite each of the numerators and\ndenominators in (7.35) as\neij0(f 2\n0 )\n=\nuT\ni D0uj = aij + bijf 2\n0 ,\n(7.37)\neij1(f 2\n1 )\n=\nσiσjvT\ni D1vj = cij + dijf 2\n1 .\n(7.38)\nNotice that each of these is afﬁne (linear plus constant) in either f 2\n0 or f 2\n1 .\nHence, we\ncan cross-multiply these equations to obtain quadratic equations in f 2\nj , which can readily\nbe solved. (See also the work by Bougnoux (1998) for some alternative formulations.)\nAn alternative solution technique is to observe that we have a set of three equations related\nby an unknown scalar λ, i.e.,\neij0(f 2\n0 ) = λeij1(f 2\n1 )\n(7.39)\n(Richard Hartley, personal communication, July 2009). These can readily be solved to yield\n(f 2\n0 , λf 2\n1 , λ) and hence (f0, f1).\nHow well does this approach work in practice? There are certain degenerate conﬁgura-\ntions, such as when there is no rotation or when the optical axes intersect, when it does not\nwork at all. (In such a situation, you can vary the focal lengths of the cameras and obtain\n9 In Photo Tourism, our system registered photographs of an information sign outside Notre Dame with real\npictures of the cathedral.",
  "image_path": "page_377.jpg",
  "pages": [
    376,
    377,
    378
  ]
}