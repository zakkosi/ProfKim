{
  "doc_id": "pages_088_090",
  "text": "66\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n-90\n-80\n-70\n-60\n-50\n-40\n-30\n-20\n-10\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nAmbient\nDiffuse\nExp=10\nExp=100\nExp=1000\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAmbient\nDiffuse\nExp=10\nExp=100\nExp=1000\n(a)\n(b)\nFigure 2.18 Cross-section through a Phong shading model BRDF for a ﬁxed incident illu-\nmination direction: (a) component values as a function of angle away from surface normal;\n(b) polar plot. The value of the Phong exponent ke is indicated by the “Exp” labels and the\nlight source is at an angle of 30◦away from the normal.\nblue sky. In the Phong model, the ambient term does not depend on surface orientation, but\ndepends on the color of both the ambient illumination La(λ) and the object ka(λ),\nfa(λ) = ka(λ)La(λ).\n(2.92)\nPutting all of these terms together, we arrive at the Phong shading model,\nLr(ˆvr; λ) = ka(λ)La(λ) + kd(λ)\nX\ni\nLi(λ)[ˆvi · ˆn]+ + ks(λ)\nX\ni\nLi(λ)(ˆvr · ˆsi)ke. (2.93)\nFigure 2.18 shows a typical set of Phong shading model components as a function of the\nangle away from the surface normal (in a plane containing both the lighting direction and the\nviewer).\nTypically, the ambient and diffuse reﬂection color distributions ka(λ) and kd(λ) are the\nsame, since they are both due to sub-surface scattering (body reﬂection) inside the surface\nmaterial (Shafer 1985). The specular reﬂection distribution ks(λ) is often uniform (white),\nsince it is caused by interface reﬂections that do not change the light color. (The exception\nto this are metallic materials, such as copper, as opposed to the more common dielectric\nmaterials, such as plastics.)\nThe ambient illumination La(λ) often has a different color cast from the direct light\nsources Li(λ), e.g., it may be blue for a sunny outdoor scene or yellow for an interior lit\nwith candles or incandescent lights. (The presence of ambient sky illumination in shadowed\nareas is what often causes shadows to appear bluer than the corresponding lit portions of a\nscene). Note also that the diffuse component of the Phong model (or of any shading model)\ndepends on the angle of the incoming light source ˆvi, while the specular component depends\non the relative angle between the viewer vr and the specular reﬂection direction ˆsi (which\nitself depends on the incoming light direction ˆvi and the surface normal ˆn).\n2.2 Photometric image formation\n67\nThe Phong shading model has been superseded in terms of physical accuracy by a number\nof more recently developed models in computer graphics, including the model developed by\nCook and Torrance (1982) based on the original micro-facet model of Torrance and Sparrow\n(1967). Until recently, most computer graphics hardware implemented the Phong model but\nthe recent advent of programmable pixel shaders makes the use of more complex models\nfeasible.\nDi-chromatic reﬂection model\nThe Torrance and Sparrow (1967) model of reﬂection also forms the basis of Shafer’s (1985)\ndi-chromatic reﬂection model, which states that the apparent color of a uniform material lit\nfrom a single source depends on the sum of two terms,\nLr(ˆvr; λ)\n=\nLi(ˆvr, ˆvi, ˆn; λ) + Lb(ˆvr, ˆvi, ˆn; λ)\n(2.94)\n=\nci(λ)mi(ˆvr, ˆvi, ˆn) + cb(λ)mb(ˆvr, ˆvi, ˆn),\n(2.95)\ni.e., the radiance of the light reﬂected at the interface, Li, and the radiance reﬂected at the sur-\nface body, Lb. Each of these, in turn, is a simple product between a relative power spectrum\nc(λ), which depends only on wavelength, and a magnitude m(ˆvr, ˆvi, ˆn), which depends only\non geometry. (This model can easily be derived from a generalized version of Phong’s model\nby assuming a single light source and no ambient illumination, and re-arranging terms.) The\ndi-chromatic model has been successfully used in computer vision to segment specular col-\nored objects with large variations in shading (Klinker 1993) and more recently has inspired\nlocal two-color models for applications such Bayer pattern demosaicing (Bennett, Uytten-\ndaele, Zitnick et al. 2006).\nGlobal illumination (ray tracing and radiosity)\nThe simple shading model presented thus far assumes that light rays leave the light sources,\nbounce off surfaces visible to the camera, thereby changing in intensity or color, and arrive\nat the camera. In reality, light sources can be shadowed by occluders and rays can bounce\nmultiple times around a scene while making their trip from a light source to the camera.\nTwo methods have traditionally been used to model such effects. If the scene is mostly\nspecular (the classic example being scenes made of glass objects and mirrored or highly pol-\nished balls), the preferred approach is ray tracing or path tracing (Glassner 1995; Akenine-\nM¨oller and Haines 2002; Shirley 2005), which follows individual rays from the camera across\nmultiple bounces towards the light sources (or vice versa). If the scene is composed mostly\nof uniform albedo simple geometry illuminators and surfaces, radiosity (global illumination)\ntechniques are preferred (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995).\n68\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nCombinations of the two techniques have also been developed (Wallace, Cohen, and Green-\nberg 1987), as well as more general light transport techniques for simulating effects such as\nthe caustics cast by rippling water.\nThe basic ray tracing algorithm associates a light ray with each pixel in the camera im-\nage and ﬁnds its intersection with the nearest surface. A primary contribution can then be\ncomputed using the simple shading equations presented previously (e.g., Equation (2.93))\nfor all light sources that are visible for that surface element. (An alternative technique for\ncomputing which surfaces are illuminated by a light source is to compute a shadow map,\nor shadow buffer, i.e., a rendering of the scene from the light source’s perspective, and then\ncompare the depth of pixels being rendered with the map (Williams 1983; Akenine-M¨oller\nand Haines 2002).) Additional secondary rays can then be cast along the specular direction\ntowards other objects in the scene, keeping track of any attenuation or color change that the\nspecular reﬂection induces.\nRadiosity works by associating lightness values with rectangular surface areas in the scene\n(including area light sources). The amount of light interchanged between any two (mutually\nvisible) areas in the scene can be captured as a form factor, which depends on their relative\norientation and surface reﬂectance properties, as well as the 1/r2 fall-off as light is distributed\nover a larger effective sphere the further away it is (Cohen and Wallace 1993; Sillion and\nPuech 1994; Glassner 1995). A large linear system can then be set up to solve for the ﬁnal\nlightness of each area patch, using the light sources as the forcing function (right hand side).\nOnce the system has been solved, the scene can be rendered from any desired point of view.\nUnder certain circumstances, it is possible to recover the global illumination in a scene from\nphotographs using computer vision techniques (Yu, Debevec, Malik et al. 1999).\nThe basic radiosity algorithm does not take into account certain near ﬁeld effects, such\nas the darkening inside corners and scratches, or the limited ambient illumination caused\nby partial shadowing from other surfaces. Such effects have been exploited in a number of\ncomputer vision algorithms (Nayar, Ikeuchi, and Kanade 1991; Langer and Zucker 1994).\nWhile all of these global illumination effects can have a strong effect on the appearance\nof a scene, and hence its 3D interpretation, they are not covered in more detail in this book.\n(But see Section 12.7.1 for a discussion of recovering BRDFs from real scenes and objects.)\n2.2.3 Optics\nOnce the light from a scene reaches the camera, it must still pass through the lens before\nreaching the sensor (analog ﬁlm or digital silicon). For many applications, it sufﬁces to\ntreat the lens as an ideal pinhole that simply projects all rays through a common center of\nprojection (Figures 2.8 and 2.9).\nHowever, if we want to deal with issues such as focus, exposure, vignetting, and aber-",
  "image_path": "page_089.jpg",
  "pages": [
    88,
    89,
    90
  ]
}