{
  "doc_id": "pages_808_810",
  "text": "786\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSection 14.4.1: Bag of words\nTwo bag of words classiﬁers, http://people.csail.mit.edu/fergus/iccv2005/bagwords.html\n(Fei-Fei and Perona 2005; Sivic, Russell, Efros et al. 2005).\nBag of features and hierarchical k-means, http://www.vlfeat.org/ (Nist´er and Stew´enius\n2006; Nowak, Jurie, and Triggs 2006).\nSection 14.4.2: Part-based models\nA simple parts and structure object detector, http://people.csail.mit.edu/fergus/iccv2005/\npartsstructure.html (Fischler and Elschlager 1973; Felzenszwalb and Huttenlocher 2005).\nSection 14.5.1: Machine learning software\nSupport vector machines (SVM) software (http://www.support-vector-machines.org/\nSVM soft.html) has pointers to lots of SVM libraries, including SVMlight, http://\nsvmlight.joachims.org/; LIBSVM, http://www.csie.ntu.edu.tw/∼cjlin/libsvm/ (Fan, Chen,\nand Lin 2005); and LIBLINEAR, http://www.csie.ntu.edu.tw/∼cjlin/liblinear/ (Fan,\nChang, Hsieh et al. 2008).\nKernel Machines: links to SVM, Gaussian processes, boosting, and other machine\nlearning algorithms, http://www.kernel-machines.org/software.\nMultiple kernels for image classiﬁcation, http://www.robots.ox.ac.uk/∼vgg/software/\nMKL/ (Varma and Ray 2007; Vedaldi, Gulshan, Varma et al. 2009).\nAppendix A.1–A.2: Matrix decompositions and linear least squares2\nBLAS (Basic Linear Algebra Subprograms), http://www.netlib.org/blas/ (Blackford,\nDemmel, Dongarra et al. 2002).\nLAPACK (Linear Algebra PACKage), http://www.netlib.org/lapack/ (Anderson, Bai,\nBischof et al. 1999).\nGotoBLAS, http://www.tacc.utexas.edu/tacc-projects/.\nATLAS (Automatically Tuned Linear Algebra Software), http://math-atlas.sourceforge.\nnet/ (Demmel, Dongarra, Eijkhout et al. 2005).\nIntel Math Kernel Library (MKL), http://software.intel.com/en-us/intel-mkl/.\nAMD Core Math Library (ACML), http://developer.amd.com/cpu/Libraries/acml/Pages/\ndefault.aspx.\n2 Thanks to Sameer Agarwal for suggesting and describing most of these sites.\nC.2 Software\n787\nRobust PCA code, http://www.salle.url.edu/∼ftorre/papers/rpca2.html (De la Torre and\nBlack 2003).\nAppendix A.3: Non-linear least squares\nMINPACK, http://www.netlib.org/minpack/.\nlevmar: Levenberg–Marquardt nonlinear least squares algorithms, http://www.ics.forth.\ngr/∼lourakis/levmar/ (Madsen, Nielsen, and Tingleff 2004).\nAppendix A.4–A.5: Direct and iterative sparse matrix solvers\nSuiteSparse (various reordering algorithms, CHOLMOD) and SuiteSparse QR, http:\n//www.cise.uﬂ.edu/research/sparse/SuiteSparse/ (Davis 2006, 2008).\nPARDISO (iterative and sparse direct solution), http://www.pardiso-project.org/.\nTAUCS (sparse direct, iterative, out of core, preconditioners), http://www.tau.ac.il/\n∼stoledo/taucs/.\nHSL Mathematical Software Library, http://www.hsl.rl.ac.uk/index.html.\nTemplates for the solution of linear systems, http://www.netlib.org/linalg/html templates/\nTemplates.html (Barrett, Berry, Chan et al. 1994). Download the PDF for instructions\non how to get the software.\nITSOL, MIQR, and other sparse solvers, http://www-users.cs.umn.edu/∼saad/software/\n(Saad 2003).\nILUPACK, http://www-public.tu-bs.de/∼bolle/ilupack/.\nAppendix B: Bayesian modeling and inference\nMiddlebury source code for MRF minimization, http://vision.middlebury.edu/MRF/\ncode/ (Szeliski, Zabih, Scharstein et al. 2008).\nC++ code for efﬁcient belief propagation for early vision, http://people.cs.uchicago.\nedu/∼pff/bp/ (Felzenszwalb and Huttenlocher 2006).\nFastPD MRF optimization code, http://www.csd.uoc.gr/∼komod/FastPD (Komodakis\nand Tziritas 2007a; Komodakis, Tziritas, and Paragios 2008)\n788\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ndouble urand()\n{\nreturn ((double) rand()) / ((double) RAND MAX);\n}\nvoid grand(double& g1, double& g2)\n{\n#ifndef M PI\n#define M PI 3.14159265358979323846\n#endif // M PI\ndouble n1 = urand();\ndouble n2 = urand();\ndouble x1 = n1 + (n1 == 0); /* guard against log(0) */\ndouble sqlogn1 = sqrt(-2.0 * log (x1));\ndouble angl = (2.0 * M PI) * n2;\ng1 = sqlogn1 * cos(angl);\ng2 = sqlogn1 * sin(angl);\n}\nAlgorithm C.1 C algorithm for Gaussian random noise generation, using the Box–Muller\ntransform.\nGaussian noise generation.\nA lot of basic software packages come with a uniform random\nnoise generator (e.g., the rand() routine in Unix), but not all have a Gaussian random\nnoise generator. To compute a normally distributed random variable, you can use the Box–\nMuller transform (Box and Muller 1958), whose C code is given in Algorithm C.1—note that\nthis routine returns pairs of random variables. Alternative methods for generating Gaussian\nrandom numbers are given by Thomas, Luk, Leong et al. (2007).\nPseudocolor generation.\nIn many applications, it is convenient to be able to visualize the\nset of labels assigned to an image (or to image features such as lines). One of the easiest\nways to do this is to assign a unique color to each integer label. In my work, I have found it\nconvenient to distribute these labels in a quasi-uniform fashion around the RGB color cube\nusing the following idea.\nFor each (non-negative) label value, consider the bits as being split among the three color\nchannels, e.g., for a nine-bit value, the bits could be labeled RGBRGBRGB. After collecting\neach of the three color values, reverse the bits so that the low-order bits vary the most quickly.",
  "image_path": "page_809.jpg",
  "pages": [
    808,
    809,
    810
  ]
}