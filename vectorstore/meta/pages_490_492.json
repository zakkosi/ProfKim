{
  "doc_id": "pages_490_492",
  "text": "468\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 10.1\nComputational photography: (a) merging multiple exposures to create high\ndynamic range images (Debevec and Malik 1997) c⃝1997 ACM; (b) merging ﬂash and non-\nﬂash photographs; (Petschnigg, Agrawala, Hoppe et al. 2004) c⃝2004 ACM; (c) image mat-\nting and compositing; (Chuang, Curless, Salesin et al. 2001) c⃝2001 IEEE; (d) hole ﬁlling\nwith inpainting (Criminisi, P´erez, and Toyama 2004) c⃝2004 IEEE.\n10 Computational photography\n469\nStitching multiple images into wide ﬁeld of view panoramas, which we covered in Chapter 9,\nallows us create photographs that could not be captured with a regular camera. This is just\none instance of computational photography, where image analysis and processing algorithms\nare applied to one or more photographs to create images that go beyond the capabilities of\ntraditional imaging systems. Some of these techniques are now being incorporated directly\ninto digital still cameras. For example, some of the newer digital still cameras have sweep\npanorama modes and take multiple shots in low-light conditions to reduce image noise.\nIn this chapter, we cover a number of additional computational photography algorithms.\nWe begin with a review of photometric image calibration (Section 10.1), i.e., the measurement\nof camera and lens responses, which is a prerequisite for many of the algorithms we describe\nlater. We then discuss high dynamic range imaging (Section 10.2), which captures the full\nrange of brightness in a scene through the use of multiple exposures (Figure 10.1a). We also\ndiscuss tone mapping operators, which map rich images back into regular display devices,\nsuch as screens and printers, as well as algorithms that merge ﬂash and regular images to\nobtain better exposures (Figure 10.1b).\nNext, we discuss how the resolution of images can be improved either by merging mul-\ntiple photographs together or using sophisticated image priors (Section 10.3). This includes\nalgorithms for extracting full-color images from the patterned Bayer mosaics present in most\ncameras.\nIn Section 10.4, we discuss algorithms for cutting pieces of images from one photograph\nand pasting them into others (Figure 10.1c). In Section 10.5, we describe how to generate\nnovel textures from real-world samples for applications such as ﬁlling holes in images (Fig-\nure 10.1d). We close with a brief overview of non-photorealistic rendering (Section 10.5.2),\nwhich can turn regular photographs into artistic renderings that resemble traditional drawings\nand paintings.\nOne topic that we do not cover extensively in this book is novel computational sensors,\noptics, and cameras. A nice survey can be found in an article by Nayar (2006), a recently\npublished book by Raskar and Tumblin (2010), and more recent research papers (Levin,\nFergus, Durand et al. 2007). Some related discussion can also be found in Sections 10.2\nand 13.3.\nA good general-audience introduction to computational photography can be found in the\narticle by Hayes (2008) as well as survey papers by Nayar (2006), Cohen and Szeliski (2006),\nLevoy (2006), and Debevec (2006).1 Raskar and Tumblin (2010) give extensive coverage of\ntopics in this area, with particular emphasis on computational cameras and sensors. The\nsub-ﬁeld of high dynamic range imaging has its own book discussing research in this area\n(Reinhard, Ward, Pattanaik et al. 2005), as well as a wonderful book aimed more at profes-\n1 See also the two special issue journals edited by Bimber (2006) and Durand and Szeliski (2007).\n470\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nsional photographers (Freeman 2008).2 A good survey of image matting is provided by Wang\nand Cohen (2007a).\nThere are also several courses on computational photography where the instructors have\nprovided extensive on-line materials, e.g., Fr´edo Durand’s Computation Photography course\nat MIT,3 Alyosha Efros’ class at Carnegie Mellon,4 Marc Levoy’s class at Stanford,5 and a\nseries of SIGGRAPH courses on Computational Photography.6\n10.1 Photometric calibration\nBefore we can successfully merge multiple photographs, we need to characterize the func-\ntions that map incoming irradiance into pixel values and also the amounts of noise present\nin each image. In this section, we examine three components of the imaging pipeline (Fig-\nure 10.2) that affect this mapping.\nThe ﬁrst is the radiometric response function (Mitsunaga and Nayar 1999), which maps\nphotons arriving at the lens into digital values stored in the image ﬁle (Section 10.1.1). The\nsecond is vignetting, which darkens pixel values near the periphery of images, especially at\nlarge apertures (Section 10.1.3). The third is the point spread function, which characterizes\nthe blur induced by the lens, anti-aliasing ﬁlters, and ﬁnite sensor areas (Section 10.1.4).7 The\nmaterial in this section builds on the image formation processes described in Sections 2.2.3\nand 2.3.3, so if it has been a while since you looked at those sections, please go back and\nreview them.\n10.1.1 Radiometric response function\nAs we can see in Figure 10.2, a number of factors affect how the intensity of light arriving\nat the lens ends up being mapped into stored digital values. Let us ignore for now any non-\nuniform attenuation that may occur inside the lens, which we cover in Section 10.1.3.\nThe ﬁrst factors to affect this mapping are the aperture and shutter speed (Section 2.3),\nwhich can be modeled as global multipliers on the incoming light, most conveniently mea-\nsured in exposure values (log2 brightness ratios). Next, the analog to digital (A/D) converter\non the sensing chip applies an electronic gain, usually controlled by the ISO setting on your\ncamera. While in theory this gain is linear, as with any electronics non-linearities may be\n2 Gulbins and Gulbins (2009) discuss related photographic techniques.\n3 MIT 6.815/6.865, http://stellar.mit.edu/S/course/6/sp08/6.815/materials.html.\n4 CMU 15-463, http://graphics.cs.cmu.edu/courses/15-463/.\n5 Stanford CS 448A, http://graphics.stanford.edu/courses/cs448a-10/.\n6 http://web.media.mit.edu/∼raskar/photo/.\n7 Additional photometric camera and lens effects include sensor glare, blooming, and chromatic aberration, which\ncan also be thought of as a spectrally varying form of geometric aberration (Section 2.2.3).",
  "image_path": "page_491.jpg",
  "pages": [
    490,
    491,
    492
  ]
}