{
  "doc_id": "pages_240_242",
  "text": "218\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n Scale\n (first\n octave)\nScale\n(next\noctave)\nGaussian\nDifference of\nGaussian (DOG)\n. . .\nScale\n(a)\n(b)\nFigure 4.11 Scale-space feature detection using a sub-octave Difference of Gaussian pyra-\nmid (Lowe 2004) c⃝2004 Springer: (a) Adjacent levels of a sub-octave Gaussian pyramid\nare subtracted to produce Difference of Gaussian images; (b) extrema (maxima and minima)\nin the resulting 3D volume are detected by comparing a pixel to its 26 neighbors.\nWhile Lowe’s Scale Invariant Feature Transform (SIFT) performs well in practice, it is not\nbased on the same theoretical foundation of maximum spatial stability as the auto-correlation-\nbased detectors. (In fact, its detection locations are often complementary to those produced\nby such techniques and can therefore be used in conjunction with these other approaches.)\nIn order to add a scale selection mechanism to the Harris corner detector, Mikolajczyk and\nSchmid (2004) evaluate the Laplacian of Gaussian function at each detected Harris point (in\na multi-scale pyramid) and keep only those points for which the Laplacian is extremal (larger\nor smaller than both its coarser and ﬁner-level values). An optional iterative reﬁnement for\nboth scale and position is also proposed and evaluated. Additional examples of scale invariant\nregion detectors are discussed by Mikolajczyk, Tuytelaars, Schmid et al. (2005); Tuytelaars\nand Mikolajczyk (2007).\nRotational invariance and orientation estimation\nIn addition to dealing with scale changes, most image matching and object recognition algo-\nrithms need to deal with (at least) in-plane image rotation. One way to deal with this problem\nis to design descriptors that are rotationally invariant (Schmid and Mohr 1997), but such\ndescriptors have poor discriminability, i.e. they map different looking patches to the same\ndescriptor.\n4.1 Points and patches\n219\nFigure 4.12\nA dominant orientation estimate can be computed by creating a histogram of\nall the gradient orientations (weighted by their magnitudes or after thresholding out small\ngradients) and then ﬁnding the signiﬁcant peaks in this distribution (Lowe 2004) c⃝2004\nSpringer.\nA better method is to estimate a dominant orientation at each detected keypoint. Once\nthe local orientation and scale of a keypoint have been estimated, a scaled and oriented patch\naround the detected point can be extracted and used to form a feature descriptor (Figures 4.10\nand 4.17).\nThe simplest possible orientation estimate is the average gradient within a region around\nthe keypoint. If a Gaussian weighting function is used (Brown, Szeliski, and Winder 2005),\nthis average gradient is equivalent to a ﬁrst-order steerable ﬁlter (Section 3.2.3), i.e., it can be\ncomputed using an image convolution with the horizontal and vertical derivatives of Gaus-\nsian ﬁlter (Freeman and Adelson 1991). In order to make this estimate more reliable, it is\nusually preferable to use a larger aggregation window (Gaussian kernel size) than detection\nwindow (Brown, Szeliski, and Winder 2005). The orientations of the square boxes shown in\nFigure 4.10 were computed using this technique.\nSometimes, however, the averaged (signed) gradient in a region can be small and therefore\nan unreliable indicator of orientation. A more reliable technique is to look at the histogram\nof orientations computed around the keypoint. Lowe (2004) computes a 36-bin histogram\nof edge orientations weighted by both gradient magnitude and Gaussian distance to the cen-\nter, ﬁnds all peaks within 80% of the global maximum, and then computes a more accurate\norientation estimate using a three-bin parabolic ﬁt (Figure 4.12).\nAfﬁne invariance\nWhile scale and rotation invariance are highly desirable, for many applications such as wide\nbaseline stereo matching (Pritchett and Zisserman 1998; Schaffalitzky and Zisserman 2002)\nor location recognition (Chum, Philbin, Sivic et al. 2007), full afﬁne invariance is preferred.\n220\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.13\nAfﬁne region detectors used to match two images taken from dramatically\ndifferent viewpoints (Mikolajczyk and Schmid 2004) c⃝2004 Springer.\nx0 →\nA−1/2\n0\nx′\n0\nx′\n0 →\nRx′\n1\nA−1/2\n1\nx′\n1\n←x1\nFigure 4.14\nAfﬁne normalization using the second moment matrices, as described by Miko-\nlajczyk, Tuytelaars, Schmid et al. (2005) c⃝2005 Springer. After image coordinates are trans-\nformed using the matrices A−1/2\n0\nand A−1/2\n1\n, they are related by a pure rotation R, which\ncan be estimated using a dominant orientation technique.\nAfﬁne-invariant detectors not only respond at consistent locations after scale and orientation\nchanges, they also respond consistently across afﬁne deformations such as (local) perspective\nforeshortening (Figure 4.13). In fact, for a small enough patch, any continuous image warping\ncan be well approximated by an afﬁne deformation.\nTo introduce afﬁne invariance, several authors have proposed ﬁtting an ellipse to the auto-\ncorrelation or Hessian matrix (using eigenvalue analysis) and then using the principal axes\nand ratios of this ﬁt as the afﬁne coordinate frame (Lindeberg and Garding 1997; Baumberg\n2000; Mikolajczyk and Schmid 2004; Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuyte-\nlaars and Mikolajczyk 2007). Figure 4.14 shows how the square root of the moment matrix\ncan be used to transform local patches into a frame which is similar up to rotation.\nAnother important afﬁne invariant region detector is the maximally stable extremal region\n(MSER) detector developed by Matas, Chum, Urban et al. (2004). To detect MSERs, binary\nregions are computed by thresholding the image at all possible gray levels (the technique\ntherefore only works for grayscale images). This operation can be performed efﬁciently by\nﬁrst sorting all pixels by gray value and then incrementally adding pixels to each connected\ncomponent as the threshold is changed (Nist´er and Stew´enius 2008). As the threshold is\nchanged, the area of each component (region) is monitored; regions whose rate of change of\narea with respect to the threshold is minimal are deﬁned as maximally stable. Such regions",
  "image_path": "page_241.jpg",
  "pages": [
    240,
    241,
    242
  ]
}