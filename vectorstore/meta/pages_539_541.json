{
  "doc_id": "pages_539_541",
  "text": "10.4 Image matting and compositing\n517\n(a)\n(b)\n(c)\n(d)\nFigure 10.47\nSmoke matting (Chuang, Agarwala, Curless et al. 2002) c⃝2002 ACM: (a)\ninput video frame; (b) after removing the foreground object; (c) estimated alpha matte; (d)\ninsertion of new objects into the background.\nFigure 10.48 Shadow matting (Chuang, Goldman, Curless et al. 2003) c⃝2003 ACM. In-\nstead of simply darkening the new scene with the shadow (c), shadow matting correctly dims\nthe lit scene with the new shadow and drapes the shadow over 3D geometry (d).\nspace is used to estimate this foreground color and the distance along each color line is used\nto estimate the per-pixel temporally varying alpha (Figure 10.47).\nExtracting and re-inserting shadows is also possible using a related technique (Chuang,\nGoldman, Curless et al. 2003). Here, instead of assuming a constant foreground color, each\npixel is assumed to vary between its fully lit and fully shadowed colors, which can be esti-\nmated by taking (robust) minimum and maximum values over time as a shadow passes over\nthe scene (Exercise 10.9). The resulting fractional shadow matte can be used to re-project\nthe shadow into a new scene. If the destination scene has a non-planar geometry, it can be\nscanned by waving a straight stick shadow across the scene. The new shadow matte can then\nbe warped with the computed deformation ﬁeld to have it drape correctly over the new scene\n(Figure 10.48).\nThe quality and reliability of matting algorithms can also be enhanced using more sophis-\nticated acquisition systems. For example, taking a ﬂash and non-ﬂash image pair supports\nthe reliable extraction of foreground mattes, which show up as regions of large illumination\nchange between the two images (Sun, Li, Kang et al. 2006). Taking simultaneous video\nstreams focused at different distances (McGuire, Matusik, Pﬁster et al. 2005) or using multi-\ncamera arrays (Joshi, Matusik, and Avidan 2006) are also good approaches to producing\n518\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nhigh-quality mattes. These techniques are described in more detail in (Wang and Cohen\n2007a).\nLastly, photographing a refractive object in front of a number of patterned backgrounds\nallows the object to be placed in novel 3D environments. These environment matting tech-\nniques (Zongker, Werner, Curless et al. 1999; Chuang, Zongker, Hindorff et al. 2000) are\ndiscussed in Section 13.4.\n10.4.5 Video matting\nWhile regular single-frame matting techniques such as blue or green screen matting (Smith\nand Blinn 1996; Wright 2006; Brinkmann 2008) can be applied to video sequences, the pres-\nence of moving objects can sometimes make the matting process easier, as portions of the\nbackground may get revealed in preceding or subsequent frames.\nChuang, Agarwala, Curless et al. (2002) describe a nice approach to this video matting\nproblem, where foreground objects are ﬁrst removed using a conservative garbage matte and\nthe resulting background plates are aligned and composited to yield a high-quality back-\nground estimate. They also describe how trimaps drawn at sparse keyframes can be inter-\npolated to in-between frames using bi-direction optic ﬂow. Alternative approaches to video\nmatting, such as rotoscoping, which involves drawing and tracking curves in video sequences\n(Agarwala, Hertzmann, Seitz et al. 2004), are discussed in the matting survey paper by Wang\nand Cohen (2007a).\n10.5 Texture analysis and synthesis\nWhile texture analysis and synthesis may not at ﬁrst seem like computational photography\ntechniques, they are, in fact, widely used to repair defects, such as small holes, in images or\nto create non-photorealistic painterly renderings from regular photographs.\nThe problem of texture synthesis can be formulated as follows: given a small sample of\na “texture” (Figure 10.49a), generate a larger similar-looking image (Figure 10.49b). As you\ncan imagine, for certain sample textures, this problem can be quite challenging.\nTraditional approaches to texture analysis and synthesis try to match the spectrum of the\nsource image while generating shaped noise. Matching the frequency characteristics, which\nis equivalent to matching spatial correlations, is in itself not sufﬁcient. The distributions of\nthe responses at different frequencies must also match. Heeger and Bergen (1995) develop an\nalgorithm that alternates between matching the histograms of multi-scale (steerable pyramid)\nresponses and matching the ﬁnal image histogram. Portilla and Simoncelli (2000) improve\non this technique by also matching pairwise statistics across scale and orientations. De Bonet\n(1997) uses a coarse-to-ﬁne strategy to ﬁnd locations in the source texture with a similar\n10.5 Texture analysis and synthesis\n519\nradishes\nlots more radishes\nrocks\nyogurt\n(a)\n(b)\n(c)\nFigure 10.49 Texture synthesis: (a) given a small patch of texture, the task is to synthesize\n(b) a similar-looking larger patch; (c) other semi-structured textures that are challenging to\nsynthesize. (Images courtesy of Alyosha Efros.)\nparent structure, i.e., similar multi-scale oriented ﬁlter responses, and then randomly chooses\none of these matching locations as the current sample value.\nMore recent texture synthesis algorithms sequentially generate texture pixels by looking\nfor neighborhoods in the source texture that are similar to the currently synthesized image\n(Efros and Leung 1999). Consider the (as yet) unknown pixel p in the partially constructed\ntexture on the left side of Figure 10.50. Since some of its neighboring pixels have been\nalready been synthesized, we can look for similar partial neighborhoods in the sample texture\nimage on the right and randomly select one of these as the new value of p. This process\ncan be repeated down the new image either in a raster fashion or by scanning around the\nperiphery (“onion peeling”) when ﬁlling holes, as discussed in (Section 10.5.1). In their\nactual implementation, Efros and Leung (1999) ﬁnd the most similar neighborhood and then\ninclude all other neighborhoods within a d = (1 + ϵ) distance, with ϵ = 0.1. They also\noptionally weight the random pixel selections by the similarity metric d.\nTo accelerate this process and improve its visual quality, Wei and Levoy (2000) extend\nthis technique using a coarse-to-ﬁne generation process, where coarser levels of the pyramid,\nwhich have already been synthesized, are also considered during the matching (De Bonet\n1997). To accelerate the nearest neighbor ﬁnding, tree-structured vector quantization is used.\nEfros and Freeman (2001) propose an alternative acceleration and visual quality improve-",
  "image_path": "page_540.jpg",
  "pages": [
    539,
    540,
    541
  ]
}