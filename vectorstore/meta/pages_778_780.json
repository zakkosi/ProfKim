{
  "doc_id": "pages_778_780",
  "text": "756\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe following problem commonly recurs in this book: Given a number of measurements\n(images, feature positions, etc.), estimate the values of some unknown structure or parameter\n(camera positions, object shape, etc.). These kinds of problems are in general called inverse\nproblems because they involve estimating unknown model parameters instead of simulating\nthe forward formation equations.1 Computer graphics is a classic forward modeling problem\n(given some objects, cameras, and lighting, simulate the images that would result), while\ncomputer vision problems are usually of the inverse kind (given one or more images, recover\nthe scene that gave rise to these images).\nGiven an instance of an inverse problem, there are, in general, several ways to proceed.\nFor instance, through clever (or sometimes straightforward) algebraic manipulation, a closed\nform solution for the unknowns can sometimes be derived. Consider, for example, the camera\nmatrix calibration problem (Section 6.2.1): given an image of a calibration pattern consisting\nof known 3D point positions, compute the 3×4 camera matrix P that maps these points onto\nthe image plane.\nIn more detail, we can write this problem as (6.33–6.34)\nxi\n=\np00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(B.1)\nyi\n=\np10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n,\n(B.2)\nwhere (xi, yi) is the feature position of the ith point measured in the image plane, (Xi, Yi, Zi)\nis the corresponding 3D point position, and the pij are the unknown entries of the camera\nmatrix P . Moving the denominator over to the left hand side, we end up with a set of\nsimultaneous linear equations,\nxi(p20Xi + p21Yi + p22Zi + p23)\n=\np00Xi + p01Yi + p02Zi + p03,\n(B.3)\nyi(p20Xi + p21Yi + p22Zi + p23)\n=\np10Xi + p11Yi + p12Zi + p13,\n(B.4)\nwhich we can solve using linear least squares (Appendix A.2) to obtain an estimate of P .\nThe question then arises: is this set of equations the right ones to be solving? If the\nmeasurements are totally noise-free or we do not care about getting the best possible answer,\nthen the answer is yes. However, in general, we cannot be sure that we have a reasonable\nalgorithm unless we make a model of the likely sources of error and devise an algorithm that\nperforms as well as possible given these potential errors.\n1 In machine learning, these problems are called regression problems, because we are trying to estimate a contin-\nuous quantity from noisy inputs, as opposed to a discrete classiﬁcation task (Bishop 2006).\nB.1 Estimation theory\n757\nB.1 Estimation theory\nThe study of such inference problems from noisy data is often called estimation theory (Gelb\n1974), and its extension to problems where we explicitly choose a loss function is called sta-\ntistical decision theory (Berger 1993; Hastie, Tibshirani, and Friedman 2001; Bishop 2006;\nRobert 2007). We ﬁrst start by writing down the forward process that leads from our un-\nknowns (and knowns) to a set of noise-corrupted measurements. We then devise an algorithm\nthat will give us an estimate (or set of estimates) that are both insensitive to the noise (as best\nthey can be) and also quantify the reliability of these estimates.\nThe speciﬁc equations above (B.1) are just a particular instance of a more general set of\nmeasurement equations,\nyi = f i(x) + ni.\n(B.5)\nHere, the yi are the noise-corrupted measurements, e.g., (xi, yi) in Equation (B.1), and x is\nthe unknown state vector.2\nEach measurement comes with its associated measurement model f i(x), which maps the\nunknown into that particular measurement. An alternative formulation would be to have one\ngeneral function f(x, pi) and to use a per-measurement parameter vector pi to distinguish\nbetween different measurements, e.g., (Xi, Yi, Zi) in Equation (B.1). Note that the use of the\nf i(x) form makes it straightforward to have measurements of different dimensions, which\nbecomes useful when we start adding in prior information (Appendix B.4).\nEach measurement is also contaminated with some noise ni. In Equation (B.5), we have\nindicated that ni is a zero-mean normal (Gaussian) random variable with a covariance matrix\nΣi. In general, the noise need not be Gaussian and, in fact, it is usually prudent to assume\nthat some measurements may be outliers. However, we defer this discussion to Appendix B.3,\nafter we have explored the simpler Gaussian noise case more fully. We also assume that the\nnoise vectors ni are independent. In the case where they are not (e.g., when some constant\ngain or offset contaminates all of the pixels in a given image), we can add this effect as a\nnuisance parameter to our state vector x and later estimate its value (and discard it, if so\ndesired).\nB.1.1 Likelihood for multivariate Gaussian noise\nGiven all of the noisy measurements y = {yi}, we would like to infer a probability distribu-\ntion on the unknown x vector. We can write the likelihood of having observed the {yi} given\na particular value of x as\nL = p(y|x) =\nY\ni\np(yi|x) =\nY\ni\np(yi|f i(x)) =\nY\ni\np(ni).\n(B.6)\n2 In the Kalman ﬁltering literature (Gelb 1974), it is more common to use z instead of y to denote measurements.\n758\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen each noise vector ni is a multivariate Gaussian with covariance Σi,\nni ∼N(0, Σi),\n(B.7)\nwe can write this likelihood as\nL\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2(yi −f i(x))T Σ−1\ni (yi −f i(x))\n\u0013\n(B.8)\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2∥yi −f i(x)∥2\nΣ\n−1\ni\n\u0013\n,\nwhere the matrix norm ∥x∥2\nA is a shorthand notation for xT Ax.\nThe norm ∥yi −yi∥Σ\n−1\ni\nis often called the Mahalanobis distance (5.26 and 14.14) and is\nused to measure the distance between a measurement and the mean of a multivariate Gaussian\ndistribution. Contours of equal Mahalanobis distance are equi-probability contours. Note\nthat when the measurement covariance is isotropic (the same in all directions), i.e., when\nΣi = σ2\ni I, the likelihood can be written as\nL =\nY\ni\n(2πσ2\ni )−Ni/2 exp\n\u0012\n−1\n2σ2\ni\n∥yi −f i(x)∥2\n\u0013\n,\n(B.9)\nwhere Ni is the length of the ith measurement vector yi.\nWe can more easily visualize the structure of the covariance matrix and the correspond-\ning Mahalanobis distance if we ﬁrst perform an eigenvalue or principal component analysis\n(PCA) of the covariance matrix (A.6),\nΣ = Φ diag(λ0 . . . λN−1) ΦT .\n(B.10)\nEqual-probability contours of the corresponding multi-variate Gaussian, which are also equi-\ndistance contours in the Mahalanobis distance (Figure 14.14), are multi-dimensional ellip-\nsoids whose axis directions are given by the columns of Φ (the eigenvectors) and whose\nlengths are given by the σj =\np\nλj (Figure A.1).\nIt is usually more convenient to work with the negative log likelihood, which we can think\nof as a cost or energy\nE = −log L\n=\n1\n2\nX\ni\n(yi −f i(x))T Σ−1\ni (yi −f i(x)) + k\n(B.11)\n=\n1\n2\nX\ni\n∥yi −f i(x)∥2\nΣ\n−1\ni\n+ k,\n(B.12)\nwhere k = P\ni log |2πΣi| is a constant that depends on the measurement variances, but is\nindependent of x.",
  "image_path": "page_779.jpg",
  "pages": [
    778,
    779,
    780
  ]
}