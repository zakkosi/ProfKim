{
  "doc_id": "pages_114_116",
  "text": "92\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 2.33 Image compressed with JPEG at three quality settings. Note how the amount\nof block artifact and high-frequency aliasing (“mosquito noise”) increases from left to right.\nstatistically independent). Both MPEG and JPEG use 8 × 8 DCT transforms (Wallace 1991;\nLe Gall 1991), although newer variants use smaller 4×4 blocks or alternative transformations,\nsuch as wavelets (Taubman and Marcellin 2002) and lapped transforms (Malvar 1990, 1998,\n2000) are now used.\nAfter transform coding, the coefﬁcient values are quantized into a set of small integer\nvalues that can be coded using a variable bit length scheme such as a Huffman code or an\narithmetic code (Wallace 1991). (The DC (lowest frequency) coefﬁcients are also adaptively\npredicted from the previous block’s DC values. The term “DC” comes from “direct current”,\ni.e., the non-sinusoidal or non-alternating part of a signal.) The step size in the quantization\nis the main variable controlled by the quality setting on the JPEG ﬁle (Figure 2.33).\nWith video, it is also usual to perform block-based motion compensation, i.e., to encode\nthe difference between each block and a predicted set of pixel values obtained from a shifted\nblock in the previous frame. (The exception is the motion-JPEG scheme used in older DV\ncamcorders, which is nothing more than a series of individually JPEG compressed image\nframes.) While basic MPEG uses 16 × 16 motion compensation blocks with integer motion\nvalues (Le Gall 1991), newer standards use adaptively sized block, sub-pixel motions, and\nthe ability to reference blocks from older frames. In order to recover more gracefully from\nfailures and to allow for random access to the video stream, predicted P frames are interleaved\namong independently coded I frames. (Bi-directional B frames are also sometimes used.)\nThe quality of a compression algorithm is usually reported using its peak signal-to-noise\nratio (PSNR), which is derived from the average mean square error,\nMSE = 1\nn\nX\nx\nh\nI(x) −ˆI(x)\ni2\n,\n(2.117)\nwhere I(x) is the original uncompressed image and ˆI(x) is its compressed counterpart, or\nequivalently, the root mean square error (RMS error), which is deﬁned as\nRMS =\n√\nMSE.\n(2.118)\n2.4 Additional reading\n93\nThe PSNR is deﬁned as\nPSNR = 10 log10\nI2\nmax\nMSE = 20 log10\nImax\nRMS ,\n(2.119)\nwhere Imax is the maximum signal extent, e.g., 255 for eight-bit images.\nWhile this is just a high-level sketch of how image compression works, it is useful to\nunderstand so that the artifacts introduced by such techniques can be compensated for in\nvarious computer vision applications.\n2.4 Additional reading\nAs we mentioned at the beginning of this chapter, it provides but a brief summary of a very\nrich and deep set of topics, traditionally covered in a number of separate ﬁelds.\nA more thorough introduction to the geometry of points, lines, planes, and projections\ncan be found in textbooks on multi-view geometry (Hartley and Zisserman 2004; Faugeras\nand Luong 2001) and computer graphics (Foley, van Dam, Feiner et al. 1995; Watt 1995;\nOpenGL-ARB 1997). Topics covered in more depth include higher-order primitives such as\nquadrics, conics, and cubics, as well as three-view and multi-view geometry.\nThe image formation (synthesis) process is traditionally taught as part of a computer\ngraphics curriculum (Foley, van Dam, Feiner et al. 1995; Glassner 1995; Watt 1995; Shirley\n2005) but it is also studied in physics-based computer vision (Wolff, Shafer, and Healey\n1992a).\nThe behavior of camera lens systems is studied in optics (M¨oller 1988; Hecht 2001; Ray\n2002).\nSome good books on color theory have been written by Healey and Shafer (1992); Wyszecki\nand Stiles (2000); Fairchild (2005), with Livingstone (2008) providing a more fun and infor-\nmal introduction to the topic of color perception. Mark Fairchild’s page of color books and\nlinks25 lists many other sources.\nTopics relating to sampling and aliasing are covered in textbooks on signal and image\nprocessing (Crane 1997; J¨ahne 1997; Oppenheim and Schafer 1996; Oppenheim, Schafer,\nand Buck 1999; Pratt 2007; Russ 2007; Burger and Burge 2008; Gonzales and Woods 2008).\n2.5 Exercises\nA note to students: This chapter is relatively light on exercises since it contains mostly\nbackground material and not that many usable techniques. If you really want to understand\n25 http://www.cis.rit.edu/fairchild/WhyIsColor/books links.html.\n94\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmulti-view geometry in a thorough way, I encourage you to read and do the exercises provided\nby Hartley and Zisserman (2004). Similarly, if you want some exercises related to the image\nformation process, Glassner’s (1995) book is full of challenging problems.\nEx 2.1: Least squares intersection point and line ﬁtting—advanced\nEquation (2.4) shows\nhow the intersection of two 2D lines can be expressed as their cross product, assuming the\nlines are expressed as homogeneous coordinates.\n1. If you are given more than two lines and want to ﬁnd a point ˜x that minimizes the sum\nof squared distances to each line,\nD =\nX\ni\n(˜x · ˜li)2,\n(2.120)\nhow can you compute this quantity? (Hint: Write the dot product as ˜xT˜li and turn the\nsquared quantity into a quadratic form, ˜xT A˜x.)\n2. To ﬁt a line to a bunch of points, you can compute the centroid (mean) of the points\nas well as the covariance matrix of the points around this mean. Show that the line\npassing through the centroid along the major axis of the covariance ellipsoid (largest\neigenvector) minimizes the sum of squared distances to the points.\n3. These two approaches are fundamentally different, even though projective duality tells\nus that points and lines are interchangeable. Why are these two algorithms so appar-\nently different? Are they actually minimizing different objectives?\nEx 2.2: 2D transform editor\nWrite a program that lets you interactively create a set of\nrectangles and then modify their “pose” (2D transform). You should implement the following\nsteps:\n1. Open an empty window (“canvas”).\n2. Shift drag (rubber-band) to create a new rectangle.\n3. Select the deformation mode (motion model): translation, rigid, similarity, afﬁne, or\nperspective.\n4. Drag any corner of the outline to change its transformation.\nThis exercise should be built on a set of pixel coordinate and transformation classes, either\nimplemented by yourself or from a software library. Persistence of the created representation\n(save and load) should also be supported (for each rectangle, save its transformation).",
  "image_path": "page_115.jpg",
  "pages": [
    114,
    115,
    116
  ]
}