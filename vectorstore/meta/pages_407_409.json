{
  "doc_id": "pages_407_409",
  "text": "8.1 Translational alignment\n385\nHowever, since this function is not differentiable at the origin, it is not well suited to gradient-\ndescent approaches such as the ones presented in Section 8.1.3.\nInstead, a smoothly varying function that is quadratic for small values but grows more\nslowly away from the origin is often used. Black and Rangarajan (1996) discuss a variety of\nsuch functions, including the Geman–McClure function,\nρGM(x) =\nx2\n1 + x2/a2 ,\n(8.4)\nwhere a is a constant that can be thought of as an outlier threshold. An appropriate value for\nthe threshold can itself be derived using robust statistics (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Rousseeuw and Leroy 1987), e.g., by computing the median absolute\ndeviation, MAD = medi|ei|, and multiplying it by 1.4 to obtain a robust estimate of the\nstandard deviation of the inlier noise process (Stewart 1999).\nSpatially varying weights.\nThe error metrics above ignore that fact that for a given align-\nment, some of the pixels being compared may lie outside the original image boundaries.\nFurthermore, we may want to partially or completely downweight the contributions of cer-\ntain pixels. For example, we may want to selectively “erase” some parts of an image from\nconsideration when stitching a mosaic where unwanted foreground objects have been cut out.\nFor applications such as background stabilization, we may want to downweight the middle\npart of the image, which often contains independently moving objects being tracked by the\ncamera.\nAll of these tasks can be accomplished by associating a spatially varying per-pixel weight\nvalue with each of the two images being matched.\nThe error metric then becomes the\nweighted (or windowed) SSD function,\nEWSSD(u) =\nX\ni\nw0(xi)w1(xi + u)[I1(xi + u) −I0(xi)]2,\n(8.5)\nwhere the weighting functions w0 and w1 are zero outside the image boundaries.\nIf a large range of potential motions is allowed, the above metric can have a bias towards\nsmaller overlap solutions. To counteract this bias, the windowed SSD score can be divided\nby the overlap area\nA =\nX\ni\nw0(xi)w1(xi + u)\n(8.6)\nto compute a per-pixel (or mean) squared pixel error EWSSD/A. The square root of this\nquantity is the root mean square intensity error\nRMS =\np\nEWSSD/A\n(8.7)\noften reported in comparative studies.\n386\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBias and gain (exposure differences).\nOften, the two images being aligned were not taken\nwith the same exposure. A simple model of linear (afﬁne) intensity variation between the two\nimages is the bias and gain model,\nI1(x + u) = (1 + α)I0(x) + β,\n(8.8)\nwhere β is the bias and α is the gain (Lucas and Kanade 1981; Gennert 1988; Fuh and\nMaragos 1991; Baker, Gross, and Matthews 2003; Evangelidis and Psarakis 2008). The least\nsquares formulation then becomes\nEBG(u) =\nX\ni\n[I1(xi + u) −(1 + α)I0(xi) −β]2 =\nX\ni\n[αI0(xi) + β −ei]2.\n(8.9)\nRather than taking a simple squared difference between corresponding patches, it becomes\nnecessary to perform a linear regression (Appendix A.2), which is somewhat more costly.\nNote that for color images, it may be necessary to estimate a different bias and gain for each\ncolor channel to compensate for the automatic color correction performed by some digital\ncameras (Section 2.3.2). Bias and gain compensation is also used in video codecs, where it is\nknown as weighted prediction (Richardson 2003).\nA more general (spatially varying, non-parametric) model of intensity variation, which is\ncomputed as part of the registration process, is used in (Negahdaripour 1998; Jia and Tang\n2003; Seitz and Baker 2009). This can be useful for dealing with local variations such as\nthe vignetting caused by wide-angle lenses, wide apertures, or lens housings. It is also pos-\nsible to pre-process the images before comparing their values, e.g., using band-pass ﬁltered\nimages (Anandan 1989; Bergen, Anandan, Hanna et al. 1992), gradients (Scharstein 1994;\nPapenberg, Bruhn, Brox et al. 2006), or using other local transformations such as histograms\nor rank transforms (Cox, Roy, and Hingorani 1995; Zabih and Woodﬁll 1994), or to max-\nimize mutual information (Viola and Wells III 1997; Kim, Kolmogorov, and Zabih 2003).\nHirschm¨uller and Scharstein (2009) compare a number of these approaches and report on\ntheir relative performance in scenes with exposure differences.\nCorrelation.\nAn alternative to taking intensity differences is to perform correlation, i.e., to\nmaximize the product (or cross-correlation) of the two aligned images,\nECC(u) =\nX\ni\nI0(xi)I1(xi + u).\n(8.10)\nAt ﬁrst glance, this may appear to make bias and gain modeling unnecessary, since the images\nwill prefer to line up regardless of their relative scales and offsets. However, this is actually\nnot true. If a very bright patch exists in I1(x), the maximum product may actually lie in that\narea.\n8.1 Translational alignment\n387\nFor this reason, normalized cross-correlation is more commonly used,\nENCC(u) =\nP\ni[I0(xi) −I0] [I1(xi + u) −I1]\nqP\ni[I0(xi) −I0]2\nqP\ni[I1(xi + u) −I1]2\n,\n(8.11)\nwhere\nI0\n=\n1\nN\nX\ni\nI0(xi)\nand\n(8.12)\nI1\n=\n1\nN\nX\ni\nI1(xi + u)\n(8.13)\nare the mean images of the corresponding patches and N is the number of pixels in the patch.\nThe normalized cross-correlation score is always guaranteed to be in the range [−1, 1], which\nmakes it easier to handle in some higher-level applications, such as deciding which patches\ntruly match. Normalized correlation works well when matching images taken with different\nexposures, e.g., when creating high dynamic range images (Section 10.2). Note, however,\nthat the NCC score is undeﬁned if either of the two patches has zero variance (and, in fact, its\nperformance degrades for noisy low-contrast regions).\nA variant on NCC, which is related to the bias–gain regression implicit in the matching\nscore (8.9), is the normalized SSD score\nENSSD(u) = 1\n2\nP\ni\n\u0002\n[I0(xi) −I0] −[I1(xi + u) −I1]\n\u00032\nqP\ni[I0(xi) −I0]2 + [I1(xi + u) −I1]2\n(8.14)\nrecently proposed by Criminisi, Shotton, Blake et al. (2007). In their experiments, they ﬁnd\nthat it produces comparable results to NCC, but is more efﬁcient when applied to a large\nnumber of overlapping patches using a moving average technique (Section 3.2.2).\n8.1.1 Hierarchical motion estimation\nNow that we have a well-deﬁned alignment cost function to optimize, how can we ﬁnd its\nminimum? The simplest solution is to do a full search over some range of shifts, using ei-\nther integer or sub-pixel steps. This is often the approach used for block matching in motion\ncompensated video compression, where a range of possible motions (say, ±16 pixels) is ex-\nplored.4\nTo accelerate this search process, hierarchical motion estimation is often used: an image\npyramid (Section 3.5) is constructed and a search over a smaller number of discrete pixels\n4 In stereo matching (Section 11.1.2), an explicit search over all possible disparities (i.e., a plane sweep) is almost\nalways performed, since the number of search hypotheses is much smaller due to the 1D nature of the potential\ndisplacements.",
  "image_path": "page_408.jpg",
  "pages": [
    407,
    408,
    409
  ]
}