{
  "doc_id": "pages_452_454",
  "text": "430\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\npanoramas taken by casual users (Brown and Lowe 2007).\nWhat, then, are the essential problems in image stitching? As with image alignment, we\nmust ﬁrst determine the appropriate mathematical model relating pixel coordinates in one im-\nage to pixel coordinates in another; Section 9.1 reviews the basic models we have studied and\npresents some new motion models related speciﬁcally to panoramic image stitching. Next,\nwe must somehow estimate the correct alignments relating various pairs (or collections) of\nimages. Chapter 4 discussed how distinctive features can be found in each image and then\nefﬁciently matched to rapidly establish correspondences between pairs of images. Chapter 8\ndiscussed how direct pixel-to-pixel comparisons combined with gradient descent (and other\noptimization techniques) can also be used to estimate these parameters. When multiple im-\nages exist in a panorama, bundle adjustment (Section 7.4) can be used to compute a globally\nconsistent set of alignments and to efﬁciently discover which images overlap one another. In\nSection 9.2, we look at how each of these previously developed techniques can be modiﬁed\nto take advantage of the imaging setups commonly used to create panoramas.\nOnce we have aligned the images, we must choose a ﬁnal compositing surface for warping\nthe aligned images (Section 9.3.1). We also need algorithms to seamlessly cut and blend over-\nlapping images, even in the presence of parallax, lens distortion, scene motion, and exposure\ndifferences (Section 9.3.2–9.3.4).\n9.1 Motion models\nBefore we can register and align images, we need to establish the mathematical relationships\nthat map pixel coordinates from one image to another. A variety of such parametric motion\nmodels are possible, from simple 2D transforms, to planar perspective models, 3D camera\nrotations, lens distortions, and mapping to non-planar (e.g., cylindrical) surfaces.\nWe already covered several of these models in Sections 2.1 and 6.1. In particular, we saw\nin Section 2.1.5 how the parametric motion describing the deformation of a planar surfaced\nas viewed from different positions can be described with an eight-parameter homography\n(2.71) (Mann and Picard 1994; Szeliski 1996). We also saw how a camera undergoing a pure\nrotation induces a different kind of homography (2.72).\nIn this section, we review both of these models and show how they can be applied to dif-\nferent stitching situations. We also introduce spherical and cylindrical compositing surfaces\nand show how, under favorable circumstances, they can be used to perform alignment using\npure translations (Section 9.1.6). Deciding which alignment model is most appropriate for a\ngiven situation or set of data is a model selection problem (Hastie, Tibshirani, and Friedman\n2001; Torr 2002; Bishop 2006; Robert 2007), an important topic we do not cover in this book.\n9.1 Motion models\n431\n(a) translation [2 dof]\n(b) afﬁne [6 dof]\n(c) perspective [8 dof]\n(d) 3D rotation [3+ dof]\nFigure 9.2 Two-dimensional motion models and how they can be used for image stitching.\n9.1.1 Planar perspective motion\nThe simplest possible motion model to use when aligning images is to simply translate and\nrotate them in 2D (Figure 9.2a). This is exactly the same kind of motion that you would\nuse if you had overlapping photographic prints. It is also the kind of technique favored by\nDavid Hockney to create the collages that he calls joiners (Zelnik-Manor and Perona 2007;\nNomura, Zhang, and Nayar 2007). Creating such collages, which show visible seams and\ninconsistencies that add to the artistic effect, is popular on Web sites such as Flickr, where they\nmore commonly go under the name panography (Section 6.1.2). Translation and rotation are\nalso usually adequate motion models to compensate for small camera motions in applications\nsuch as photo and video stabilization and merging (Exercise 6.1 and Section 8.2.1).\nIn Section 6.1.3, we saw how the mapping between two cameras viewing a common plane\ncan be described using a 3×3 homography (2.71). Consider the matrix M 10 that arises when\nmapping a pixel in one image to a 3D point and then back onto a second image,\n˜x1 ∼˜\nP 1 ˜\nP\n−1\n0 ˜x0 = M 10˜x0.\n(9.1)\nWhen the last row of the P 0 matrix is replaced with a plane equation ˆn0·p+c0 and points are\nassumed to lie on this plane, i.e., their disparity is d0 = 0, we can ignore the last column of\nM 10 and also its last row, since we do not care about the ﬁnal z-buffer depth. The resulting\nhomography matrix ˜\nH10 (the upper left 3 × 3 sub-matrix of M 10) describes the mapping\nbetween pixels in the two images,\n˜x1 ∼˜\nH10˜x0.\n(9.2)\nThis observation formed the basis of some of the earliest automated image stitching al-\ngorithms (Mann and Picard 1994; Szeliski 1994, 1996). Because reliable feature matching\ntechniques had not yet been developed, these algorithms used direct pixel value matching, i.e.,\ndirect parametric motion estimation, as described in Section 8.2 and Equations (6.19–6.20).\n432\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMore recent stitching algorithms ﬁrst extract features and then match them up, often using\nrobust techniques such as RANSAC (Section 6.1.4) to compute a good set of inliers. The ﬁnal\ncomputation of the homography (9.2), i.e., the solution of the least squares ﬁtting problem\ngiven pairs of corresponding features,\nx1 = (1 + h00)x0 + h01y0 + h02\nh20x0 + h21y0 + 1\nand y1 = h10x0 + (1 + h11)y0 + h12\nh20x0 + h21y0 + 1\n,\n(9.3)\nuses iterative least squares, as described in Section 6.1.3 and Equations (6.21–6.23).\n9.1.2 Application: Whiteboard and document scanning\nThe simplest image-stitching application is to stitch together a number of image scans taken\non a ﬂatbed scanner. Say you have a large map, or a piece of child’s artwork, that is too large\nto ﬁt on your scanner. Simply take multiple scans of the document, making sure to overlap\nthe scans by a large enough amount to ensure that there are enough common features. Next,\ntake successive pairs of images that you know overlap, extract features, match them up, and\nestimate the 2D rigid transform (2.16),\nxk+1 = Rkxk + tk,\n(9.4)\nthat best matches the features, using two-point RANSAC, if necessary, to ﬁnd a good set\nof inliers. Then, on a ﬁnal compositing surface (aligned with the ﬁrst scan, for example),\nresample your images (Section 3.6.1) and average them together. Can you see any potential\nproblems with this scheme?\nOne complication is that a 2D rigid transformation is non-linear in the rotation angle θ,\nso you will have to either use non-linear least squares or constrain R to be orthonormal, as\ndescribed in Section 6.1.3.\nA bigger problem lies in the pairwise alignment process. As you align more and more\npairs, the solution may drift so that it is no longer globally consistent. In this case, a global op-\ntimization procedure, as described in Section 9.2, may be required. Such global optimization\noften requires a large system of non-linear equations to be solved, although in some cases,\nsuch as linearized homographies (Section 9.1.3) or similarity transforms (Section 6.1.2), reg-\nular least squares may be an option.\nA slightly more complex scenario is when you take multiple overlapping handheld pic-\ntures of a whiteboard or other large planar object (He and Zhang 2005; Zhang and He 2007).\nHere, the natural motion model to use is a homography, although a more complex model that\nestimates the 3D rigid motion relative to the plane (plus the focal length, if unknown), could\nin principle be used.",
  "image_path": "page_453.jpg",
  "pages": [
    452,
    453,
    454
  ]
}