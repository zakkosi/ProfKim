{
  "doc_id": "pages_775_777",
  "text": "A.5 Iterative techniques\n753\nA.5.3 Multigrid\nOne other class of iterative techniques widely used in computer vision is multigrid techniques\n(Briggs, Henson, and McCormick 2000; Trottenberg, Oosterlee, and Schuller 2000), which\nhave been applied to problems such as surface interpolation (Terzopoulos 1986a), optical\nﬂow (Terzopoulos 1986a; Bruhn, Weickert, Kohlberger et al. 2006), high dynamic range tone\nmapping (Fattal, Lischinski, and Werman 2002), colorization (Levin, Lischinski, and Weiss\n2004), natural image matting (Levin, Lischinski, and Weiss 2008), and segmentation (Grady\n2008).\nThe main idea behind multigrid is to form coarser (lower-resolution) versions of the prob-\nlems and use them to compute the low-frequency components of the solution. However,\nunlike simple coarse-to-ﬁne techniques, which use the coarse solutions to initialize the ﬁne\nsolution, multigrid techniques only correct the low-frequency component of the current solu-\ntion and use multiple rounds of coarsening and reﬁnement (in what are often called “V” and\n“W” patterns of motion across the pyramid) to obtain rapid convergence.\nOn certain simple homogeneous problems (such as solving Poisson equations), multigrid\ntechniques can achieve optimal performance, i.e., computation times linear in the number\nof variables. However, for more inhomogeneous problems or problems on irregular grids,\nvariants on these techniques, such as algebraic multigrid (AMG) approaches, which look at\nthe structure of C to derive coarse level problems, may be preferable. Saad (2003) has a\nnice discussion of the relationship between multigrid and parallel preconditioners and on the\nrelative merits of using multigrid or conjugate gradient approaches.\n754\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAppendix B\nBayesian modeling and inference\nB.1\nEstimation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757\nB.1.1\nLikelihood for multivariate Gaussian noise\n. . . . . . . . . . . . . . 757\nB.2\nMaximum likelihood estimation and least squares . . . . . . . . . . . . . . . 759\nB.3\nRobust statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 760\nB.4\nPrior models and Bayesian inference . . . . . . . . . . . . . . . . . . . . . . 762\nB.5\nMarkov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763\nB.5.1\nGradient descent and simulated annealing . . . . . . . . . . . . . . . 765\nB.5.2\nDynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 766\nB.5.3\nBelief propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\nB.5.4\nGraph cuts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770\nB.5.5\nLinear programming . . . . . . . . . . . . . . . . . . . . . . . . . . 773\nB.6\nUncertainty estimation (error analysis) . . . . . . . . . . . . . . . . . . . . . 775",
  "image_path": "page_776.jpg",
  "pages": [
    775,
    776,
    777
  ]
}