{
  "doc_id": "pages_377_379",
  "text": "7.2 Two-frame structure from motion\n355\n7.2.2 Self-calibration\nThe results of structure from motion computation are much more useful (and intelligible) if\na metric reconstruction is obtained, i.e., one in which parallel lines are parallel, orthogonal\nwalls are at right angles, and the reconstructed model is a scaled version of reality. Over\nthe years, a large number of self-calibration (or auto-calibration) techniques have been de-\nveloped for converting a projective reconstruction into a metric one, which is equivalent to\nrecovering the unknown calibration matrices Kj associated with each image (Hartley and\nZisserman 2004; Moons, Van Gool, and Vergauwen 2010).\nIn situations where certain additional information is known about the scene, different\nmethods may be employed. For example, if there are parallel lines in the scene (usually,\nhaving several lines converge on the same vanishing point is good evidence), three or more\nvanishing points, which are the images of points at inﬁnity, can be used to establish the ho-\nmography for the plane at inﬁnity, from which focal lengths and rotations can be recovered.\nIf two or more ﬁnite orthogonal vanishing points have been observed, the single-image cali-\nbration method based on vanishing points (Section 6.3.2) can be used instead.\nIn the absence of such external information, it is not possible to recover a fully parameter-\nized independent calibration matrix Kj for each image from correspondences alone. To see\nthis, consider the set of all camera matrices P j = Kj[Rj|tj] projecting world coordinates\npi = (Xi, Yi, Zi, Wi) into screen coordinates xij ∼P jpi. Now consider transforming the\n3D scene {pi} through an arbitrary 4 × 4 projective transformation ˜\nH, yielding a new model\nconsisting of points p′\ni = ˜\nHpi. Post-multiplying each P j matrix by ˜\nH\n−1 still produces the\nsame screen coordinates and a new set calibration matrices can be computed by applying RQ\ndecomposition to the new camera matrix P ′\nj = P j ˜\nH\n−1.\nFor this reason, all self-calibration methods assume some restricted form of the calibration\nmatrix, either by setting or equating some of their elements or by assuming that they do not\nvary over time. While most of the techniques discussed by Hartley and Zisserman (2004);\nMoons, Van Gool, and Vergauwen (2010) require three or more frames, in this section we\npresent a simple technique that can recover the focal lengths (f0, f1) of both images from the\nfundamental matrix F in a two-frame reconstruction (Hartley and Zisserman 2004, p. 456).\nTo accomplish this, we assume that the camera has zero skew, a known aspect ratio (usu-\nally set to 1), and a known optical center, as in Equation (2.59). How reasonable is this\nassumption in practice? The answer, as with many questions, is “it depends”.\nIf absolute metric accuracy is required, as in photogrammetry applications, it is imperative\nto pre-calibrate the cameras using one of the techniques from Section 6.3 and to use ground\ncontrol points to pin down the reconstruction. If instead, we simply wish to reconstruct the\nworld for visualization or image-based rendering applications, as in the Photo Tourism system\nof Snavely, Seitz, and Szeliski (2006), this assumption is quite reasonable in practice.\n356\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMost cameras today have square pixels and an optical center near the middle of the image,\nand are much more likely to deviate from a simple camera model due to radial distortion\n(Section 6.3.5), which should be compensated for whenever possible. The biggest problems\noccur when images have been cropped off-center, in which case the optical center will no\nlonger be in the middle, or when perspective pictures have been taken of a different picture,\nin which case a general camera matrix becomes necessary.9\nGiven these caveats, the two-frame focal length estimation algorithm based on the Kruppa\nequations developed by Hartley and Zisserman (2004, p. 456) proceeds as follows. Take the\nleft and right singular vectors {u0, u1, v0, v1} of the fundamental matrix F (7.30) and their\nassociated singular values {σ0, σ1) and form the following set of equations:\nuT\n1 D0u1\nσ2\n0vT\n0 D1v0\n= −\nuT\n0 D0u1\nσ0σ1vT\n0 D1v1\n=\nuT\n0 D0u0\nσ2\n1vT\n1 D1v1\n,\n(7.35)\nwhere the two matrices\nDj = KjKT\nj = diag(f 2\nj , f 2\nj , 1) =\n\n\nf 2\nj\nf 2\nj\n1\n\n\n(7.36)\nencode the unknown focal lengths. For simplicity, let us rewrite each of the numerators and\ndenominators in (7.35) as\neij0(f 2\n0 )\n=\nuT\ni D0uj = aij + bijf 2\n0 ,\n(7.37)\neij1(f 2\n1 )\n=\nσiσjvT\ni D1vj = cij + dijf 2\n1 .\n(7.38)\nNotice that each of these is afﬁne (linear plus constant) in either f 2\n0 or f 2\n1 .\nHence, we\ncan cross-multiply these equations to obtain quadratic equations in f 2\nj , which can readily\nbe solved. (See also the work by Bougnoux (1998) for some alternative formulations.)\nAn alternative solution technique is to observe that we have a set of three equations related\nby an unknown scalar λ, i.e.,\neij0(f 2\n0 ) = λeij1(f 2\n1 )\n(7.39)\n(Richard Hartley, personal communication, July 2009). These can readily be solved to yield\n(f 2\n0 , λf 2\n1 , λ) and hence (f0, f1).\nHow well does this approach work in practice? There are certain degenerate conﬁgura-\ntions, such as when there is no rotation or when the optical axes intersect, when it does not\nwork at all. (In such a situation, you can vary the focal lengths of the cameras and obtain\n9 In Photo Tourism, our system registered photographs of an information sign outside Notre Dame with real\npictures of the cathedral.\n7.3 Factorization\n357\na deeper or shallower reconstruction, which is an example of a bas-relief ambiguity (Sec-\ntion 7.4.3).) Hartley and Zisserman (2004) recommend using techniques based on three or\nmore frames. However, if you ﬁnd two images for which the estimates of (f 2\n0 , λf 2\n1 , λ) are\nwell conditioned, they can be used to initialize a more complete bundle adjustment of all\nthe parameters (Section 7.4). An alternative, which is often used in systems such as Photo\nTourism, is to use camera EXIF tags or generic default values to initialize focal length esti-\nmates and reﬁne them as part of bundle adjustment.\n7.2.3 Application: View morphing\nAn interesting application of basic two-frame structure from motion is view morphing (also\nknown as view interpolation, see Section 13.1), which can be used to generate a smooth 3D\nanimation from one view of a 3D scene to another (Chen and Williams 1993; Seitz and Dyer\n1996).\nTo create such a transition, you must ﬁrst smoothly interpolate the camera matrices, i.e.,\nthe camera positions, orientations, and focal lengths. While simple linear interpolation can be\nused (representing rotations as quaternions (Section 2.1.4)), a more pleasing effect is obtained\nby easing in and easing out the camera parameters, e.g., using a raised cosine, as well as\nmoving the camera along a more circular trajectory (Snavely, Seitz, and Szeliski 2006).\nTo generate in-between frames, either a full set of 3D correspondences needs to be es-\ntablished (Section 11.3) or 3D models (proxies) must be created for each reference view.\nSection 13.1 describes several widely used approaches to this problem. One of the simplest\nis to just triangulate the set of matched feature points in each image, e.g., using Delaunay\ntriangulation. As the 3D points are re-projected into their intermediate views, pixels can be\nmapped from their original source images to their new views using afﬁne or projective map-\nping (Szeliski and Shum 1997). The ﬁnal image is then composited using a linear blend of\nthe two reference images, as with usual morphing (Section 3.6.3).\n7.3 Factorization\nWhen processing video sequences, we often get extended feature tracks (Section 4.1.4) from\nwhich it is possible to recover the structure and motion using a process called factorization.\nConsider the tracks generated by a rotating ping pong ball, which has been marked with\ndots to make its shape and motion more discernable (Figure 7.5). We can readily see from\nthe shape of the tracks that the moving object must be a sphere, but how can we infer this\nmathematically?\nIt turns out that, under orthography or related models we discuss below, the shape and\nmotion can be recovered simultaneously using a singular value decomposition (Tomasi and",
  "image_path": "page_378.jpg",
  "pages": [
    377,
    378,
    379
  ]
}