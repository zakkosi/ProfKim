{
  "doc_id": "pages_369_371",
  "text": "7.2 Two-frame structure from motion\n347\nwe use homogeneous coordinates p = (X, Y, Z, W), the resulting set of equations is homo-\ngeneous and is best solved as a singular value decomposition (SVD) or eigenvalue problem\n(looking for the smallest singular vector or eigenvector). If we set W = 1, we can use regular\nlinear least squares, but the resulting system may be singular or poorly conditioned, i.e., if all\nof the viewing rays are parallel, as occurs for points far away from the camera.\nFor this reason, it is generally preferable to parameterize 3D points using homogeneous\ncoordinates, especially if we know that there are likely to be points at greatly varying dis-\ntances from the cameras. Of course, minimizing the set of observations (7.5–7.6) using non-\nlinear least squares, as described in (6.14 and 6.23), is preferable to using linear least squares,\nregardless of the representation chosen.\nFor the case of two observations, it turns out that the location of the point p that exactly\nminimizes the true reprojection error (7.5–7.6) can be computed using the solution of degree\nsix equations (Hartley and Sturm 1997). Another problem to watch out for with triangulation\nis the issue of chirality, i.e., ensuring that the reconstructed points lie in front of all the\ncameras (Hartley 1998). While this cannot always be guaranteed, a useful heuristic is to take\nthe points that lie behind the cameras because their rays are diverging (imagine Figure 7.2\nwhere the rays were pointing away from each other) and to place them on the plane at inﬁnity\nby setting their W values to 0.\n7.2 Two-frame structure from motion\nSo far in our study of 3D reconstruction, we have always assumed that either the 3D point\npositions or the 3D camera poses are known in advance. In this section, we take our ﬁrst look\nat structure from motion, which is the simultaneous recovery of 3D structure and pose from\nimage correspondences.\nConsider Figure 7.3, which shows a 3D point p being viewed from two cameras whose\nrelative position can be encoded by a rotation R and a translation t. Since we do not know\nanything about the camera positions, without loss of generality, we can set the ﬁrst camera at\nthe origin c0 = 0 and at a canonical orientation R0 = I.\nNow notice that the observed location of point p in the ﬁrst image, p0 = d0ˆx0 is mapped\ninto the second image by the transformation\nd1ˆx1 = p1 = Rp0 + t = R(d0ˆx0) + t,\n(7.7)\nwhere ˆxj = K−1\nj xj are the (local) ray direction vectors. Taking the cross product of both\nsides with t in order to annihilate it on the right hand side yields1\nd1[t]×ˆx1 = d0[t]×Rˆx0.\n(7.8)\n1 The cross-product operator [ ]× was introduced in (2.32).\n348\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n       epipolar plane\np∞\np\n(R,t)\nc0\nc1\nepipolar\nlines\nx\n0\ne0\ne1\nx1\nl1\nl0\nFigure 7.3 Epipolar geometry: The vectors t = c1 −c0, p −c0 and p −c1 are co-planar\nand deﬁne the basic epipolar constraint expressed in terms of the pixel measurements x0 and\nx1.\nTaking the dot product of both sides with ˆx1 yields\nd0ˆxT\n1 ([t]×R)ˆx0 = d1ˆxT\n1 [t]×ˆx1 = 0,\n(7.9)\nsince the right hand side is a triple product with two identical entries. (Another way to say\nthis is that the cross product matrix [t]× is skew symmetric and returns 0 when pre- and\npost-multiplied by the same vector.)\nWe therefore arrive at the basic epipolar constraint\nˆxT\n1 E ˆx0 = 0,\n(7.10)\nwhere\nE = [t]×R\n(7.11)\nis called the essential matrix (Longuet-Higgins 1981).\nAn alternative way to derive the epipolar constraint is to notice that in order for the cam-\neras to be oriented so that the rays ˆx0 and ˆx1 intersect in 3D at point p, the vectors connecting\nthe two camera centers c1 −c0 = −R−1\n1 t and the rays corresponding to pixels x0 and x1,\nnamely R−1\nj\nˆxj, must be co-planar. This requires that the triple product\n(ˆx0, R−1ˆx1, −R−1t) = (Rˆx0, ˆx1, −t) = ˆx1 · (t × Rˆx0) = ˆxT\n1 ([t]×R)ˆx0 = 0.\n(7.12)\nNotice that the essential matrix E maps a point ˆx0 in image 0 into a line l1 = Eˆx0\nin image 1, since ˆxT\n1 l1 = 0 (Figure 7.3). All such lines must pass through the second\nepipole e1, which is therefore deﬁned as the left singular vector of E with a 0 singular value,\nor, equivalently, the projection of the vector t into image 1. The dual (transpose) of these\n7.2 Two-frame structure from motion\n349\nrelationships gives us the epipolar line in the ﬁrst image as l0 = ET ˆx1 and e0 as the zero-\nvalue right singular vector of E.\nGiven this fundamental relationship (7.10), how can we use it to recover the camera\nmotion encoded in the essential matrix E?\nIf we have N corresponding measurements\n{(xi0, xi1)}, we can form N homogeneous equations in the nine elements of E = {e00 . . . e22},\nxi0xi1e00\n+\nyi0xi1e01\n+\nxi1e02\n+\nxi0yi1e00\n+\nyi0yi1e11\n+\nyi1e12\n+\nxi0e20\n+\nyi0e21\n+\ne22\n=\n0\n(7.13)\nwhere xij = (xij, yij, 1). This can be written more compactly as\n[xi1 xT\ni0] ⊗E = Zi ⊗E = zi · f = 0,\n(7.14)\nwhere ⊗indicates an element-wise multiplication and summation of matrix elements, and zi\nand f are the rasterized (vector) forms of the Zi = ˆxi1ˆxT\ni0 and E matrices.2 Given N ≥8\nsuch equations, we can compute an estimate (up to scale) for the entries in E using an SVD.\nIn the presence of noisy measurements, how close is this estimate to being statistically\noptimal? If you look at the entries in (7.13), you can see that some entries are the products\nof image measurements such as xi0yi1 and others are direct image measurements (or even\nthe identity). If the measurements have comparable noise, the terms that are products of\nmeasurements have their noise ampliﬁed by the other element in the product, which can lead\nto very poor scaling, e.g., an inordinately large inﬂuence of points with large coordinates (far\naway from the image center).\nIn order to counteract this trend, Hartley (1997a) suggests that the point coordinates\nshould be translated and scaled so that their centroid lies at the origin and their variance\nis unity, i.e.,\n˜xi\n=\ns(xi −µx)\n(7.15)\n˜yi\n=\ns(xi −µy)\n(7.16)\nsuch that P\ni ˜xi = P\ni ˜yi = 0 and P\ni ˜x2\ni + P\ni ˜y2\ni = 2n, where n is the number of points.3\nOnce the essential matrix ˜E has been computed from the transformed coordinates\n{(˜xi0, ˜xi1)}, where ˜xij = T j ˆxij, the original essential matrix E can be recovered as\nE = T 1 ˜ET 0.\n(7.17)\n2 We use f instead of e to denote the rasterized form of E to avoid confusion with the epipoles ej.\n3 More precisely, Hartley (1997a) suggests scaling the points “so that the average distance from the origin is equal\nto\n√\n2” but the heuristic of unit variance is faster to compute (does not require per-point square roots) and should\nyield comparable improvements.",
  "image_path": "page_370.jpg",
  "pages": [
    369,
    370,
    371
  ]
}