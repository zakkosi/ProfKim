{
  "doc_id": "pages_150_152",
  "text": "128\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3.21\nBinary image morphology: (a) original image; (b) dilation; (c) erosion; (d)\nmajority; (e) opening; (f) closing. The structuring element for all examples is a 5 × 5 square.\nThe effects of majority are a subtle rounding of sharp corners. Opening fails to eliminate the\ndot, since it is not wide enough.\noperation,\nθ(f, t) =\n(\n1\nif f ≥t,\n0\nelse,\n(3.41)\ne.g., converting a scanned grayscale document into a binary image for further processing such\nas optical character recognition.\nThe most common binary image operations are called morphological operations, since\nthey change the shape of the underlying binary objects (Ritter and Wilson 2000, Chapter 7).\nTo perform such an operation, we ﬁrst convolve the binary image with a binary structuring\nelement and then select a binary output value depending on the thresholded result of the\nconvolution. (This is not the usual way in which these operations are described, but I ﬁnd it\na nice simple way to unify the processes.) The structuring element can be any shape, from\na simple 3 × 3 box ﬁlter, to more complicated disc structures. It can even correspond to a\nparticular shape that is being sought for in the image.\nFigure 3.21 shows a close-up of the convolution of a binary image f with a 3 × 3 struc-\nturing element s and the resulting images for the operations described below. Let\nc = f ⊗s\n(3.42)\nbe the integer-valued count of the number of 1s inside each structuring element as it is scanned\nover the image and S be the size of the structuring element (number of pixels). The standard\noperations used in binary morphology include:\n• dilation: dilate(f, s) = θ(c, 1);\n• erosion: erode(f, s) = θ(c, S);\n• majority: maj(f, s) = θ(c, S/2);\n• opening: open(f, s) = dilate(erode(f, s), s);\n3.3 More neighborhood operators\n129\n• closing: close(f, s) = erode(dilate(f, s), s).\nAs we can see from Figure 3.21, dilation grows (thickens) objects consisting of 1s, while\nerosion shrinks (thins) them. The opening and closing operations tend to leave large regions\nand smooth boundaries unaffected, while removing small objects or holes and smoothing\nboundaries.\nWhile we will not use mathematical morphology much in the rest of this book, it is a\nhandy tool to have around whenever you need to clean up some thresholded images. You\ncan ﬁnd additional details on morphology in other textbooks on computer vision and image\nprocessing (Haralick and Shapiro 1992, Section 5.2) (Bovik 2000, Section 2.2) (Ritter and\nWilson 2000, Section 7) as well as articles and books speciﬁcally on this topic (Serra 1982;\nSerra and Vincent 1992; Yuille, Vincent, and Geiger 1992; Soille 2006).\n3.3.3 Distance transforms\nThe distance transform is useful in quickly precomputing the distance to a curve or set of\npoints using a two-pass raster algorithm (Rosenfeld and Pfaltz 1966; Danielsson 1980; Borge-\nfors 1986; Paglieroni 1992; Breu, Gil, Kirkpatrick et al. 1995; Felzenszwalb and Huttenlocher\n2004a; Fabbri, Costa, Torelli et al. 2008). It has many applications, including level sets (Sec-\ntion 5.1.4), fast chamfer matching (binary image alignment) (Huttenlocher, Klanderman, and\nRucklidge 1993), feathering in image stitching and blending (Section 9.3.2), and nearest point\nalignment (Section 12.2.1).\nThe distance transform D(i, j) of a binary image b(i, j) is deﬁned as follows. Let d(k, l)\nbe some distance metric between pixel offsets. Two commonly used metrics include the city\nblock or Manhattan distance\nd1(k, l) = |k| + |l|\n(3.43)\nand the Euclidean distance\nd2(k, l) =\np\nk2 + l2.\n(3.44)\nThe distance transform is then deﬁned as\nD(i, j) =\nmin\nk,l:b(k,l)=0 d(i −k, j −l),\n(3.45)\ni.e., it is the distance to the nearest background pixel whose value is 0.\nThe D1 city block distance transform can be efﬁciently computed using a forward and\nbackward pass of a simple raster-scan algorithm, as shown in Figure 3.22. During the forward\npass, each non-zero pixel in b is replaced by the minimum of 1 + the distance of its north or\nwest neighbor. During the backward pass, the same occurs, except that the minimum is both\nover the current value D and 1 + the distance of the south and east neighbors (Figure 3.22).\n130\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n.\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n1\n2\n0\n0\n0\n0\n1\n1\n2\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1\n2\n2\n3\n1\n0\n0\n1\n2\n2\n3\n1\n0\n0\n1\n2\n2\n2\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1\n2\n3\n0\n1\n2\n2\n1\n1\n0\n0\n1\n2\n2\n1\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n2\n1\n0\n0\n0\n0\n1\n2\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n(a)\n(b)\n(c)\n(d)\nFigure 3.22\nCity block distance transform: (a) original binary image; (b) top to bottom\n(forward) raster sweep: green values are used to compute the orange value; (c) bottom to top\n(backward) raster sweep: green values are merged with old orange value; (d) ﬁnal distance\ntransform.\nEfﬁciently computing the Euclidean distance transform is more complicated. Here, just\nkeeping the minimum scalar distance to the boundary during the two passes is not sufﬁcient.\nInstead, a vector-valued distance consisting of both the x and y coordinates of the distance\nto the boundary must be kept and compared using the squared distance (hypotenuse) rule. As\nwell, larger search regions need to be used to obtain reasonable results. Rather than explaining\nthe algorithm (Danielsson 1980; Borgefors 1986) in more detail, we leave it as an exercise\nfor the motivated reader (Exercise 3.13).\nFigure 3.11g shows a distance transform computed from a binary image. Notice how\nthe values grow away from the black (ink) regions and form ridges in the white area of the\noriginal image. Because of this linear growth from the starting boundary pixels, the distance\ntransform is also sometimes known as the grassﬁre transform, since it describes the time at\nwhich a ﬁre starting inside the black region would consume any given pixel, or a chamfer,\nbecause it resembles similar shapes used in woodworking and industrial design. The ridges\nin the distance transform become the skeleton (or medial axis transform (MAT)) of the region\nwhere the transform is computed, and consist of pixels that are of equal distance to two (or\nmore) boundaries (Tek and Kimia 2003; Sebastian and Kimia 2005).\nA useful extension of the basic distance transform is the signed distance transform, which\ncomputes distances to boundary pixels for all the pixels (Lavall´ee and Szeliski 1995). The\nsimplest way to create this is to compute the distance transforms for both the original bi-\nnary image and its complement and to negate one of them before combining. Because such\ndistance ﬁelds tend to be smooth, it is possible to store them more compactly (with mini-\nmal loss in relative accuracy) using a spline deﬁned over a quadtree or octree data structure\n(Lavall´ee and Szeliski 1995; Szeliski and Lavall´ee 1996; Frisken, Perry, Rockwood et al.\n2000). Such precomputed signed distance transforms can be extremely useful in efﬁciently\naligning and merging 2D curves and 3D surfaces (Huttenlocher, Klanderman, and Rucklidge",
  "image_path": "page_151.jpg",
  "pages": [
    150,
    151,
    152
  ]
}