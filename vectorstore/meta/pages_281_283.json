{
  "doc_id": "pages_281_283",
  "text": "4.5 Exercises\n259\nGrauman 2009; Raginsky and Lazebnik 2009).\nThe classic reference on feature detection and tracking is (Shi and Tomasi 1994). More\nrecent work in this ﬁeld has focused on learning better matching functions for speciﬁc features\n(Avidan 2001; Jurie and Dhome 2002; Williams, Blake, and Cipolla 2003; Lepetit and Fua\n2005; Lepetit, Pilet, and Fua 2006; Hinterstoisser, Benhimane, Navab et al. 2008; Rogez,\nRihan, Ramalingam et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010).\nA highly cited and widely used edge detector is the one developed by Canny (1986).\nAlternative edge detectors as well as experimental comparisons can be found in publica-\ntions by Nalwa and Binford (1986); Nalwa (1987); Deriche (1987); Freeman and Adelson\n(1991); Nalwa (1993); Heath, Sarkar, Sanocki et al. (1998); Crane (1997); Ritter and Wilson\n(2000); Bowyer, Kranenburg, and Dougherty (2001); Arbel´aez, Maire, Fowlkes et al. (2010).\nThe topic of scale selection in edge detection is nicely treated by Elder and Zucker (1998),\nwhile approaches to color and texture edge detection can be found in (Ruzon and Tomasi\n2001; Martin, Fowlkes, and Malik 2004; Gevers, van de Weijer, and Stokman 2006). Edge\ndetectors have also recently been combined with region segmentation techniques to further\nimprove the detection of semantically salient boundaries (Maire, Arbelaez, Fowlkes et al.\n2008; Arbel´aez, Maire, Fowlkes et al. 2010). Edges linked into contours can be smoothed\nand manipulated for artistic effect (Lowe 1989; Finkelstein and Salesin 1994; Taubin 1995)\nand used for recognition (Belongie, Malik, and Puzicha 2002; Tek and Kimia 2003; Sebastian\nand Kimia 2005).\nAn early, well-regarded paper on straight line extraction in images was written by Burns,\nHanson, and Riseman (1986). More recent techniques often combine line detection with van-\nishing point detection (Quan and Mohr 1989; Collins and Weiss 1990; Brillaut-O’Mahoney\n1991; McLean and Kotturi 1995; Becker and Bove 1995; Shufelt 1999; Tuytelaars, Van Gool,\nand Proesmans 1997; Schaffalitzky and Zisserman 2000; Antone and Teller 2002; Rother\n2002; Koˇseck´a and Zhang 2005; Pﬂugfelder 2008; Sinha, Steedly, Szeliski et al. 2008; Tardif\n2009).\n4.5 Exercises\nEx 4.1: Interest point detector\nImplement one or more keypoint detectors and compare\ntheir performance (with your own or with a classmate’s detector).\nPossible detectors:\n• Laplacian or Difference of Gaussian;\n• F¨orstner–Harris Hessian (try different formula variants given in (4.9–4.11));\n260\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n• oriented/steerable ﬁlter, looking for either second-order high second response or two\nedges in a window (Koethe 2003), as discussed in Section 4.1.1.\nOther detectors are described by Mikolajczyk, Tuytelaars, Schmid et al. (2005); Tuytelaars\nand Mikolajczyk (2007). Additional optional steps could include:\n1. Compute the detections on a sub-octave pyramid and ﬁnd 3D maxima.\n2. Find local orientation estimates using steerable ﬁlter responses or a gradient histogram-\nming method.\n3. Implement non-maximal suppression, such as the adaptive technique of Brown, Szeliski,\nand Winder (2005).\n4. Vary the window shape and size (pre-ﬁlter and aggregation).\nTo test for repeatability, download the code from http://www.robots.ox.ac.uk/∼vgg/research/\nafﬁne/ (Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuytelaars and Mikolajczyk 2007) or\nsimply rotate or shear your own test images. (Pick a domain you may want to use later, e.g.,\nfor outdoor stitching.)\nBe sure to measure and report the stability of your scale and orientation estimates.\nEx 4.2: Interest point descriptor\nImplement one or more descriptors (steered to local scale\nand orientation) and compare their performance (with your own or with a classmate’s detec-\ntor).\nSome possible descriptors include\n• contrast-normalized patches (Brown, Szeliski, and Winder 2005);\n• SIFT (Lowe 2004);\n• GLOH (Mikolajczyk and Schmid 2005);\n• DAISY (Winder and Brown 2007; Tola, Lepetit, and Fua 2010).\nOther detectors are described by Mikolajczyk and Schmid (2005).\nEx 4.3: ROC curve computation\nGiven a pair of curves (histograms) plotting the number\nof matching and non-matching features as a function of Euclidean distance d as shown in\nFigure 4.23b, derive an algorithm for plotting a ROC curve (Figure 4.23a). In particular, let\nt(d) be the distribution of true matches and f(d) be the distribution of (false) non-matches.\nWrite down the equations for the ROC, i.e., TPR(FPR), and the AUC.\n(Hint: Plot the cumulative distributions T(d) =\nR\nt(d) and F(d) =\nR\nf(d) and see if\nthese help you derive the TPR and FPR at a given threshold θ.)\n4.5 Exercises\n261\nEx 4.4: Feature matcher\nAfter extracting features from a collection of overlapping or dis-\ntorted images,10 match them up by their descriptors either using nearest neighbor matching\nor a more efﬁcient matching strategy such as a k-d tree.\nSee whether you can improve the accuracy of your matches using techniques such as the\nnearest neighbor distance ratio.\nEx 4.5: Feature tracker\nInstead of ﬁnding feature points independently in multiple images\nand then matching them, ﬁnd features in the ﬁrst image of a video or image sequence and\nthen re-locate the corresponding points in the next frames using either search and gradient\ndescent (Shi and Tomasi 1994) or learned feature detectors (Lepetit, Pilet, and Fua 2006;\nFossati, Dimitrijevic, Lepetit et al. 2007). When the number of tracked points drops below a\nthreshold or new regions in the image become visible, ﬁnd additional points to track.\n(Optional) Winnow out incorrect matches by estimating a homography (6.19–6.23) or\nfundamental matrix (Section 7.2.1).\n(Optional) Reﬁne the accuracy of your matches using the iterative registration algorithm\ndescribed in Section 8.2 and Exercise 8.2.\nEx 4.6: Facial feature tracker\nApply your feature tracker to tracking points on a person’s\nface, either manually initialized to interesting locations such as eye corners or automatically\ninitialized at interest points.\n(Optional) Match features between two people and use these features to perform image\nmorphing (Exercise 3.25).\nEx 4.7: Edge detector\nImplement an edge detector of your choice. Compare its perfor-\nmance to that of your classmates’ detectors or code downloaded from the Internet.\nA simple but well-performing sub-pixel edge detector can be created as follows:\n1. Blur the input image a little,\nBσ(x) = Gσ(x) ∗I(x).\n2. Construct a Gaussian pyramid (Exercise 3.19),\nP = Pyramid{Bσ(x)}\n3. Subtract an interpolated coarser-level pyramid image from the original resolution blurred\nimage,\nS(x) = Bσ(x) −P.InterpolatedLevel(L).\n10 http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/.",
  "image_path": "page_282.jpg",
  "pages": [
    281,
    282,
    283
  ]
}