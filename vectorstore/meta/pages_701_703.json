{
  "doc_id": "pages_701_703",
  "text": "14.2 Face recognition\n679\n(a)\n(b)\nFigure 14.19 View-based eigenspace (Moghaddam and Pentland 1997) c⃝1997 IEEE. (a)\nComparison between a regular (parametric) eigenspace reconstruction (middle column) and\na view-based eigenspace reconstruction (right column) corresponding to the input image (left\ncolumn). The top row is from a training image, the bottom row is from the test set. (b) A\nschematic representation of the two approaches, showing how each view computes its own\nlocal basis representation.\nSection 14.2.2, which can learn deformations across all individuals.\nIt is also possible to generalize the bilinear factorization implicit in PCA and SVD ap-\nproaches to multilinear (tensor) formulations that can model several interacting factors si-\nmultaneously (Vasilescu and Terzopoulos 2007). These ideas are related to currently active\ntopics in machine learning such as subspace learning (Cai, He, Hu et al. 2007), local distance\nfunctions (Frome, Singer, Sha et al. 2007), and metric learning (Ramanan and Baker 2009).\nLearning approaches play an increasingly important role in face recognition, e.g., in the work\nof Sivic, Everingham, and Zisserman (2009) and Guillaumin, Verbeek, and Schmid (2009).\n14.2.2 Active appearance and 3D shape models\nThe need to use modular or view-based eigenspaces for face recognition is symptomatic of\na more general observation, i.e., that facial appearance and identiﬁability depend as much\non shape as they do on color or texture (which is what eigenfaces capture). Furthermore,\nwhen dealing with 3D head rotations, the pose of a person’s head should be discounted when\nperforming recognition.\nIn fact, the earliest face recognition systems, such as those by Fischler and Elschlager\n(1973), Kanade (1977), and Yuille (1991), found distinctive feature points on facial images\nand performed recognition on the basis of their relative positions or distances. Newer tech-\nniques such as local feature analysis (Penev and Atick 1996) and elastic bunch graph match-\ning (Wiskott, Fellous, Kr¨uger et al. 1997) combine local ﬁlter responses (jets) at distinctive\n680\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.20 Manipulating facial appearance through shape and color (Rowland and Perrett\n1995) c⃝1995 IEEE. By adding or subtracting gender-speciﬁc shape and color characteristics\nto (b) an input image, different amounts of gender variation can be induced. The amounts\nadded (from the mean) are: (a) +50% (gender enhancement), (c) -50% (near “androgyny”),\n(d) -100% (gender switched), and (e) -150% (opposite gender attributes enhanced).\nfeature locations together with shape models to perform recognition.\nA visually compelling example of why both shape and texture are important is the work\nof Rowland and Perrett (1995), who manually traced the contours of facial features and then\nused these contours to normalize (warp) each image to a canonical shape. After analyzing\nboth the shape and color images for deviations from the mean, they were able to associate\ncertain shape and color deformations with personal characteristics such as age and gender\n(Figure 14.20). Their work demonstrates that both shape and color have an important inﬂu-\nence on the perception of such characteristics.\nAround the same time, researchers in computer vision were beginning to use simultane-\nous shape deformations and texture interpolation to model the variability in facial appearance\ncaused by identity or expression (Beymer 1996; Vetter and Poggio 1997), developing tech-\nniques such as Active Shape Models (Lanitis, Taylor, and Cootes 1997), 3D Morphable Mod-\nels (Blanz and Vetter 1999), and Elastic Bunch Graph Matching (Wiskott, Fellous, Kr¨uger et\nal. 1997).14\nOf all these techniques, the active appearance models (AAMs) of Cootes, Edwards, and\nTaylor (2001) are among the most widely used for face recognition and tracking. Like other\nshape and texture models, an AAM models both the variation in the shape of an image s,\nwhich is normally encoded by the location of key feature points on the image (Figure 14.21b),\n14 We have already seen the application of PCA to 3D head and face modeling and animation in Section 12.6.3.\n14.2 Face recognition\n681\n(a)\n(b)\n(c)\nFigure 14.21\nActive Appearance Models (Cootes, Edwards, and Taylor 2001) c⃝2001\nIEEE: (a) input image with registered feature points; (b) the feature points (shape vector\ns); (c) the shape-free appearance image (texture vector t).\nas well as the variation in texture t, which is normalized to a canonical shape before being\nanalyzed (Figure 14.21c).15\nBoth shape and texture are represented as deviations from a mean shape ¯s and texture ¯t,\ns\n=\n¯s + U sa\n(14.29)\nt\n=\n¯t + U ta,\n(14.30)\nwhere the eigenvectors in U s and U t have been pre-scaled (whitened) so that unit vectors in\na represent one standard deviation of variation observed in the training data. In addition to\nthese principal deformations, the shape parameters are transformed by a global similarity to\nmatch the location, size, and orientation of a given face. Similarly, the texture image contains\na scale and offset to best match novel illumination conditions.\nAs you can see, the same appearance parameters a in (14.29–14.30) simultaneously con-\ntrol both the shape and texture deformations from the mean, which makes sense if we believe\nthem to be correlated. Figure 14.22 shows how moving three standard deviations along each\nof the ﬁrst four principal directions ends up changing several correlated factors in a person’s\nappearance, including expression, gender, age, and identity.\nIn order to ﬁt an active appearance model to a novel image, Cootes, Edwards, and Taylor\n(2001) pre-compute a set of “difference decomposition” images, using an approach related to\nother fast techniques for incremental tracking, such as those we discussed in Sections 4.1.4,\n8.1.3, and 8.2 (Gleicher 1997; Hager and Belhumeur 1998), which often learn a discrimi-\nnative mapping between matching errors and incremental displacements (Avidan 2001; Jurie\nand Dhome 2002; Liu, Chen, and Kumar 2003; Sclaroff and Isidoro 2003; Romdhani and\nVetter 2003; Williams, Blake, and Cipolla 2003).\n15 When only the shape variation is being captured, such models are called active shape models (ASMs) (Cootes,\nCooper, Taylor et al. 1995; Davies, Twining, and Taylor 2008). These were already discussed in Section 5.1.1\n(5.13–5.17).",
  "image_path": "page_702.jpg",
  "pages": [
    701,
    702,
    703
  ]
}