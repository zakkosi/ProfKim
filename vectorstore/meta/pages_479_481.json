{
  "doc_id": "pages_479_481",
  "text": "9.3 Compositing\n457\nFigure 9.16\nPhotomontage (Agarwala, Dontcheva, Agrawala et al. 2004) c⃝2004 ACM.\nFrom a set of ﬁve source images (of which four are shown on the left), Photomontage quickly\ncreates a composite family portrait in which everyone is smiling and looking at the camera\n(right). Users simply ﬂip through the stack and coarsely draw strokes using the designated\nsource image objective over the people they wish to add to the composite. The user-applied\nstrokes and computed regions (middle) are color-coded by the borders of the source images\non the left.\nalso possible to infer which object in a region of difference is the foreground object by the\n“edginess” (pixel differences) across the ROD boundary, which should be higher when an\nobject is present (Herley 2005).) Once the desired excess regions of difference have been\nremoved, the ﬁnal composite can be created by feathering (Figure 9.14f).\nA different approach to pixel selection and seam placement is described by Agarwala,\nDontcheva, Agrawala et al. (2004). Their system computes the label assignment that opti-\nmizes the sum of two objective functions. The ﬁrst is a per-pixel image objective that deter-\nmines which pixels are likely to produce good composites,\nCD =\nX\nx\nD(x, l(x)),\n(9.41)\nwhere D(x, l) is the data penalty associated with choosing image l at pixel x. In their system,\nusers can select which pixels to use by “painting” over an image with the desired object or\nappearance, which sets D(x, l) to a large value for all labels l other than the one selected\nby the user (Figure 9.16). Alternatively, automated selection criteria can be used, such as\nmaximum likelihood, which prefers pixels that occur repeatedly in the background (for object\nremoval), or minimum likelihood for objects that occur infrequently, i.e., for moving object\nretention. Using a more traditional center-weighted data term tends to favor objects that are\ncentered in the input images (Figure 9.17).\nThe second term is a seam objective that penalizes differences in labelings between adja-\ncent images,\nCS =\nX\n(x,y)∈N\nS(x, y, l(x), l(y)),\n(9.42)\n458\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 9.17\nSet of ﬁve photos tracking a snowboarder’s jump stitched together into a seam-\nless composite. Because the algorithm prefers pixels near the center of the image, multiple\ncopies of the boarder are retained.\nwhere S(x, y, lx, ly) is the image-dependent interaction penalty or seam cost of placing a\nseam between pixels x and y, and N is the set of N4 neighboring pixels. For example,\nthe simple color-based seam penalty used in (Kwatra, Sch¨odl, Essa et al. 2003; Agarwala,\nDontcheva, Agrawala et al. 2004) can be written as\nS(x, y, lx, ly) = ∥˜Ilx(x) −˜Ily(x)∥+ ∥˜Ilx(y) −˜Ily(y)∥.\n(9.43)\nMore sophisticated seam penalties can also look at image gradients or the presence of image\nedges (Agarwala, Dontcheva, Agrawala et al. 2004). Seam penalties are widely used in other\ncomputer vision applications such as stereo matching (Boykov, Veksler, and Zabih 2001) to\ngive the labeling function its coherence or smoothness. An alternative approach, which places\nseams along strong consistent edges in overlapping images using a watershed computation is\ndescribed by Soille (2006).\nThe sum of these two objective functions gives rise to a Markov random ﬁeld (MRF),\nfor which good optimization algorithms are described in Sections 3.7.2 and 5.5 and Ap-\npendix B.5. For label computations of this kind, the α-expansion algorithm developed by\nBoykov, Veksler, and Zabih (2001) works particularly well (Szeliski, Zabih, Scharstein et al.\n2008).\nFor the result shown in Figure 9.14g, Agarwala, Dontcheva, Agrawala et al. (2004) use\na large data penalty for invalid pixels and 0 for valid pixels. Notice how the seam placement\nalgorithm avoids regions of difference, including those that border the image and that might\nresult in objects being cut off. Graph cuts (Agarwala, Dontcheva, Agrawala et al. 2004) and\n9.3 Compositing\n459\nvertex cover (Uyttendaele, Eden, and Szeliski 2001) often produce similar looking results,\nalthough the former is signiﬁcantly slower since it optimizes over all pixels, while the latter\nis more sensitive to the thresholds used to determine regions of difference.\n9.3.3 Application: Photomontage\nWhile image stitching is normally used to composite partially overlapping photographs, it\ncan also be used to composite repeated shots of a scene taken with the aim of obtaining the\nbest possible composition and appearance of each element.\nFigure 9.16 shows the Photomontage system developed by Agarwala, Dontcheva, Agrawala\net al. (2004), where users draw strokes over a set of pre-aligned images to indicate which re-\ngions they wish to keep from each image. Once the system solves the resulting multi-label\ngraph cut (9.41–9.42), the various pieces taken from each source photo are blended together\nusing a variant of Poisson image blending (9.44–9.46). Their system can also be used to au-\ntomatically composite an all-focus image from a series of bracketed focus images (Hasinoff,\nKutulakos, Durand et al. 2009) or to remove wires and other unwanted elements from sets of\nphotographs. Exercise 9.10 has you implement this system and try out some of its variants.\n9.3.4 Blending\nOnce the seams between images have been determined and unwanted objects removed, we\nstill need to blend the images to compensate for exposure differences and other mis-alignments.\nThe spatially varying weighting (feathering) previously discussed can often be used to accom-\nplish this. However, it is difﬁcult in practice to achieve a pleasing balance between smoothing\nout low-frequency exposure variations and retaining sharp enough transitions to prevent blur-\nring (although using a high exponent in feathering can help).\nLaplacian pyramid blending.\nAn attractive solution to this problem is the Laplacian pyra-\nmid blending technique developed by Burt and Adelson (1983b), which we discussed in Sec-\ntion 3.5.5. Instead of using a single transition width, a frequency-adaptive width is used by\ncreating a band-pass (Laplacian) pyramid and making the transition widths within each level\na function of the level, i.e., the same width in pixels. In practice, a small number of levels,\ni.e., as few as two (Brown and Lowe 2007), may be adequate to compensate for differences\nin exposure. The result of applying this pyramid blending is shown in Figure 9.14h.\nGradient domain blending.\nAn alternative approach to multi-band image blending is to\nperform the operations in the gradient domain. Reconstructing images from their gradient\nﬁelds has a long history in computer vision (Horn 1986), starting originally with work in",
  "image_path": "page_480.jpg",
  "pages": [
    479,
    480,
    481
  ]
}