{
  "doc_id": "pages_591_593",
  "text": "11.6 Multi-view stereo\n569\n(a)\n(b)\n(c)\n(d)\nFigure 11.22\nVolumetric octree reconstruction from binary silhouettes (Szeliski 1993) c⃝\n1993 Elsevier: (a) octree representation and its corresponding (b) tree structure; (c) input\nimage of an object on a turntable; (d) computed 3D volumetric octree model.\nnodes are exterior, and gray nodes are of mixed occupancy. Examples of octree-based re-\nconstruction approaches include those by Potmesil (1987), Noborio, Fukada, and Arimoto\n(1988), Srivasan, Liang, and Hackwood (1990), and Szeliski (1993).\nThe approach of Szeliski (1993) ﬁrst converts each binary silhouette into a one-sided\nvariant of a distance map, where each pixel in the map indicates the largest square that is\ncompletely inside (or outside) the silhouette. This makes it fast to project an octree cell\ninto the silhouette to conﬁrm whether it is completely inside or outside the object, so that\nit can be colored black, white, or left as gray (mixed) for further reﬁnement on a smaller\ngrid. The octree construction algorithm proceeds in a coarse-to-ﬁne manner, ﬁrst building an\noctree at a relatively coarse resolution, and then reﬁning it by revisiting and subdividing all\nthe input images for the gray (mixed) cells whose occupancy has not yet been determined.\nFigure 11.22d shows the resulting octree model computed from a coffee cup rotating on a\nturntable.\nMore recent work on visual hull computation borrows ideas from image-based rendering,\nand is hence called an image-based visual hull (Matusik, Buehler, Raskar et al. 2000). Instead\nof precomputing a global 3D model, an image-based visual hull is recomputed for each new\n570\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nviewpoint, by successively intersecting viewing ray segments with the binary silhouettes in\neach image. This not only leads to a fast computation algorithm but also enables fast texturing\nof the recovered model with color values from the input images. This approach can also\nbe combined with high-quality deformable templates to capture and re-animate whole body\nmotion (Vlasic, Baran, Matusik et al. 2008).\n11.7 Additional reading\nThe ﬁeld of stereo correspondence and depth estimation is one of the oldest and most widely\nstudied topics in computer vision. A number of good surveys have been written over the years\n(Marr and Poggio 1976; Barnard and Fischler 1982; Dhond and Aggarwal 1989; Scharstein\nand Szeliski 2002; Brown, Burschka, and Hager 2003; Seitz, Curless, Diebel et al. 2006) and\nthey can serve as good guides to this extensive literature.\nBecause of computational limitations and the desire to ﬁnd appearance-invariant cor-\nrespondences, early algorithms often focused on ﬁnding sparse correspondences (Hannah\n1974; Marr and Poggio 1979; Mayhew and Frisby 1980; Baker and Binford 1981; Arnold\n1983; Grimson 1985; Ohta and Kanade 1985; Bolles, Baker, and Marimont 1987; Matthies,\nKanade, and Szeliski 1989; Hsieh, McKeown, and Perlant 1992; Bolles, Baker, and Hannah\n1993).\nThe topic of computing epipolar geometry and pre-rectifying images is covered in Sec-\ntions 7.2 and 11.1 and is also treated in textbooks on multi-view geometry (Faugeras and\nLuong 2001; Hartley and Zisserman 2004) and articles speciﬁcally on this topic (Torr and\nMurray 1997; Zhang 1998a,b). The concepts of the disparity space and disparity space im-\nage are often associated with the seminal work by Marr (1982) and the papers of Yang, Yuille,\nand Lu (1993) and Intille and Bobick (1994). The plane sweep algorithm was ﬁrst popular-\nized by Collins (1996) and then generalized to a full arbitrary projective setting by Szeliski\nand Golland (1999) and Saito and Kanade (1999). Plane sweeps can also be formulated using\ncylindrical surfaces (Ishiguro, Yamamoto, and Tsuji 1992; Kang and Szeliski 1997; Shum\nand Szeliski 1999; Li, Shum, Tang et al. 2004; Zheng, Kang, Cohen et al. 2007) or even more\ngeneral topologies (Seitz 2001).\nOnce the topology for the cost volume or DSI has been set up, we need to compute the\nactual photoconsistency measures for each pixel and potential depth. A wide range of such\nmeasures have been proposed, as discussed in Section 11.3.1. Some of these are compared in\nrecent surveys and evaluations of matching costs (Scharstein and Szeliski 2002; Hirschm¨uller\nand Scharstein 2009).\nTo compute an actual depth map from these costs, some form of optimization or selection\ncriterion must be used. The simplest of these are sliding windows of various kinds, which\n11.8 Exercises\n571\nare discussed in Section 11.4 and surveyed by Gong, Yang, Wang et al. (2007) and Tombari,\nMattoccia, Di Stefano et al. (2008). More commonly, global optimization frameworks are\nused to compute the best disparity ﬁeld, as described in Section 11.5. These techniques\ninclude dynamic programming and truly global optimization algorithms, such as graph cuts\nand loopy belief propagation. Because the literature on this is so extensive, it is described in\nmore detail in Section 11.5. A good place to ﬁnd pointers to the latest results in this ﬁeld is\nthe Middlebury Stereo Vision Page at http://vision.middlebury.edu/stereo.\nAlgorithms for multi-view stereo typically fall into two categories. The ﬁrst include al-\ngorithms that compute traditional depth maps using several images for computing photocon-\nsistency measures (Okutomi and Kanade 1993; Kang, Webb, Zitnick et al. 1995; Nakamura,\nMatsuura, Satoh et al. 1996; Szeliski and Golland 1999; Kang, Szeliski, and Chai 2001;\nVaish, Szeliski, Zitnick et al. 2006; Gallup, Frahm, Mordohai et al. 2008). Optionally, some\nof these techniques compute multiple depth maps and use additional constraints to encourage\nthe different depth maps to be consistent (Szeliski 1999; Kolmogorov and Zabih 2002; Kang\nand Szeliski 2004; Maitre, Shinagawa, and Do 2008; Zhang, Jia, Wong et al. 2008).\nThe second category consists of papers that compute true 3D volumetric or surface-based\nobject models. Again, because of the large number of papers published on this topic, rather\nthan citing them here, we refer you to the material in Section 11.6.1, the survey by Seitz,\nCurless, Diebel et al. (2006), and the on-line evaluation Web site at http://vision.middlebury.\nedu/mview/.\n11.8 Exercises\nEx 11.1: Stereo pair rectiﬁcation\nImplement the following simple algorithm (Section 11.1.1):\n1. Rotate both cameras so that they are looking perpendicular to the line joining the two\ncamera centers c0 and c1. The smallest rotation can be computed from the cross prod-\nuct between the original and desired optical axes.\n2. Twist the optical axes so that the horizontal axis of each camera looks in the direction\nof the other camera. (Again, the cross product between the current x-axis after the ﬁrst\nrotation and the line joining the cameras gives the rotation.)\n3. If needed, scale up the smaller (less detailed) image so that it has the same resolution\n(and hence line-to-line correspondence) as the other image.\nNow compare your results to the algorithm proposed by Loop and Zhang (1999). Can you\nthink of situations where their approach may be preferable?",
  "image_path": "page_592.jpg",
  "pages": [
    591,
    592,
    593
  ]
}