{
  "doc_id": "pages_311_313",
  "text": "5.3 Mean shift and mode ﬁnding\n289\nwith φ usually set to 0.2. The intensity and texture similarity statistics for the coarser nodes\nare recursively computed using weighted averaging, where the relative strengths (couplings)\nbetween coarse- and ﬁne-level nodes are based on their merge probabilities pij. This allows\nthe algorithm to run in essentially O(N) time, using the same kind of hierarchical aggrega-\ntion operations that are used in pyramid-based ﬁltering or preconditioning algorithms. After\na segmentation has been identiﬁed at a coarser level, the exact memberships of each pixel are\ncomputed by propagating coarse-level assignments to their ﬁner-level “children” (Sharon,\nGalun, Sharon et al. 2006; Alpert, Galun, Basri et al. 2007). Figure 5.22 shows the segmen-\ntations produced by this algorithm compared to other popular segmentation algorithms.\n5.3 Mean shift and mode ﬁnding\nMean-shift and mode ﬁnding techniques, such as k-means and mixtures of Gaussians, model\nthe feature vectors associated with each pixel (e.g., color and position) as samples from an\nunknown probability density function and then try to ﬁnd clusters (modes) in this distribution.\nConsider the color image shown in Figure 5.16a. How would you segment this image\nbased on color alone? Figure 5.16b shows the distribution of pixels in L*u*v* space, which\nis equivalent to what a vision algorithm that ignores spatial location would see. To make the\nvisualization simpler, let us only consider the L*u* coordinates, as shown in Figure 5.16c.\nHow many obvious (elongated) clusters do you see? How would you go about ﬁnding these\nclusters?\nThe k-means and mixtures of Gaussians techniques use a parametric model of the den-\nsity function to answer this question, i.e., they assume the density is the superposition of a\nsmall number of simpler distributions (e.g., Gaussians) whose locations (centers) and shape\n(covariance) can be estimated. Mean shift, on the other hand, smoothes the distribution and\nﬁnds its peaks as well as the regions of feature space that correspond to each peak. Since\na complete density is being modeled, this approach is called non-parametric (Bishop 2006).\nLet us look at these techniques in more detail.\n5.3.1 K-means and mixtures of Gaussians\nWhile k-means implicitly models the probability density as a superposition of spherically\nsymmetric distributions, it does not require any probabilistic reasoning or modeling (Bishop\n2006). Instead, the algorithm is given the number of clusters k it is supposed to ﬁnd; it\nthen iteratively updates the cluster center location based on the samples that are closest to\neach center. The algorithm can be initialized by randomly sampling k centers from the input\nfeature vectors. Techniques have also been developed for splitting or merging cluster centers\n290\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 5.16\nMean-shift image segmentation (Comaniciu and Meer 2002) c⃝2002 IEEE:\n(a) input color image; (b) pixels plotted in L*u*v* space; (c) L*u* space distribution; (d)\nclustered results after 159 mean-shift procedures; (e) corresponding trajectories with peaks\nmarked as red dots.\n5.3 Mean shift and mode ﬁnding\n291\nbased on their statistics, and for accelerating the process of ﬁnding the nearest mean center\n(Bishop 2006).\nIn mixtures of Gaussians, each cluster center is augmented by a covariance matrix whose\nvalues are re-estimated from the corresponding samples. Instead of using nearest neighbors\nto associate input samples with cluster centers, a Mahalanobis distance (Appendix B.1.1) is\nused:\nd(xi, µk; Σk) = ∥xi −µk∥Σ\n−1\nk\n= (xi −µk)T Σ−1\nk (xi −µk)\n(5.26)\nwhere xi are the input samples, µk are the cluster centers, and Σk are their covariance es-\ntimates. Samples can be associated with the nearest cluster center (a hard assignment of\nmembership) or can be softly assigned to several nearby clusters.\nThis latter, more commonly used, approach corresponds to iteratively re-estimating the\nparameters for a mixture of Gaussians density function,\np(x|{πk, µk, Σk}) =\nX\nk\nπk N(x|µk, Σk),\n(5.27)\nwhere πk are the mixing coefﬁcients, µk and Σk are the Gaussian means and covariances,\nand\nN(x|µk, Σk) =\n1\n|Σk|e−d(x,µk;Σk)\n(5.28)\nis the normal (Gaussian) distribution (Bishop 2006).\nTo iteratively compute (a local) maximum likely estimate for the unknown mixture pa-\nrameters {πk, µk, Σk}, the expectation maximization (EM) algorithm (Dempster, Laird, and\nRubin 1977) proceeds in two alternating stages:\n1. The expectation stage (E step) estimates the responsibilities\nzik = 1\nZi\nπk N(x|µk, Σk)\nwith\nX\nk\nzik = 1,\n(5.29)\nwhich are the estimates of how likely a sample xi was generated from the kth Gaussian\ncluster.\n2. The maximization stage (M step) updates the parameter values\nµk\n=\n1\nNk\nX\ni\nzikxi,\n(5.30)\nΣk\n=\n1\nNk\nX\ni\nzik(xi −µk)(xi −µk)T ,\n(5.31)\nπk\n=\nNk\nN ,\n(5.32)",
  "image_path": "page_312.jpg",
  "pages": [
    311,
    312,
    313
  ]
}