{
  "doc_id": "pages_785_787",
  "text": "B.5 Markov random ﬁelds\n763\nthe measurement likelihood p(y|x) by the prior distribution p(x),\np(x|y) = p(y|x)p(x)\np(y)\n,\n(B.21)\nwhere p(y) =\nR\nx p(y|x)p(x) is a normalizing constant used to make the p(x|y) distribution\nproper (integrate to 1). Taking the negative logarithm of both sides of Equation (B.21), we\nget\n−log p(x|y) = −log p(y|x) −log p(x) + log p(y),\n(B.22)\nwhich is the negative posterior log likelihood. It is common to drop the constant log p(y) be-\ncause its value does not matter during energy minimization. However, if the prior distribution\np(x) depends on some unknown parameters, we may wish to keep log p(y) in order to com-\npute the most likely value of these parameters using Occam’s razor, i.e., by maximizing the\nlikelihood of the observations, or to select the correct number of free parameters using model\nselection (Hastie, Tibshirani, and Friedman 2001; Torr 2002; Bishop 2006; Robert 2007).\nTo ﬁnd the most likely (maximum a posteriori or MAP) solution x given some measure-\nments y, we simply minimize this negative log likelihood, which can also be thought of as an\nenergy,\nE(x, y) = Ed(x, y) + Ep(x).\n(B.23)\nThe ﬁrst term Ed(x, y) is the data energy or data penalty and measures the negative log\nlikelihood that the measurements y were observed given the unknown state x. The second\nterm Ep(x) is the prior energy and it plays a role analogous to the smoothness energy in\nregularization. Note that the MAP estimate may not always be desirable, since it selects the\n“peak” in the posterior distribution rather than some more stable statistic such as MSE—see\nthe discussion in Appendix B.2 about loss functions and decision theory.\nB.5 Markov random ﬁelds\nMarkov random ﬁelds (Blake, Kohli, and Rother 2010) are the most popular types of prior\nmodel for gridded image-like data,5 which include not only regular natural images (Sec-\ntion 3.7.2) but also two-dimensional ﬁelds such as optic ﬂow (Chapter 8) or depth maps\n(Chapter 11), as well as binary ﬁelds, such as segmentations (Section 5.5).\nAs we discussed in Section 3.7.2, the prior probability p(x) for a Markov random ﬁeld is\na Gibbs or Boltzmann distribution, whose negative log likelihood (according to the Hammer-\n5 Alternative formulations include power spectra (Section 3.4.3) and non-local means (Buades, Coll, and Morel\n2008).\n764\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure B.1\nGraphical model for an N4 neighborhood Markov random ﬁeld. The white\ncircles are the unknowns f(i, j), while the dark circles are the input data d(i, j). The sx(i, j)\nand sy(i, j) black boxes denote arbitrary interaction potentials between adjacent nodes in the\nrandom ﬁeld, and the w(i, j) denote the data penalty functions. They are all examples of the\ngeneral potentials Vi,j,k,l(f(i, j), f(k, l)) used in Equation (B.24).\nsley–Clifford Theorem) can be written as a sum of pairwise interaction potentials,\nEp(x) =\nX\n{(i,j),(k,l)}∈N\nVi,j,k,l(f(i, j), f(k, l)),\n(B.24)\nwhere N(i, j) denotes the neighbors of pixel (i, j). In the more general case, MRFs can also\ncontain unary potentials, as well as higher-order potentials deﬁned over larger cardinality\ncliques (Kindermann and Snell 1980; Geman and Geman 1984; Bishop 2006; Potetz and Lee\n2008; Kohli, Kumar, and Torr 2009; Kohli, Ladick´y, and Torr 2009; Rother, Kohli, Feng et\nal. 2009; Alahari, Kohli, and Torr 2011). They can also contain line processes, i.e., additional\nbinary variables that mediate discontinuities between adjacent elements (Geman and Geman\n1984). Black and Rangarajan (1996) show how independent line process variables can be\neliminated and incorporated into regular MRFs using robust pairwise penalty functions.\nThe most commonly used neighborhood in Markov random ﬁeld modeling is the N4\nneighborhood, where each pixel in the ﬁeld f(i, j) interacts only with its immediate neighbors—\nFigure B.1 shows such an N4 MRF. The sx(i, j) and sy(i, j) black boxes denote arbitrary\ninteraction potentials between adjacent nodes in the random ﬁeld and the w(i, j) denote the\nelemental data penalty terms in Ed (B.23). These square nodes can also be interpreted as fac-\ntors in a factor graph version of the undirected graphical model (Bishop 2006; Wainwright\nand Jordan 2008; Koller and Friedman 2009), which is another name for interaction poten-\ntials. (Strictly speaking, the factors are improper probability functions whose product is the\nun-normalized posterior distribution.)\nMore complex and higher-dimensional interaction models and neighborhoods are also\nB.5 Markov random ﬁelds\n765\npossible. For example, 2D grids can be enhanced with the addition of diagonal connections\n(an N8 neighborhood) or even larger numbers of pairwise terms (Boykov and Kolmogorov\n2003; Rother, Kolmogorov, Lempitsky et al. 2007). 3D grids can be used to compute glob-\nally optimal segmentations in 3D volumetric medical images (Boykov and Funka-Lea 2006)\n(Section 5.5.1). Higher-order cliques can also be used to develop more sophisticated models\n(Potetz and Lee 2008; Kohli, Ladick´y, and Torr 2009; Kohli, Kumar, and Torr 2009).\nOne of the biggest challenges in using MRF models is to develop efﬁcient inference algo-\nrithms that will ﬁnd low-energy solutions (Veksler 1999; Boykov, Veksler, and Zabih 2001;\nKohli 2007; Kumar 2008). Over the years, a large variety of such algorithms have been de-\nveloped, including simulated annealing, graph cuts, and loopy belief propagation. The choice\nof inference technique can greatly affect the overall performance of a vision system. For\nexample, most of the top-performing algorithms on the Middlebury Stereo Evaluation page\neither use belief propagation or graph cuts.\nIn the next few subsections, we review some of the more widely used MRF inference\ntechniques. More in-depth descriptions of most of these algorithms can be found in a re-\ncently published book on advances in MRF techniques (Blake, Kohli, and Rother 2010).\nExperimental comparisons, along with test datasets and reference software, are provided by\nSzeliski, Zabih, Scharstein et al. (2008).6\nB.5.1 Gradient descent and simulated annealing\nThe simplest optimization technique is gradient descent, which minimizes the energy by\nchanging independent subsets of nodes to take on lower-energy conﬁgurations. Such tech-\nniques go under a variety of names, including contextual classiﬁcation (Kittler and F¨oglein\n1984) and iterated conditional modes (ICM) (Besag 1986).7 Variables can either be updated\nsequentially, e.g., in raster scan, or in parallel, e.g., using red–black coloring on a checker-\nboard. Chou and Brown (1990) suggests using highest conﬁdence ﬁrst (HCF), i.e., choosing\nvariables based on how large a difference they make in reducing the energy.\nThe problem with gradient descent is that it is prone to getting stuck in local minima,\nwhich is almost always the case with MRF problems. One way around this is to use stochastic\ngradient descent or Markov chain Monte Carlo (MCMC) (Metropolis, Rosenbluth, Rosen-\nbluth et al. 1953), i.e., to randomly take occasional uphill steps in order to get out of such\nminima. One popular update rule is the Gibbs sampler (Geman and Geman 1984); rather\nthan choosing the lowest energy state for a variable being updated, it chooses the state with\n6 http://vision.middlebury.edu/MRF/.\n7 The name comes from iteratively setting variables to the mode (most likely, i.e., lowest energy) state conditioned\non its currently ﬁxed neighbors.",
  "image_path": "page_786.jpg",
  "pages": [
    785,
    786,
    787
  ]
}