{
  "doc_id": "pages_207_209",
  "text": "3.7 Global optimization\n185\n(a) initial labeling\n(b) standard move\n(c) α-β-swap\n(d) α-expansion\nFigure 3.58\nMulti-level graph optimization from (Boykov, Veksler, and Zabih 2001) c⃝\n2001 IEEE: (a) initial problem conﬁguration; (b) the standard move only changes one pixel;\n(c) the α-β-swap optimally exchanges all α and β-labeled pixels; (d) the α-expansion move\noptimally selects among current pixel values and the α label.\nWhen ρd or ρp are non-quadratic functions, gradient descent techniques such as non-\nlinear least squares or iteratively re-weighted least squares can sometimes be used (Ap-\npendix A.3). However, if the search space has lots of local minima, as is the case for stereo\nmatching (Barnard 1989; Boykov, Veksler, and Zabih 2001), more sophisticated techniques\nare required.\nThe extension of graph cut techniques to multi-valued problems was ﬁrst proposed by\nBoykov, Veksler, and Zabih (2001). In their paper, they develop two different algorithms,\ncalled the swap move and the expansion move, which iterate among a series of binary labeling\nsub-problems to ﬁnd a good solution (Figure 3.58). Note that a global solution is generally not\nachievable, as the problem is provably NP-hard for general energy functions. Because both\nthese algorithms use a binary MRF optimization inside their inner loop, they are subject to the\nkind of constraints on the energy functions that occur in the binary labeling case (Kolmogorov\nand Zabih 2004). Appendix B.5.4 discusses these algorithms in more detail, along with some\nmore recently developed approaches to this problem.\nAnother MRF inference technique is belief propagation (BP). While belief propagation\nwas originally developed for inference over trees, where it is exact (Pearl 1988), it has more\nrecently been applied to graphs with loops such as Markov random ﬁelds (Freeman, Pasz-\ntor, and Carmichael 2000; Yedidia, Freeman, and Weiss 2001). In fact, some of the better\nperforming stereo-matching algorithms use loopy belief propagation (LBP) to perform their\ninference (Sun, Zheng, and Shum 2003). LBP is discussed in more detail in Appendix B.5.3\nas well as the comparative survey paper on MRF optimization (Szeliski, Zabih, Scharstein et\nal. 2008).\nFigure 3.57 shows an example of image denoising and inpainting (hole ﬁlling) using a\nnon-quadratic energy function (non-Gaussian MRF). The original image has been corrupted\nby noise and a portion of the data has been removed (the black bar). In this case, the loopy\n186\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nd (i, j+1)\nFigure 3.59\nGraphical model for a Markov random ﬁeld with a more complex measurement\nmodel. The additional colored edges show how combinations of unknown values (say, in a\nsharp image) produce the measured values (a noisy blurred image). The resulting graphical\nmodel is still a classic MRF and is just as easy to sample from, but some inference algorithms\n(e.g., those based on graph cuts) may not be applicable because of the increased network\ncomplexity, since state changes during the inference become more entangled and the posterior\nMRF has much larger cliques.\nbelief propagation algorithm computes a slightly lower energy and also a smoother image\nthan the alpha-expansion graph cut algorithm.\nOf course, the above formula (3.113) for the smoothness term Ep(i, j) just shows the\nsimplest case. In more recent work, Roth and Black (2009) propose a Field of Experts (FoE)\nmodel, which sums up a large number of exponentiated local ﬁlter outputs to arrive at the\nsmoothness penalty. Weiss and Freeman (2007) analyze this approach and compare it to the\nsimpler hyper-Laplacian model of natural image statistics. Lyu and Simoncelli (2009) use\nGaussian Scale Mixtures (GSMs) to construct an inhomogeneous multi-scale MRF, with one\n(positive exponential) GMRF modulating the variance (amplitude) of another Gaussian MRF.\nIt is also possible to extend the measurement model to make the sampled (noise-corrupted)\ninput pixels correspond to blends of unknown (latent) image pixels, as in Figure 3.59. This is\nthe commonly occurring case when trying to de-blur an image. While this kind of a model is\nstill a traditional generative Markov random ﬁeld, ﬁnding an optimal solution can be difﬁcult\nbecause the clique sizes get larger. In such situations, gradient descent techniques, such\nas iteratively reweighted least squares, can be used (Joshi, Zitnick, Szeliski et al. 2009).\nExercise 3.31 has you explore some of these issues.\n3.7 Global optimization\n187\nFigure 3.60\nAn unordered label MRF (Agarwala, Dontcheva, Agrawala et al. 2004) c⃝\n2004 ACM: Strokes in each of the source images on the left are used as constraints on an\nMRF optimization, which is solved using graph cuts. The resulting multi-valued label ﬁeld is\nshown as a color overlay in the middle image, and the ﬁnal composite is shown on the right.\nUnordered labels\nAnother case with multi-valued labels where Markov random ﬁelds are often applied are\nunordered labels, i.e., labels where there is no semantic meaning to the numerical difference\nbetween the values of two labels. For example, if we are classifying terrain from aerial\nimagery, it makes no sense to take the numeric difference between the labels assigned to\nforest, ﬁeld, water, and pavement. In fact, the adjacencies of these various kinds of terrain\neach have different likelihoods, so it makes more sense to use a prior of the form\nEp(i, j) = sx(i, j)V (l(i, j), l(i + 1, j)) + sy(i, j)V (l(i, j), l(i, j + 1)),\n(3.115)\nwhere V (l0, l1) is a general compatibility or potential function. (Note that we have also\nreplaced f(i, j) with l(i, j) to make it clearer that these are labels rather than discrete function\nsamples.) An alternative way to write this prior energy (Boykov, Veksler, and Zabih 2001;\nSzeliski, Zabih, Scharstein et al. 2008) is\nEp =\nX\n(p,q)∈N\nVp,q(lp, lq),\n(3.116)\nwhere the (p, q) are neighboring pixels and a spatially varying potential function Vp,q is eval-\nuated for each neighboring pair.\nAn important application of unordered MRF labeling is seam ﬁnding in image composit-\ning (Davis 1998; Agarwala, Dontcheva, Agrawala et al. 2004) (see Figure 3.60, which is\nexplained in more detail in Section 9.3.2). Here, the compatibility Vp,q(lp, lq) measures the\nquality of the visual appearance that would result from placing a pixel p from image lp next\nto a pixel q from image lq. As with most MRFs, we assume that Vp,q(l, l) = 0, i.e., it is per-\nfectly ﬁne to choose contiguous pixels from the same image. For different labels, however,",
  "image_path": "page_208.jpg",
  "pages": [
    207,
    208,
    209
  ]
}