{
  "doc_id": "pages_623_625",
  "text": "12.6 Model-based reconstruction\n601\n(a)\n(b)\n(c)\n(d)\nFigure 12.16\nAutomated architectural reconstruction using 3D lines and planes (Werner\nand Zisserman 2002) c⃝2002 Springer: (a) reconstructed 3D lines, color coded by their van-\nishing directions; (b) wire-frame model superimposed onto an input image; (c) triangulated\npiecewise-planar model with windows; (d) ﬁnal texture-mapped model.\nof plane-sweep stereo aligned to dominant planes and depth map fusion. Cornelis, Leibe,\nCornelis et al. (2008) present a related system that also uses plane-sweep stereo (aligned to\nvertical building fac¸ades) combined with object recognition and segmentation for vehicles.\nMiˇcuˇs´ık and Koˇseck´a (2009) build on these results using omni-directional images and super-\npixel-based stereo matching along dominant plane orientations. Reconstruction directly from\nactive range scanning data combined with color imagery that has been compensated for ex-\nposure and lighting variations is also possible (Chen and Chen 2008; Stamos, Liu, Chen et\nal. 2008; Troccoli and Allen 2008).\n12.6.2 Heads and faces\nAnother area in which specialized shape and appearance models are extremely helpful is in\nthe modeling of heads and faces. Even though the appearance of people seems at ﬁrst glance\nto be inﬁnitely variable, the actual shape of a person’s head and face can be described rea-\nsonably well using a few dozen parameters (Pighin, Hecker, Lischinski et al. 1998; Guenter,\nGrimm, Wood et al. 1998; DeCarlo, Metaxas, and Stone 1998; Blanz and Vetter 1999; Shan,\nLiu, and Zhang 2001).\nFigure 12.17 shows an example of an image-based modeling system, where user-speciﬁed\nkeypoints in several images are used to ﬁt a generic head model to a person’s face. As you\ncan see in Figure 12.17c, after specifying just over 100 keypoints, the shape of the face has\nbecome quite adapted and recognizable. Extracting a texture map from the original images\nand then applying it to the head model results in an animatable model with striking visual\nﬁdelity (Figure 12.18a).\nA more powerful system can be built by applying principal component analysis (PCA) to\na collection of 3D scanned faces, which is a topic we discuss in Section 12.6.3. As you can\nsee in Figure 12.19, it is then possible to ﬁt morphable 3D models to single images and to\n602\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 12.17\n3D model ﬁtting to a collection of images: (Pighin, Hecker, Lischinski et\nal. 1998) c⃝1998 ACM: (a) set of ﬁve input images along with user-selected keypoints; (b)\nthe complete set of keypoints and curves; (c) three meshes—the original, adapted after 13\nkeypoints, and after an additional 99 keypoints; (d) the partition of the image into separately\nanimatable regions.\n(a)\n(b)\nFigure 12.18\nHead and expression tracking and re-animation using deformable 3D models.\n(a) Models ﬁt directly to ﬁve input video streams (Pighin, Szeliski, and Salesin 2002) c⃝\n2002 Springer: The bottom row shows the results of re-animating a synthetic texture-mapped\n3D model with pose and expression parameters ﬁtted to the input images in the top row. (b)\nModels ﬁt to frame-rate spacetime stereo surface models (Zhang, Snavely, Curless et al. 2004)\nc⃝2004 ACM: The top row shows the input images with synthetic green markers overlaid,\nwhile the bottom row shows the ﬁtted 3D surface model.\n12.6 Model-based reconstruction\n603\nuse such models for a variety of animation and visual effects (Blanz and Vetter 1999). It is\nalso possible to design stereo matching algorithms that optimize directly for the head model\nparameters (Shan, Liu, and Zhang 2001; Kang and Jones 2002) or to use the output of real-\ntime stereo with active illumination (Zhang, Snavely, Curless et al. 2004) (Figures 12.7 and\n12.18b).\nAs the sophistication of 3D facial capture systems evolves, so does the detail and realism\nin the reconstructed models. Newer systems can capture (in real-time) not only surface details\nsuch as wrinkles and creases, but also accurate models of skin reﬂection, translucency, and\nsub-surface scattering (Weyrich, Matusik, Pﬁster et al. 2006; Golovinskiy, Matusik, ster et al.\n2006; Bickel, Botsch, Angst et al. 2007; Igarashi, Nishino, and Nayar 2007).\nOnce a 3D head model has been constructed, it can be used in a variety of applications,\nsuch as head tracking (Toyama 1998; Lepetit, Pilet, and Fua 2004; Matthews, Xiao, and Baker\n2007), as shown in Figures 4.29 and 14.24, and face transfer, i.e., replacing one person’s\nface with another in a video (Bregler, Covell, and Slaney 1997; Vlasic, Brand, Pﬁster et al.\n2005). Additional applications include face beautiﬁcation by warping face images toward a\nmore attractive “standard” (Leyvand, Cohen-Or, Dror et al. 2008), face de-identiﬁcation for\nprivacy protection (Gross, Sweeney, De la Torre et al. 2008), and face swapping (Bitouk,\nKumar, Dhillon et al. 2008).\n12.6.3 Application: Facial animation\nPerhaps the most widely used application of 3D head modeling is facial animation. Once\na parameterized 3D model of shape and appearance (surface texture) has been constructed,\nit can be used directly to track a person’s facial motions (Figure 12.18a) and to animate a\ndifferent character with these same motions and expressions (Pighin, Szeliski, and Salesin\n2002).\nAn improved version of such a system can be constructed by ﬁrst applying principal com-\nponent analysis (PCA) to the space of possible head shapes and facial appearances. Blanz\nand Vetter (1999) describe a system where they ﬁrst capture a set of 200 colored range scans\nof faces (Figure 12.19a), which can be represented as a large collection of (X, Y, Z, R, G, B)\nsamples (vertices).9 In order for 3D morphing to be meaningful, corresponding vertices in\ndifferent people’s scans must ﬁrst be put into correspondence (Pighin, Hecker, Lischinski et\nal. 1998). Once this is done, PCA can be applied to more naturally parameterize the 3D mor-\nphable model. The ﬂexibility of this model can be increased by performing separate analyses\nin different subregions, such as the eyes, nose, and mouth, just as in modular eigenspaces\n(Moghaddam and Pentland 1997).\n9 A cylindrical coordinate system provides a natural two-dimensional embedding for this collection, but such an\nembedding is not necessary to perform PCA.",
  "image_path": "page_624.jpg",
  "pages": [
    623,
    624,
    625
  ]
}