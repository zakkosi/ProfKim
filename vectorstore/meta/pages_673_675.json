{
  "doc_id": "pages_673_675",
  "text": "13.7 Exercises\n651\n4. (Optional) Take a single image, such as the Mona Lisa or a friend’s picture, and create\nan animated 3D view morph (Seitz and Dyer 1996).\n(a) Find the vertical axis of symmetry in the image and reﬂect your reference image\nto provide a virtual pair (assuming the person’s hairstyle is somewhat symmetric).\n(b) Use structure from motion to determine the relative camera pose of the pair.\n(c) Use dense stereo matching to estimate the 3D shape.\n(d) Use view morphing to create a 3D animation.\nEx 13.4: View dependent texture mapping\nUse a 3D model you created along with the\noriginal images to implement a view-dependent texture mapping system.\n1. Use one of the 3D reconstruction techniques you developed in Exercises 7.3, 11.9,\n11.10, or 12.8 to build a triangulated 3D image-based model from multiple photographs.\n2. Extract textures for each model face from your photographs, either by performing the\nappropriate resampling or by ﬁguring out how to use the texture mapping software to\ndirectly access the source images.\n3. At run time, for each new camera view, select the best source image for each visible\nmodel face.\n4. Extend this to blend between the top two or three textures. This is trickier, since it\ninvolves the use of texture blending or pixel shading (Debevec, Taylor, and Malik 1996;\nDebevec, Yu, and Borshukov 1998; Pighin, Hecker, Lischinski et al. 1998).\nEx 13.5: Layered depth images\nExtend your view interpolation algorithm (Exercise 13.2)\nto store more than one depth or color value per pixel (Shade, Gortler, He et al. 1998), i.e., a\nlayered depth image (LDI). Modify your rendering algorithm accordingly. For your data, you\ncan use synthetic ray tracing, a layered reconstructed model, or a volumetric reconstruction.\nEx 13.6: Rendering from sprites or layers\nExtend your view interpolation algorithm to\nhandle multiple planes or sprites (Section 13.2.1) (Shade, Gortler, He et al. 1998).\n1. Extract your layers using the technique you developed in Exercise 8.9.\n2. Alternatively, use an interactive painting and 3D placement system to extract your lay-\ners (Kang 1998; Oh, Chen, Dorsey et al. 2001; Shum, Sun, Yamazaki et al. 2004).\n3. Determine a back-to-front order based on expected visibility or add a z-buffer to your\nrendering algorithm to handle occlusions.\n652\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n4. Render and composite all of the resulting layers, with optional alpha matting to handle\nthe edges of layers and sprites.\nEx 13.7: Light ﬁeld transformations\nDerive the equations relating regular images to 4D\nlight ﬁeld coordinates.\n1. Determine the mapping between the far plane (u, v) coordinates and a virtual camera’s\n(x, y) coordinates.\n(a) Start by parameterizing a 3D point on the uv plane in terms of its (u, v) coordi-\nnates.\n(b) Project the resulting 3D point to the camera pixels (x, y, 1) using the usual 3 × 4\ncamera matrix P (2.63).\n(c) Derive the 2D homography relating (u, v) and (x, y) coordinates.\n2. Write down a similar transformation for (s, t) to (x, y) coordinates.\n3. Prove that if the virtual camera is actually on the (s, t) plane, the (s, t) value depends\nonly on the camera’s optical center and is independent of (x, y).\n4. Prove that an image taken by a regular orthographic or perspective camera, i.e., one that\nhas a linear projective relationship between 3D points and (x, y) pixels (2.63), samples\nthe (s, t, u, v) light ﬁeld along a two-dimensional hyperplane.\nEx 13.8: Light ﬁeld and Lumigraph rendering\nImplement a light ﬁeld or Lumigraph ren-\ndering system:\n1. Download one of the light ﬁeld data sets from http://lightﬁeld.stanford.edu/.\n2. Write an algorithm to synthesize a new view from this light ﬁeld, using quadri-linear\ninterpolation of (s, t, u, v) ray samples.\n3. Try varying the focal plane corresponding to your desired view (Isaksen, McMillan,\nand Gortler 2000) and see if the resulting image looks sharper.\n4. Determine a 3D proxy for the objects in your scene. You can do this by running multi-\nview stereo over one of your light ﬁelds to obtain a depth map per image.\n5. Implement the Lumigraph rendering algorithm, which modiﬁes the sampling of rays\naccording to the 3D location of each surface element.\n6. Collect a set of images yourself and determine their pose using structure from motion.\n13.7 Exercises\n653\n7. Implement the unstructured Lumigraph rendering algorithm from Buehler, Bosse, McMil-\nlan et al. (2001).\nEx 13.9: Surface light ﬁelds\nConstruct a surface light ﬁeld (Wood, Azuma, Aldinger et al.\n2000) and see how well you can compress it.\n1. Acquire an interesting light ﬁeld of a specular scene or object, or download one from\nhttp://lightﬁeld.stanford.edu/.\n2. Build a 3D model of the object using a multi-view stereo algorithm that is robust to\noutliers due to specularities.\n3. Estimate the Lumisphere for each surface point on the object.\n4. Estimate its diffuse components. Is the median the best way to do this? Why not use\nthe minimum color value? What happens if there is Lambertian shading on the diffuse\ncomponent?\n5. Model and compress the remaining portion of the Lumisphere using one of the tech-\nniques suggested by Wood, Azuma, Aldinger et al. (2000) or invent one of your own.\n6. Study how well your compression algorithm works and what artifacts it produces.\n7. (Optional) Develop a system to edit and manipulate your surface light ﬁeld.\nEx 13.10: Handheld concentric mosaics\nDevelop a system to navigate a handheld con-\ncentric mosaic.\n1. Stand in the middle of a room with a camcorder held at arm’s length in front of you and\nspin in a circle.\n2. Use a structure from motion system to determine the camera pose and sparse 3D struc-\nture for each input frame.\n3. (Optional) Re-bin your image pixels into a more regular concentric mosaic structure.\n4. At view time, determine from the new camera’s view (which should be near the plane\nof your original capture) which source pixels to display. You can simplify your com-\nputations to determine a source column (and scaling) for each output column.\n5. (Optional) Use your sparse 3D structure, interpolated to a dense depth map, to improve\nyour rendering (Zheng, Kang, Cohen et al. 2007).",
  "image_path": "page_674.jpg",
  "pages": [
    673,
    674,
    675
  ]
}