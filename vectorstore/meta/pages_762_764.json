{
  "doc_id": "pages_762_764",
  "text": "740\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(This can be derived from (A.6) by post-multiplying both sides by ui.) Since the latter equa-\ntion is homogeneous, i.e., it has a zero right-hand-side, it can only have a non-zero (non-\ntrivial) solution for ui if the system is rank deﬁcient, i.e.,\n|(λI −C)| = 0.\n(A.16)\nEvaluating this determinant yields a characteristic polynomial equation in λ, which can be\nsolved for small problems, e.g., 2 × 2 or 3 × 3 matrices, in closed form.\nFor larger matrices, iterative algorithms that ﬁrst reduce the matrix C to a real symmetric\ntridiagonal form using orthogonal transforms and then perform QR iterations are normally\nused (Golub and Van Loan 1996; Trefethen and Bau 1997; Bj¨orck and Dahlquist 2010). Since\nthese techniques are rather involved, it is best to use a linear algebra package such as LAPACK\n(Anderson, Bai, Bischof et al. 1999)—see Appendix C.2.\nFactorization with missing data requires different kinds of iterative algorithms, which of-\nten involve either hallucinating the missing terms or minimizing some weighted reconstruc-\ntion metric, which is intrinsically much more challenging than regular factorization. This\narea has been widely studied in computer vision (Shum, Ikeuchi, and Reddy 1995; De la\nTorre and Black 2003; Huynh, Hartley, and Heyden 2003; Buchanan and Fitzgibbon 2005;\nGross, Matthews, and Baker 2006; Torresani, Hertzmann, and Bregler 2008) and is some-\ntimes called generalized PCA. However, this term is also sometimes used to denote algebraic\nsubspace clustering techniques, which is the subject of a forthcoming monograph by Vidal,\nMa, and Sastry (2010).\nA.1.3 QR factorization\nA widely used technique for stably solving poorly conditioned least squares problems (Bj¨orck\n1996) and as the basis of more complex algorithms, such as computing the SVD and eigen-\nvalue decompositions, is the QR factorization,\nA = QR,\n(A.17)\nwhere Q is an orthonormal (or unitary) matrix QQT = I and R is upper triangular.3 In\ncomputer vision, QR can be used to convert a camera matrix into a rotation matrix and\nan upper-triangular calibration matrix (6.35) and also in various self-calibration algorithms\n(Section 7.2.2). The most common algorithms for computing QR decompositions, modiﬁed\nGram–Schmidt, Householder transformations, and Givens rotations, are described by Golub\nand Van Loan (1996), Trefethen and Bau (1997), and Bj¨orck and Dahlquist (2010) and are\n3 The term “R” comes from the German name for the lower–upper (LU) decomposition, which is LR for “links”\nand “rechts” (left and right of the diagonal).\nA.1 Matrix decompositions\n741\nprocedure Cholesky(C, R):\nR = C\nfor i = 0 . . . n −1\nfor j = i + 1 . . . n −1\nRj,j:n−1 = Rj,j:n−1 −rijr−1\nii Ri,j:n−1\nRi,i:n−1 = r−1/2\nii\nRi,i:n−1\nAlgorithm A.1 Cholesky decomposition of the matrix C into its upper triangular form R.\nalso found in LAPACK. Unlike the SVD and eigenvalue decompositions, QR factorization\ndoes not require iteration and can be computed exactly in O(MN 2 + N 3) operations, where\nM is the number of rows and N is the number of columns (for a tall matrix).\nA.1.4 Cholesky factorization\nCholesky factorization can be applied to any symmetric positive deﬁnite matrix C to convert\nit into a product of symmetric lower and upper triangular matrices,\nC = LLT = RT R,\n(A.18)\nwhere L is a lower-triangular matrix and R is an upper-triangular matrix. Unlike Gaussian\nelimination, which may require pivoting (row and column reordering) or may become un-\nstable (sensitive to roundoff errors or reordering), Cholesky factorization remains stable for\npositive deﬁnite matrices, such as those that arise from normal equations in least squares prob-\nlems (Appendix A.2). Because of the form of (A.18), the matrices L and R are sometimes\ncalled matrix square roots.4\nThe algorithm to compute an upper triangular Cholesky decomposition of C is a straight-\nforward symmetric generalization of Gaussian elimination and is based on the decomposition\n(Bj¨orck 1996; Golub and Van Loan 1996)\nC\n=\n\"\nγ\ncT\nc\nC11\n#\n(A.19)\n=\n\"\nγ1/2\n0T\ncγ−1/2\nI\n# \"\n1\n0T\n0\nC11 −cγ−1cT\n# \"\nγ1/2\nγ−1/2cT\n0\nI\n#\n(A.20)\n4 In fact, there exists a whole family of matrix square roots. Any matrix of the form LQ or QR, where Q is a\nunitary matrix, is a square root of C.\n742\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n=\nRT\n0 C1R0,\n(A.21)\nwhich, through recursion, can be turned into\nC = RT\n0 . . . RT\nn−1Rn−1 . . . R0 = RT R.\n(A.22)\nAlgorithm A.1 provides a more procedural deﬁnition, which can store the upper-triangular\nmatrix R in the same space as C, if desired. The total operation count for Cholesky factor-\nization is O(N 3) for a dense matrix but can be signiﬁcantly lower for sparse matrices with\nlow ﬁll-in (Appendix A.4).\nNote that Cholesky decomposition can also be applied to block-structured matrices, where\nthe term γ in (A.19) is now a square block sub-matrix and c is a rectangular matrix (Golub\nand Van Loan 1996). The computation of square roots can be avoided by leaving the γ on\nthe diagonal of the middle factor in (A.20), which results in the C = LDLT factorization,\nwhere D is a diagonal matrix. However, since square roots are relatively fast on modern\ncomputers, this is not worth the bother and Cholesky factorization is usually preferred.\nA.2 Linear least squares\nLeast squares ﬁtting problems are pervasive in computer vision. For example, the alignment\nof images based on matching feature points involves the minimization of a squared distance\nobjective function (6.2),\nELS =\nX\ni\n∥ri∥2 =\nX\ni\n∥f(xi; p) −x′\ni∥2,\n(A.23)\nwhere\nri = f(xi; p) −x′\ni = ˆx′\ni −˜x′\ni\n(A.24)\nis the residual between the measured location ˆx′\ni and its corresponding current predicted lo-\ncation ˜x′\ni = f(xi; p). More complex versions of least squares problems, such as large-scale\nstructure from motion (Section 7.4), may involve the minimization of functions of thousands\nof variables. Even problems such as image ﬁltering (Section 3.4.3) and regularization (Sec-\ntion 3.7.1) may involve the minimization of sums of squared errors.\nFigure A.2a shows an example of a simple least squares line ﬁtting problem, where the\nquantities being estimated are the line equation parameters (m, b). When the sampled vertical\nvalues yi are assumed to be noisy versions of points on the line y = mx + b, the optimal\nestimates for (m, b) can be found by minimizing the squared vertical residuals\nEVLS =\nX\ni\n|yi −(mxi + b)|2.\n(A.25)",
  "image_path": "page_763.jpg",
  "pages": [
    762,
    763,
    764
  ]
}