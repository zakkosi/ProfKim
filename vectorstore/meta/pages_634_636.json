{
  "doc_id": "pages_634_636",
  "text": "612\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.22\nEstimating the diffuse albedo and reﬂectance parameters for a scanned 3D\nmodel (Sato, Wheeler, and Ikeuchi 1997) c⃝1997 ACM: (a) set of input images projected\nonto the model; (b) the complete diffuse reﬂection (albedo) model; (c) rendering from the\nreﬂectance model including the specular component.\n12.7.1 Estimating BRDFs\nA more ambitious approach to the problem of view-dependent appearance modeling is to\nestimate a general bidirectional reﬂectance distribution function (BRDF) for each point on an\nobject’s surface. Dana, van Ginneken, Nayar et al. (1999), Jensen, Marschner, Levoy et al.\n(2001), and Lensch, Kautz, Goesele et al. (2003) present different techniques for estimating\nsuch functions, while Dorsey, Rushmeier, and Sillion (2007) and Weyrich, Lawrence, Lensch\net al. (2008) present more recent surveys of the topics of BRDF modeling, recovery, and\nrendering.\nAs we saw in Section 2.2.2 (2.81), the BRDF can be written as\nfr(θi, φi, θr, φr; λ),\n(12.9)\nwhere (θi, φi) and (θr, φr) are the angles the incident ˆvi and reﬂected ˆvr light ray directions\nmake with the local surface coordinate frame ( ˆdx, ˆdy, ˆn) shown in Figure 2.15. When mod-\neling the appearance of an object, as opposed to the appearance of a patch of material, we\nneed to estimate this function at every point (x, y) on the object’s surface, which gives us the\nspatially varying BRDF, or SVBRDF (Weyrich, Lawrence, Lensch et al. 2008),\nfv(x, y, θi, φi, θr, φr; λ).\n(12.10)\nIf sub-surface scattering effects are being modeled, such as the long-range transmission\nof light through materials such as alabaster, the eight-dimensional bidirectional scattering-\nsurface reﬂectance-distribution function (BSSRDF) is used instead,\nfe(xi, yi, θi, φi, xe, ye, θe, φe; λ),\n(12.11)\nwhere the e subscript now represents the emitted rather than the reﬂected light directions.\n12.7 Recovering texture maps and albedos\n613\n(a)\n(b)\nFigure 12.23\nImage-based reconstruction of appearance and detailed geometry (Lensch,\nKautz, Goesele et al. 2003) c⃝2003 ACM. (a) Appearance models (BRDFs) are re-estimated\nusing divisive clustering. (b) In order to model detailed spatially varying appearance, each\nlumitexel is projected onto the basis formed by the clustered materials.\nWeyrich, Lawrence, Lensch et al. (2008) provide a nice survey of these and related topics,\nincluding basic photometry, BRDF models, traditional BRDF acquisition using gonio reﬂec-\ntometry (the precise measurement of visual angles and reﬂectances), multiplexed illumination\n(Schechner, Nayar, and Belhumeur 2009), skin modeling (Debevec, Hawkins, Tchou et al.\n2000; Weyrich, Matusik, Pﬁster et al. 2006), and image-based acquisition techniques, which\nsimultaneously recover an object’s 3D shape and reﬂectometry from multiple photographs.\nA nice example of this latter approach is the system developed by Lensch, Kautz, Goesele\net al. (2003), who estimate locally varying BRDFs and reﬁne their shape models using local\nestimates of surface normals. To build up their models, they ﬁrst associate a lumitexels, which\ncontains a 3D position, a surface normal, and a set of sparse radiance samples, with each\nsurface point. Next, they cluster such lumitexels into materials that share common properties,\nusing a Lafortune reﬂectance model (Lafortune, Foo, Torrance et al. 1997) and a divisive\nclustering approach (Figure 12.23a). Finally, in order to model detailed spatially varying\nappearance, each lumitexel (surface point) is projected onto the basis of clustered appearance\nmodels (Figure 12.23b).\nWhile most of the techniques discussed in this section require large numbers of views\nto estimate surface properties, a challenging future direction will be to take these techniques\nout of the lab and into the real world, and to combine them with regular and Internet photo\nimage-based modeling approaches.\n12.7.2 Application: 3D photography\nThe techniques described in this chapter for building complete 3D models from multiple im-\nages and then recovering their surface appearance have opened up a whole new range of\napplications that often go under the name 3D photography. Pollefeys and Van Gool (2002)\nprovide a nice introduction to this ﬁeld, including the processing steps of feature matching,\n614\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nstructure from motion recovery,15 dense depth map estimation, 3D model building, and tex-\nture map recovery. A complete Web-based system for automatically performing all of these\ntasks, called ARC3D, is described by Vergauwen and Van Gool (2006) and Moons, Van Gool,\nand Vergauwen (2010). The latter paper provides not only an in-depth survey of this whole\nﬁeld but also a detailed description of their complete end-to-end system.\nAn alternative to such fully automated systems is to put the user in the loop in what is\nsometimes called interactive computer vision. van den Hengel, Dick, Thormhlen et al. (2007)\ndescribe their VideoTrace system, which performs automated point tracking and 3D structure\nrecovery from video and then lets the user draw triangles and surfaces on top of the resulting\npoint cloud, as well as interactively adjusting the locations of model vertices. Sinha, Steedly,\nSzeliski et al. (2008) describe a related system that uses matched vanishing points in multiple\nimages (Figure 4.45) to infer 3D line orientations and plane normals. These are then used to\nguide the user drawing axis-aligned planes, which are automatically ﬁtted to the recovered\n3D point cloud. Fully automated variants on these ideas are described by Zebedin, Bauer,\nKarner et al. (2008), Furukawa, Curless, Seitz et al. (2009a), Furukawa, Curless, Seitz et al.\n(2009b), Miˇcuˇs´ık and Koˇseck´a (2009), and Sinha, Steedly, and Szeliski (2009).\nAs the sophistication and reliability of these techniques continues to improve, we can ex-\npect to see even more user-friendly applications for photorealistic 3D modeling from images\n(Exercise 12.8).\n12.8 Additional reading\nShape from shading is one of the classic problems in computer vision (Horn 1975). Some\nrepresentative papers in this area include those by Horn (1977), Ikeuchi and Horn (1981),\nPentland (1984), Horn and Brooks (1986), Horn (1990), Szeliski (1991a), Mancini and Wolff\n(1992), Dupuis and Oliensis (1994), and Fua and Leclerc (1995). The collection of papers\nedited by Horn and Brooks (1989) is a great source of information on this topic, especially\nthe chapter on variational approaches. The survey by Zhang, Tsai, Cryer et al. (1999) not\nonly reviews more recent techniques but also provides some comparative results.\nWoodham (1981) wrote the seminal paper of photometric stereo. Shape from texture\ntechniques include those by Witkin (1981), Ikeuchi (1981), Blostein and Ahuja (1987), Gard-\ning (1992), Malik and Rosenholtz (1997), Liu, Collins, and Tsin (2004), Liu, Lin, and Hays\n(2004), Hays, Leordeanu, Efros et al. (2006), Lin, Hays, Wu et al. (2006), Lobay and Forsyth\n(2006), White and Forsyth (2006), White, Crane, and Forsyth (2007), and Park, Brockle-\nhurst, Collins et al. (2009). Good papers and books on depth from defocus have been written\nby Pentland (1987), Nayar and Nakagawa (1994), Nayar, Watanabe, and Noguchi (1996),\n15 These earlier steps are also discussed in Section 7.4.4.",
  "image_path": "page_635.jpg",
  "pages": [
    634,
    635,
    636
  ]
}