{
  "doc_id": "pages_694_696",
  "text": "672\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx\nDFFS\nDIFS\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nF\nF\n_\nm\n~x\nFigure 14.14 Projection onto the linear subspace spanned by the eigenface images (Moghad-\ndam and Pentland 1997) c⃝1997 IEEE. The distance from face space (DFFS) is the orthog-\nonal distance to the plane, while the distance in face space (DIFS) is the distance along the\nplane from the mean image. Both distances can be turned into Mahalanobis distances and\ngiven probabilistic interpretations.\n(1991a) call them eigenfaces (Figure 14.13b).8\nTwo important properties of the eigenvalue decomposition are that the optimal (best ap-\nproximation) coefﬁcients ai for any new image x can be computed as\nai = (x −m) · ui,\n(14.11)\nand that, assuming the eigenvalues {λi} are sorted in decreasing order, truncating the ap-\nproximation given in (14.8) at any point M gives the best possible approximation (least er-\nror) between ˜x and x. Figure 14.13c shows the resulting approximation corresponding to\nFigure 14.13a and shows how much better it is at compressing a face image than JPEG.\nTruncating the eigenface decomposition of a face image (14.8) after M components is\nequivalent to projecting the image onto a linear subspace F, which we can call the face space\n(Figure 14.14). Because the eigenvectors (eigenfaces) are orthogonal and of unit norm, the\ndistance of a projected face ˜x to the mean face m can be written as\nDIFS = ∥˜x −m∥=\nv\nu\nu\nt\nM−1\nX\ni=0\na2\ni ,\n(14.12)\nwhere DIFS stands for distance in face space (Moghaddam and Pentland 1997). The re-\nmaining distance between the original image x and its projection onto face space ˜x, i.e., the\n8 In actual practice, the full P ×P scatter matrix (14.9) is never computed. Instead, a smaller N ×N matrix con-\nsisting of the inner products between all the signed deviations (xi−m) is accumulated instead. See Appendix A.1.2\n(A.13–A.14) for details.\n14.2 Face recognition\n673\ndistance from face space (DFFS), can be computed directly in pixel space and represents the\n“faceness” of a particular image.9 It is also possible to measure the distance between two\ndifferent faces in face space as\nDIFS(x, y) = ∥˜x −˜y∥=\nv\nu\nu\nt\nM−1\nX\ni=0\n(ai −bi)2,\n(14.13)\nwhere the bi = (y −m) · ui are the eigenface coefﬁcients corresponding to y.\nComputing such distances in Euclidean vector space, however, does not exploit the ad-\nditional information that the eigenvalue decomposition of our covariance matrix (14.10) pro-\nvides. If we interpret the covariance matrix C as the covariance of a multi-variate Gaussian\n(Appendix B.1.1),10 we can turn the DIFS into a log likelihood by computing the Maha-\nlanobis distance\nDIFS′ = ∥˜x −m∥C\n−1 =\nv\nu\nu\nt\nM−1\nX\ni=0\na2\ni /λ2\ni .\n(14.14)\nInstead of measuring the squared distance along each principal component in face space F,\nthe Mahalanobis distance measures the ratio between the squared distance and the corre-\nsponding variance σ2\ni = λi and then sums these squared ratios (per-component log-likelihoods).\nAn alternative way to implement this is to pre-scale each eigenvector by the inverse square\nroot of its corresponding eigenvalue,\nˆU = UΛ−1/2.\n(14.15)\nThis whitening transformation then means that Euclidean distances in feature (face) space\nnow correspond directly to log likelihoods (Moghaddam, Jebara, and Pentland 2000). (This\nsame whitening approach can also be used in feature-based matching algorithms, as discussed\nin Section 4.1.3.)\nIf the distribution in eigenface space is very elongated, the Mahalanobis distance properly\nscales the components to come up with a sensible (probabilistic) distance from the mean.\nA similar analysis can be performed for computing a sensible difference from face space\n(DFFS) (Moghaddam and Pentland 1997) and the two terms can be combined to produce an\nestimate of the likelihood of being a true face, which can be useful in doing face detection\n(Section 14.1.1). More detailed explanations of probabilistic and Bayesian PCA can be found\nin textbooks on statistical learning (Hastie, Tibshirani, and Friedman 2001; Bishop 2006),\nwhich also discuss techniques for selecting the optimum number of components M to use in\nmodeling a distribution.\n9 This can be used to form a simple face detector, as mentioned in Section 14.1.1.\n10 The ellipse shown in Figure 14.14 denotes an equi-probability contour of this multi-variate Gaussian.\n674\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.15 Images from the Harvard database used by Belhumeur, Hespanha, and Krieg-\nman (1997) c⃝1997 IEEE. Note the wide range of illumination variation, which can be more\ndramatic than inter-personal variations.\nOne of the biggest advantages of using eigenfaces is that they reduce the comparison\nof a new face image x to a prototype (training) face image xk (one of the colored xs in\nFigure 14.14) from a P-dimensional difference in pixel space to an M-dimensional difference\nin face space,\n∥x −xk∥= ∥a −ak∥,\n(14.16)\nwhere a = U T (x −m) (14.11) involves computing a dot product between the signed\ndifference-from-mean image (x −m) and each of the eigenfaces ui. Once again, however,\nthis Euclidean distance ignores the fact that we have more information about face likelihoods\navailable in the distribution of training images.\nConsider the set of images of one person taken under a wide range of illuminations shown\nin Figure 14.15. As you can see, the intrapersonal variability within these images is much\ngreater than the typical extrapersonal variability between any two people taken under the\nsame illumination. Regular PCA analysis fails to distinguish between these two sources of\nvariability and may, in fact, devote most of its principal components to modeling the intrap-\nersonal variability.\nIf we are going to approximate faces by a linear subspace, it is more useful to have a\nspace that discriminates between different classes (people) and is less sensitive to within-class\nvariations (Belhumeur, Hespanha, and Kriegman 1997). Consider the three classes shown as\ndifferent colors in Figure 14.16. As you can see, the distributions within a class (indicated\nby the tilted colored axes) are elongated and tilted with respect to the main face space PCA,",
  "image_path": "page_695.jpg",
  "pages": [
    694,
    695,
    696
  ]
}