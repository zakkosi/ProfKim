{
  "doc_id": "pages_053_055",
  "text": "2.1 Geometric primitives and transformations\n31\nBefore we can intelligently analyze and manipulate images, we need to establish a vocabulary\nfor describing the geometry of a scene. We also need to understand the image formation\nprocess that produced a particular image given a set of lighting conditions, scene geometry,\nsurface properties, and camera optics. In this chapter, we present a simpliﬁed model of such\nan image formation process.\nSection 2.1 introduces the basic geometric primitives used throughout the book (points,\nlines, and planes) and the geometric transformations that project these 3D quantities into 2D\nimage features (Figure 2.1a). Section 2.2 describes how lighting, surface properties (Fig-\nure 2.1b), and camera optics (Figure 2.1c) interact in order to produce the color values that\nfall onto the image sensor. Section 2.3 describes how continuous color images are turned into\ndiscrete digital samples inside the image sensor (Figure 2.1d) and how to avoid (or at least\ncharacterize) sampling deﬁciencies, such as aliasing.\nThe material covered in this chapter is but a brief summary of a very rich and deep set of\ntopics, traditionally covered in a number of separate ﬁelds. A more thorough introduction to\nthe geometry of points, lines, planes, and projections can be found in textbooks on multi-view\ngeometry (Hartley and Zisserman 2004; Faugeras and Luong 2001) and computer graphics\n(Foley, van Dam, Feiner et al. 1995). The image formation (synthesis) process is traditionally\ntaught as part of a computer graphics curriculum (Foley, van Dam, Feiner et al. 1995; Glass-\nner 1995; Watt 1995; Shirley 2005) but it is also studied in physics-based computer vision\n(Wolff, Shafer, and Healey 1992a). The behavior of camera lens systems is studied in optics\n(M¨oller 1988; Hecht 2001; Ray 2002). Two good books on color theory are (Wyszecki and\nStiles 2000; Healey and Shafer 1992), with (Livingstone 2008) providing a more fun and in-\nformal introduction to the topic of color perception. Topics relating to sampling and aliasing\nare covered in textbooks on signal and image processing (Crane 1997; J¨ahne 1997; Oppen-\nheim and Schafer 1996; Oppenheim, Schafer, and Buck 1999; Pratt 2007; Russ 2007; Burger\nand Burge 2008; Gonzales and Woods 2008).\nA note to students: If you have already studied computer graphics, you may want to\nskim the material in Section 2.1, although the sections on projective depth and object-centered\nprojection near the end of Section 2.1.5 may be new to you. Similarly, physics students (as\nwell as computer graphics students) will mostly be familiar with Section 2.2. Finally, students\nwith a good background in image processing will already be familiar with sampling issues\n(Section 2.3) as well as some of the material in Chapter 3.\n2.1 Geometric primitives and transformations\nIn this section, we introduce the basic 2D and 3D primitives used in this textbook, namely\npoints, lines, and planes. We also describe how 3D features are projected into 2D features.\n32\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMore detailed descriptions of these topics (along with a gentler and more intuitive introduc-\ntion) can be found in textbooks on multiple-view geometry (Hartley and Zisserman 2004;\nFaugeras and Luong 2001).\n2.1.1 Geometric primitives\nGeometric primitives form the basic building blocks used to describe three-dimensional shapes.\nIn this section, we introduce points, lines, and planes. Later sections of the book discuss\ncurves (Sections 5.1 and 11.2), surfaces (Section 12.3), and volumes (Section 12.5).\n2D points.\n2D points (pixel coordinates in an image) can be denoted using a pair of values,\nx = (x, y) ∈R2, or alternatively,\nx =\n\"\nx\ny\n#\n.\n(2.1)\n(As stated in the introduction, we use the (x1, x2, . . .) notation to denote column vectors.)\n2D points can also be represented using homogeneous coordinates, ˜x = (˜x, ˜y, ˜w) ∈P2,\nwhere vectors that differ only by scale are considered to be equivalent. P2 = R3 −(0, 0, 0)\nis called the 2D projective space.\nA homogeneous vector ˜x can be converted back into an inhomogeneous vector x by\ndividing through by the last element ˜w, i.e.,\n˜x = (˜x, ˜y, ˜w) = ˜w(x, y, 1) = ˜w¯x,\n(2.2)\nwhere ¯x = (x, y, 1) is the augmented vector. Homogeneous points whose last element is ˜w =\n0 are called ideal points or points at inﬁnity and do not have an equivalent inhomogeneous\nrepresentation.\n2D lines.\n2D lines can also be represented using homogeneous coordinates ˜l = (a, b, c).\nThe corresponding line equation is\n¯x · ˜l = ax + by + c = 0.\n(2.3)\nWe can normalize the line equation vector so that l = (ˆnx, ˆny, d) = (ˆn, d) with ∥ˆn∥= 1. In\nthis case, ˆn is the normal vector perpendicular to the line and d is its distance to the origin\n(Figure 2.2). (The one exception to this normalization is the line at inﬁnity ˜l = (0, 0, 1),\nwhich includes all (ideal) points at inﬁnity.)\nWe can also express ˆn as a function of rotation angle θ, ˆn = (ˆnx, ˆny) = (cos θ, sin θ)\n(Figure 2.2a). This representation is commonly used in the Hough transform line-ﬁnding\n2.1 Geometric primitives and transformations\n33\ny\nx\nd\nθ\nn\nl\n^\nz\nx\nd\nn\nm\ny\n^\n(a)\n(b)\nFigure 2.2 (a) 2D line equation and (b) 3D plane equation, expressed in terms of the normal\nˆn and distance to the origin d.\nalgorithm, which is discussed in Section 4.3.2. The combination (θ, d) is also known as\npolar coordinates.\nWhen using homogeneous coordinates, we can compute the intersection of two lines as\n˜x = ˜l1 × ˜l2,\n(2.4)\nwhere × is the cross product operator. Similarly, the line joining two points can be written as\n˜l = ˜x1 × ˜x2.\n(2.5)\nWhen trying to ﬁt an intersection point to multiple lines or, conversely, a line to multiple\npoints, least squares techniques (Section 6.1.1 and Appendix A.2) can be used, as discussed\nin Exercise 2.1.\n2D conics.\nThere are other algebraic curves that can be expressed with simple polynomial\nhomogeneous equations. For example, the conic sections (so called because they arise as the\nintersection of a plane and a 3D cone) can be written using a quadric equation\n˜xT Q˜x = 0.\n(2.6)\nQuadric equations play useful roles in the study of multi-view geometry and camera calibra-\ntion (Hartley and Zisserman 2004; Faugeras and Luong 2001) but are not used extensively in\nthis book.\n3D points.\nPoint coordinates in three dimensions can be written using inhomogeneous co-\nordinates x = (x, y, z) ∈R3 or homogeneous coordinates ˜x = (˜x, ˜y, ˜z, ˜w) ∈P3. As before,\nit is sometimes useful to denote a 3D point using the augmented vector ¯x = (x, y, z, 1) with\n˜x = ˜w¯x.",
  "image_path": "page_054.jpg",
  "pages": [
    53,
    54,
    55
  ]
}