{
  "doc_id": "pages_777_779",
  "text": "Appendix B\nBayesian modeling and inference\nB.1\nEstimation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757\nB.1.1\nLikelihood for multivariate Gaussian noise\n. . . . . . . . . . . . . . 757\nB.2\nMaximum likelihood estimation and least squares . . . . . . . . . . . . . . . 759\nB.3\nRobust statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 760\nB.4\nPrior models and Bayesian inference . . . . . . . . . . . . . . . . . . . . . . 762\nB.5\nMarkov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763\nB.5.1\nGradient descent and simulated annealing . . . . . . . . . . . . . . . 765\nB.5.2\nDynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 766\nB.5.3\nBelief propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\nB.5.4\nGraph cuts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770\nB.5.5\nLinear programming . . . . . . . . . . . . . . . . . . . . . . . . . . 773\nB.6\nUncertainty estimation (error analysis) . . . . . . . . . . . . . . . . . . . . . 775\n756\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe following problem commonly recurs in this book: Given a number of measurements\n(images, feature positions, etc.), estimate the values of some unknown structure or parameter\n(camera positions, object shape, etc.). These kinds of problems are in general called inverse\nproblems because they involve estimating unknown model parameters instead of simulating\nthe forward formation equations.1 Computer graphics is a classic forward modeling problem\n(given some objects, cameras, and lighting, simulate the images that would result), while\ncomputer vision problems are usually of the inverse kind (given one or more images, recover\nthe scene that gave rise to these images).\nGiven an instance of an inverse problem, there are, in general, several ways to proceed.\nFor instance, through clever (or sometimes straightforward) algebraic manipulation, a closed\nform solution for the unknowns can sometimes be derived. Consider, for example, the camera\nmatrix calibration problem (Section 6.2.1): given an image of a calibration pattern consisting\nof known 3D point positions, compute the 3×4 camera matrix P that maps these points onto\nthe image plane.\nIn more detail, we can write this problem as (6.33–6.34)\nxi\n=\np00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(B.1)\nyi\n=\np10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n,\n(B.2)\nwhere (xi, yi) is the feature position of the ith point measured in the image plane, (Xi, Yi, Zi)\nis the corresponding 3D point position, and the pij are the unknown entries of the camera\nmatrix P . Moving the denominator over to the left hand side, we end up with a set of\nsimultaneous linear equations,\nxi(p20Xi + p21Yi + p22Zi + p23)\n=\np00Xi + p01Yi + p02Zi + p03,\n(B.3)\nyi(p20Xi + p21Yi + p22Zi + p23)\n=\np10Xi + p11Yi + p12Zi + p13,\n(B.4)\nwhich we can solve using linear least squares (Appendix A.2) to obtain an estimate of P .\nThe question then arises: is this set of equations the right ones to be solving? If the\nmeasurements are totally noise-free or we do not care about getting the best possible answer,\nthen the answer is yes. However, in general, we cannot be sure that we have a reasonable\nalgorithm unless we make a model of the likely sources of error and devise an algorithm that\nperforms as well as possible given these potential errors.\n1 In machine learning, these problems are called regression problems, because we are trying to estimate a contin-\nuous quantity from noisy inputs, as opposed to a discrete classiﬁcation task (Bishop 2006).\nB.1 Estimation theory\n757\nB.1 Estimation theory\nThe study of such inference problems from noisy data is often called estimation theory (Gelb\n1974), and its extension to problems where we explicitly choose a loss function is called sta-\ntistical decision theory (Berger 1993; Hastie, Tibshirani, and Friedman 2001; Bishop 2006;\nRobert 2007). We ﬁrst start by writing down the forward process that leads from our un-\nknowns (and knowns) to a set of noise-corrupted measurements. We then devise an algorithm\nthat will give us an estimate (or set of estimates) that are both insensitive to the noise (as best\nthey can be) and also quantify the reliability of these estimates.\nThe speciﬁc equations above (B.1) are just a particular instance of a more general set of\nmeasurement equations,\nyi = f i(x) + ni.\n(B.5)\nHere, the yi are the noise-corrupted measurements, e.g., (xi, yi) in Equation (B.1), and x is\nthe unknown state vector.2\nEach measurement comes with its associated measurement model f i(x), which maps the\nunknown into that particular measurement. An alternative formulation would be to have one\ngeneral function f(x, pi) and to use a per-measurement parameter vector pi to distinguish\nbetween different measurements, e.g., (Xi, Yi, Zi) in Equation (B.1). Note that the use of the\nf i(x) form makes it straightforward to have measurements of different dimensions, which\nbecomes useful when we start adding in prior information (Appendix B.4).\nEach measurement is also contaminated with some noise ni. In Equation (B.5), we have\nindicated that ni is a zero-mean normal (Gaussian) random variable with a covariance matrix\nΣi. In general, the noise need not be Gaussian and, in fact, it is usually prudent to assume\nthat some measurements may be outliers. However, we defer this discussion to Appendix B.3,\nafter we have explored the simpler Gaussian noise case more fully. We also assume that the\nnoise vectors ni are independent. In the case where they are not (e.g., when some constant\ngain or offset contaminates all of the pixels in a given image), we can add this effect as a\nnuisance parameter to our state vector x and later estimate its value (and discard it, if so\ndesired).\nB.1.1 Likelihood for multivariate Gaussian noise\nGiven all of the noisy measurements y = {yi}, we would like to infer a probability distribu-\ntion on the unknown x vector. We can write the likelihood of having observed the {yi} given\na particular value of x as\nL = p(y|x) =\nY\ni\np(yi|x) =\nY\ni\np(yi|f i(x)) =\nY\ni\np(ni).\n(B.6)\n2 In the Kalman ﬁltering literature (Gelb 1974), it is more common to use z instead of y to denote measurements.",
  "image_path": "page_778.jpg",
  "pages": [
    777,
    778,
    779
  ]
}