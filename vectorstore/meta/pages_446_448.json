{
  "doc_id": "pages_446_448",
  "text": "424\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Compute the translation (and, optionally, rotation) between successive frames with ro-\nbust outlier rejection.\n2. Perform temporal high-pass ﬁltering on the motion parameters to remove the low-\nfrequency component (smooth the motion).\n3. Compensate for the high-frequency motion, zooming in slightly (a user-speciﬁed amount)\nto avoid missing edge pixels.\n4. (Optional) Do not zoom in, but instead borrow pixels from previous or subsequent\nframes to ﬁll in.\n5. (Optional) Compensate for images that are blurry because of fast motion by “stealing”\nhigher frequencies from adjacent frames.\nEx 8.4: Optical ﬂow\nCompute optical ﬂow (spline-based or per-pixel) between two im-\nages, using one or more of the techniques described in this chapter.\n1. Test your algorithms on the motion sequences available at http://vision.middlebury.\nedu/ﬂow/ or http://people.csail.mit.edu/celiu/motionAnnotation/ and compare your re-\nsults (visually) to those available on these Web sites. If you think your algorithm is\ncompetitive with the best, consider submitting it for formal evaluation.\n2. Visualize the quality of your results by generating in-between images using frame in-\nterpolation (Exercise 8.5).\n3. What can you say about the relative efﬁciency (speed) of your approach?\nEx 8.5: Automated morphing / frame interpolation\nWrite a program to automatically morph\nbetween pairs of images. Implement the following steps, as sketched out in Section 8.5.1 and\nby Baker, Scharstein, Lewis et al. (2009):\n1. Compute the ﬂow both ways (previous exercise). Consider using a multi-frame (n > 2)\ntechnique to better deal with occluded regions.\n2. For each intermediate (morphed) image, compute a set of ﬂow vectors and which im-\nages should be used in the ﬁnal composition.\n3. Blend (cross-dissolve) the images and view with a sequence viewer.\nTry this out on images of your friends and colleagues and see what kinds of morphs you get.\nAlternatively, take a video sequence and do a high-quality slow-motion effect. Compare your\nalgorithm with simple cross-fading.\n8.7 Exercises\n425\nEx 8.6: Motion-based user interaction\nWrite a program to compute a low-resolution mo-\ntion ﬁeld in order to interactively control a simple application (Cutler and Turk 1998). For\nexample:\n1. Downsample each image using a pyramid and compute the optical ﬂow (spline-based\nor pixel-based) from the previous frame.\n2. Segment each training video sequence into different “actions” (e.g., hand moving in-\nwards, moving up, no motion) and “learn” the velocity ﬁelds associated with each one.\n(You can simply ﬁnd the mean and variance for each motion ﬁeld or use something\nmore sophisticated, such as a support vector machine (SVM).)\n3. Write a recognizer that ﬁnds successive actions of approximately the right duration and\nhook it up to an interactive application (e.g., a sound generator or a computer game).\n4. Ask your friends to test it out.\nEx 8.7: Video denoising\nImplement the algorithm sketched in Application 8.4.2. Your al-\ngorithm should contain the following steps:\n1. Compute accurate per-pixel ﬂow.\n2. Determine which pixels in the reference image have good matches with other frames.\n3. Either average all of the matched pixels or choose the sharpest image, if trying to\ncompensate for blur. Don’t forget to use regular single-frame denoising techniques as\npart of your solution, (see Section 3.4.4, Section 3.7.3, and Exercise 3.11).\n4. Devise a fall-back strategy for areas where you don’t think the ﬂow estimates are accu-\nrate enough.\nEx 8.8: Motion segmentation\nWrite a program to segment an image into separately mov-\ning regions or to reliably ﬁnd motion boundaries.\nUse the human-assisted motion segmentation database at http://people.csail.mit.edu/celiu/\nmotionAnnotation/ as some of your test data.\nEx 8.9: Layered motion estimation\nDecompose into separate layers (Section 8.5) a video\nsequence of a scene taken with a moving camera:\n1. Find the set of dominant (afﬁne or planar perspective) motions, either by computing\nthem in blocks or ﬁnding a robust estimate and then iteratively re-ﬁtting outliers.\n2. Determine which pixels go with each motion.\n426\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Construct the layers by blending pixels from different frames.\n4. (Optional) Add per-pixel residual ﬂows or depths.\n5. (Optional) Reﬁne your estimates using an iterative global optimization technique.\n6. (Optional) Write an interactive renderer to generate in-between frames or view the\nscene from different viewpoints (Shade, Gortler, He et al. 1998).\n7. (Optional) Construct an unwrap mosaic from a more complex scene and use this to do\nsome video editing (Rav-Acha, Kohli, Fitzgibbon et al. 2008).\nEx 8.10: Transparent motion and reﬂection estimation\nTake a video sequence looking\nthrough a window (or picture frame) and see if you can remove the reﬂection in order to\nbetter see what is inside.\nThe steps are described in Section 8.5.2 and by Szeliski, Avidan, and Anandan (2000).\nAlternative approaches can be found in work by Shizawa and Mase (1991), Bergen, Burt,\nHingorani et al. (1992), Darrell and Simoncelli (1993), Darrell and Pentland (1995), Irani,\nRousso, and Peleg (1994), Black and Anandan (1996), and Ju, Black, and Jepson (1996).",
  "image_path": "page_447.jpg",
  "pages": [
    446,
    447,
    448
  ]
}