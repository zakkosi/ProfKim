{
  "doc_id": "pages_800_802",
  "text": "778\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn this ﬁnal appendix, I summarize some of the supplementary materials that may be use-\nful to students, instructors, and researchers. The book’s Web site at http://szeliski.org/Book\ncontains updated lists of datasets and software, so please check there as well.\nC.1 Data sets\nOne of the keys to developing reliable vision algorithms is to test your procedures on chal-\nlenging and representative data sets. When ground truth or other people’s results are available,\nsuch test can be even more informative (and quantitative).\nOver the years, a large number of datasets have been developed for testing and evaluating\ncomputer vision algorithms. A number of these datasets (and software) are indexed on the\nComputer Vision Homepage.1 Some newer Web sites, such as CVonline (http://homepages.\ninf.ed.ac.uk/rbf/CVonline/), VisionBib.Com (http://datasets.visionbib.com/), and Computer\nVision online (http://computervisiononline.com/), have more recent pointers.\nBelow, I list some of the more popular data sets, grouped by the book chapters to which\nthey most closely correspond:\nChapter 2: Image formation\nCUReT: Columbia-Utrecht Reﬂectance and Texture Database, http://www1.cs.columbia.\nedu/CAVE/software/curet/ (Dana, van Ginneken, Nayar et al. 1999).\nMiddlebury Color Datasets: registered color images taken by different cameras to\nstudy how they transform gamuts and colors, http://vision.middlebury.edu/color/data/\n(Chakrabarti, Scharstein, and Zickler 2009).\nChapter 3: Image processing\nMiddlebury test datasets for evaluating MRF minimization/inference algorithms, http:\n//vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharstein et al. 2008).\nChapter 4: Feature detection and matching\nAfﬁne Covariant Features database for evaluating feature detector and descriptor match-\ning quality and repeatability, http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/ (Miko-\nlajczyk and Schmid 2005; Mikolajczyk, Tuytelaars, Schmid et al. 2005).\nDatabase of matched image patches for learning and feature descriptor evaluation,\nhttp://cvlab.epﬂ.ch/∼brown/patchdata/patchdata.html (Winder and Brown 2007; Hua,\nBrown, and Winder 2007).\n1 http://www.cs.cmu.edu/∼cil/vision.html, although it has not been maintained since 2004.\nC.1 Data sets\n779\nChapter 5: Segmentation\nBerkeley Segmentation Dataset and Benchmark of 1000 images labeled by 30 humans,\nalong with an evaluation, http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\ngrouping/segbench/ (Martin, Fowlkes, Tal et al. 2001).\nWeizmann segmentation evaluation database of 100 grayscale images with ground\ntruth segmentations, http://www.wisdom.weizmann.ac.il/∼vision/Seg Evaluation DB/\nindex.html (Alpert, Galun, Basri et al. 2007).\nChapter 8: Dense motion estimation\nThe Middlebury optic ﬂow evaluation Web site, http://vision.middlebury.edu/ﬂow/data\n(Baker, Scharstein, Lewis et al. 2009).\nThe Human-Assisted Motion Annotation database,\nhttp://people.csail.mit.edu/celiu/motionAnnotation/ (Liu, Freeman, Adelson et al. 2008)\nChapter 10: Computational photography\nHigh Dynamic Range radiance maps, http://www.debevec.org/Research/HDR/ (De-\nbevec and Malik 1997).\nAlpha matting evaluation Web site, http://alphamatting.com/ (Rhemann, Rother, Wang\net al. 2009).\nChapter 11: Stereo correspondence\nMiddlebury Stereo Datasets and Evaluation, http://vision.middlebury.edu/stereo/ (Scharstein\nand Szeliski 2002).\nStereo Classiﬁcation and Performance Evaluation of different aggregation costs for\nstereo matching, http://www.vision.deis.unibo.it/spe/SPEHome.aspx (Tombari, Mat-\ntoccia, Di Stefano et al. 2008).\nMiddlebury Multi-View Stereo Datasets, http://vision.middlebury.edu/mview/data/ (Seitz,\nCurless, Diebel et al. 2006).\nMulti-view and Oxford Colleges building reconstructions, http://www.robots.ox.ac.uk/\n∼vgg/data/data-mview.html.\nMulti-View Stereo Datasets, http://cvlab.epﬂ.ch/data/strechamvs/ (Strecha, Fransens,\nand Van Gool 2006).\n780\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMulti-View Evaluation, http://cvlab.epﬂ.ch/∼strecha/multiview/ (Strecha, von Hansen,\nVan Gool et al. 2008).\nChapter 12: 3D reconstruction\nHumanEva: synchronized video and motion capture dataset for evaluation of artic-\nulated human motion, http://vision.cs.brown.edu/humaneva/ (Sigal, Balan, and Black\n2010).\nChapter 13: Image-based rendering\nThe (New) Stanford Light Field Archive, http://lightﬁeld.stanford.edu/ (Wilburn, Joshi,\nVaish et al. 2005).\nVirtual Viewpoint Video: multi-viewpoint video with per-frame depth maps, http:\n//research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/ (Zitnick, Kang, Uytten-\ndaele et al. 2004).\nChapter 14: Recognition\nFor a list of visual recognition datasets, see Tables 14.1–14.2. In addition to those,\nthere are also:\nBuffy pose classes, http://www.robots.ox.ac.uk/∼vgg/data/buffy pose classes/ and Buffy\nstickmen V2.1, http://www.robots.ox.ac.uk/∼vgg/data/stickmen/index.html (Ferrari, Marin-\nJimenez, and Zisserman 2009; Eichner and Ferrari 2009).\nH3D database of pose/joint annotated photographs of humans, http://www.eecs.berkeley.\nedu/∼lbourdev/h3d/ (Bourdev and Malik 2009).\nAction Recognition Datasets, http://www.cs.berkeley.edu/projects/vision/action, has point-\ners to several datasets for action and activity recognition, as well as some papers. The\nhuman action database at http://www.nada.kth.se/cvap/actions/ contains more action\nsequences.\nC.2 Software\nOne of the best sources for computer vision algorithms is the Open Source Computer Vision\n(OpenCV) library (http://opencv.willowgarage.com/wiki/), which was developed by Gary\nBradski and his colleagues at Intel and is now being maintained and extended at Willow\nGarage (Bradsky and Kaehler 2008). A partial list of the available functions, taken from\nhttp://opencv.willowgarage.com/documentation/cpp/ includes:",
  "image_path": "page_801.jpg",
  "pages": [
    800,
    801,
    802
  ]
}