{
  "doc_id": "pages_656_658",
  "text": "634\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe decomposition into a diffuse and specular component can also be used to perform\nediting or manipulation operations, such as re-painting the surface, changing the specular\ncomponent of the reﬂection (e.g., by blurring or sharpening the specular Lumispheres), or\neven geometrically deforming the object while preserving detailed surface appearance.\n13.3.3 Application: Concentric mosaics\nA useful and simple version of light ﬁeld rendering is a panoramic image with parallax, i.e., a\nvideo or series of photographs taken from a camera swinging in front of some rotation point.\nSuch panoramas can be captured by placing a camera on a boom on a tripod, or even more\nsimply, by holding a camera at arm’s length while rotating your body around a ﬁxed axis.\nThe resulting set of images can be thought of as a concentric mosaic (Shum and He 1999;\nShum, Wang, Chai et al. 2002) or a layered depth panorama (Zheng, Kang, Cohen et al.\n2007). The term “concentric mosaic” comes from a particular structure that can be used to\nre-bin all of the sampled rays, essentially associating each column of pixels with the “radius”\nof the concentric circle to which it is tangent (Shum and He 1999; Peleg, Ben-Ezra, and Pritch\n2001).\nRendering from such data structures is fast and straightforward. If we assume that the\nscene is far enough away, for any virtual camera location, we can associate each column of\npixels in the virtual camera with the nearest column of pixels in the input image set. (For\na regularly captured set of images, this computation can be performed analytically.) If we\nhave some rough knowledge of the depth of such pixels, columns can be stretched vertically\nto compensate for the change in depth between the two cameras. If we have an even more\ndetailed depth map (Peleg, Ben-Ezra, and Pritch 2001; Li, Shum, Tang et al. 2004; Zheng,\nKang, Cohen et al. 2007), we can perform pixel-by-pixel depth corrections.\nWhile the virtual camera’s motion is constrained to lie in the plane of the original cameras\nand within the radius of the original capture ring, the resulting experience can exhibit complex\nrendering phenomena, such as reﬂections and translucencies, which cannot be captured using\na texture-mapped 3D model of the world. Exercise 13.10 has you construct a concentric\nmosaic rendering system from a series of hand-held photos or video.\n13.4 Environment mattes\nSo far in this chapter, we have dealt with view interpolation and light ﬁelds, which are tech-\nniques for modeling and rendering complex static scenes seen from different viewpoints.\nWhat if instead of moving around a virtual camera, we take a complex, refractive object,\nsuch as the water goblet shown in Figure 13.10, and place it in front of a new background?\n13.4 Environment mattes\n635\n(a)\n(b)\n(c)\n(d)\nFigure 13.10 Environment mattes: (a–b) a refractive object can be placed in front of a series\nof backgrounds and their light patterns will be correctly refracted (Zongker, Werner, Cur-\nless et al. 1999) (c) multiple refractions can be handled using a mixture of Gaussians model\nand (d) real-time mattes can be pulled using a single graded colored background (Chuang,\nZongker, Hindorff et al. 2000) c⃝2000 ACM.\nInstead of modeling the 4D space of rays emanating from a scene, we now need to model\nhow each pixel in our view of this object refracts incident light coming from its environment.\nWhat is the intrinsic dimensionality of such a representation and how do we go about\ncapturing it? Let us assume that if we trace a light ray from the camera at pixel (x, y) toward\nthe object, it is reﬂected or refracted back out toward its environment at an angle (φ, θ). If\nwe assume that other objects and illuminants are sufﬁciently distant (the same assumption we\nmade for surface light ﬁelds in Section 13.3.2), this 4D mapping (x, y) →(φ, θ) captures all\nthe information between a refractive object and its environment. Zongker, Werner, Curless et\nal. (1999) call such a representation an environment matte, since it generalizes the process of\nobject matting (Section 10.4) to not only cut and paste an object from one image into another\nbut also take into account the subtle refractive or reﬂective interplay between the object and\nits environment.\nRecall from Equations (3.8) and (10.30) that a foreground object can be represented by\nits premultiplied colors and opacities (αF, α). Such a matte can then be composited onto a\nnew background B using\nCi = αiFi + (1 −αi)Bi,\n(13.1)\nwhere i is the pixel under consideration. In environment matting, we augment this equation\nwith a reﬂective or refractive term to model indirect light paths between the environment\nand the camera. In the original work of Zongker, Werner, Curless et al. (1999), this indirect\ncomponent Ii is modeled as\nIi = Ri\nZ\nAi(x)B(x)dx,\n(13.2)\nwhere Ai is the rectangular area of support for that pixel, Ri is the colored reﬂectance or\n636\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntransmittance (for colored glossy surfaces or glass), and B(x) is the background (environ-\nment) image, which is integrated over the area Ai(x). In follow-on work, Chuang, Zongker,\nHindorff et al. (2000) use a superposition of oriented Gaussians,\nIi =\nX\nj\nRij\nZ\nGij(x)B(x)dx,\n(13.3)\nwhere each 2D Gaussian\nGij(x) = G2D(x; cij, σij, θij)\n(13.4)\nis modeled by its center cij, unrotated widths σij = (σx\nij, σy\nij), and orientation θij.\nGiven a representation for an environment matte, how can we go about estimating it for a\nparticular object? The trick is to place the object in front of a monitor (or surrounded by a set\nof monitors), where we can change the illumination patterns B(x) and observe the value of\neach composite pixel Ci.7\nAs with traditional two-screen matting (Section 10.4.1), we can use a variety of solid\ncolored backgrounds to estimate each pixel’s foreground color αiFi and partial coverage\n(opacity) αi. To estimate the area of support Ai in (13.2), Zongker, Werner, Curless et al.\n(1999) use a series of periodic horizontal and vertical solid stripes at different frequencies and\nphases, which is reminiscent of the structured light patterns used in active rangeﬁnding (Sec-\ntion 12.2). For the more sophisticated mixture of Gaussian model (13.3), Chuang, Zongker,\nHindorff et al. (2000) sweep a series of narrow Gaussian stripes at four different orientations\n(horizontal, vertical, and two diagonals), which enables them to estimate multiple oriented\nGaussian responses at each pixel.\nOnce an environment matte has been “pulled”, it is then a simple matter to replace the\nbackground with a new image B(x) to obtain a novel composite of the object placed in a\ndifferent environment (Figure 13.10a–c). The use of multiple backgrounds during the matting\nprocess, however, precludes the use of this technique with dynamic scenes, e.g., water pouring\ninto a glass (Figure 13.10d). In this case, a single graded color background can be used to\nestimate a single 2D monochromatic displacement for each pixel (Chuang, Zongker, Hindorff\net al. 2000).\n13.4.1 Higher-dimensional light ﬁelds\nAs you can tell from the preceding discussion, an environment matte in principle maps every\npixel (x, y) into a 4D distribution over light rays and is, hence, a six-dimensional representa-\ntion. (In practice, each 2D pixel’s response is parameterized using a dozen or so parameters,\n7 If we relax the assumption that the environment is distant, the monitor can be placed at several depths to estimate\na depth-dependent mapping function (Zongker, Werner, Curless et al. 1999).",
  "image_path": "page_657.jpg",
  "pages": [
    656,
    657,
    658
  ]
}