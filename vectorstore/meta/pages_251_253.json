{
  "doc_id": "pages_251_253",
  "text": "4.1 Points and patches\n229\nfalse positive rate\ntrue positive rate\n0.1\n0.8\n0\n1\n1\nequal error\n \nrate\nrandom chance\nTP\nFP FN\nTN\nθ\nd\n#\n(a)\n(b)\nFigure 4.23 ROC curve and its related rates: (a) The ROC curve plots the true positive rate\nagainst the false positive rate for a particular combination of feature extraction and match-\ning algorithms. Ideally, the true positive rate should be close to 1, while the false positive\nrate is close to 0. The area under the ROC curve (AUC) is often used as a single (scalar)\nmeasure of algorithm performance. Alternatively, the equal error rate is sometimes used. (b)\nThe distribution of positives (matches) and negatives (non-matches) as a function of inter-\nfeature distance d. As the threshold θ is increased, the number of true positives (TP) and false\npositives (FP) increases.\nIn the information retrieval (or document retrieval) literature (Baeza-Yates and Ribeiro-\nNeto 1999; Manning, Raghavan, and Sch¨utze 2008), the term precision (how many returned\ndocuments are relevant) is used instead of PPV and recall (what fraction of relevant docu-\nments was found) is used instead of TPR.\nAny particular matching strategy (at a particular threshold or parameter setting) can be\nrated by the TPR and FPR numbers; ideally, the true positive rate will be close to 1 and the\nfalse positive rate close to 0. As we vary the matching threshold, we obtain a family of such\npoints, which are collectively known as the receiver operating characteristic (ROC curve)\n(Fawcett 2006) (Figure 4.23a). The closer this curve lies to the upper left corner, i.e., the\nlarger the area under the curve (AUC), the better its performance. Figure 4.23b shows how\nwe can plot the number of matches and non-matches as a function of inter-feature distance d.\nThese curves can then be used to plot an ROC curve (Exercise 4.3). The ROC curve can also\nbe used to calculate the mean average precision, which is the average precision (PPV) as you\nvary the threshold to select the best results, then the two top results, etc.\nThe problem with using a ﬁxed threshold is that it is difﬁcult to set; the useful range\n230\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nDB\nDA\nd1\nDD\nDC\nd2\nDE\nd1\n \n’\nd2\n \n’\nFigure 4.24 Fixed threshold, nearest neighbor, and nearest neighbor distance ratio matching.\nAt a ﬁxed distance threshold (dashed circles), descriptor DA fails to match DB and DD\nincorrectly matches DC and DE. If we pick the nearest neighbor, DA correctly matches DB\nbut DD incorrectly matches DC. Using nearest neighbor distance ratio (NNDR) matching,\nthe small NNDR d1/d2 correctly matches DA with DB, and the large NNDR d′\n1/d′\n2 correctly\nrejects matches for DD.\nof thresholds can vary a lot as we move to different parts of the feature space (Lowe 2004;\nMikolajczyk and Schmid 2005). A better strategy in such cases is to simply match the nearest\nneighbor in feature space. Since some features may have no matches (e.g., they may be part\nof background clutter in object recognition or they may be occluded in the other image), a\nthreshold is still used to reduce the number of false positives.\nIdeally, this threshold itself will adapt to different regions of the feature space. If sufﬁcient\ntraining data is available (Hua, Brown, and Winder 2007), it is sometimes possible to learn\ndifferent thresholds for different features. Often, however, we are simply given a collection\nof images to match, e.g., when stitching images or constructing 3D models from unordered\nphoto collections (Brown and Lowe 2007, 2003; Snavely, Seitz, and Szeliski 2006). In this\ncase, a useful heuristic can be to compare the nearest neighbor distance to that of the second\nnearest neighbor, preferably taken from an image that is known not to match the target (e.g.,\na different object in the database) (Brown and Lowe 2002; Lowe 2004). We can deﬁne this\nnearest neighbor distance ratio (Mikolajczyk and Schmid 2005) as\nNNDR = d1\nd2\n= ∥DA −DB|\n∥DA −DC|,\n(4.18)\nwhere d1 and d2 are the nearest and second nearest neighbor distances, DA is the target\ndescriptor, and DB and DC are its closest two neighbors (Figure 4.24).\nThe effects of using these three different matching strategies for the feature descriptors\nevaluated by Mikolajczyk and Schmid (2005) are shown in Figure 4.25. As you can see, the\nnearest neighbor and NNDR strategies produce improved ROC curves.\n4.1 Points and patches\n231\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 3708\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(b)\n(c)\nFigure 4.25\nPerformance of the feature descriptors evaluated by Mikolajczyk and Schmid\n(2005) c⃝2005 IEEE, shown for three matching strategies: (a) ﬁxed threshold; (b) nearest\nneighbor; (c) nearest neighbor distance ratio (NNDR). Note how the ordering of the algo-\nrithms does not change that much, but the overall performance varies signiﬁcantly between\nthe different matching strategies.",
  "image_path": "page_252.jpg",
  "pages": [
    251,
    252,
    253
  ]
}