{
  "doc_id": "pages_236_238",
  "text": "214\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.7\nIsocontours of popular keypoint detection functions (Brown, Szeliski, and\nWinder 2004).\nEach detector looks for points where the eigenvalues λ0, λ1 of A =\nw ∗∇I∇IT are both large.\n1. Compute the horizontal and vertical derivatives of the image Ix and Iy by con-\nvolving the original image with derivatives of Gaussians (Section 3.2.3).\n2. Compute the three images corresponding to the outer products of these gradients.\n(The matrix A is symmetric, so only three entries are needed.)\n3. Convolve each of these images with a larger Gaussian.\n4. Compute a scalar interest measure using one of the formulas discussed above.\n5. Find local maxima above a certain threshold and report them as detected feature\npoint locations.\nAlgorithm 4.1 Outline of a basic feature detection algorithm.\n4.1 Points and patches\n215\n(a)\n(b)\n(c)\nFigure 4.8 Interest operator responses: (a) Sample image, (b) Harris response, and (c) DoG\nresponse. The circle sizes and colors indicate the scale at which each interest point was\ndetected. Notice how the two detectors tend to respond at complementary locations.\nThe steps in the basic auto-correlation-based keypoint detector are summarized in Algo-\nrithm 4.1. Figure 4.8 shows the resulting interest operator responses for the classic Harris\ndetector as well as the difference of Gaussian (DoG) detector discussed below.\nAdaptive non-maximal suppression (ANMS).\nWhile most feature detectors simply look\nfor local maxima in the interest function, this can lead to an uneven distribution of feature\npoints across the image, e.g., points will be denser in regions of higher contrast. To mitigate\nthis problem, Brown, Szeliski, and Winder (2005) only detect features that are both local\nmaxima and whose response value is signiﬁcantly (10%) greater than that of all of its neigh-\nbors within a radius r (Figure 4.9c–d). They devise an efﬁcient way to associate suppression\nradii with all local maxima by ﬁrst sorting them by their response strength and then creating\na second list sorted by decreasing suppression radius (Brown, Szeliski, and Winder 2005).\nFigure 4.9 shows a qualitative comparison of selecting the top n features and using ANMS.\nMeasuring repeatability.\nGiven the large number of feature detectors that have been de-\nveloped in computer vision, how can we decide which ones to use? Schmid, Mohr, and\nBauckhage (2000) were the ﬁrst to propose measuring the repeatability of feature detectors,\nwhich they deﬁne as the frequency with which keypoints detected in one image are found\nwithin ϵ (say, ϵ = 1.5) pixels of the corresponding location in a transformed image. In their\npaper, they transform their planar images by applying rotations, scale changes, illumination\nchanges, viewpoint changes, and adding noise. They also measure the information content\navailable at each detected feature point, which they deﬁne as the entropy of a set of rotation-\nally invariant local grayscale descriptors. Among the techniques they survey, they ﬁnd that\nthe improved (Gaussian derivative) version of the Harris operator with σd = 1 (scale of the\nderivative Gaussian) and σi = 2 (scale of the integration Gaussian) works best.\n216\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a) Strongest 250\n(b) Strongest 500\n(c) ANMS 250, r = 24\n(d) ANMS 500, r = 16\nFigure 4.9\nAdaptive non-maximal suppression (ANMS) (Brown, Szeliski, and Winder\n2005) c⃝2005 IEEE: The upper two images show the strongest 250 and 500 interest points,\nwhile the lower two images show the interest points selected with adaptive non-maximal sup-\npression, along with the corresponding suppression radius r. Note how the latter features\nhave a much more uniform spatial distribution across the image.\nScale invariance\nIn many situations, detecting features at the ﬁnest stable scale possible may not be appro-\npriate. For example, when matching images with little high frequency detail (e.g., clouds),\nﬁne-scale features may not exist.\nOne solution to the problem is to extract features at a variety of scales, e.g., by performing\nthe same operations at multiple resolutions in a pyramid and then matching features at the\nsame level. This kind of approach is suitable when the images being matched do not undergo\nlarge scale changes, e.g., when matching successive aerial images taken from an airplane or\nstitching panoramas taken with a ﬁxed-focal-length camera. Figure 4.10 shows the output of\none such approach, the multi-scale, oriented patch detector of Brown, Szeliski, and Winder\n(2005), for which responses at ﬁve different scales are shown.\nHowever, for most object recognition applications, the scale of the object in the image",
  "image_path": "page_237.jpg",
  "pages": [
    236,
    237,
    238
  ]
}