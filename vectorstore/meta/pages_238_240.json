{
  "doc_id": "pages_238_240",
  "text": "216\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a) Strongest 250\n(b) Strongest 500\n(c) ANMS 250, r = 24\n(d) ANMS 500, r = 16\nFigure 4.9\nAdaptive non-maximal suppression (ANMS) (Brown, Szeliski, and Winder\n2005) c⃝2005 IEEE: The upper two images show the strongest 250 and 500 interest points,\nwhile the lower two images show the interest points selected with adaptive non-maximal sup-\npression, along with the corresponding suppression radius r. Note how the latter features\nhave a much more uniform spatial distribution across the image.\nScale invariance\nIn many situations, detecting features at the ﬁnest stable scale possible may not be appro-\npriate. For example, when matching images with little high frequency detail (e.g., clouds),\nﬁne-scale features may not exist.\nOne solution to the problem is to extract features at a variety of scales, e.g., by performing\nthe same operations at multiple resolutions in a pyramid and then matching features at the\nsame level. This kind of approach is suitable when the images being matched do not undergo\nlarge scale changes, e.g., when matching successive aerial images taken from an airplane or\nstitching panoramas taken with a ﬁxed-focal-length camera. Figure 4.10 shows the output of\none such approach, the multi-scale, oriented patch detector of Brown, Szeliski, and Winder\n(2005), for which responses at ﬁve different scales are shown.\nHowever, for most object recognition applications, the scale of the object in the image\n4.1 Points and patches\n217\nFigure 4.10\nMulti-scale oriented patches (MOPS) extracted at ﬁve pyramid levels (Brown,\nSzeliski, and Winder 2005) c⃝2005 IEEE. The boxes show the feature orientation and the\nregion from which the descriptor vectors are sampled.\nis unknown. Instead of extracting features at many different scales and then matching all of\nthem, it is more efﬁcient to extract features that are stable in both location and scale (Lowe\n2004; Mikolajczyk and Schmid 2004).\nEarly investigations into scale selection were performed by Lindeberg (1993; 1998b),\nwho ﬁrst proposed using extrema in the Laplacian of Gaussian (LoG) function as interest\npoint locations. Based on this work, Lowe (2004) proposed computing a set of sub-octave\nDifference of Gaussian ﬁlters (Figure 4.11a), looking for 3D (space+scale) maxima in the re-\nsulting structure (Figure 4.11b), and then computing a sub-pixel space+scale location using a\nquadratic ﬁt (Brown and Lowe 2002). The number of sub-octave levels was determined, after\ncareful empirical investigation, to be three, which corresponds to a quarter-octave pyramid,\nwhich is the same as used by Triggs (2004).\nAs with the Harris operator, pixels where there is strong asymmetry in the local curvature\nof the indicator function (in this case, the DoG) are rejected. This is implemented by ﬁrst\ncomputing the local Hessian of the difference image D,\nH =\n\"\nDxx\nDxy\nDxy\nDyy\n#\n,\n(4.12)\nand then rejecting keypoints for which\nTr(H)2\nDet(H) > 10.\n(4.13)\n218\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n Scale\n (first\n octave)\nScale\n(next\noctave)\nGaussian\nDifference of\nGaussian (DOG)\n. . .\nScale\n(a)\n(b)\nFigure 4.11 Scale-space feature detection using a sub-octave Difference of Gaussian pyra-\nmid (Lowe 2004) c⃝2004 Springer: (a) Adjacent levels of a sub-octave Gaussian pyramid\nare subtracted to produce Difference of Gaussian images; (b) extrema (maxima and minima)\nin the resulting 3D volume are detected by comparing a pixel to its 26 neighbors.\nWhile Lowe’s Scale Invariant Feature Transform (SIFT) performs well in practice, it is not\nbased on the same theoretical foundation of maximum spatial stability as the auto-correlation-\nbased detectors. (In fact, its detection locations are often complementary to those produced\nby such techniques and can therefore be used in conjunction with these other approaches.)\nIn order to add a scale selection mechanism to the Harris corner detector, Mikolajczyk and\nSchmid (2004) evaluate the Laplacian of Gaussian function at each detected Harris point (in\na multi-scale pyramid) and keep only those points for which the Laplacian is extremal (larger\nor smaller than both its coarser and ﬁner-level values). An optional iterative reﬁnement for\nboth scale and position is also proposed and evaluated. Additional examples of scale invariant\nregion detectors are discussed by Mikolajczyk, Tuytelaars, Schmid et al. (2005); Tuytelaars\nand Mikolajczyk (2007).\nRotational invariance and orientation estimation\nIn addition to dealing with scale changes, most image matching and object recognition algo-\nrithms need to deal with (at least) in-plane image rotation. One way to deal with this problem\nis to design descriptors that are rotationally invariant (Schmid and Mohr 1997), but such\ndescriptors have poor discriminability, i.e. they map different looking patches to the same\ndescriptor.",
  "image_path": "page_239.jpg",
  "pages": [
    238,
    239,
    240
  ]
}