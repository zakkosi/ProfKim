{
  "doc_id": "pages_248_250",
  "text": "226\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 4.20 Spatial summation blocks for SIFT, GLOH, and some newly developed feature\ndescriptors (Winder and Brown 2007) c⃝2007 IEEE: (a) The parameters for the new features,\ne.g., their Gaussian weights, are learned from a training database of (b) matched real-world\nimage patches obtained from robust structure from motion applied to Internet photo collec-\ntions (Hua, Brown, and Winder 2007).\nMatching strategy and error rates\nDetermining which feature matches are reasonable to process further depends on the context\nin which the matching is being performed. Say we are given two images that overlap to a fair\namount (e.g., for image stitching, as in Figure 4.16, or for tracking objects in a video). We\nknow that most features in one image are likely to match the other image, although some may\nnot match because they are occluded or their appearance has changed too much.\nOn the other hand, if we are trying to recognize how many known objects appear in a clut-\ntered scene (Figure 4.21), most of the features may not match. Furthermore, a large number\nof potentially matching objects must be searched, which requires more efﬁcient strategies, as\ndescribed below.\nTo begin with, we assume that the feature descriptors have been designed so that Eu-\nclidean (vector magnitude) distances in feature space can be directly used for ranking poten-\ntial matches. If it turns out that certain parameters (axes) in a descriptor are more reliable\nthan others, it is usually preferable to re-scale these axes ahead of time, e.g., by determin-\ning how much they vary when compared against other known good matches (Hua, Brown,\nand Winder 2007). A more general process, which involves transforming feature vectors\ninto a new scaled basis, is called whitening and is discussed in more detail in the context of\neigenface-based face recognition (Section 14.2.1).\nGiven a Euclidean distance metric, the simplest matching strategy is to set a threshold\n(maximum distance) and to return all matches from other images within this threshold. Set-\nting the threshold too high results in too many false positives, i.e., incorrect matches being\nreturned. Setting the threshold too low results in too many false negatives, i.e., too many\ncorrect matches being missed (Figure 4.22).\nWe can quantify the performance of a matching algorithm at a particular threshold by\n4.1 Points and patches\n227\nFigure 4.21 Recognizing objects in a cluttered scene (Lowe 2004) c⃝2004 Springer. Two of\nthe training images in the database are shown on the left. These are matched to the cluttered\nscene in the middle using SIFT features, shown as small squares in the right image. The afﬁne\nwarp of each recognized database image onto the scene is shown as a larger parallelogram in\nthe right image.\n1\n1\n2\n1\n3\n4\nFigure 4.22\nFalse positives and negatives: The black digits 1 and 2 are features being\nmatched against a database of features in other images. At the current threshold setting (the\nsolid circles), the green 1 is a true positive (good match), the blue 1 is a false negative (failure\nto match), and the red 3 is a false positive (incorrect match). If we set the threshold higher\n(the dashed circles), the blue 1 becomes a true positive but the brown 4 becomes an additional\nfalse positive.\n228\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPredicted matches\nTP = 18\nFP = 4\nP' = 22\nPPV = 0.82\nPredicted non-matches\nFN = 2\nTN = 76\nN' = 78\nP = 20\nN = 80\nTotal = 100\nTPR = 0.90\nFPR = 0.05\nACC = 0.94\nTrue matches\nTrue non-matches\nTable 4.1 The number of matches correctly and incorrectly estimated by a feature matching\nalgorithm, showing the number of true positives (TP), false positives (FP), false negatives\n(FN) and true negatives (TN). The columns sum up to the actual number of positives (P) and\nnegatives (N), while the rows sum up to the predicted number of positives (P’) and negatives\n(N’). The formulas for the true positive rate (TPR), the false positive rate (FPR), the positive\npredictive value (PPV), and the accuracy (ACC) are given in the text.\nﬁrst counting the number of true and false matches and match failures, using the following\ndeﬁnitions (Fawcett 2006):\n• TP: true positives, i.e., number of correct matches;\n• FN: false negatives, matches that were not correctly detected;\n• FP: false positives, proposed matches that are incorrect;\n• TN: true negatives, non-matches that were correctly rejected.\nTable 4.1 shows a sample confusion matrix (contingency table) containing such numbers.\nWe can convert these numbers into unit rates by deﬁning the following quantities (Fawcett\n2006):\n• true positive rate (TPR),\nTPR =\nTP\nTP+FN = TP\nP ;\n(4.14)\n• false positive rate (FPR),\nFPR =\nFP\nFP+TN = FP\nN ;\n(4.15)\n• positive predictive value (PPV),\nPPV =\nTP\nTP+FP = TP\nP’ ;\n(4.16)\n• accuracy (ACC),\nACC = TP+TN\nP+N .\n(4.17)",
  "image_path": "page_249.jpg",
  "pages": [
    248,
    249,
    250
  ]
}