{
  "doc_id": "pages_681_683",
  "text": "14.1 Object detection\n659\nFigure 14.2\nFace detection results produced by Rowley, Baluja, and Kanade (1998a) c⃝\n1998 IEEE. Can you ﬁnd the one false positive (a box around a non-face) among the 57 true\npositive results?\nOver the years, a wide variety of fast face detection algorithms have been developed.\nYang, Kriegman, and Ahuja (2002) provide a comprehensive survey of earlier work in this\nﬁeld; Yang’s ICPR 2004 tutorial2 and the Torralba (2007) short course provide more recent\nreviews.3\nAccording to the taxonomy of Yang, Kriegman, and Ahuja (2002), face detection tech-\nniques can be classiﬁed as feature-based, template-based, or appearance-based. Feature-\nbased techniques attempt to ﬁnd the locations of distinctive image features such as the eyes,\nnose, and mouth, and then verify whether these features are in a plausible geometrical ar-\nrangement. These techniques include some of the early approaches to face recognition (Fis-\nchler and Elschlager 1973; Kanade 1977; Yuille 1991), as well as more recent approaches\nbased on modular eigenspaces (Moghaddam and Pentland 1997), local ﬁlter jets (Leung,\nBurl, and Perona 1995; Penev and Atick 1996; Wiskott, Fellous, Kr¨uger et al. 1997), support\nvector machines (Heisele, Ho, Wu et al. 2003; Heisele, Serre, and Poggio 2007), and boosting\n(Schneiderman and Kanade 2004).\nTemplate-based approaches, such as active appearance models (AAMs) (Section 14.2.2),\ncan deal with a wide range of pose and expression variability. Typically, they require good\ninitialization near a real face and are therefore not suitable as fast face detectors.\n2 http://vision.ai.uiuc.edu/mhyang/face-detection-survey.html.\n3 An alternative approach to detecting faces is to look for regions of skin color in the image (Forsyth and Fleck\n1999; Jones and Rehg 2001). See Exercise 2.8 for some additional discussion and references.\n660\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 14.3\nPre-processing stages for face detector training (Rowley, Baluja, and Kanade\n1998a) c⃝1998 IEEE: (a) artiﬁcially mirroring, rotating, scaling, and translating training\nimages for greater variability; (b) using images without faces (looking up at a tree) to generate\nnon-face examples; (c) pre-processing the patches by subtracting a best ﬁt linear function\n(constant gradient) and histogram equalizing.\nAppearance-based approaches scan over small overlapping rectangular patches of the im-\nage searching for likely face candidates, which can then be reﬁned using a cascade of more\nexpensive but selective detection algorithms (Sung and Poggio 1998; Rowley, Baluja, and\nKanade 1998a; Romdhani, Torr, Sch¨olkopf et al. 2001; Fleuret and Geman 2001; Viola and\nJones 2004). In order to deal with scale variation, the image is usually converted into a\nsub-octave pyramid and a separate scan is performed on each level. Most appearance-based\napproaches today rely heavily on training classiﬁers using sets of labeled face and non-face\npatches.\nSung and Poggio (1998) and Rowley, Baluja, and Kanade (1998a) present two of the ear-\nliest appearance-based face detectors and introduce a number of innovations that are widely\nused in later work by others.\nTo start with, both systems collect a set of labeled face patches (Figure 14.2) as well as a\nset of patches taken from images that are known not to contain faces, such as aerial images or\nvegetation (Figure 14.3b). The collected face images are augmented by artiﬁcially mirroring,\nrotating, scaling, and translating the images by small amounts to make the face detectors less\nsensitive to such effects (Figure 14.3a).\nAfter an initial set of training images has been collected, some optional pre-processing\ncan be performed, such as subtracting an average gradient (linear function) from the image\nto compensate for global shading effects and using histogram equalization to compensate for\nvarying camera contrast (Figure 14.3c).\n14.1 Object detection\n661\nFigure 14.4\nLearning a mixture of Gaussians model for face detection (Sung and Poggio\n1998) c⃝1998 IEEE. The face and non-face images (192-long vectors) are ﬁrst clustered into\nsix separate clusters (each) using k-means and then analyzed using PCA. The cluster centers\nare shown in the right-hand columns.\nClustering and PCA.\nOnce the face and non-face patterns have been pre-processed, Sung\nand Poggio (1998) cluster each of these datasets into six separate clusters using k-means\nand then ﬁt PCA subspaces to each of the resulting 12 clusters (Figure 14.4). At detection\ntime, the DIFS and DFFS metrics ﬁrst developed by Moghaddam and Pentland (1997) (see\nFigure 14.14 and (14.14)) are used to produce 24 Mahalanobis distance measurements (two\nper cluster). The resulting 24 measurements are input to a multi-layer perceptron (MLP),\nwhich is a neural network with alternating layers of weighted summations and sigmoidal non-\nlinearities trained using the “backpropagation” algorithm (Rumelhart, Hinton, and Williams\n1986).\nNeural networks.\nInstead of ﬁrst clustering the data and computing Mahalanobis distances\nto the cluster centers, Rowley, Baluja, and Kanade (1998a) apply a neural network (MLP) di-\nrectly to the 20×20 pixel patches of gray-level intensities, using a variety of differently sized\nhand-crafted “receptive ﬁelds” to capture both large-scale and smaller scale structure (Fig-\nure 14.5). The resulting neural network directly outputs the likelihood of a face at the center",
  "image_path": "page_682.jpg",
  "pages": [
    681,
    682,
    683
  ]
}