{
  "doc_id": "pages_767_769",
  "text": "A.2 Linear least squares\n745\nThe above error metric has a trivial minimum solution at x = 0 and is, in fact, homoge-\nneous in x. For this reason, we augment this minimization problem with the requirement that\n∥x∥2 = 1. which results in the eigenvalue problem\nx = arg min\nx xT (AT A)x\nsuch that\n∥x∥2 = 1.\n(A.36)\nThe value of x that minimizes this constrained problem is the eigenvector associated with the\nsmallest eigenvalue of AT A. This is the same as the last right singular vector of A, since\nA\n=\nUΣV ,\n(A.37)\nAT A\n=\nV Σ2V ,\n(A.38)\nAT Avk\n=\nσ2\nk,\n(A.39)\nwhich is minimized by selecting the smallest σk value.\nFigure A.2b shows a line ﬁtting problem where, in this case, the measurement errors are\nassumed to be isotropic in (x, y). The solution for the best line equation ax + by + c = 0 is\nfound by minimizing\nETLS−2D =\nX\ni\n(axi + byi + c)2,\n(A.40)\ni.e., ﬁnding the eigenvector associated with the smallest eigenvalue of6\nC = AT A =\nX\ni\n\n\nxi\nyi\n1\n\n\nh\nxi\nyi\n1\ni\n.\n(A.41)\nNotice, however, that minimizing P\ni(aix)2 in (A.35) is only statistically optimal (Ap-\npendix B.1.1) if all of the measured terms in the ai, e.g., the (xi, yi, 1) measurements, have\nequal noise. This is deﬁnitely not the case in the line-ﬁtting example of Figure A.2b (A.40),\nsince the 1 values are noise-free. To mitigate this, we ﬁrst subtract the mean x and y values\nfrom all the measured points\nˆxi\n=\nxi −¯x\n(A.42)\nˆyi\n=\nyi −¯y\n(A.43)\nand then ﬁt the 2D line equation a(x −¯x) + b(y −¯y) = 0 by minimizing\nETLS−2Dm =\nX\ni\n(aˆxi + bˆyi)2.\n(A.44)\n6 Again, be careful with the variable names here. The measurement equation is ai = (xi, yi, 1) and the unknown\nparameters are x = (a, b, c).\n746\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe more general case where each individual measurement component can have different\nnoise level, as is the case in estimating essential and fundamental matrices (Section 7.2), is\ncalled the heteroscedastic errors-in-variable (HEIV) model and is discussed by Matei and\nMeer (2006).\nA.3 Non-linear least squares\nIn many vision problems, such as structure from motion, the least squares problem formulated\nin (A.23) involves functions f(xi; p) that are not linear in the unknown parameters p. This\nproblem is known as non-linear least squares or non-linear regression (Bj¨orck 1996; Madsen,\nNielsen, and Tingleff 2004; Nocedal and Wright 2006). It is usually solved by iteratively re-\nlinearizing (A.23) around the current estimate of p using the gradient derivative (Jacobian)\nJ = ∂f/∂p and computing an incremental improvement ∆p.\nAs shown in Equations (6.13–6.17), this results in\nENLS(∆p)\n=\nX\ni\n∥f(xi; p + ∆p) −x′\ni∥2\n(A.45)\n≈\nX\ni\n∥J(xi; p)∆p −ri∥2,\n(A.46)\nwhere the Jacobians J(xi; p) and residual vectors ri play the same role in forming the normal\nequations as ai and bi in (A.28).\nBecause the above approximation only holds near a local minimum or for small values\nof ∆p, the update p ←p + ∆p may not always decrease the summed square residual error\n(A.45). One way to mitigate this problem is to take a smaller step,\np ←p + α∆p,\n0 < α ≤1.\n(A.47)\nA simple way to determine a reasonable value of α is to start with 1 and successively halve\nthe value, which is a simple form of line search (Al-Baali and Fletcher. 1986; Bj¨orck 1996;\nNocedal and Wright 2006).\nAnother approach to ensuring a downhill step in error is to add a diagonal damping term\nto the approximate Hessian\nC =\nX\ni\nJT (xi)J(xi),\n(A.48)\ni.e., to solve\n[C + λ diag(C)]∆p = d,\n(A.49)\nwhere\nd =\nX\ni\nJT (xi)ri,\n(A.50)\nA.4 Direct sparse matrix techniques\n747\nwhich is called a damped Gauss–Newton method. The damping parameter λ is increased if\nthe squared residual is not decreasing as fast as expected, i.e., as predicted by (A.46), and\nis decreased if the expected decrease is obtained (Madsen, Nielsen, and Tingleff 2004). The\ncombination of the Newton (ﬁrst-order Taylor series) approximation (A.46) and the adaptive\ndamping parameter λ is commonly known as the Levenberg–Marquardt algorithm (Leven-\nberg 1944; Marquardt 1963) and is an example of more general trust region methods, which\nare discussed in more detail in (Bj¨orck 1996; Conn, Gould, and Toint 2000; Madsen, Nielsen,\nand Tingleff 2004; Nocedal and Wright 2006).\nWhen the initial solution is far away from its quadratic region of convergence around a\nlocal minimum, large residual methods, e.g., Newton-type methods, which add a second-order\nterm to the Taylor series expansion in (A.46), may converge faster. Quasi-Newton methods\nsuch as BFGS, which require only gradient evaluations, can also be useful if memory size is\nan issue. Such techniques are discussed in textbooks and papers on numerical optimization\n(Toint 1987; Bj¨orck 1996; Conn, Gould, and Toint 2000; Nocedal and Wright 2006).\nA.4 Direct sparse matrix techniques\nMany optimization problems in computer vision, such as bundle adjustment (Szeliski and\nKang 1994; Triggs, McLauchlan, Hartley et al. 1999; Hartley and Zisserman 2004; Snavely,\nSeitz, and Szeliski 2008b; Agarwal, Snavely, Simon et al. 2009) have Jacobian and (approx-\nimate) Hessian matrices that are extremely sparse (Section 7.4.1). For example, Figure 7.9a\nshows the bipartite model typical of structure from motion problems, in which most points\nare only observed by a subset of the cameras, which results in the sparsity patterns for the\nJacobian and Hessian shown in Figure 7.9b–c.\nWhenever the Hessian matrix is sparse enough, it is more efﬁcient to use sparse Cholesky\nfactorization instead of regular Cholesky factorization. In such sparse direct techniques, the\nHessian matrix C and its associated Cholesky factor R are stored in compressed form, in\nwhich the amount of storage is proportional to the number of (potentially) non-zero entries\n(Bj¨orck 1996; Davis 2006).7 Algorithms for computing the non-zero elements in C and R\nfrom the sparsity pattern of the Jacobian matrix J are given by Bj¨orck (1996, Section 6.4),\nand algorithms for computing the numerical Cholesky and QR decompositions (once the\nsparsity pattern has been computed and storage allocated) are discussed by Bj¨orck (1996,\nSection 6.5).\n7 For example, you can store a list of (i, j, cij) triples. One example of such a scheme is compressed sparse\nrow (CSR) storage. An alternative storage method called skyline, which stores adjacent vertical spans of non-zero\nelements (Bathe 2007), is sometimes used in ﬁnite element analysis. Banded systems such as snakes (5.3) can store\njust the non-zero band elements (Bj¨orck 1996, Section 6.2) and can be solved in O(nb2), where n is the number of\nvariables and b is the bandwidth.",
  "image_path": "page_768.jpg",
  "pages": [
    767,
    768,
    769
  ]
}