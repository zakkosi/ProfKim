{
  "doc_id": "pages_112_114",
  "text": "90\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprovide you with the eight-bit gamma-compressed R’G’B’ values. However, if you are trying\nto do careful image deblocking (Exercise 3.30), this information may be useful.\nAnother color space you may come across is hue, saturation, value (HSV), which is a pro-\njection of the RGB color cube onto a non-linear chroma angle, a radial saturation percentage,\nand a luminance-inspired value. In more detail, value is deﬁned as either the mean or maxi-\nmum color value, saturation is deﬁned as scaled distance from the diagonal, and hue is deﬁned\nas the direction around a color wheel (the exact formulas are described by Hall (1989); Foley,\nvan Dam, Feiner et al. (1995)). Such a decomposition is quite natural in graphics applications\nsuch as color picking (it approximates the Munsell chart for color description). Figure 2.32l–\nn shows an HSV representation of a sample color image, where saturation is encoded using a\ngray scale (saturated = darker) and hue is depicted as a color.\nIf you want your computer vision algorithm to only affect the value (luminance) of an\nimage and not its saturation or hue, a simpler solution is to use either the Y xy (luminance +\nchromaticity) coordinates deﬁned in (2.104) or the even simpler color ratios,\nr =\nR\nR + G + B , g =\nG\nR + G + B , b =\nB\nR + G + B\n(2.116)\n(Figure 2.32e–h). After manipulating the luma (2.112), e.g., through the process of histogram\nequalization (Section 3.1.4), you can multiply each color ratio by the ratio of the new to old\nluma to obtain an adjusted RGB triplet.\nWhile all of these color systems may sound confusing, in the end, it often may not mat-\nter that much which one you use. Poynton, in his Color FAQ, http://www.poynton.com/\nColorFAQ.html, notes that the perceptually motivated L*a*b* system is qualitatively similar\nto the gamma-compressed R’G’B’ system we mostly deal with, since both have a fractional\npower scaling (which approximates a logarithmic response) between the actual intensity val-\nues and the numbers being manipulated. As in all cases, think carefully about what you are\ntrying to accomplish before deciding on a technique to use.24\n2.3.3 Compression\nThe last stage in a camera’s processing pipeline is usually some form of image compression\n(unless you are using a lossless compression scheme such as camera RAW or PNG).\nAll color video and image compression algorithms start by converting the signal into\nYCbCr (or some closely related variant), so that they can compress the luminance signal with\nhigher ﬁdelity than the chrominance signal. (Recall that the human visual system has poorer\n24 If you are at a loss for questions at a conference, you can always ask why the speaker did not use a perceptual\ncolor space, such as L*a*b*. Conversely, if they did use L*a*b*, you can ask if they have any concrete evidence that\nthis works better than regular colors.\n2.3 The digital camera\n91\n(a) RGB\n(b) R\n(c) G\n(d) B\n(e) rgb\n(f) r\n(g) g\n(h) b\n(i) L*\n(j) a*\n(k) b*\n(l) H\n(m) S\n(n) V\nFigure 2.32 Color space transformations: (a–d) RGB; (e–h) rgb. (i–k) L*a*b*; (l–n) HSV.\nNote that the rgb, L*a*b*, and HSV values are all re-scaled to ﬁt the dynamic range of the\nprinted page.\nfrequency response to color than to luminance changes.) In video, it is common to subsam-\nple Cb and Cr by a factor of two horizontally; with still images (JPEG), the subsampling\n(averaging) occurs both horizontally and vertically.\nOnce the luminance and chrominance images have been appropriately subsampled and\nseparated into individual images, they are then passed to a block transform stage. The most\ncommon technique used here is the discrete cosine transform (DCT), which is a real-valued\nvariant of the discrete Fourier transform (DFT) (see Section 3.4.3). The DCT is a reasonable\napproximation to the Karhunen–Lo`eve or eigenvalue decomposition of natural image patches,\ni.e., the decomposition that simultaneously packs the most energy into the ﬁrst coefﬁcients\nand diagonalizes the joint covariance matrix among the pixels (makes transform coefﬁcients\n92\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 2.33 Image compressed with JPEG at three quality settings. Note how the amount\nof block artifact and high-frequency aliasing (“mosquito noise”) increases from left to right.\nstatistically independent). Both MPEG and JPEG use 8 × 8 DCT transforms (Wallace 1991;\nLe Gall 1991), although newer variants use smaller 4×4 blocks or alternative transformations,\nsuch as wavelets (Taubman and Marcellin 2002) and lapped transforms (Malvar 1990, 1998,\n2000) are now used.\nAfter transform coding, the coefﬁcient values are quantized into a set of small integer\nvalues that can be coded using a variable bit length scheme such as a Huffman code or an\narithmetic code (Wallace 1991). (The DC (lowest frequency) coefﬁcients are also adaptively\npredicted from the previous block’s DC values. The term “DC” comes from “direct current”,\ni.e., the non-sinusoidal or non-alternating part of a signal.) The step size in the quantization\nis the main variable controlled by the quality setting on the JPEG ﬁle (Figure 2.33).\nWith video, it is also usual to perform block-based motion compensation, i.e., to encode\nthe difference between each block and a predicted set of pixel values obtained from a shifted\nblock in the previous frame. (The exception is the motion-JPEG scheme used in older DV\ncamcorders, which is nothing more than a series of individually JPEG compressed image\nframes.) While basic MPEG uses 16 × 16 motion compensation blocks with integer motion\nvalues (Le Gall 1991), newer standards use adaptively sized block, sub-pixel motions, and\nthe ability to reference blocks from older frames. In order to recover more gracefully from\nfailures and to allow for random access to the video stream, predicted P frames are interleaved\namong independently coded I frames. (Bi-directional B frames are also sometimes used.)\nThe quality of a compression algorithm is usually reported using its peak signal-to-noise\nratio (PSNR), which is derived from the average mean square error,\nMSE = 1\nn\nX\nx\nh\nI(x) −ˆI(x)\ni2\n,\n(2.117)\nwhere I(x) is the original uncompressed image and ˆI(x) is its compressed counterpart, or\nequivalently, the root mean square error (RMS error), which is deﬁned as\nRMS =\n√\nMSE.\n(2.118)",
  "image_path": "page_113.jpg",
  "pages": [
    112,
    113,
    114
  ]
}