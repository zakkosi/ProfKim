{
  "doc_id": "pages_239_241",
  "text": "4.1 Points and patches\n217\nFigure 4.10\nMulti-scale oriented patches (MOPS) extracted at ﬁve pyramid levels (Brown,\nSzeliski, and Winder 2005) c⃝2005 IEEE. The boxes show the feature orientation and the\nregion from which the descriptor vectors are sampled.\nis unknown. Instead of extracting features at many different scales and then matching all of\nthem, it is more efﬁcient to extract features that are stable in both location and scale (Lowe\n2004; Mikolajczyk and Schmid 2004).\nEarly investigations into scale selection were performed by Lindeberg (1993; 1998b),\nwho ﬁrst proposed using extrema in the Laplacian of Gaussian (LoG) function as interest\npoint locations. Based on this work, Lowe (2004) proposed computing a set of sub-octave\nDifference of Gaussian ﬁlters (Figure 4.11a), looking for 3D (space+scale) maxima in the re-\nsulting structure (Figure 4.11b), and then computing a sub-pixel space+scale location using a\nquadratic ﬁt (Brown and Lowe 2002). The number of sub-octave levels was determined, after\ncareful empirical investigation, to be three, which corresponds to a quarter-octave pyramid,\nwhich is the same as used by Triggs (2004).\nAs with the Harris operator, pixels where there is strong asymmetry in the local curvature\nof the indicator function (in this case, the DoG) are rejected. This is implemented by ﬁrst\ncomputing the local Hessian of the difference image D,\nH =\n\"\nDxx\nDxy\nDxy\nDyy\n#\n,\n(4.12)\nand then rejecting keypoints for which\nTr(H)2\nDet(H) > 10.\n(4.13)\n218\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n Scale\n (first\n octave)\nScale\n(next\noctave)\nGaussian\nDifference of\nGaussian (DOG)\n. . .\nScale\n(a)\n(b)\nFigure 4.11 Scale-space feature detection using a sub-octave Difference of Gaussian pyra-\nmid (Lowe 2004) c⃝2004 Springer: (a) Adjacent levels of a sub-octave Gaussian pyramid\nare subtracted to produce Difference of Gaussian images; (b) extrema (maxima and minima)\nin the resulting 3D volume are detected by comparing a pixel to its 26 neighbors.\nWhile Lowe’s Scale Invariant Feature Transform (SIFT) performs well in practice, it is not\nbased on the same theoretical foundation of maximum spatial stability as the auto-correlation-\nbased detectors. (In fact, its detection locations are often complementary to those produced\nby such techniques and can therefore be used in conjunction with these other approaches.)\nIn order to add a scale selection mechanism to the Harris corner detector, Mikolajczyk and\nSchmid (2004) evaluate the Laplacian of Gaussian function at each detected Harris point (in\na multi-scale pyramid) and keep only those points for which the Laplacian is extremal (larger\nor smaller than both its coarser and ﬁner-level values). An optional iterative reﬁnement for\nboth scale and position is also proposed and evaluated. Additional examples of scale invariant\nregion detectors are discussed by Mikolajczyk, Tuytelaars, Schmid et al. (2005); Tuytelaars\nand Mikolajczyk (2007).\nRotational invariance and orientation estimation\nIn addition to dealing with scale changes, most image matching and object recognition algo-\nrithms need to deal with (at least) in-plane image rotation. One way to deal with this problem\nis to design descriptors that are rotationally invariant (Schmid and Mohr 1997), but such\ndescriptors have poor discriminability, i.e. they map different looking patches to the same\ndescriptor.\n4.1 Points and patches\n219\nFigure 4.12\nA dominant orientation estimate can be computed by creating a histogram of\nall the gradient orientations (weighted by their magnitudes or after thresholding out small\ngradients) and then ﬁnding the signiﬁcant peaks in this distribution (Lowe 2004) c⃝2004\nSpringer.\nA better method is to estimate a dominant orientation at each detected keypoint. Once\nthe local orientation and scale of a keypoint have been estimated, a scaled and oriented patch\naround the detected point can be extracted and used to form a feature descriptor (Figures 4.10\nand 4.17).\nThe simplest possible orientation estimate is the average gradient within a region around\nthe keypoint. If a Gaussian weighting function is used (Brown, Szeliski, and Winder 2005),\nthis average gradient is equivalent to a ﬁrst-order steerable ﬁlter (Section 3.2.3), i.e., it can be\ncomputed using an image convolution with the horizontal and vertical derivatives of Gaus-\nsian ﬁlter (Freeman and Adelson 1991). In order to make this estimate more reliable, it is\nusually preferable to use a larger aggregation window (Gaussian kernel size) than detection\nwindow (Brown, Szeliski, and Winder 2005). The orientations of the square boxes shown in\nFigure 4.10 were computed using this technique.\nSometimes, however, the averaged (signed) gradient in a region can be small and therefore\nan unreliable indicator of orientation. A more reliable technique is to look at the histogram\nof orientations computed around the keypoint. Lowe (2004) computes a 36-bin histogram\nof edge orientations weighted by both gradient magnitude and Gaussian distance to the cen-\nter, ﬁnds all peaks within 80% of the global maximum, and then computes a more accurate\norientation estimate using a three-bin parabolic ﬁt (Figure 4.12).\nAfﬁne invariance\nWhile scale and rotation invariance are highly desirable, for many applications such as wide\nbaseline stereo matching (Pritchett and Zisserman 1998; Schaffalitzky and Zisserman 2002)\nor location recognition (Chum, Philbin, Sivic et al. 2007), full afﬁne invariance is preferred.",
  "image_path": "page_240.jpg",
  "pages": [
    239,
    240,
    241
  ]
}