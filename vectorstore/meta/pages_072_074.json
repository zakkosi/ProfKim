{
  "doc_id": "pages_072_074",
  "text": "50\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\ncs\nyc\nxs\nys\nsx\nsy\npc\np\nOc\nFigure 2.8 Projection of a 3D camera-centered point pc onto the sensor planes at location\np. Oc is the camera center (nodal point), cs is the 3D origin of the sensor plane coordinate\nsystem, and sx and sy are the pixel spacings.\nCamera intrinsics\nOnce we have projected a 3D point through an ideal pinhole using a projection matrix, we\nmust still transform the resulting coordinates according to the pixel sensor spacing and the\nrelative position of the sensor plane to the origin. Figure 2.8 shows an illustration of the\ngeometry involved. In this section, we ﬁrst present a mapping from 2D pixel coordinates to\n3D rays using a sensor homography M s, since this is easier to explain in terms of physically\nmeasurable quantities. We then relate these quantities to the more commonly used camera in-\ntrinsic matrix K, which is used to map 3D camera-centered points pc to 2D pixel coordinates\n˜xs.\nImage sensors return pixel values indexed by integer pixel coordinates (xs, ys), often\nwith the coordinates starting at the upper-left corner of the image and moving down and to\nthe right. (This convention is not obeyed by all imaging libraries, but the adjustment for\nother coordinate systems is straightforward.) To map pixel centers to 3D coordinates, we ﬁrst\nscale the (xs, ys) values by the pixel spacings (sx, sy) (sometimes expressed in microns for\nsolid-state sensors) and then describe the orientation of the sensor array relative to the camera\nprojection center Oc with an origin cs and a 3D rotation Rs (Figure 2.8).\nThe combined 2D to 3D projection can then be written as\np =\nh\nRs\ncs\ni\n\n\nsx\n0\n0\n0\nsy\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\nxs\nys\n1\n\n= M s¯xs.\n(2.53)\nThe ﬁrst two columns of the 3 × 3 matrix M s are the 3D vectors corresponding to unit steps\nin the image pixel array along the xs and ys directions, while the third column is the 3D\nimage array origin cs.\n2.1 Geometric primitives and transformations\n51\nThe matrix M s is parameterized by eight unknowns: the three parameters describing\nthe rotation Rs, the three parameters describing the translation cs, and the two scale factors\n(sx, sy). Note that we ignore here the possibility of skew between the two axes on the image\nplane, since solid-state manufacturing techniques render this negligible. In practice, unless\nwe have accurate external knowledge of the sensor spacing or sensor orientation, there are\nonly seven degrees of freedom, since the distance of the sensor from the origin cannot be\nteased apart from the sensor spacing, based on external image measurement alone.\nHowever, estimating a camera model M s with the required seven degrees of freedom\n(i.e., where the ﬁrst two columns are orthogonal after an appropriate re-scaling) is impractical,\nso most practitioners assume a general 3 × 3 homogeneous matrix form.\nThe relationship between the 3D pixel center p and the 3D camera-centered point pc is\ngiven by an unknown scaling s, p = spc. We can therefore write the complete projection\nbetween pc and a homogeneous version of the pixel address ˜xs as\n˜xs = αM −1\ns pc = Kpc.\n(2.54)\nThe 3 × 3 matrix K is called the calibration matrix and describes the camera intrinsics (as\nopposed to the camera’s orientation in space, which are called the extrinsics).\nFrom the above discussion, we see that K has seven degrees of freedom in theory and\neight degrees of freedom (the full dimensionality of a 3×3 homogeneous matrix) in practice.\nWhy, then, do most textbooks on 3D computer vision and multi-view geometry (Faugeras\n1993; Hartley and Zisserman 2004; Faugeras and Luong 2001) treat K as an upper-triangular\nmatrix with ﬁve degrees of freedom?\nWhile this is usually not made explicit in these books, it is because we cannot recover\nthe full K matrix based on external measurement alone. When calibrating a camera (Chap-\nter 6) based on external 3D points or other measurements (Tsai 1987), we end up estimating\nthe intrinsic (K) and extrinsic (R, t) camera parameters simultaneously using a series of\nmeasurements,\n˜xs = K\nh\nR\nt\ni\npw = P pw,\n(2.55)\nwhere pw are known 3D world coordinates and\nP = K[R|t]\n(2.56)\nis known as the camera matrix. Inspecting this equation, we see that we can post-multiply\nK by R1 and pre-multiply [R|t] by RT\n1 , and still end up with a valid calibration. Thus, it\nis impossible based on image measurements alone to know the true orientation of the sensor\nand the true camera intrinsics.\nThe choice of an upper-triangular form for K seems to be conventional. Given a full\n3 × 4 camera matrix P = K[R|t], we can compute an upper-triangular K matrix using QR\n52\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\n0\nyc\nxs\nys\nW-1\nH-1\n(cx,cy)\n0\nf\nFigure 2.9\nSimpliﬁed camera intrinsics showing the focal length f and the optical center\n(cx, cy). The image width and height are W and H.\nfactorization (Golub and Van Loan 1996). (Note the unfortunate clash of terminologies: In\nmatrix algebra textbooks, R represents an upper-triangular (right of the diagonal) matrix; in\ncomputer vision, R is an orthogonal rotation.)\nThere are several ways to write the upper-triangular form of K. One possibility is\nK =\n\n\nfx\ns\ncx\n0\nfy\ncy\n0\n0\n1\n\n,\n(2.57)\nwhich uses independent focal lengths fx and fy for the sensor x and y dimensions. The entry\ns encodes any possible skew between the sensor axes due to the sensor not being mounted\nperpendicular to the optical axis and (cx, cy) denotes the optical center expressed in pixel\ncoordinates. Another possibility is\nK =\n\n\nf\ns\ncx\n0\naf\ncy\n0\n0\n1\n\n,\n(2.58)\nwhere the aspect ratio a has been made explicit and a common focal length f is used.\nIn practice, for many applications an even simpler form can be obtained by setting a = 1\nand s = 0,\nK =\n\n\nf\n0\ncx\n0\nf\ncy\n0\n0\n1\n\n.\n(2.59)\nOften, setting the origin at roughly the center of the image, e.g., (cx, cy) = (W/2, H/2),\nwhere W and H are the image height and width, can result in a perfectly usable camera\nmodel with a single unknown, i.e., the focal length f.",
  "image_path": "page_073.jpg",
  "pages": [
    72,
    73,
    74
  ]
}