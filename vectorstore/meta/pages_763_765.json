{
  "doc_id": "pages_763_765",
  "text": "A.1 Matrix decompositions\n741\nprocedure Cholesky(C, R):\nR = C\nfor i = 0 . . . n −1\nfor j = i + 1 . . . n −1\nRj,j:n−1 = Rj,j:n−1 −rijr−1\nii Ri,j:n−1\nRi,i:n−1 = r−1/2\nii\nRi,i:n−1\nAlgorithm A.1 Cholesky decomposition of the matrix C into its upper triangular form R.\nalso found in LAPACK. Unlike the SVD and eigenvalue decompositions, QR factorization\ndoes not require iteration and can be computed exactly in O(MN 2 + N 3) operations, where\nM is the number of rows and N is the number of columns (for a tall matrix).\nA.1.4 Cholesky factorization\nCholesky factorization can be applied to any symmetric positive deﬁnite matrix C to convert\nit into a product of symmetric lower and upper triangular matrices,\nC = LLT = RT R,\n(A.18)\nwhere L is a lower-triangular matrix and R is an upper-triangular matrix. Unlike Gaussian\nelimination, which may require pivoting (row and column reordering) or may become un-\nstable (sensitive to roundoff errors or reordering), Cholesky factorization remains stable for\npositive deﬁnite matrices, such as those that arise from normal equations in least squares prob-\nlems (Appendix A.2). Because of the form of (A.18), the matrices L and R are sometimes\ncalled matrix square roots.4\nThe algorithm to compute an upper triangular Cholesky decomposition of C is a straight-\nforward symmetric generalization of Gaussian elimination and is based on the decomposition\n(Bj¨orck 1996; Golub and Van Loan 1996)\nC\n=\n\"\nγ\ncT\nc\nC11\n#\n(A.19)\n=\n\"\nγ1/2\n0T\ncγ−1/2\nI\n# \"\n1\n0T\n0\nC11 −cγ−1cT\n# \"\nγ1/2\nγ−1/2cT\n0\nI\n#\n(A.20)\n4 In fact, there exists a whole family of matrix square roots. Any matrix of the form LQ or QR, where Q is a\nunitary matrix, is a square root of C.\n742\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n=\nRT\n0 C1R0,\n(A.21)\nwhich, through recursion, can be turned into\nC = RT\n0 . . . RT\nn−1Rn−1 . . . R0 = RT R.\n(A.22)\nAlgorithm A.1 provides a more procedural deﬁnition, which can store the upper-triangular\nmatrix R in the same space as C, if desired. The total operation count for Cholesky factor-\nization is O(N 3) for a dense matrix but can be signiﬁcantly lower for sparse matrices with\nlow ﬁll-in (Appendix A.4).\nNote that Cholesky decomposition can also be applied to block-structured matrices, where\nthe term γ in (A.19) is now a square block sub-matrix and c is a rectangular matrix (Golub\nand Van Loan 1996). The computation of square roots can be avoided by leaving the γ on\nthe diagonal of the middle factor in (A.20), which results in the C = LDLT factorization,\nwhere D is a diagonal matrix. However, since square roots are relatively fast on modern\ncomputers, this is not worth the bother and Cholesky factorization is usually preferred.\nA.2 Linear least squares\nLeast squares ﬁtting problems are pervasive in computer vision. For example, the alignment\nof images based on matching feature points involves the minimization of a squared distance\nobjective function (6.2),\nELS =\nX\ni\n∥ri∥2 =\nX\ni\n∥f(xi; p) −x′\ni∥2,\n(A.23)\nwhere\nri = f(xi; p) −x′\ni = ˆx′\ni −˜x′\ni\n(A.24)\nis the residual between the measured location ˆx′\ni and its corresponding current predicted lo-\ncation ˜x′\ni = f(xi; p). More complex versions of least squares problems, such as large-scale\nstructure from motion (Section 7.4), may involve the minimization of functions of thousands\nof variables. Even problems such as image ﬁltering (Section 3.4.3) and regularization (Sec-\ntion 3.7.1) may involve the minimization of sums of squared errors.\nFigure A.2a shows an example of a simple least squares line ﬁtting problem, where the\nquantities being estimated are the line equation parameters (m, b). When the sampled vertical\nvalues yi are assumed to be noisy versions of points on the line y = mx + b, the optimal\nestimates for (m, b) can be found by minimizing the squared vertical residuals\nEVLS =\nX\ni\n|yi −(mxi + b)|2.\n(A.25)\nA.2 Linear least squares\n743\nNote that the function being ﬁtted need not itself be linear to use linear least squares. All that\nis required is that the function be linear in the unknown parameters. For example, polynomial\nﬁtting can be written as\nEPLS =\nX\ni\n|yi −(\np\nX\nj=0\najxj\ni)|2,\n(A.26)\nwhile sinusoid ﬁtting with unknown amplitude A and phase φ (but known frequency f) can\nbe written as\nESLS =\nX\ni\n|yi −A sin(2πfxi +φ)|2 =\nX\ni\n|yi −(B sin 2πfxi +C cos 2πfxi)|2, (A.27)\nwhich is linear in (B, C).\nIn general, it is more common to denote the unknown parameters using x and to write the\ngeneral form of linear least squares as5\nELLS =\nX\ni\n|aix −bi|2 = ∥Ax −b∥2.\n(A.28)\nExpanding the above equation gives us\nELLS = xT (AT A)x −2xT (AT b) + ∥b∥2,\n(A.29)\nwhose minimum value for x can be found by solving the associated normal equations (Bj¨orck\n1996; Golub and Van Loan 1996)\n(AT A)x = AT b.\n(A.30)\nThe preferred way to solve the normal equations is to use Cholesky factorization. Let\nC = AT A = RT R,\n(A.31)\nwhere R is the upper-triangular Cholesky factor of the Hessian C, and\nd = AT b.\n(A.32)\nAfter factorization, the solution for x can be obtained as\nRT z = d,\nRx = z,\n(A.33)\nwhich involves the solution of two triangular systems, i.e., forward and backward substitution\n(Bj¨orck 1996).\n5 Be extra careful in interpreting the variable names here. In the 2D line-ﬁtting example, x is used to denote the\nhorizontal axis, but in the general least squares problem, x = (m, b) denotes the unknown parameter vector.",
  "image_path": "page_764.jpg",
  "pages": [
    763,
    764,
    765
  ]
}