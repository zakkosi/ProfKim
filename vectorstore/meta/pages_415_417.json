{
  "doc_id": "pages_415_417",
  "text": "8.1 Translational alignment\n393\nA more commonly used approach, ﬁrst proposed by Lucas and Kanade (1981), is to\nperform gradient descent on the SSD energy function (8.1), using a Taylor series expansion\nof the image function (Figure 8.2),\nELK−SSD(u + ∆u)\n=\nX\ni\n[I1(xi + u + ∆u) −I0(xi)]2\n(8.33)\n≈\nX\ni\n[I1(xi + u) + J1(xi + u)∆u −I0(xi)]2\n(8.34)\n=\nX\ni\n[J1(xi + u)∆u + ei]2,\n(8.35)\nwhere\nJ1(xi + u) = ∇I1(xi + u) = (∂I1\n∂x , ∂I1\n∂y )(xi + u)\n(8.36)\nis the image gradient or Jacobian at (xi + u) and\nei = I1(xi + u) −I0(xi),\n(8.37)\nﬁrst introduced in (8.1), is the current intensity error.7 The gradient at a particular sub-pixel\nlocation (xi + u) can be computed using a variety of techniques, the simplest of which is\nto simply take the horizontal and vertical differences between pixels x and x + (1, 0) or\nx + (0, 1). More sophisticated derivatives can sometimes lead to noticeable performance\nimprovements.\nThe linearized form of the incremental update to the SSD error (8.35) is often called the\noptical ﬂow constraint or brightness constancy constraint equation\nIxu + Iyv + It = 0,\n(8.38)\nwhere the subscripts in Ix and Iy denote spatial derivatives, and It is called the temporal\nderivative, which makes sense if we are computing instantaneous velocity in a video se-\nquence. When squared and summed or integrated over a region, it can be used to compute\noptic ﬂow (Horn and Schunck 1981).\nThe above least squares problem (8.35) can be minimized by solving the associated nor-\nmal equations (Appendix A.2),\nA∆u = b\n(8.39)\nwhere\nA =\nX\ni\nJT\n1 (xi + u)J1(xi + u)\n(8.40)\n7 We follow the convention, commonly used in robotics and by Baker and Matthews (2004), that derivatives with\nrespect to (column) vectors result in row vectors, so that fewer transposes are needed in the formulas.\n394\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nand\nb = −\nX\ni\neiJT\n1 (xi + u)\n(8.41)\nare called the (Gauss–Newton approximation of the) Hessian and gradient-weighted residual\nvector, respectively.8 These matrices are also often written as\nA =\n\"\nP I2\nx\nP IxIy\nP IxIy\nP I2\ny\n#\nand b = −\n\" P IxIt\nP IyIt\n#\n.\n(8.42)\nThe gradients required for J1(xi + u) can be evaluated at the same time as the image\nwarps required to estimate I1(xi + u) (Section 3.6.1 (3.89)) and, in fact, are often computed\nas a side-product of image interpolation. If efﬁciency is a concern, these gradients can be\nreplaced by the gradients in the template image,\nJ1(xi + u) ≈J0(xi),\n(8.43)\nsince near the correct alignment, the template and displaced target images should look sim-\nilar. This has the advantage of allowing the pre-computation of the Hessian and Jacobian\nimages, which can result in signiﬁcant computational savings (Hager and Belhumeur 1998;\nBaker and Matthews 2004). A further reduction in computation can be obtained by writing\nthe warped image I1(xi + u) used to compute ei in (8.37) as a convolution of a sub-pixel\ninterpolation ﬁlter with the discrete samples in I1 (Peleg and Rav-Acha 2006). Precomput-\ning the inner product between the gradient ﬁeld and shifted version of I1 allows the iterative\nre-computation of ei to be performed in constant time (independent of the number of pixels).\nThe effectiveness of the above incremental update rule relies on the quality of the Taylor\nseries approximation. When far away from the true displacement (say, 1–2 pixels), several\niterations may be needed. It is possible, however, to estimate a value for J1 using a least\nsquares ﬁt to a series of larger displacements in order to increase the range of convergence\n(Jurie and Dhome 2002) or to “learn” a special-purpose recognizer for a given patch (Avi-\ndan 2001; Williams, Blake, and Cipolla 2003; Lepetit, Pilet, and Fua 2006; Hinterstoisser,\nBenhimane, Navab et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010) as discussed in Sec-\ntion 4.1.4.\nA commonly used stopping criterion for incremental updating is to monitor the magnitude\nof the displacement correction ∥u∥and to stop when it drops below a certain threshold (say,\n1/10 of a pixel). For larger motions, it is usual to combine the incremental update rule with a\nhierarchical coarse-to-ﬁne search strategy, as described in Section 8.1.1.\n8 The true Hessian is the full second derivative of the error function E, which may not be positive deﬁnite—see\nSection 6.1.3 and Appendix A.3.\n8.1 Translational alignment\n395\nxi\nxi+u\nu\ni\n(a)\n(b)\n(c)\nFigure 8.3\nAperture problems for different image regions, denoted by the orange and red\nL-shaped structures, overlaid in the same image to make it easier to diagram the ﬂow. (a) A\nwindow w(xi) centered at xi (black circle) can uniquely be matched to its corresponding\nstructure at xi +u in the second (red) image. (b) A window centered on the edge exhibits the\nclassic aperture problem, since it can be matched to a 1D family of possible locations. (c) In\na completely textureless region, the matches become totally unconstrained.\nConditioning and aperture problems.\nSometimes, the inversion of the linear system (8.39)\ncan be poorly conditioned because of lack of two-dimensional texture in the patch being\naligned. A commonly occurring example of this is the aperture problem, ﬁrst identiﬁed in\nsome of the early papers on optical ﬂow (Horn and Schunck 1981) and then studied more ex-\ntensively by Anandan (1989). Consider an image patch that consists of a slanted edge moving\nto the right (Figure 8.3). Only the normal component of the velocity (displacement) can be\nreliably recovered in this case. This manifests itself in (8.39) as a rank-deﬁcient matrix A,\ni.e., one whose smaller eigenvalue is very close to zero.9\nWhen Equation (8.39) is solved, the component of the displacement along the edge is very\npoorly conditioned and can result in wild guesses under small noise perturbations. One way\nto mitigate this problem is to add a prior (soft constraint) on the expected range of motions\n(Simoncelli, Adelson, and Heeger 1991; Baker, Gross, and Matthews 2004; Govindu 2006).\nThis can be accomplished by adding a small value to the diagonal of A, which essentially\nbiases the solution towards smaller ∆u values that still (mostly) minimize the squared error.\nHowever, the pure Gaussian model assumed when using a simple (ﬁxed) quadratic prior,\nas in (Simoncelli, Adelson, and Heeger 1991), does not always hold in practice, e.g., because\nof aliasing along strong edges (Triggs 2004). For this reason, it may be prudent to add some\nsmall fraction (say, 5%) of the larger eigenvalue to the smaller one before doing the matrix\ninversion.\n9The matrix A is by construction always guaranteed to be symmetric positive semi-deﬁnite, i.e., it has real\nnon-negative eigenvalues.",
  "image_path": "page_416.jpg",
  "pages": [
    415,
    416,
    417
  ]
}