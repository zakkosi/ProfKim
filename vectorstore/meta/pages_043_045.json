{
  "doc_id": "pages_043_045",
  "text": "1.3 Book overview\n21\nFigure 1.11 shows a rough layout of the contents of this book. Since computer vision\ninvolves going from images to a structural description of the scene (and computer graphics\nthe converse), I have positioned the chapters horizontally in terms of which major component\nthey address, in addition to vertically according to their dependence.\nGoing from left to right, we see the major column headings as Images (which are 2D\nin nature), Geometry (which encompasses 3D descriptions), and Photometry (which encom-\npasses object appearance). (An alternative labeling for these latter two could also be shape\nand appearance—see, e.g., Chapter 13 and Kang, Szeliski, and Anandan (2000).) Going\nfrom top to bottom, we see increasing levels of modeling and abstraction, as well as tech-\nniques that build on previously developed algorithms. Of course, this taxonomy should be\ntaken with a large grain of salt, as the processing and dependencies in this diagram are not\nstrictly sequential and subtle additional dependencies and relationships also exist (e.g., some\nrecognition techniques make use of 3D information). The placement of topics along the hor-\nizontal axis should also be taken lightly, as most vision algorithms involve mapping between\nat least two different representations.9\nInterspersed throughout the book are sample applications, which relate the algorithms\nand mathematical material being presented in various chapters to useful, real-world applica-\ntions. Many of these applications are also presented in the exercises sections, so that students\ncan write their own.\nAt the end of each section, I provide a set of exercises that the students can use to imple-\nment, test, and reﬁne the algorithms and techniques presented in each section. Some of the\nexercises are suitable as written homework assignments, others as shorter one-week projects,\nand still others as open-ended research problems that make for challenging ﬁnal projects.\nMotivated students who implement a reasonable subset of these exercises will, by the end of\nthe book, have a computer vision software library that can be used for a variety of interesting\ntasks and projects.\nAs a reference book, I try wherever possible to discuss which techniques and algorithms\nwork well in practice, as well as providing up-to-date pointers to the latest research results in\nthe areas that I cover. The exercises can be used to build up your own personal library of self-\ntested and validated vision algorithms, which is more worthwhile in the long term (assuming\nyou have the time) than simply pulling algorithms out of a library whose performance you do\nnot really understand.\nThe book begins in Chapter 2 with a review of the image formation processes that create\nthe images that we see and capture. Understanding this process is fundamental if you want\nto take a scientiﬁc (model-based) approach to computer vision. Students who are eager to\njust start implementing algorithms (or courses that have limited time) can skip ahead to the\n9 For an interesting comparison with what is known about the human visual system, e.g., the largely parallel what\nand where pathways, see some textbooks on human perception (Palmer 1999; Livingstone 2008).\n22\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nn^\n2. Image Formation\n3. Image Processing\n4. Features\n5. Segmentation\n6-7. Structure from Motion\n8. Motion\n9. Stitching\n10. Computational Photography\n11. Stereo\n12. 3D Shape\n13. Image-based Rendering\n14. Recognition\nFigure 1.12\nA pictorial summary of the chapter contents. Sources: Brown, Szeliski, and\nWinder (2005); Comaniciu and Meer (2002); Snavely, Seitz, and Szeliski (2006); Nagel\nand Enkelmann (1986); Szeliski and Shum (1997); Debevec and Malik (1997); Gortler,\nGrzeszczuk, Szeliski et al. (1996); Viola and Jones (2004)—see the ﬁgures in the respec-\ntive chapters for copyright information.\n1.3 Book overview\n23\nnext chapter and dip into this material later. In Chapter 2, we break down image formation\ninto three major components. Geometric image formation (Section 2.1) deals with points,\nlines, and planes, and how these are mapped onto images using projective geometry and other\nmodels (including radial lens distortion). Photometric image formation (Section 2.2) covers\nradiometry, which describes how light interacts with surfaces in the world, and optics, which\nprojects light onto the sensor plane. Finally, Section 2.3 covers how sensors work, including\ntopics such as sampling and aliasing, color sensing, and in-camera compression.\nChapter 3 covers image processing, which is needed in almost all computer vision appli-\ncations. This includes topics such as linear and non-linear ﬁltering (Section 3.3), the Fourier\ntransform (Section 3.4), image pyramids and wavelets (Section 3.5), geometric transforma-\ntions such as image warping (Section 3.6), and global optimization techniques such as regu-\nlarization and Markov Random Fields (MRFs) (Section 3.7). While most of this material is\ncovered in courses and textbooks on image processing, the use of optimization techniques is\nmore typically associated with computer vision (although MRFs are now being widely used\nin image processing as well). The section on MRFs is also the ﬁrst introduction to the use\nof Bayesian inference techniques, which are covered at a more abstract level in Appendix B.\nChapter 3 also presents applications such as seamless image blending and image restoration.\nIn Chapter 4, we cover feature detection and matching. A lot of current 3D reconstruction\nand recognition techniques are built on extracting and matching feature points (Section 4.1),\nso this is a fundamental technique required by many subsequent chapters (Chapters 6, 7, 9\nand 14). We also cover edge and straight line detection in Sections 4.2 and 4.3.\nChapter 5 covers region segmentation techniques, including active contour detection and\ntracking (Section 5.1). Segmentation techniques include top-down (split) and bottom-up\n(merge) techniques, mean shift techniques that ﬁnd modes of clusters, and various graph-\nbased segmentation approaches. All of these techniques are essential building blocks that are\nwidely used in a variety of applications, including performance-driven animation, interactive\nimage editing, and recognition.\nIn Chapter 6, we cover geometric alignment and camera calibration. We introduce the\nbasic techniques of feature-based alignment in Section 6.1 and show how this problem can\nbe solved using either linear or non-linear least squares, depending on the motion involved.\nWe also introduce additional concepts, such as uncertainty weighting and robust regression,\nwhich are essential to making real-world systems work. Feature-based alignment is then used\nas a building block for 3D pose estimation (extrinsic calibration) in Section 6.2 and camera\n(intrinsic) calibration in Section 6.3. Chapter 6 also describes applications of these techniques\nto photo alignment for ﬂip-book animations, 3D pose estimation from a hand-held camera,\nand single-view reconstruction of building models.\nChapter 7 covers the topic of structure from motion, which involves the simultaneous\nrecovery of 3D camera motion and 3D scene structure from a collection of tracked 2D fea-",
  "image_path": "page_044.jpg",
  "pages": [
    43,
    44,
    45
  ]
}