{
  "doc_id": "pages_646_648",
  "text": "624\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 13.3 View-dependent texture mapping (Debevec, Taylor, and Malik 1996) c⃝1996\nACM. (a) The weighting given to each input view depends on the relative angles between the\nnovel (virtual) view and the original views; (b) simpliﬁed 3D model geometry; (c) with view-\ndependent texture mapping, the geometry appears to have more detail (recessed windows).\nHecker, Lischinski et al. 1998) and 3D scanning and visualization (Pulli, Abi-Rached, Duchamp\net al. 1998). Closely related to view-dependent texture mapping is the idea of blending be-\ntween light rays in 4D space, which forms the basis of the Lumigraph and unstructured Lu-\nmigraph systems (Section 13.3) (Gortler, Grzeszczuk, Szeliski et al. 1996; Buehler, Bosse,\nMcMillan et al. 2001).\nIn order to provide even more realism in their Fac¸ade system, Debevec, Taylor, and Malik\n(1996) also include a model-based stereo component, which optionally computes an offset\n(parallax) map for each coarse planar facet of their 3D model. They call the resulting analysis\nand rendering system a hybrid geometry- and image-based approach, since it uses traditional\n3D geometric modeling to create the global 3D model, but then uses local depth offsets, along\nwith view interpolation, to add visual realism.\n13.1.2 Application: Photo Tourism\nWhile view interpolation was originally developed to accelerate the rendering of 3D scenes\non low-powered processors and systems without graphics acceleration, it turns out that it\ncan be applied directly to large collections of casually acquired photographs. The Photo\nTourism system developed by Snavely, Seitz, and Szeliski (2006) uses structure from motion\nto compute the 3D locations and poses of all the cameras taking the images, along with a\nsparse 3D point-cloud model of the scene (Section 7.4.4, Figure 7.11).\nTo perform an image-based exploration of the resulting sea of images (Aliaga, Funkhouser,\nYanovsky et al. 2003), Photo Tourism ﬁrst associates a 3D proxy with each image. While a\ntriangulated mesh obtained from the point cloud can sometimes form a suitable proxy, e.g.,\nfor outdoor terrain models, a simple dominant plane ﬁt to the 3D points visible in each image\n13.1 View interpolation\n625\n(a)\n(b)\nFigure 13.4\nPhoto Tourism (Snavely, Seitz, and Szeliski 2006): c⃝2006 ACM: (a) a 3D\noverview of the scene, with translucent washes and lines painted onto the planar impostors;\n(b) once the user has selected a region of interest, a set of related thumbnails is displayed\nalong the bottom; (c) planar proxy selection for optimal stabilization (Snavely, Garg, Seitz et\nal. 2008) c⃝2008 ACM.\noften performs better, because it does not contain any erroneous segments or connections that\npop out as artifacts. As automated 3D modeling techniques continue to improve, however,\nthe pendulum may swing back to more detailed 3D geometry (Goesele, Snavely, Curless et\nal. 2007; Sinha, Steedly, and Szeliski 2009).\nThe resulting image-based navigation system lets users move from photo to photo, ei-\nther by selecting cameras from a top-down view of the scene (Figure 13.4a) or by selecting\nregions of interest in an image, navigating to nearby views, or selecting related thumbnails\n(Figure 13.4b). To create a background for the 3D scene, e.g., when being viewed from\nabove, non-photorealistic techniques (Section 10.5.2), such as translucent color washes or\nhighlighted 3D line segments, can be used (Figure 13.4a). The system can also be used to\nannotate regions of images and to automatically propagate such annotations to other pho-\ntographs.\nThe 3D planar proxies used in Photo Tourism and the related Photosynth system from\nMicrosoft result in non-photorealistic transitions reminiscent of visual effects such as “page\nﬂips”. Selecting a stable 3D axis for all the planes can reduce the amount of swimming and\nenhance the perception of 3D (Figure 13.4c) (Snavely, Garg, Seitz et al. 2008). It is also\npossible to automatically detect objects in the scene that are seen from multiple views and\ncreate “orbits” of viewpoints around such objects. Furthermore, nearby images in both 3D\nposition and viewing direction can be linked to create “virtual paths”, which can then be used\nto navigate between arbitrary pairs of images, such as those you might take yourself while\nwalking around a popular tourist site (Snavely, Garg, Seitz et al. 2008).\nThe spatial matching of image features and regions performed by Photo Tourism can\nalso be used to infer more information from large image collections. For example, Simon,\nSnavely, and Seitz (2007) show how the match graph between images of popular tourist sites\n626\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncan be used to ﬁnd the most iconic (commonly photographed) objects in the collection, along\nwith their related tags. In follow-on work, Simon and Seitz (2008) show how such tags can\nbe propagated to sub-regions of each image, using an analysis of which 3D points appear\nin the central portions of photographs. Extensions of these techniques to all of the world’s\nimages, including the use of GPS tags where available, have been investigated as well (Li,\nWu, Zach et al. 2008; Quack, Leibe, and Van Gool 2008; Crandall, Backstrom, Huttenlocher\net al. 2009; Li, Crandall, and Huttenlocher 2009; Zheng, Zhao, Song et al. 2009).\n13.2 Layered depth images\nTraditional view interpolation techniques associate a single depth map with each source or\nreference image. Unfortunately, when such a depth map is warped to a novel view, holes and\ncracks inevitably appear behind the foreground objects. One way to alleviate this problem is\nto keep several depth and color values (depth pixels) at every pixel in a reference image (or,\nat least for pixels near foreground–background transitions) (Figure 13.5). The resulting data\nstructure, which is called a layered depth image (LDI), can be used to render new views using\na back-to-front forward warping (splatting) algorithm (Shade, Gortler, He et al. 1998).\n13.2.1 Impostors, sprites, and layers\nAn alternative to keeping lists of color-depth values at each pixel, as is done in the LDI, is\nto organize objects into different layers or sprites. The term sprite originates in the computer\ngame industry, where it is used to designate ﬂat animated characters in games such as Pac-\nMan or Mario Bros. When put into a 3D setting, such objects are often called impostors,\nbecause they use a piece of ﬂat, alpha-matted geometry to represent simpliﬁed versions of 3D\nobjects that are far away from the camera (Shade, Lischinski, Salesin et al. 1996; Lengyel and\nSnyder 1997; Torborg and Kajiya 1996). In computer vision, such representations are usually\ncalled layers (Wang and Adelson 1994; Baker, Szeliski, and Anandan 1998; Torr, Szeliski,\nand Anandan 1999; Birchﬁeld, Natarajan, and Tomasi 2007). Section 8.5.2 discusses the\ntopics of transparent layers and reﬂections, which occur on specular and transparent surfaces\nsuch as glass.\nWhile ﬂat layers can often serve as an adequate representation of geometry and appear-\nance for far-away objects, better geometric ﬁdelity can be achieved by also modeling the\nper-pixel offsets relative to a base plane, as shown in Figures 13.5 and 13.6a–b. Such repre-\nsentations are called plane plus parallax in the computer vision literature (Kumar, Anandan,\nand Hanna 1994; Sawhney 1994; Szeliski and Coughlan 1997; Baker, Szeliski, and Anandan\n1998), as discussed in Section 8.5 (Figure 8.16). In addition to fully automated stereo tech-\nniques, it is also possible to paint in depth layers (Kang 1998; Oh, Chen, Dorsey et al. 2001;",
  "image_path": "page_647.jpg",
  "pages": [
    646,
    647,
    648
  ]
}