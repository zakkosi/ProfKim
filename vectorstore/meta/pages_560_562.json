{
  "doc_id": "pages_560_562",
  "text": "538\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np\nx1\nx0\n(R,t)\np∞\ne1\ne0\nc0\nc1\n       epipolar plane\np∞\np\n(R,t)\nc0\nc1\nepipolar\nlines\nx\n0\ne0\ne1\nx1\nl1\nl0\n(a)\n(b)\nFigure 11.3\nEpipolar geometry: (a) epipolar line segment corresponding to one ray; (b)\ncorresponding set of epipolar lines and their epipolar plane.\n11.1.1 Rectiﬁcation\nAs we saw in Section 7.2, the epipolar geometry for a pair of cameras is implicit in the\nrelative pose and calibrations of the cameras, and can easily be computed from seven or more\npoint matches using the fundamental matrix (or ﬁve or more points for the calibrated essential\nmatrix) (Zhang 1998a,b; Faugeras and Luong 2001; Hartley and Zisserman 2004). Once this\ngeometry has been computed, we can use the epipolar line corresponding to a pixel in one\nimage to constrain the search for corresponding pixels in the other image. One way to do this\nis to use a general correspondence algorithm, such as optical ﬂow (Section 8.4), but to only\nconsider locations along the epipolar line (or to project any ﬂow vectors that fall off back onto\nthe line).\nA more efﬁcient algorithm can be obtained by ﬁrst rectifying (i.e, warping) the input\nimages so that corresponding horizontal scanlines are epipolar lines (Loop and Zhang 1999;\nFaugeras and Luong 2001; Hartley and Zisserman 2004).2 Afterwards, it is possible to match\nhorizontal scanlines independently or to shift images horizontally while computing matching\nscores (Figure 11.4).\nA simple way to rectify the two images is to ﬁrst rotate both cameras so that they are\nlooking perpendicular to the line joining the camera centers c0 and c1. Since there is a de-\ngree of freedom in the tilt, the smallest rotations that achieve this should be used. Next, to\ndetermine the desired twist around the optical axes, make the up vector (the camera y axis)\n2 This makes most sense if the cameras are next to each other, although by rotating the cameras, rectiﬁcation can\nbe performed on any pair that is not verged too much or has too much of a scale change. In those latter cases, using\nplane sweep (below) or hypothesizing small planar patch locations in 3D (Goesele, Snavely, Curless et al. 2007) may\nbe preferable.\n11.1 Epipolar geometry\n539\n(a)\n(b)\n(c)\n(d)\nFigure 11.4\nThe multi-stage stereo rectiﬁcation algorithm of Loop and Zhang (1999) c⃝\n1999 IEEE. (a) Original image pair overlaid with several epipolar lines; (b) images trans-\nformed so that epipolar lines are parallel; (c) images rectiﬁed so that epipolar lines are hori-\nzontal and in vertial correspondence; (d) ﬁnal rectiﬁcation that minimizes horizontal distor-\ntions.\nperpendicular to the camera center line. This ensures that corresponding epipolar lines are\nhorizontal and that the disparity for points at inﬁnity is 0. Finally, re-scale the images, if nec-\nessary, to account for different focal lengths, magnifying the smaller image to avoid aliasing.\n(The full details of this procedure can be found in Fusiello, Trucco, and Verri (2000) and Ex-\nercise 11.1.) Note that in general, it is not possible to rectify an arbitrary collection of images\nsimultaneously unless their optical centers are collinear, although rotating the cameras so that\nthey all point in the same direction reduces the inter-camera pixel movements to scalings and\ntranslations.\nThe resulting standard rectiﬁed geometry is employed in a lot of stereo camera setups and\nstereo algorithms, and leads to a very simple inverse relationship between 3D depths Z and\ndisparities d,\nd = f B\nZ ,\n(11.1)\nwhere f is the focal length (measured in pixels), B is the baseline, and\nx′ = x + d(x, y), y′ = y\n(11.2)\ndescribes the relationship between corresponding pixel coordinates in the left and right im-\nages (Bolles, Baker, and Marimont 1987; Okutomi and Kanade 1993; Scharstein and Szeliski\n540\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 11.5\nSlices through a typical disparity space image (DSI) (Scharstein and Szeliski\n2002) c⃝2002 Springer: (a) original color image; (b) ground truth disparities; (c–e) three\n(x, y) slices for d = 10, 16, 21; (f) an (x, d) slice for y = 151 (the dashed line in (b)).\nVarious dark (matching) regions are visible in (c–e), e.g., the bookshelves, table and cans,\nand head statue, and three disparity levels can be seen as horizontal lines in (f). The dark\nbands in the DSIs indicate regions that match at this disparity. (Smaller dark regions are often\nthe result of textureless regions.) Additional examples of DSIs are discussed by Bobick and\nIntille (1999).\n2002).3 The task of extracting depth from a set of images then becomes one of estimating the\ndisparity map d(x, y).\nAfter rectiﬁcation, we can easily compare the similarity of pixels at corresponding lo-\ncations (x, y) and (x′, y′) = (x + d, y) and store them in a disparity space image (DSI)\nC(x, y, d) for further processing (Figure 11.5). The concept of the disparity space (x, y, d)\ndates back to early work in stereo matching (Marr and Poggio 1976), while the concept of a\ndisparity space image (volume) is generally associated with Yang, Yuille, and Lu (1993) and\nIntille and Bobick (1994).\n11.1.2 Plane sweep\nAn alternative to pre-rectifying the images before matching is to sweep a set of planes through\nthe scene and to measure the photoconsistency of different images as they are re-projected\nonto these planes (Figure 11.6). This process is commonly known as the plane sweep algo-\nrithm (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).\nAs we saw in Section 2.1.5, where we introduced projective depth (also known as plane\nplus parallax (Kumar, Anandan, and Hanna 1994; Sawhney 1994; Szeliski and Coughlan\n3 The term disparity was ﬁrst introduced in the human vision literature to describe the difference in location\nof corresponding features seen by the left and right eyes (Marr 1982). Horizontal disparity is the most commonly\nstudied phenomenon, but vertical disparity is possible if the eyes are verged.",
  "image_path": "page_561.jpg",
  "pages": [
    560,
    561,
    562
  ]
}