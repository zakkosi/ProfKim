{
  "doc_id": "pages_590_592",
  "text": "568\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 11.21 Reconstruction results (details) for seven algorithms (Hernandez and Schmitt\n2004; Furukawa and Ponce 2009; Pons, Keriven, and Faugeras 2005; Goesele, Curless, and\nSeitz 2006; Vogiatzis, Torr, and Cipolla 2005; Tran and Davis 2002; Kolmogorov and Zabih\n2002) evaluated by Seitz, Curless, Diebel et al. (2006) on the 47-image Temple Ring dataset.\nThe numbers underneath each detail image are the accuracy of each of these techniques mea-\nsured in millimeters.\n2003; Vlasic, Baran, Matusik et al. 2008) or to impose a convex set of constraints on multi-\nview stereo (Kolev and Cremers 2008). Over the years, a number of techniques have been\ndeveloped to reconstruct a 3D volumetric model from the intersection of the binary silhou-\nettes projected into 3D. The resulting model is called a visual hull (or sometimes a line hull),\nanalogous with the convex hull of a set of points, since the volume is maximal with respect\nto the visual silhouettes and surface elements are tangent to the viewing rays (lines) along\nthe silhouette boundaries (Laurentini 1994). It is also possible to carve away a more accu-\nrate reconstruction using multi-view stereo (Sinha and Pollefeys 2005) or by analyzing cast\nshadows (Savarese, Andreetto, Rushmeier et al. 2007).\nSome techniques ﬁrst approximate each silhouette with a polygonal representation and\nthen intersect the resulting faceted conical regions in three-space to produce polyhedral mod-\nels (Baumgart 1974; Martin and Aggarwal 1983; Matusik, Buehler, and McMillan 2001),\nwhich can later be reﬁned using triangular splines (Sullivan and Ponce 1998). Other ap-\nproaches use voxel-based representations, usually encoded as octrees (Samet 1989), because\nof the resulting space–time efﬁciency. Figures 11.22a–b show an example of a 3D octree\nmodel and its associated colored tree, where black nodes are interior to the model, white\n11.6 Multi-view stereo\n569\n(a)\n(b)\n(c)\n(d)\nFigure 11.22\nVolumetric octree reconstruction from binary silhouettes (Szeliski 1993) c⃝\n1993 Elsevier: (a) octree representation and its corresponding (b) tree structure; (c) input\nimage of an object on a turntable; (d) computed 3D volumetric octree model.\nnodes are exterior, and gray nodes are of mixed occupancy. Examples of octree-based re-\nconstruction approaches include those by Potmesil (1987), Noborio, Fukada, and Arimoto\n(1988), Srivasan, Liang, and Hackwood (1990), and Szeliski (1993).\nThe approach of Szeliski (1993) ﬁrst converts each binary silhouette into a one-sided\nvariant of a distance map, where each pixel in the map indicates the largest square that is\ncompletely inside (or outside) the silhouette. This makes it fast to project an octree cell\ninto the silhouette to conﬁrm whether it is completely inside or outside the object, so that\nit can be colored black, white, or left as gray (mixed) for further reﬁnement on a smaller\ngrid. The octree construction algorithm proceeds in a coarse-to-ﬁne manner, ﬁrst building an\noctree at a relatively coarse resolution, and then reﬁning it by revisiting and subdividing all\nthe input images for the gray (mixed) cells whose occupancy has not yet been determined.\nFigure 11.22d shows the resulting octree model computed from a coffee cup rotating on a\nturntable.\nMore recent work on visual hull computation borrows ideas from image-based rendering,\nand is hence called an image-based visual hull (Matusik, Buehler, Raskar et al. 2000). Instead\nof precomputing a global 3D model, an image-based visual hull is recomputed for each new\n570\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nviewpoint, by successively intersecting viewing ray segments with the binary silhouettes in\neach image. This not only leads to a fast computation algorithm but also enables fast texturing\nof the recovered model with color values from the input images. This approach can also\nbe combined with high-quality deformable templates to capture and re-animate whole body\nmotion (Vlasic, Baran, Matusik et al. 2008).\n11.7 Additional reading\nThe ﬁeld of stereo correspondence and depth estimation is one of the oldest and most widely\nstudied topics in computer vision. A number of good surveys have been written over the years\n(Marr and Poggio 1976; Barnard and Fischler 1982; Dhond and Aggarwal 1989; Scharstein\nand Szeliski 2002; Brown, Burschka, and Hager 2003; Seitz, Curless, Diebel et al. 2006) and\nthey can serve as good guides to this extensive literature.\nBecause of computational limitations and the desire to ﬁnd appearance-invariant cor-\nrespondences, early algorithms often focused on ﬁnding sparse correspondences (Hannah\n1974; Marr and Poggio 1979; Mayhew and Frisby 1980; Baker and Binford 1981; Arnold\n1983; Grimson 1985; Ohta and Kanade 1985; Bolles, Baker, and Marimont 1987; Matthies,\nKanade, and Szeliski 1989; Hsieh, McKeown, and Perlant 1992; Bolles, Baker, and Hannah\n1993).\nThe topic of computing epipolar geometry and pre-rectifying images is covered in Sec-\ntions 7.2 and 11.1 and is also treated in textbooks on multi-view geometry (Faugeras and\nLuong 2001; Hartley and Zisserman 2004) and articles speciﬁcally on this topic (Torr and\nMurray 1997; Zhang 1998a,b). The concepts of the disparity space and disparity space im-\nage are often associated with the seminal work by Marr (1982) and the papers of Yang, Yuille,\nand Lu (1993) and Intille and Bobick (1994). The plane sweep algorithm was ﬁrst popular-\nized by Collins (1996) and then generalized to a full arbitrary projective setting by Szeliski\nand Golland (1999) and Saito and Kanade (1999). Plane sweeps can also be formulated using\ncylindrical surfaces (Ishiguro, Yamamoto, and Tsuji 1992; Kang and Szeliski 1997; Shum\nand Szeliski 1999; Li, Shum, Tang et al. 2004; Zheng, Kang, Cohen et al. 2007) or even more\ngeneral topologies (Seitz 2001).\nOnce the topology for the cost volume or DSI has been set up, we need to compute the\nactual photoconsistency measures for each pixel and potential depth. A wide range of such\nmeasures have been proposed, as discussed in Section 11.3.1. Some of these are compared in\nrecent surveys and evaluations of matching costs (Scharstein and Szeliski 2002; Hirschm¨uller\nand Scharstein 2009).\nTo compute an actual depth map from these costs, some form of optimization or selection\ncriterion must be used. The simplest of these are sliding windows of various kinds, which",
  "image_path": "page_591.jpg",
  "pages": [
    590,
    591,
    592
  ]
}