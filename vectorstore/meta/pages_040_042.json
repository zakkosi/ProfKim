{
  "doc_id": "pages_040_042",
  "text": "18\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.10\nRecent examples of computer vision algorithms: (a) image-based rendering\n(Gortler, Grzeszczuk, Szeliski et al. 1996), (b) image-based modeling (Debevec, Taylor, and\nMalik 1996) c⃝1996 ACM, (c) interactive tone mapping (Lischinski, Farbman, Uyttendaele\net al. 2006a) (d) texture synthesis (Efros and Freeman 2001), (e) feature-based recognition\n(Fergus, Perona, and Zisserman 2007), (f) region-based recognition (Mori, Ren, Efros et al.\n2004) c⃝2004 IEEE.\n2000s.\nThis past decade has continued to see a deepening interplay between the vision and\ngraphics ﬁelds. In particular, many of the topics introduced under the rubric of image-based\nrendering, such as image stitching (see Chapter 9), light-ﬁeld capture and rendering (see\nSection 13.3), and high dynamic range (HDR) image capture through exposure bracketing\n(Figure1.5b) (see Section 10.2 and Mann and Picard 1995; Debevec and Malik 1997), were\nre-christened as computational photography (see Chapter 10) to acknowledge the increased\nuse of such techniques in everyday digital photography. For example, the rapid adoption of\nexposure bracketing to create high dynamic range images necessitated the development of\ntone mapping algorithms (Figure 1.10c) (see Section 10.2.1) to convert such images back\nto displayable results (Fattal, Lischinski, and Werman 2002; Durand and Dorsey 2002; Rein-\nhard, Stark, Shirley et al. 2002; Lischinski, Farbman, Uyttendaele et al. 2006a). In addition to\nmerging multiple exposures, techniques were developed to merge ﬂash images with non-ﬂash\ncounterparts (Eisemann and Durand 2004; Petschnigg, Agrawala, Hoppe et al. 2004) and to\ninteractively or automatically select different regions from overlapping images (Agarwala,\n1.3 Book overview\n19\nDontcheva, Agrawala et al. 2004).\nTexture synthesis (Figure 1.10d) (see Section 10.5), quilting (Efros and Leung 1999; Efros\nand Freeman 2001; Kwatra, Sch¨odl, Essa et al. 2003) and inpainting (Bertalmio, Sapiro,\nCaselles et al. 2000; Bertalmio, Vese, Sapiro et al. 2003; Criminisi, P´erez, and Toyama 2004)\nare additional topics that can be classiﬁed as computational photography techniques, since\nthey re-combine input image samples to produce new photographs.\nA second notable trend during this past decade has been the emergence of feature-based\ntechniques (combined with learning) for object recognition (see Section 14.3 and Ponce,\nHebert, Schmid et al. 2006). Some of the notable papers in this area include the constellation\nmodel of Fergus, Perona, and Zisserman (2007) (Figure 1.10e) and the pictorial structures\nof Felzenszwalb and Huttenlocher (2005). Feature-based techniques also dominate other\nrecognition tasks, such as scene recognition (Zhang, Marszalek, Lazebnik et al. 2007) and\npanorama and location recognition (Brown and Lowe 2007; Schindler, Brown, and Szeliski\n2007). And while interest point (patch-based) features tend to dominate current research,\nsome groups are pursuing recognition based on contours (Belongie, Malik, and Puzicha 2002)\nand region segmentation (Figure 1.10f) (Mori, Ren, Efros et al. 2004).\nAnother signiﬁcant trend from this past decade has been the development of more efﬁcient\nalgorithms for complex global optimization problems (see Sections 3.7 and B.5 and Szeliski,\nZabih, Scharstein et al. 2008; Blake, Kohli, and Rother 2010). While this trend began with\nwork on graph cuts (Boykov, Veksler, and Zabih 2001; Kohli and Torr 2007), a lot of progress\nhas also been made in message passing algorithms, such as loopy belief propagation (LBP)\n(Yedidia, Freeman, and Weiss 2001; Kumar and Torr 2006).\nThe ﬁnal trend, which now dominates a lot of the visual recognition research in our com-\nmunity, is the application of sophisticated machine learning techniques to computer vision\nproblems (see Section 14.5.1 and Freeman, Perona, and Sch¨olkopf 2008). This trend coin-\ncides with the increased availability of immense quantities of partially labelled data on the\nInternet, which makes it more feasible to learn object categories without the use of careful\nhuman supervision.\n1.3 Book overview\nIn the ﬁnal part of this introduction, I give a brief tour of the material in this book, as well\nas a few notes on notation and some additional general references. Since computer vision is\nsuch a broad ﬁeld, it is possible to study certain aspects of it, e.g., geometric image formation\nand 3D structure recovery, without engaging other parts, e.g., the modeling of reﬂectance and\nshading. Some of the chapters in this book are only loosely coupled with others, and it is not\nstrictly necessary to read all of the material in sequence.\n20\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nImages (2D)\nGeometry (3D) \nshape\nPhotometry \nappearance\n+\nvision\ngraphics\nimage processing\n2.1 Geometric \nimage formation\n2.2 Photometric \nimage formation\n2.3 Sampling\nand aliasing\n3 Image \nprocessing\n4 Feature \ndetection\n6 Feature-based \nalignment\n7 Structure \nfrom motion\n8 Motion\nestimation\n10 Computational \nphotography\n11 Stereo \ncorrespondence\n12 3D shape \nrecovery\n12 Texture \nrecovery\n13 Image-based \nrendering\n14 Recognition\n5 Segmentation\n9 Stitching\nFigure 1.11 Relationship between images, geometry, and photometry, as well as a taxonomy\nof the topics covered in this book. Topics are roughly positioned along the left–right axis\ndepending on whether they are more closely related to image-based (left), geometry-based\n(middle) or appearance-based (right) representations, and on the vertical axis by increasing\nlevel of abstraction. The whole ﬁgure should be taken with a large grain of salt, as there are\nmany additional subtle connections between topics not illustrated here.",
  "image_path": "page_041.jpg",
  "pages": [
    40,
    41,
    42
  ]
}