{
  "doc_id": "pages_438_440",
  "text": "416\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIntensity map\nAlpha map\nVelocity map\nIntensity map\nAlpha map\nVelocity map\nFrame 1\nFrame 2\nFrame 3\nFigure 8.14\nLayered motion estimation framework (Wang and Adelson 1994) c⃝1994\nIEEE: The top two rows describe the two layers, each of which consists of an intensity (color)\nimage, an alpha mask (black=transparent), and a parametric motion ﬁeld. The layers are com-\nposited with different amounts of motion to recreate the video sequence.\npixels near motion discontinuities. This makes them particularly suited as a representation\nfor image-based rendering (Section 13.2.1) (Shade, Gortler, He et al. 1998; Zitnick, Kang,\nUyttendaele et al. 2004) as well as object-level video editing.\nTo compute a layered representation of a video sequence, Wang and Adelson (1994) ﬁrst\nestimate afﬁne motion models over a collection of non-overlapping patches and then cluster\nthese estimates using k-means. They then alternate between assigning pixels to layers and\nrecomputing motion estimates for each layer using the assigned pixels, using a technique\nﬁrst proposed by Darrell and Pentland (1991). Once the parametric motions and pixel-wise\nlayer assignments have been computed for each frame independently, layers are constructed\nby warping and merging the various layer pieces from all of the frames together. Median\nﬁltering is used to produce sharp composite layers that are robust to small intensity variations,\nas well as to infer occlusion relationships between the layers. Figure 8.15 shows the results\nof this process on the ﬂower garden sequence. You can see both the initial and ﬁnal layer\nassignments for one of the frames, as well as the composite ﬂow and the alpha-matted layers\nwith their corresponding ﬂow vectors overlaid.\nIn follow-on work, Weiss and Adelson (1996) use a formal probabilistic mixture model\nto infer both the optimal number of layers and the per-pixel layer assignments. Weiss (1997)\n8.5 Layered motion\n417\ncolor image (input frame)\nﬂow\ninitial layers\nﬁnal layers\nlayers with pixel assignments and ﬂow\nFigure 8.15 Layered motion estimation results (Wang and Adelson 1994) c⃝1994 IEEE.\nfurther generalizes this approach by replacing the per-layer afﬁne motion models with smooth\nregularized per-pixel motion estimates, which allows the system to better handle curved and\nundulating layers, such as those seen in most real-world sequences.\nThe above approaches, however, still make a distinction between estimating the motions\nand layer assignments and then later estimating the layer colors. In the system described by\nBaker, Szeliski, and Anandan (1998), the generative model illustrated in Figure 8.14 is gen-\neralized to account for real-world rigid motion scenes. The motion of each frame is described\nusing a 3D camera model and the motion of each layer is described using a 3D plane equation\nplus per-pixel residual depth offsets (the plane plus parallax representation (Section 2.1.5)).\nThe initial layer estimation proceeds in a manner similar to that of Wang and Adelson (1994),\nexcept that rigid planar motions (homographies) are used instead of afﬁne motion models.\nThe ﬁnal model reﬁnement, however, jointly re-optimizes the layer pixel color and opacity\nvalues Ll and the 3D depth, plane, and motion parameters zl, nl, and P t by minimizing the\ndiscrepancy between the re-synthesized and observed motion sequences (Baker, Szeliski, and\nAnandan 1998).\nFigure 8.16 shows the ﬁnal results obtained with this algorithm. As you can see, the\nmotion boundaries and layer assignments are much crisper than those in Figure 8.15. Because\nof the per-pixel depth offsets, the individual layer color values are also sharper than those\nobtained with afﬁne or planar motion models. While the original system of Baker, Szeliski,\nand Anandan (1998) required a rough initial assignment of pixels to layers, Torr, Szeliski,\nand Anandan (2001) describe automated Bayesian techniques for initializing this system and\ndetermining the optimal number of layers.\nLayered motion estimation continues to be an active area of research. Representative pa-\npers in this area include (Sawhney and Ayer 1996; Jojic and Frey 2001; Xiao and Shah 2005;\nKumar, Torr, and Zisserman 2008; Thayananthan, Iwasaki, and Cipolla 2008; Schoenemann\nand Cremers 2008).\n418\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.16\nLayered stereo reconstruction (Baker, Szeliski, and Anandan 1998) c⃝1998\nIEEE: (a) ﬁrst and (b) last input images; (c) initial segmentation into six layers; (d) and\n(e) the six layer sprites; (f) depth map for planar sprites (darker denotes closer); front layer\n(g) before and (h) after residual depth estimation. Note that the colors for the ﬂower garden\nsequence are incorrect; the correct colors (yellow ﬂowers) are shown in Figure 8.15.\no\nOf course, layers are not the only way to introduce segmentation into motion estimation.\nA large number of algorithms have been developed that alternate between estimating optic\nﬂow vectors and segmenting them into coherent regions (Black and Jepson 1996; Ju, Black,\nand Jepson 1996; Chang, Tekalp, and Sezan 1997; M´emin and P´erez 2002; Cremers and\nSoatto 2005). Some of the more recent techniques rely on ﬁrst segmenting the input color\nimages and then estimating per-segment motions that produce a coherent motion ﬁeld while\nalso modeling occlusions (Zitnick, Kang, Uyttendaele et al. 2004; Zitnick, Jojic, and Kang\n2005; Stein, Hoiem, and Hebert 2007; Thayananthan, Iwasaki, and Cipolla 2008).\n8.5.1 Application: Frame interpolation\nFrame interpolation is another widely used application of motion estimation, often imple-\nmented in the same circuitry as de-interlacing hardware required to match an incoming video",
  "image_path": "page_439.jpg",
  "pages": [
    438,
    439,
    440
  ]
}