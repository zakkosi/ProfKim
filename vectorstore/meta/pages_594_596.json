{
  "doc_id": "pages_594_596",
  "text": "572\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 11.2: Rigid direct alignment\nModify your spline-based or optical ﬂow motion estima-\ntor from Exercise 8.4 to use epipolar geometry, i.e. to only estimate disparity.\n(Optional) Extend your algorithm to simultaneously estimate the epipolar geometry (with-\nout ﬁrst using point correspondences) by estimating a base homography corresponding to a\nreference plane for the dominant motion and then an epipole for the residual parallax (mo-\ntion).\nEx 11.3: Shape from proﬁles\nReconstruct a surface model from a series of edge images\n(Section 11.2.1).\n1. Extract edges and link them (Exercises 4.7–4.8).\n2. Based on previously computed epipolar geometry, match up edges in triplets (or longer\nsets) of images.\n3. Reconstruct the 3D locations of the curves using osculating circles (11.5).\n4. Render the resulting 3D surface model as a sparse mesh, i.e., drawing the reconstructed\n3D proﬁle curves and links between 3D points in neighboring images with similar\nosculating circles.\nEx 11.4: Plane sweep\nImplement a plane sweep algorithm (Section 11.1.2).\nIf the images are already pre-rectiﬁed, this consists simply of shifting images relative to\neach other and comparing pixels. If the images are not pre-rectiﬁed, compute the homography\nthat resamples the target image into the reference image’s coordinate system for each plane.\nEvaluate a subset of the following similarity measures (Section 11.3.1) and compare their\nperformance by visualizing the disparity space image (DSI), which should be dark for pixels\nat correct depths:\n• squared difference (SD);\n• absolute difference (AD);\n• truncated or robust measures;\n• gradient differences;\n• rank or census transform (the latter usually performs better);\n• mutual information from a pre-computed joint density function.\nConsider using the Birchﬁeld and Tomasi (1998) technique of comparing ranges between\nneighboring pixels (different shifted or warped images). Also, try pre-compensating images\nfor bias or gain variations using one or more of the techniques discussed in Section 11.3.1.\n11.8 Exercises\n573\nEx 11.5: Aggregation and window-based stereo\nImplement one or more of the matching\ncost aggregation strategies described in Section 11.4:\n• convolution with a box or Gaussian kernel;\n• shifting window locations by applying a min ﬁlter (Scharstein and Szeliski 2002);\n• picking a window that maximizes some match-reliability metric (Veksler 2001, 2003);\n• weighting pixels by their similarity to the central pixel (Yoon and Kweon 2006).\nOnce you have aggregated the costs in the DSI, pick the winner at each pixel (winner-take-\nall), and then optionally perform one or more of the following post-processing steps:\n1. compute matches both ways and pick only the reliable matches (draw the others in\nanother color);\n2. tag matches that are unsure (whose conﬁdence is too low);\n3. ﬁll in the matches that are unsure from neighboring values;\n4. reﬁne your matches to sub-pixel disparity by either ﬁtting a parabola to the DSI values\naround the winner or by using an iteration of Lukas–Kanade.\nEx 11.6: Optimization-based stereo\nCompute the disparity space image (DSI) volume us-\ning one of the techniques you implemented in Exercise 11.4 and then implement one (or more)\nof the global optimization techniques described in Section 11.5 to compute the depth map.\nPotential choices include:\n• dynamic programming or scanline optimization (relatively easy);\n• semi-global optimization (Hirschm¨uller 2008), which is a simple extension of scanline\noptimization and performs well;\n• graph cuts using alpha expansions (Boykov, Veksler, and Zabih 2001), for which you\nwill need to ﬁnd a max-ﬂow or min-cut algorithm (http://vision.middlebury.edu/stereo);\n• loopy belief propagation (Appendix B.5.3).\nEvaluate your algorithm by running it on the Middlebury stereo data sets.\nHow well does your algorithm do against local aggregation (Yoon and Kweon 2006)?\nCan you think of some extensions or modiﬁcations to make it even better?\n574\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 11.7: View interpolation, revisited\nCompute a dense depth map using one of the tech-\nniques you developed above and use it (or, better yet, a depth map for each source image) to\ngenerate smooth in-between views from a stereo data set.\nCompare your results against using the ground truth depth data (if available).\nWhat kinds of artifacts do you see? Can you think of ways to reduce them?\nMore details on implementing such algorithms can be found in Section 13.1 and Exercises\n13.1–13.4.\nEx 11.8: Multi-frame stereo\nExtend one of your previous techniques to use multiple input\nframes (Section 11.6) and try to improve the results you obtained with just two views.\nIf helpful, try using temporal selection (Kang and Szeliski 2004) to deal with the increased\nnumber of occlusions in multi-frame data sets.\nYou can also try to simultaneously estimate multiple depth maps and make them consis-\ntent (Kolmogorov and Zabih 2002; Kang and Szeliski 2004).\nTest your algorithms out on some standard multi-view data sets.\nEx 11.9: Volumetric stereo\nImplement voxel coloring (Seitz and Dyer 1999) as a simple\nextension to the plane sweep algorithm you implemented in Exercise 11.4.\n1. Instead of computing the complete DSI all at once, evaluate each plane one at a time\nfrom front to back.\n2. Tag every voxel whose photoconsistency is below a certain threshold as being part of\nthe object and remember its average (or robust) color (Seitz and Dyer 1999; Eisert,\nSteinbach, and Girod 2000; Kutulakos 2000; Slabaugh, Culbertson, Slabaugh et al.\n2004).\n3. Erase the input pixels corresponding to tagged voxels in the input images, e.g., by\nsetting their alpha value to 0 (or to some reduced number, depending on occupancy).\n4. As you evaluate the next plane, use the source image alpha values to modify your\nphotoconsistency score, e.g., only consider pixels that have full alpha or weight pixels\nby their alpha values.\n5. If the cameras are not all on the same side of your plane sweeps, use space carving\n(Kutulakos and Seitz 2000) to cycle through different subsets of source images while\ncarving away the volume from different directions.\nEx 11.10: Depth map merging\nUse the technique you developed for multi-frame stereo in\nExercise 11.8 or a different technique, such as the one described by Goesele, Snavely, Curless\net al. (2007), to compute a depth map for every input image.",
  "image_path": "page_595.jpg",
  "pages": [
    594,
    595,
    596
  ]
}