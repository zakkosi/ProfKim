{
  "doc_id": "pages_733_735",
  "text": "14.4 Category recognition\n711\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.48\nThe importance of context (images courtesy of Antonio Torralba). Can you\nname all of the objects in images (a–b), especially those that are circled in (c–d). Look\ncarefully at the circled objects. Did you notice that they all have the same shape (after being\nrotated), as shown in column (e)?\neveryone in a “burst mode” group shot. Leyvand, Cohen-Or, Dror et al. (2008) show how\naccurately locating facial features using an active shape model (Cootes, Edwards, and Taylor\n2001; Zhou, Gu, and Zhang 2003) can be used to warp such features (and hence the image)\ntowards conﬁgurations resembling those found in images whose facial attractiveness was\nhighly rated, thereby “beautifying” the image without completely losing a person’s identity.\nMost of these techniques rely either on a set of labeled training images, which is an\nessential component of all learning techniques, or the even more recent explosion in images\navailable on the Internet. The assumption in some of this work (and in recognition systems\nbased on such very large databases (Section 14.5.1)) is that as the collection of accessible (and\npotentially partially labeled) images gets larger, ﬁnding a close match gets easier. As Hays\nand Efros (2007) state in their abstract “Our chief insight is that while the space of images is\neffectively inﬁnite, the space of semantically differentiable scenes is actually not that large.”\nIn an interesting commentary on their paper, Levoy (2008) disputes this assertion, claiming\nthat “features in natural scenes form a heavy-tailed distribution, meaning that while some\nfeatures in photographs are more common than others, the relative occurrence of less common\nfeatures drops slowly. In other words, there are many unusual photographs in the world.” He\ndoes, however agree that in computational photography, as in many other applications such\nas speech recognition, synthesis, and translation, “simple machine learning algorithms often\noutperform more sophisticated ones if trained on large enough databases.” He also goes on\nto point out both the potential advantages of such systems, such as better automatic color\nbalancing, and potential issues and pitfalls with the kind of image fakery that these new\napproaches enable.\nFor additional examples of photo editing and computational photography applications\n712\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.49 More examples of context: read the letters in the ﬁrst group, the numbers in\nthe second, and the letters and numbers in the third. (Images courtesy of Antonio Torralba.)\nenabled by Internet computer vision, please see recent workshops on this topic,19 as well as\nthe special journal issue (Avidan, Baker, and Shan 2010), and the course on Internet Vision\nby Tamara Berg (2008).\n14.5 Context and scene understanding\nThus far, we have mostly considered the task of recognizing and localizing objects in isolation\nfrom that of understanding the scene (context) in which the object occur. This is a severe\nlimitation, as context plays a very important role in human object recognition (Oliva and\nTorralba 2007). As we will see in this section, it can greatly improve the performance of\nobject recognition algorithms (Divvala, Hoiem, Hays et al. 2009), as well as providing useful\nsemantic clues for general scene understanding (Torralba 2008).\nConsider the two photographs in Figure 14.48a–b. Can you name all of the objects,\nespecially those circled in images (c–d)? Now have a closer look at the circled objects.\nDo see any similarity in their shapes? In fact, if you rotate them by 90◦, they are all the\nsame as the “blob” shown in Figure 14.48e. So much for our ability to recognize object by\ntheir shape! Another (perhaps more artiﬁcial) example of recognition in context is shown in\nFigure 14.49. Try to name all of the letters and numbers, and then see if you guessed right.\nEven though we have not addressed context explicitly earlier in this chapter, we have\nalready seen several instances of this general idea being used. A simple way to incorporate\nspatial information into a recognition algorithm is to compute feature statistics over different\nregions, as in the spatial pyramid system of Lazebnik, Schmid, and Ponce (2006). Part-based\nmodels (Section 14.4.2, Figures 14.40–14.43), use a kind of local context, where various parts\nneed to be arranged in a proper geometric relationship to constitute an object.\nThe biggest difference between part-based and context models is that the latter combine\nobjects into scenes and the number of constituent objects from each class is not known in\nadvance. In fact, it is possible to combine part-based and context models into the same\n19 http://www.internetvisioner.org/.\n14.5 Context and scene understanding\n713\n(a)\n(b)\n(c)\nFigure 14.50 Contextual scene models for object recognition (Sudderth, Torralba, Freeman\net al. 2008) c⃝2008 Springer: (a) some street scenes and their corresponding labels (magenta\n= buildings, red = cars, green = trees, blue = road); (b) some ofﬁce scenes (red = computer\nscreen, green = keyboard, blue = mouse); (c) learned contextual models built from these\nlabeled scenes. The top row shows a sample label image and the distribution of the objects\nrelative to the center red (car or screen) object. The bottom rows show the distributions of\nparts that make up each object.\nrecognition architecture (Murphy, Torralba, and Freeman 2003; Sudderth, Torralba, Freeman\net al. 2008; Crandall and Huttenlocher 2007).\nConsider the street and ofﬁce scenes shown in Figure 14.50a–b. If we have enough train-\ning images with labeled regions, such as buildings, cars, and roads or monitors, keyboards,\nand mice, we can develop a geometric model for describing their relative positions. Sud-\nderth, Torralba, Freeman et al. (2008) develop such a model, which can be thought of as a\ntwo-level constellation model. At the top level, the distributions of objects relative to each\nother (say, buildings with respect to cars) is modeled as a Gaussian (Figure 14.50c, upper\nright corners). At the bottom level, the distribution of parts (afﬁne covariant features) with\nrespect to the object center is modeled using a mixture of Gaussians (Figure 14.50c, lower\ntwo rows). However, since the number of objects in the scene and parts in each object is\nunknown, a latent Dirichlet process (LDP) is used to model object and part creation in a gen-\nerative framework. The distributions for all of the objects and parts are learned from a large\nlabeled database and then later used during inference (recognition) to label the elements of a\nscene.\nAnother example of context is in simultaneous segmentation and recognition (Section 14.4.3)",
  "image_path": "page_734.jpg",
  "pages": [
    733,
    734,
    735
  ]
}