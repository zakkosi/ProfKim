{
  "doc_id": "pages_117_119",
  "text": "2.5 Exercises\n95\nEx 2.3: 3D viewer\nWrite a simple viewer for 3D points, lines, and polygons. Import a set\nof point and line commands (primitives) as well as a viewing transform. Interactively modify\nthe object or camera transform. This viewer can be an extension of the one you created in\n(Exercise 2.2). Simply replace the viewing transformations with their 3D equivalents.\n(Optional) Add a z-buffer to do hidden surface removal for polygons.\n(Optional) Use a 3D drawing package and just write the viewer control.\nEx 2.4: Focus distance and depth of ﬁeld\nFigure out how the focus distance and depth of\nﬁeld indicators on a lens are determined.\n1. Compute and plot the focus distance zo as a function of the distance traveled from the\nfocal length ∆zi = f −zi for a lens of focal length f (say, 100mm). Does this explain\nthe hyperbolic progression of focus distances you see on a typical lens (Figure 2.20)?\n2. Compute the depth of ﬁeld (minimum and maximum focus distances) for a given focus\nsetting zo as a function of the circle of confusion diameter c (make it a fraction of\nthe sensor width), the focal length f, and the f-stop number N (which relates to the\naperture diameter d). Does this explain the usual depth of ﬁeld markings on a lens that\nbracket the in-focus marker, as in Figure 2.20a?\n3. Now consider a zoom lens with a varying focal length f. Assume that as you zoom,\nthe lens stays in focus, i.e., the distance from the rear nodal point to the sensor plane\nzi adjusts itself automatically for a ﬁxed focus distance zo. How do the depth of ﬁeld\nindicators vary as a function of focal length? Can you reproduce a two-dimensional\nplot that mimics the curved depth of ﬁeld lines seen on the lens in Figure 2.20b?\nEx 2.5: F-numbers and shutter speeds\nList the common f-numbers and shutter speeds\nthat your camera provides. On older model SLRs, they are visible on the lens and shut-\nter speed dials. On newer cameras, you have to look at the electronic viewﬁnder (or LCD\nscreen/indicator) as you manually adjust exposures.\n1. Do these form geometric progressions; if so, what are the ratios? How do these relate\nto exposure values (EVs)?\n2. If your camera has shutter speeds of 1\n60 and\n1\n125, do you think that these two speeds are\nexactly a factor of two apart or a factor of 125/60 = 2.083 apart?\n3. How accurate do you think these numbers are? Can you devise some way to measure\nexactly how the aperture affects how much light reaches the sensor and what the exact\nexposure times actually are?\n96\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 2.6: Noise level calibration\nEstimate the amount of noise in your camera by taking re-\npeated shots of a scene with the camera mounted on a tripod. (Purchasing a remote shutter\nrelease is a good investment if you own a DSLR.) Alternatively, take a scene with constant\ncolor regions (such as a color checker chart) and estimate the variance by ﬁtting a smooth\nfunction to each color region and then taking differences from the predicted function.\n1. Plot your estimated variance as a function of level for each of your color channels\nseparately.\n2. Change the ISO setting on your camera; if you cannot do that, reduce the overall light\nin your scene (turn off lights, draw the curtains, wait until dusk). Does the amount of\nnoise vary a lot with ISO/gain?\n3. Compare your camera to another one at a different price point or year of make. Is\nthere evidence to suggest that “you get what you pay for”? Does the quality of digital\ncameras seem to be improving over time?\nEx 2.7: Gamma correction in image stitching\nHere’s a relatively simple puzzle. Assume\nyou are given two images that are part of a panorama that you want to stitch (see Chapter 9).\nThe two images were taken with different exposures, so you want to adjust the RGB values\nso that they match along the seam line. Is it necessary to undo the gamma in the color values\nin order to achieve this?\nEx 2.8: Skin color detection\nDevise a simple skin color detector (Forsyth and Fleck 1999;\nJones and Rehg 2001; Vezhnevets, Sazonov, and Andreeva 2003; Kakumanu, Makrogiannis,\nand Bourbakis 2007) based on chromaticity or other color properties.\n1. Take a variety of photographs of people and calculate the xy chromaticity values for\neach pixel.\n2. Crop the photos or otherwise indicate with a painting tool which pixels are likely to be\nskin (e.g. face and arms).\n3. Calculate a color (chromaticity) distribution for these pixels. You can use something as\nsimple as a mean and covariance measure or as complicated as a mean-shift segmenta-\ntion algorithm (see Section 5.3.2). You can optionally use non-skin pixels to model the\nbackground distribution.\n4. Use your computed distribution to ﬁnd the skin regions in an image. One easy way to\nvisualize this is to paint all non-skin pixels a given color, such as white or black.\n5. How sensitive is your algorithm to color balance (scene lighting)?\n2.5 Exercises\n97\n6. Does a simpler chromaticity measurement, such as a color ratio (2.116), work just as\nwell?\nEx 2.9: White point balancing—tricky\nA common (in-camera or post-processing) tech-\nnique for performing white point adjustment is to take a picture of a white piece of paper and\nto adjust the RGB values of an image to make this a neutral color.\n1. Describe how you would adjust the RGB values in an image given a sample “white\ncolor” of (Rw, Gw, Bw) to make this color neutral (without changing the exposure too\nmuch).\n2. Does your transformation involve a simple (per-channel) scaling of the RGB values or\ndo you need a full 3 × 3 color twist matrix (or something else)?\n3. Convert your RGB values to XYZ. Does the appropriate correction now only depend\non the XY (or xy) values? If so, when you convert back to RGB space, do you need a\nfull 3 × 3 color twist matrix to achieve the same effect?\n4. If you used pure diagonal scaling in the direct RGB mode but end up with a twist if you\nwork in XYZ space, how do you explain this apparent dichotomy? Which approach is\ncorrect? (Or is it possible that neither approach is actually correct?)\nIf you want to ﬁnd out what your camera actually does, continue on to the next exercise.\nEx 2.10: In-camera color processing—challenging\nIf your camera supports a RAW pixel\nmode, take a pair of RAW and JPEG images, and see if you can infer what the camera is doing\nwhen it converts the RAW pixel values to the ﬁnal color-corrected and gamma-compressed\neight-bit JPEG pixel values.\n1. Deduce the pattern in your color ﬁlter array from the correspondence between co-\nlocated RAW and color-mapped pixel values. Use a color checker chart at this stage\nif it makes your life easier. You may ﬁnd it helpful to split the RAW image into four\nseparate images (subsampling even and odd columns and rows) and to treat each of\nthese new images as a “virtual” sensor.\n2. Evaluate the quality of the demosaicing algorithm by taking pictures of challenging\nscenes which contain strong color edges (such as those shown in in Section 10.3.1).\n3. If you can take the same exact picture after changing the color balance values in your\ncamera, compare how these settings affect this processing.\n4. Compare your results against those presented by Chakrabarti, Scharstein, and Zickler\n(2009) or use the data available in their database of color images.26\n26 http://vision.middlebury.edu/color/.",
  "image_path": "page_118.jpg",
  "pages": [
    117,
    118,
    119
  ]
}