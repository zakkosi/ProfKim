{
  "doc_id": "pages_719_721",
  "text": "14.4 Category recognition\n697\nFigure 14.36 A typical processing pipeline for a bag-of-words category recognition system\n(Csurka, Dance, Perronnin et al. 2006) c⃝2007 Springer. Features are ﬁrst extracted at\nkeypoints and then quantized to get a distribution (histogram) over the learned visual words\n(feature cluster centers). The feature distribution histogram is used to learn a decision surface\nusing a classiﬁcation algorithm, such as a support vector machine.\nIn this section, we look at a number of approaches to solving category recognition. While\nhistorically, part-based representations and recognition algorithms (Section 14.4.2) were the\npreferred approach (Fischler and Elschlager 1973; Felzenszwalb and Huttenlocher 2005;\nFergus, Perona, and Zisserman 2007), we begin by describing simpler bag-of-features ap-\nproaches (Section 14.4.1) that represent objects and images as unordered collections of fea-\nture descriptors. We then look at the problem of simultaneously segmenting images while\nrecognizing objects (Section 14.4.3) and also present some applications of such techniques to\nphoto manipulation (Section 14.4.4). In Section 14.5, we look at how context and scene un-\nderstanding, as well as machine learning, can improve overall recognition results. Additional\ndetails on the techniques presented in this section can be found in (Pinz 2005; Ponce, Hebert,\nSchmid et al. 2006; Dickinson, Leonardis, Schiele et al. 2007; Fei-Fei, Fergus, and Torralba\n2009).\n14.4.1 Bag of words\nOne of the simplest algorithms for category recognition is the bag of words (also known as\nbag of features or bag of keypoints) approach (Csurka, Dance, Fan et al. 2004; Lazebnik,\nSchmid, and Ponce 2006; Csurka, Dance, Perronnin et al. 2006; Zhang, Marszalek, Lazeb-\nnik et al. 2007). As shown in Figure 14.36, this algorithm simply computes the distribu-\ntion (histogram) of visual words found in the query image and compares this distribution\nto those found in the training images. We have already seen elements of this approach in\nSection 14.3.2, Equations (14.33–14.35) and Algorithm 14.2. The biggest difference from\ninstance recognition is the absence of a geometric veriﬁcation stage (Section 14.3.1), since\nindividual instances of generic visual categories, such as those shown in Figure 14.35, have\nrelatively little spatial coherence to their features (but see the work by Lazebnik, Schmid, and\n698\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPonce (2006)).\nCsurka, Dance, Fan et al. (2004) were the ﬁrst to use the term bag of keypoints to describe\nsuch approaches and among the ﬁrst to demonstrate the utility of frequency-based techniques\nfor category recognition. Their original system used afﬁne covariant regions and SIFT de-\nscriptors, k-means visual vocabulary construction, and both a na¨ıve Bayesian classiﬁer and\nsupport vector machines for classiﬁcation. (The latter was found to perform better.) Their\nnewer system (Csurka, Dance, Perronnin et al. 2006) uses regular (non-afﬁne) SIFT patches,\nboosting instead of SVMs, and incorporates a small amount of geometric consistency infor-\nmation.\nZhang, Marszalek, Lazebnik et al. (2007) perform a more detailed study of such bag of\nfeatures systems. They compare a number of feature detectors (Harris–Laplace (Mikolajczyk\nand Schmid 2004) and Laplacian (Lindeberg 1998b)), descriptors (SIFT, RIFT, and SPIN\n(Lazebnik, Schmid, and Ponce 2005)), and SVM kernel functions. To estimate distances for\nthe kernel function, they form an image signature\nS = ((t1, m1), . . . , (tm, mm)),\n(14.36)\nanalogous to the tf-idf vector t in (14.34), where the cluster centers mi are made explicit.\nThey then investigate two different kernels for comparing such image signatures. The ﬁrst is\nthe earth mover’s distance (EMD) (Rubner, Tomasi, and Guibas 2000),\nEMD(S, S′) =\nP\ni\nP\nj fijd(mi, m′\nj)\nP\ni\nP\nj fij\n,\n(14.37)\nwhere fij is a ﬂow value that can be computed using a linear program and d(mi, m′\nj) is the\nground distance (Euclidean distance) between mi and m′\nj. Note that the EMD can be used\nto compare two signatures of different lengths, where the entries do not need to correspond.\nThe second is a χ2 distance\nχ2(S, S′) = 1\n2\nX\ni\n(ti −t′\ni)2\nti + t′\ni\n,\n(14.38)\nwhich measures the likelihood that the two signatures were generated from consistent random\nprocesses. These distance metrics are then converted into SVM kernels using a generalized\nGaussian kernel\nK(S, S′) = exp\n\u0012\n−1\nAD(S, S′)\n\u0013\n,\n(14.39)\nwhere A is a scaling parameter set to the mean distance between training images. In their\nexperiments, they ﬁnd that the EMD works best for visual category recognition and the χ2\nmeasure is best for texture recognition.\n14.4 Category recognition\n699\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nlevel 2\nlevel 1\nlevel 0\n\u0001 1/4\n\u0001 1/4\n\u0001 1/2\n+\n+\n+\n(a)\n(b)\nFigure 14.37\nComparing collections of feature vectors using pyramid matching. (a) The\nfeature-space pyramid match kernel (Grauman and Darrell 2007b) constructs a pyramid in\nhigh-dimensional feature space and uses it to compute distances (and implicit correspon-\ndences) between sets of feature vectors. (b) Spatial pyramid matching (Lazebnik, Schmid,\nand Ponce 2006) c⃝2006 IEEE divides the image into a pyramid of pooling regions and\ncomputes separate visual word histograms (distributions) inside each spatial bin.\nInstead of quantizing feature vectors to visual words, Grauman and Darrell (2007b) de-\nvelop a technique for directly computing an approximate distance between two variably sized\ncollections of feature vectors. Their approach is to bin the feature vectors into a multi-\nresolution pyramid deﬁned in feature space (Figure 14.37a) and count the number of features\nthat land in corresponding bins Bil and B′\nil (Figure 14.38a–c). The distance between the two\nsets of feature vectors (which can be thought of as points in a high-dimensional space) is\ncomputed using histogram intersection between corresponding bins\nCl =\nX\ni\nmin(Bil, B′\nil)\n(14.40)\n(Figure 14.38d). These per-level counts are then summed up in a weighted fashion\nD∆=\nX\nl\nwlNl\nwith\nNl = Cl −Cl−1\nand\nwl =\n1\nd2l\n(14.41)\n(Figure 14.38e), which discounts matches already found at ﬁner levels while weighting ﬁner\nmatches more heavily. (d is the dimension of the embedding space, i.e., the length of the\nfeature vectors.) In follow-on work, Grauman and Darrell (2007a) show how an explicit\nconstruction of the pyramid can be avoided using hashing techniques.\nInspired by this work, Lazebnik, Schmid, and Ponce (2006) show how a similar idea\ncan be employed to augment bags of keypoints with loose notions of 2D spatial location",
  "image_path": "page_720.jpg",
  "pages": [
    719,
    720,
    721
  ]
}