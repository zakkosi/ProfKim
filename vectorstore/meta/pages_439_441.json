{
  "doc_id": "pages_439_441",
  "text": "8.5 Layered motion\n417\ncolor image (input frame)\nﬂow\ninitial layers\nﬁnal layers\nlayers with pixel assignments and ﬂow\nFigure 8.15 Layered motion estimation results (Wang and Adelson 1994) c⃝1994 IEEE.\nfurther generalizes this approach by replacing the per-layer afﬁne motion models with smooth\nregularized per-pixel motion estimates, which allows the system to better handle curved and\nundulating layers, such as those seen in most real-world sequences.\nThe above approaches, however, still make a distinction between estimating the motions\nand layer assignments and then later estimating the layer colors. In the system described by\nBaker, Szeliski, and Anandan (1998), the generative model illustrated in Figure 8.14 is gen-\neralized to account for real-world rigid motion scenes. The motion of each frame is described\nusing a 3D camera model and the motion of each layer is described using a 3D plane equation\nplus per-pixel residual depth offsets (the plane plus parallax representation (Section 2.1.5)).\nThe initial layer estimation proceeds in a manner similar to that of Wang and Adelson (1994),\nexcept that rigid planar motions (homographies) are used instead of afﬁne motion models.\nThe ﬁnal model reﬁnement, however, jointly re-optimizes the layer pixel color and opacity\nvalues Ll and the 3D depth, plane, and motion parameters zl, nl, and P t by minimizing the\ndiscrepancy between the re-synthesized and observed motion sequences (Baker, Szeliski, and\nAnandan 1998).\nFigure 8.16 shows the ﬁnal results obtained with this algorithm. As you can see, the\nmotion boundaries and layer assignments are much crisper than those in Figure 8.15. Because\nof the per-pixel depth offsets, the individual layer color values are also sharper than those\nobtained with afﬁne or planar motion models. While the original system of Baker, Szeliski,\nand Anandan (1998) required a rough initial assignment of pixels to layers, Torr, Szeliski,\nand Anandan (2001) describe automated Bayesian techniques for initializing this system and\ndetermining the optimal number of layers.\nLayered motion estimation continues to be an active area of research. Representative pa-\npers in this area include (Sawhney and Ayer 1996; Jojic and Frey 2001; Xiao and Shah 2005;\nKumar, Torr, and Zisserman 2008; Thayananthan, Iwasaki, and Cipolla 2008; Schoenemann\nand Cremers 2008).\n418\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.16\nLayered stereo reconstruction (Baker, Szeliski, and Anandan 1998) c⃝1998\nIEEE: (a) ﬁrst and (b) last input images; (c) initial segmentation into six layers; (d) and\n(e) the six layer sprites; (f) depth map for planar sprites (darker denotes closer); front layer\n(g) before and (h) after residual depth estimation. Note that the colors for the ﬂower garden\nsequence are incorrect; the correct colors (yellow ﬂowers) are shown in Figure 8.15.\no\nOf course, layers are not the only way to introduce segmentation into motion estimation.\nA large number of algorithms have been developed that alternate between estimating optic\nﬂow vectors and segmenting them into coherent regions (Black and Jepson 1996; Ju, Black,\nand Jepson 1996; Chang, Tekalp, and Sezan 1997; M´emin and P´erez 2002; Cremers and\nSoatto 2005). Some of the more recent techniques rely on ﬁrst segmenting the input color\nimages and then estimating per-segment motions that produce a coherent motion ﬁeld while\nalso modeling occlusions (Zitnick, Kang, Uyttendaele et al. 2004; Zitnick, Jojic, and Kang\n2005; Stein, Hoiem, and Hebert 2007; Thayananthan, Iwasaki, and Cipolla 2008).\n8.5.1 Application: Frame interpolation\nFrame interpolation is another widely used application of motion estimation, often imple-\nmented in the same circuitry as de-interlacing hardware required to match an incoming video\n8.5 Layered motion\n419\nto a monitor’s actual refresh rate. As with de-interlacing, information from novel in-between\nframes needs to be interpolated from preceding and subsequent frames. The best results can\nbe obtained if an accurate motion estimate can be computed at each unknown pixel’s lo-\ncation. However, in addition to computing the motion, occlusion information is critical to\nprevent colors from being contaminated by moving foreground objects that might obscure a\nparticular pixel in a preceding or subsequent frame.\nIn a little more detail, consider Figure 8.13c and assume that the arrows denote keyframes\nbetween which we wish to interpolate additional images. The orientations of the streaks\nin this ﬁgure encode the velocities of individual pixels. If the same motion estimate u0 is\nobtained at location x0 in image I0 as is obtained at location x0 + u0 in image I1, the ﬂow\nvectors are said to be consistent. This motion estimate can be transferred to location x0 +tu0\nin the image It being generated, where t ∈(0, 1) is the time of interpolation. The ﬁnal color\nvalue at pixel x0 + tu0 can be computed as a linear blend,\nIt(x0 + tu0) = (1 −t)I0(x0) + tI1(x0 + u0).\n(8.72)\nIf, however, the motion vectors are different at corresponding locations, some method must\nbe used to determine which is correct and which image contains colors that are occluded.\nThe actual reasoning is even more subtle than this. One example of such an interpolation\nalgorithm, based on earlier work in depth map interpolation (Shade, Gortler, He et al. 1998;\nZitnick, Kang, Uyttendaele et al. 2004) which is the one used in the ﬂow evaluation paper of\nBaker, Black, Lewis et al. (2007); Baker, Scharstein, Lewis et al. (2009). An even higher-\nquality frame interpolation algorithm, which uses gradient-based reconstruction, is presented\nby Mahajan, Huang, Matusik et al. (2009).\n8.5.2 Transparent layers and reﬂections\nA special case of layered motion that occurs quite often is transparent motion, which is usu-\nally caused by reﬂections seen in windows and picture frames (Figures 8.17 and 8.18).\nSome of the early work in this area handles transparent motion by either just estimating\nthe component motions (Shizawa and Mase 1991; Bergen, Burt, Hingorani et al. 1992; Darrell\nand Simoncelli 1993; Irani, Rousso, and Peleg 1994) or by assigning individual pixels to\ncompeting motion layers (Darrell and Pentland 1995; Black and Anandan 1996; Ju, Black,\nand Jepson 1996), which is appropriate for scenes partially seen through a ﬁne occluder\n(e.g., foliage). However, to accurately separate truly transparent layers, a better model for\nmotion due to reﬂections is required. Because of the way that light is both reﬂected from\nand transmitted through a glass surface, the correct model for reﬂections is an additive one,\nwhere each moving layer contributes some intensity to the ﬁnal image (Szeliski, Avidan, and\nAnandan 2000).",
  "image_path": "page_440.jpg",
  "pages": [
    439,
    440,
    441
  ]
}