{
  "doc_id": "pages_458_460",
  "text": "436\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 9.5 Gap closing (Szeliski and Shum 1997) c⃝1997 ACM: (a) A gap is visible when\nthe focal length is wrong (f = 510). (b) No gap is visible for the correct focal length\n(f = 468).\nUnfortunately, this particular gap-closing heuristic only works for the kind of “one-dimensional”\npanorama where the camera is continuously turning in the same direction. In Section 9.2, we\ndescribe a different approach to removing gaps and overlaps that works for arbitrary camera\nmotions.\n9.1.5 Application: Video summarization and compression\nAn interesting application of image stitching is the ability to summarize and compress videos\ntaken with a panning camera. This application was ﬁrst suggested by Teodosio and Bender\n(1993), who called their mosaic-based summaries salient stills. These ideas were then ex-\ntended by Irani, Hsu, and Anandan (1995), Kumar, Anandan, Irani et al. (1995), and Irani and\nAnandan (1998) to additional applications, such as video compression and video indexing.\nWhile these early approaches used afﬁne motion models and were therefore restricted to long\nfocal lengths, the techniques were generalized by Lee, ge Chen, lung Bruce Lin et al. (1997)\nto full eight-parameter homographies and incorporated into the MPEG-4 video compression\nstandard, where the stitched background layers were called video sprites (Figure 9.6).\nWhile video stitching is in many ways a straightforward generalization of multiple-image\nstitching (Steedly, Pal, and Szeliski 2005; Baudisch, Tan, Steedly et al. 2006), the potential\npresence of large amounts of independent motion, camera zoom, and the desire to visualize\ndynamic events impose additional challenges. For example, moving foreground objects can\noften be removed using median ﬁltering. Alternatively, foreground objects can be extracted\ninto a separate layer (Sawhney and Ayer 1996) and later composited back into the stitched\npanoramas, sometimes as multiple instances to give the impressions of a “Chronophotograph”\n9.1 Motion models\n437\n+\n+\n+ · · · +\n=\nFigure 9.6 Video stitching the background scene to create a single sprite image that can be\ntransmitted and used to re-create the background in each frame (Lee, ge Chen, lung Bruce Lin\net al. 1997) c⃝1997 IEEE.\n(Massey and Bender 1996) and sometimes as video overlays (Irani and Anandan 1998).\nVideos can also be used to create animated panoramic video textures (Section 13.5.2), in\nwhich different portions of a panoramic scene are animated with independently moving video\nloops (Agarwala, Zheng, Pal et al. 2005; Rav-Acha, Pritch, Lischinski et al. 2005), or to shine\n“video ﬂashlights” onto a composite mosaic of a scene (Sawhney, Arpa, Kumar et al. 2002).\nVideo can also provide an interesting source of content for creating panoramas taken from\nmoving cameras. While this invalidates the usual assumption of a single point of view (opti-\ncal center), interesting results can still be obtained. For example, the VideoBrush system of\nSawhney, Kumar, Gendel et al. (1998) uses thin strips taken from the center of the image to\ncreate a panorama taken from a horizontally moving camera. This idea can be generalized\nto other camera motions and compositing surfaces using the concept of mosaics on adap-\ntive manifold (Peleg, Rousso, Rav-Acha et al. 2000), and also used to generate panoramic\nstereograms (Peleg, Ben-Ezra, and Pritch 2001). Related ideas have been used to create\npanoramic matte paintings for multi-plane cel animation (Wood, Finkelstein, Hughes et al.\n1997), for creating stitched images of scenes with parallax (Kumar, Anandan, Irani et al.\n1995), and as 3D representations of more complex scenes using multiple-center-of-projection\nimages (Rademacher and Bishop 1998) and multi-perspective panoramas (Rom´an, Garg, and\nLevoy 2004; Rom´an and Lensch 2006; Agarwala, Agrawala, Cohen et al. 2006).\nAnother interesting variant on video-based panoramas are concentric mosaics (Section 13.3.3)\n(Shum and He 1999). Here, rather than trying to produce a single panoramic image, the com-\nplete original video is kept and used to re-synthesize views (from different camera origins)\nusing ray remapping (light ﬁeld rendering), thus endowing the panorama with a sense of 3D\n438\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z)\nx = (sinθ,h,cosθ)\nθ\nh\nx\ny\np = (X,Y,Z)\nx = (sinθ cosφ, sinφ,\ncosθ cosφ)\nθ\nφ\nx\ny\n(a)\n(b)\nFigure 9.7 Projection from 3D to (a) cylindrical and (b) spherical coordinates.\ndepth. The same data set can also be used to explicitly reconstruct the depth using multi-\nbaseline stereo (Peleg, Ben-Ezra, and Pritch 2001; Li, Shum, Tang et al. 2004; Zheng, Kang,\nCohen et al. 2007).\n9.1.6 Cylindrical and spherical coordinates\nAn alternative to using homographies or 3D motions to align images is to ﬁrst warp the images\ninto cylindrical coordinates and then use a pure translational model to align them (Chen 1995;\nSzeliski 1996). Unfortunately, this only works if the images are all taken with a level camera\nor with a known tilt angle.\nAssume for now that the camera is in its canonical position, i.e., its rotation matrix is the\nidentity, R = I, so that the optical axis is aligned with the z axis and the y axis is aligned\nvertically. The 3D ray corresponding to an (x, y) pixel is therefore (x, y, f).\nWe wish to project this image onto a cylindrical surface of unit radius (Szeliski 1996).\nPoints on this surface are parameterized by an angle θ and a height h, with the 3D cylindrical\ncoordinates corresponding to (θ, h) given by\n(sin θ, h, cos θ) ∝(x, y, f),\n(9.12)\nas shown in Figure 9.7a. From this correspondence, we can compute the formula for the\nwarped or mapped coordinates (Szeliski and Shum 1997),\nx′\n=\nsθ = s tan−1 x\nf ,\n(9.13)\ny′\n=\nsh = s\ny\np\nx2 + f 2 ,\n(9.14)\nwhere s is an arbitrary scaling factor (sometimes called the radius of the cylinder) that can be\nset to s = f to minimize the distortion (scaling) near the center of the image.5 The inverse of\n5 The scale can also be set to a larger or smaller value for the ﬁnal compositing surface, depending on the desired\noutput panorama resolution—see Section 9.3.",
  "image_path": "page_459.jpg",
  "pages": [
    458,
    459,
    460
  ]
}