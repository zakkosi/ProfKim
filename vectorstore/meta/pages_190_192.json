{
  "doc_id": "pages_190_192",
  "text": "168\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\npyramid (Figure 3.32), where each level is pre-ﬁltered with a high-quality ﬁlter rather than\na poorer quality approximation, such as Burt and Adelson’s (1983b) ﬁve-tap binomial. To\nresample an image from a MIP-map, a scalar estimate of the resampling rate r is ﬁrst com-\nputed. For example, r can be the maximum of the absolute values in A (which suppresses\naliasing) or it can be the minimum (which reduces blurring). Akenine-M¨oller and Haines\n(2002) discuss these issues in more detail.\nOnce a resampling rate has been speciﬁed, a fractional pyramid level is computed using\nthe base 2 logarithm,\nl = log2 r.\n(3.91)\nOne simple solution is to resample the texture from the next higher or lower pyramid level,\ndepending on whether it is preferable to reduce aliasing or blur. A better solution is to re-\nsample both images and blend them linearly using the fractional component of l. Since most\nMIP-map implementations use bilinear resampling within each level, this approach is usu-\nally called trilinear MIP-mapping. Computer graphics rendering APIs, such as OpenGL and\nDirect3D, have parameters that can be used to select which variant of MIP-mapping (and of\nthe sampling rate r computation) should be used, depending on the desired tradeoff between\nspeed and quality. Exercise 3.22 has you examine some of these tradeoffs in more detail.\nElliptical Weighted Average\nThe Elliptical Weighted Average (EWA) ﬁlter invented by Greene and Heckbert (1986) is\nbased on the observation that the afﬁne mapping x = Ax′ deﬁnes a skewed two-dimensional\ncoordinate system in the vicinity of each source pixel x (Figure 3.48a). For every destina-\ntion pixel x′, the ellipsoidal projection of a small pixel grid in x′ onto x is computed (Fig-\nure 3.48b). This is then used to ﬁlter the source image g(x) with a Gaussian whose inverse\ncovariance matrix is this ellipsoid.\nDespite its reputation as a high-quality ﬁlter (Akenine-M¨oller and Haines 2002), we have\nfound in our work (Szeliski, Winder, and Uyttendaele 2010) that because a Gaussian kernel\nis used, the technique suffers simultaneously from both blurring and aliasing, compared to\nhigher-quality ﬁlters. The EWA is also quite slow, although faster variants based on MIP-\nmapping have been proposed (Szeliski, Winder, and Uyttendaele (2010) provide some addi-\ntional references).\nAnisotropic ﬁltering\nAn alternative approach to ﬁltering oriented textures, which is sometimes implemented in\ngraphics hardware (GPUs), is to use anisotropic ﬁltering (Barkans 1997; Akenine-M¨oller and\nHaines 2002). In this approach, several samples at different resolutions (fractional levels in\nthe MIP-map) are combined along the major axis of the EWA Gaussian (Figure 3.48c).\n3.6 Geometric transformations\n169\nH2\ni\nf\nx\nx\nx\ni\nf’\ng1\ng2\ng3\nu\nF\nu\nG1\nu\nG2\nu\nG3\nu\nF’\nH1\ninterpolate\n* h1(x)\nwarp\nax+t\nfilter\n* h2(x)\nsample\n* δ(x)\n(f)\n(g)\n(h)\n(i)\n(j)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 3.49 One-dimensional signal resampling (Szeliski, Winder, and Uyttendaele 2010):\n(a) original sampled signal f(i); (b) interpolated signal g1(x); (c) warped signal g2(x); (d)\nﬁltered signal g3(x); (e) sampled signal f ′(i). The corresponding spectra are shown below\nthe signals, with the aliased portions shown in red.\nMulti-pass transforms\nThe optimal approach to warping images without excessive blurring or aliasing is to adap-\ntively pre-ﬁlter the source image at each pixel using an ideal low-pass ﬁlter, i.e., an oriented\nskewed sinc or low-order (e.g., cubic) approximation (Figure 3.48a). Figure 3.49 shows how\nthis works in one dimension. The signal is ﬁrst (theoretically) interpolated to a continuous\nwaveform, (ideally) low-pass ﬁltered to below the new Nyquist rate, and then re-sampled to\nthe ﬁnal desired resolution. In practice, the interpolation and decimation steps are concate-\nnated into a single polyphase digital ﬁltering operation (Szeliski, Winder, and Uyttendaele\n2010).\nFor parametric transforms, the oriented two-dimensional ﬁltering and resampling opera-\ntions can be approximated using a series of one-dimensional resampling and shearing trans-\nforms (Catmull and Smith 1980; Heckbert 1989; Wolberg 1990; Gomes, Darsa, Costa et al.\n1999; Szeliski, Winder, and Uyttendaele 2010). The advantage of using a series of one-\ndimensional transforms is that they are much more efﬁcient (in terms of basic arithmetic\noperations) than large, non-separable, two-dimensional ﬁlter kernels.\nIn order to prevent aliasing, however, it may be necessary to upsample in the opposite di-\nrection before applying a shearing transformation (Szeliski, Winder, and Uyttendaele 2010).\nFigure 3.50 shows this process for a rotation, where a vertical upsampling stage is added be-\nfore the horizontal shearing (and upsampling) stage. The upper image shows the appearance\nof the letter being rotated, while the lower image shows its corresponding Fourier transform.\n170\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nvertical shear\n+ downsample\n(a)\n(b)\n(c)\n(d)\nvertical \nupsample\nhorizontal shear\n+ upsample\nhorizontal  \ndownsample\n(e)\nFigure 3.50 Four-pass rotation (Szeliski, Winder, and Uyttendaele 2010): (a) original pixel\ngrid, image, and its Fourier transform; (b) vertical upsampling; (c) horizontal shear and up-\nsampling; (d) vertical shear and downsampling; (e) horizontal downsampling. The general\nafﬁne case looks similar except that the ﬁrst two stages perform general resampling.\n3.6.2 Mesh-based warping\nWhile parametric transforms speciﬁed by a small number of global parameters have many\nuses, local deformations with more degrees of freedom are often required.\nConsider, for example, changing the appearance of a face from a frown to a smile (Fig-\nure 3.51a). What is needed in this case is to curve the corners of the mouth upwards while\nleaving the rest of the face intact.19 To perform such a transformation, different amounts of\nmotion are required in different parts of the image. Figure 3.51 shows some of the commonly\nused approaches.\nThe ﬁrst approach, shown in Figure 3.51a–b, is to specify a sparse set of corresponding\npoints. The displacement of these points can then be interpolated to a dense displacement ﬁeld\n(Chapter 8) using a variety of techniques (Nielson 1993). One possibility is to triangulate\nthe set of points in one image (de Berg, Cheong, van Kreveld et al. 2006; Litwinowicz and\nWilliams 1994; Buck, Finkelstein, Jacobs et al. 2000) and to use an afﬁne motion model\n(Table 3.5), speciﬁed by the three triangle vertices, inside each triangle. If the destination\n19 Rowland and Perrett (1995); Pighin, Hecker, Lischinski et al. (1998); Blanz and Vetter (1999); Leyvand, Cohen-\nOr, Dror et al. (2008) show more sophisticated examples of changing facial expression and appearance.",
  "image_path": "page_191.jpg",
  "pages": [
    190,
    191,
    192
  ]
}