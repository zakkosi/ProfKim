{
  "doc_id": "pages_550_552",
  "text": "528\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Analyze the radial gradients (especially in low-gradient regions) and ﬁt the robust\nmeans of these gradients to the derivative of the vignetting function, as described by\nZheng, Yu, Kang et al. (2008).\nFor the parametric form of your vignetting function, you can either use a simple radial func-\ntion, e.g.,\nf(r) = 1 + α1r + α2r2 + · · ·\n(10.42)\nor one of the specialized equations developed by Kang and Weiss (2000) and Zheng, Lin, and\nKang (2006).\nIn all of these cases, be sure that you are using linearized intensity measurements, by\nusing either RAW images or images linearized through a radiometric response function, or at\nleast images where the gamma curve has been removed.\n(Optional) What happens if you forget to undo the gamma before ﬁtting a (multiplicative)\nvignetting function?\nEx 10.4: Optical blur (PSF) estimation\nCompute the optical PSF either using a known\ntarget (Figure 10.7) or by detecting and ﬁtting step edges (Section 10.1.4) (Joshi, Szeliski,\nand Kriegman 2008).\n1. Detect strong edges to sub-pixel precision.\n2. Fit a local proﬁle to each oriented edge and ﬁll these pixels into an ideal target image,\neither at image resolution or at a higher resolution (Figure 10.9c–d).\n3. Use least squares (10.1) at valid pixels to estimate the PSF kernel K, either globally or\nin locally overlapping sub-regions of the image.\n4. Visualize the recovered PSFs and use them to remove chromatic aberration or de-blur\nthe image.\nEx 10.5: Tone mapping\nImplement one of the tone mapping algorithms discussed in Sec-\ntion 10.2.1 (Durand and Dorsey 2002; Fattal, Lischinski, and Werman 2002; Reinhard, Stark,\nShirley et al. 2002; Lischinski, Farbman, Uyttendaele et al. 2006b) or any of the numer-\nous additional algorithms discussed by Reinhard, Ward, Pattanaik et al. (2005) and http:\n//stellar.mit.edu/S/course/6/sp08/6.815/materials.html.\n(Optional) Compare your algorithm to local histogram equalization (Section 3.1.4).\nEx 10.6: Flash enhancement\nDevelop an algorithm to combine ﬂash and non-ﬂash pho-\ntographs to best effect. You can use ideas from Eisemann and Durand (2004) and Petschnigg,\nAgrawala, Hoppe et al. (2004) or anything else you think might work well.\n10.7 Exercises\n529\nEx 10.7: Super-resolution\nImplement one or more super-resolution algorithms and com-\npare their performance.\n1. Take a set of photographs of the same scene using a hand-held camera (to ensure that\nthere is some jitter between the photographs).\n2. Determine the PSF for the images you are trying to super-resolve using one of the\ntechniques in Exercise 10.4.\n3. Alternatively, simulate a collection of lower-resolution images by taking a high-quality\nphotograph (avoid those with compression artifacts) and applying your own pre-ﬁlter\nkernel and downsampling.\n4. Estimate the relative motion between the images using a parametric translation and\nrotation motion estimation algorithm (Sections 6.1.3 or 8.2).\n5. Implement a basic least squares super-resolution algorithm by minimizing the differ-\nence between the observed and downsampled images (10.27–10.28).\n6. Add in a gradient image prior, either as another least squares term or as a robust term\nthat can be minimized using iteratively reweighted least squares (Appendix A.3).\n7. (Optional) Implement one of the example-based super-resolution techniques, where\nmatching against a set of exemplar images is used either to infer higher-frequency\ninformation to be added to the reconstruction (Freeman, Jones, and Pasztor 2002)\nor higher-frequency gradients to be matched in the super-resolved image (Baker and\nKanade 2002).\n8. (Optional) Use local edge statistic information to improve the quality of the super-\nresolved image (Fattal 2007).\nEx 10.8: Image matting\nDevelop an algorithm for pulling a foreground matte from natural\nimages, as described in Section 10.4.\n1. Make sure that the images you are taking are linearized (Exercise 10.1 and Section 10.1)\nand that your camera exposure is ﬁxed (full manual mode), at least when taking multi-\nple shots of the same scene.\n2. To acquire ground truth data, place your object in front of a computer monitor and\ndisplay a variety of solid background colors as well as some natural imagery.\n3. Remove your object and re-display the same images to acquire known background\ncolors.\n530\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n4. Use triangulation matting (Smith and Blinn 1996) to estimate the ground truth opacities\nα and pre-multiplied foreground colors αF for your objects.\n5. Implement one or more of the natural image matting algorithms described in Sec-\ntion 10.4 and compare your results to the ground truth values you computed. Alter-\nnatively, use the matting test images published on http://alphamatting.com/.\n6. (Optional) Run your algorithms on other images taken with the same calibrated camera\n(or other images you ﬁnd interesting).\nEx 10.9: Smoke and shadow matting\nExtract smoke or shadow mattes from one scene\nand insert them into another (Chuang, Agarwala, Curless et al. 2002; Chuang, Goldman,\nCurless et al. 2003).\n1. Take a still or video sequence of images with and without some intermittent smoke and\nshadows. (Remember to linearize your images before proceeding with any computa-\ntions.)\n2. For each pixel, ﬁt a line to the observed color values.\n3. If performing smoke matting, robustly compute the intersection of these lines to obtain\nthe smoke color estimate. Then, estimate the background color as the other extremum\n(unless you already took a smoke-free background image).\nIf performing shadow matting, compute robust shadow (minimum) and lit (maximum)\nvalues for each pixel.\n4. Extract the smoke or shadow mattes from each frame as the fraction between these two\nvalues (background and smoke or shadowed and lit).\n5. Scan a new (destination) scene or modify the original background with an image editor.\n6. Re-insert the smoke or shadow matte, along with any other foreground objects you may\nhave extracted.\n7. (Optional) Using a series of cast stick shadows, estimate the deformation ﬁeld for the\ndestination scene in order to correctly warp (drape) the shadows across the new ge-\nometry. (This is related to the shadow scanning technique developed by Bouguet and\nPerona (1999) and implemented in Exercise 12.2.)\n8. (Optional) Chuang, Goldman, Curless et al. (2003) only demonstrated their technique\nfor planar source geometries. Can you extend their technique to capture shadows ac-\nquired from an irregular source geometry?",
  "image_path": "page_551.jpg",
  "pages": [
    550,
    551,
    552
  ]
}