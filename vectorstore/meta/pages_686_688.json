{
  "doc_id": "pages_686_688",
  "text": "664\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWeak classifier 1\nWeights increased\nWeak classifier 2\nWeights increased\nWeak classifier 3\nFinal classifier\nFigure 14.7 Schematic illustration of boosting, courtesy of Svetlana Lazebnik, after origi-\nnal illustrations from Paul Viola and David Lowe. After each weak classiﬁer (decision stump\nor hyperplane) is selected, data points that are erroneously classiﬁed have their weights in-\ncreased. The ﬁnal classiﬁer is a linear combination of the simple weak classiﬁers.\nIn many applications of boosting, the features are simply coordinate axes xk, i.e., the\nboosting algorithm selects one of the input vector components as the best one to threshold. In\nViola and Jones’ face detector, the features are differences of rectangular regions in the input\npatch, as shown in Figure 14.6. The advantage of using these features is that, while they are\nmore discriminating than single pixels, they are extremely fast to compute once a summed\narea table has been pre-computed, as described in Section 3.2.3 (3.31–3.32). Essentially, for\nthe cost of an O(N) pre-computation phase (where N is the number of pixels in the image),\nsubsequent differences of rectangles can be computed in 4r additions or subtractions, where\nr ∈{2, 3, 4} is the number of rectangles in the feature.\nThe key to the success of boosting is the method for incrementally selecting the weak\nlearners and for re-weighting the training examples after each stage (Figure 14.7). The Ad-\naBoost (Adaptive Boosting) algorithm (Hastie, Tibshirani, and Friedman 2001; Bishop 2006)\ndoes this by re-weighting each sample as a function of whether it is correctly classiﬁed at each\nstage, and using the stage-wise average classiﬁcation error to determine the ﬁnal weightings\nαj among the weak classiﬁers, as described in Algorithm 14.1. While the resulting classi-\nﬁer is extremely fast in practice, the training time can be quite slow (in the order of weeks),\nbecause of the large number of feature (difference of rectangle) hypotheses that need to be\nexamined at each stage.\nTo further increase the speed of the detector, it is possible to create a cascade of classiﬁers,\nwhere each classiﬁer uses a small number of tests (say, a two-term AdaBoost classiﬁer) to\nreject a large fraction of non-faces while trying to pass through all potential face candidates\n(Fleuret and Geman 2001; Viola and Jones 2004). An even faster algorithm for performing\ncascade learning has recently been developed by Brubaker, Wu, Sun et al. (2008).\naccordingly.\n14.1 Object detection\n665\n1. Input the positive and negative training examples along with their labels {(xi, yi)},\nwhere yi = 1 for positive (face) examples and yi = −1 for negative examples.\n2. Initialize all the weights to wi,1 ←\n1\nN , where N is the number of training exam-\nples. (Viola and Jones (2004) use a separate N1 and N2 for positive and negative\nexamples.)\n3. For each training stage j = 1 . . . M:\n(a) Renormalize the weights so that they sum up to 1 (divide them by their sum).\n(b) Select the best classiﬁer hj(x; fj, θj, sj) by ﬁnding the one that minimizes\nthe weighted classiﬁcation error\nej\n=\nN−1\nX\ni=0\nwi,jei,j,\n(14.3)\nei,j\n=\n1 −δ(yi, hj(xi; fj, θj, sj)).\n(14.4)\nFor any given fj function, the optimal values of (θj, sj) can be found in\nlinear time using a variant of weighted median computation (Exercise 14.2).\n(c) Compute the modiﬁed error rate βj and classiﬁer weight αj,\nβj =\nej\n1 −ej\nand\nαj = −log βj.\n(14.5)\n(d) Update the weights according to the classiﬁcation errors ei,j\nwi,j+1 ←wi,jβ1−ei,j\nj\n,\n(14.6)\ni.e., downweight the training samples that were correctly classiﬁed in pro-\nportion to the overall classiﬁcation error.\n4. Set the ﬁnal classiﬁer to\nh(x) = sign\n\n\nm−1\nX\nj=0\nαjhj(x)\n\n.\n(14.7)\nAlgorithm 14.1\nThe AdaBoost training algorithm, adapted from Hastie, Tibshirani, and\nFriedman (2001), Viola and Jones (2004), and Bishop (2006).\n666\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 14.8 Pedestrian detection using histograms of oriented gradients (Dalal and Triggs\n2005) c⃝2005 IEEE: (a) the average gradient image over the training examples; (b) each\n“pixel” shows the maximum positive SVM weight in the block centered on the pixel; (c) like-\nwise, for the negative SVM weights; (d) a test image; (e) the computed R-HOG (rectangular\nhistogram of gradients) descriptor; (f) the R-HOG descriptor weighted by the positive SVM\nweights; (g) the R-HOG descriptor weighted by the negative SVM weights.\n14.1.2 Pedestrian detection\nWhile a lot of the research on object detection has focused on faces, the detection of other\nobjects, such as pedestrians and cars, has also received widespread attention (Gavrila and\nPhilomin 1999; Gavrila 1999; Papageorgiou and Poggio 2000; Mohan, Papageorgiou, and\nPoggio 2001; Schneiderman and Kanade 2004). Some of these techniques maintain the same\nfocus as face detection on speed and efﬁciency. Others, however, focus instead on accuracy,\nviewing detection as a more challenging variant of generic class recognition (Section 14.4)\nin which the locations and extents of objects are to be determined as accurately as possible.\n(See, for example, the PASCAL VOC detection challenge, http://pascallin.ecs.soton.ac.uk/\nchallenges/VOC/.)\nAn example of a well-known pedestrian detector is the algorithm developed by Dalal\nand Triggs (2005), who use a set of overlapping histogram of oriented gradients (HOG) de-\nscriptors fed into a support vector machine (Figure 14.8). Each HOG has cells to accumulate\nmagnitude-weighted votes for gradients at particular orientations, just as in the scale invariant\nfeature transform (SIFT) developed by Lowe (2004), which we discussed in Section 4.1.2 and\nFigure 4.18. Unlike SIFT, however, which is only evaluated at interest point locations, HOGs\nare evaluated on a regular overlapping grid and their descriptor magnitudes are normalized\nusing an even coarser grid; they are only computed at a single scale and a ﬁxed orientation. In\norder to capture the subtle variations in orientation around a person’s outline, a large number\nof orientation bins is used and no smoothing is performed in the central difference gradi-\nent computation—see the work of Dalal and Triggs (2005) for more implementation details.",
  "image_path": "page_687.jpg",
  "pages": [
    686,
    687,
    688
  ]
}