{
  "doc_id": "pages_766_768",
  "text": "744\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx\ny\nb\nm\ny=mx+b\n×\n×\n×\n×\nx\ny\nax+by+c=0\n×\n×\n×\n×\n(a)\n(b)\nFigure A.2 Least squares regression. (a) The line y = mx + b is ﬁt to the four noisy data\npoints, {(xi, yi)}, denoted by × by minimizing the squared vertical residuals between the\ndata points and the line, P\ni ∥yi −(mxi + b)∥2. (b) When the measurements {(xi, yi)} are\nassumed to have noise in all directions, the sum of orthogonal squared distances to the line\nP\ni ∥axi + byi + c∥2 is minimized using total least squares.\nIn cases where the least squares problem is numerically poorly conditioned (which should\ngenerally be avoided by adding sufﬁcient regularization or prior knowledge about the param-\neters, (Appendix A.3)), it is possible to use QR factorization or SVD directly on the matrix\nA (Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Nocedal and Wright\n2006; Bj¨orck and Dahlquist 2010), e.g.,\nAx = QRx = b\n−→\nRx = QT b.\n(A.34)\nNote that the upper triangular matrices R produced by the Cholesky factorization of C =\nAT A and the QR factorization of A are the same, but that solving (A.34) is generally more\nstable (less sensitive to roundoff error) but slower (by a constant factor).\nA.2.1 Total least squares\nIn some problems, e.g., when performing geometric line ﬁtting in 2D images or 3D plane\nﬁtting to point cloud data, instead of having measurement error along one particular axis, the\nmeasured points have uncertainty in all directions, which is known as the errors-in-variables\nmodel (Van Huffel and Lemmerling 2002; Matei and Meer 2006). In this case, it makes more\nsense to minimize a set of homogeneous squared errors of the form\nETLS =\nX\ni\n(aix)2 = ∥Ax∥2,\n(A.35)\nwhich is known as total least squares (TLS) (Van Huffel and Vandewalle 1991; Bj¨orck 1996;\nGolub and Van Loan 1996; Van Huffel and Lemmerling 2002).\nA.2 Linear least squares\n745\nThe above error metric has a trivial minimum solution at x = 0 and is, in fact, homoge-\nneous in x. For this reason, we augment this minimization problem with the requirement that\n∥x∥2 = 1. which results in the eigenvalue problem\nx = arg min\nx xT (AT A)x\nsuch that\n∥x∥2 = 1.\n(A.36)\nThe value of x that minimizes this constrained problem is the eigenvector associated with the\nsmallest eigenvalue of AT A. This is the same as the last right singular vector of A, since\nA\n=\nUΣV ,\n(A.37)\nAT A\n=\nV Σ2V ,\n(A.38)\nAT Avk\n=\nσ2\nk,\n(A.39)\nwhich is minimized by selecting the smallest σk value.\nFigure A.2b shows a line ﬁtting problem where, in this case, the measurement errors are\nassumed to be isotropic in (x, y). The solution for the best line equation ax + by + c = 0 is\nfound by minimizing\nETLS−2D =\nX\ni\n(axi + byi + c)2,\n(A.40)\ni.e., ﬁnding the eigenvector associated with the smallest eigenvalue of6\nC = AT A =\nX\ni\n\n\nxi\nyi\n1\n\n\nh\nxi\nyi\n1\ni\n.\n(A.41)\nNotice, however, that minimizing P\ni(aix)2 in (A.35) is only statistically optimal (Ap-\npendix B.1.1) if all of the measured terms in the ai, e.g., the (xi, yi, 1) measurements, have\nequal noise. This is deﬁnitely not the case in the line-ﬁtting example of Figure A.2b (A.40),\nsince the 1 values are noise-free. To mitigate this, we ﬁrst subtract the mean x and y values\nfrom all the measured points\nˆxi\n=\nxi −¯x\n(A.42)\nˆyi\n=\nyi −¯y\n(A.43)\nand then ﬁt the 2D line equation a(x −¯x) + b(y −¯y) = 0 by minimizing\nETLS−2Dm =\nX\ni\n(aˆxi + bˆyi)2.\n(A.44)\n6 Again, be careful with the variable names here. The measurement equation is ai = (xi, yi, 1) and the unknown\nparameters are x = (a, b, c).\n746\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe more general case where each individual measurement component can have different\nnoise level, as is the case in estimating essential and fundamental matrices (Section 7.2), is\ncalled the heteroscedastic errors-in-variable (HEIV) model and is discussed by Matei and\nMeer (2006).\nA.3 Non-linear least squares\nIn many vision problems, such as structure from motion, the least squares problem formulated\nin (A.23) involves functions f(xi; p) that are not linear in the unknown parameters p. This\nproblem is known as non-linear least squares or non-linear regression (Bj¨orck 1996; Madsen,\nNielsen, and Tingleff 2004; Nocedal and Wright 2006). It is usually solved by iteratively re-\nlinearizing (A.23) around the current estimate of p using the gradient derivative (Jacobian)\nJ = ∂f/∂p and computing an incremental improvement ∆p.\nAs shown in Equations (6.13–6.17), this results in\nENLS(∆p)\n=\nX\ni\n∥f(xi; p + ∆p) −x′\ni∥2\n(A.45)\n≈\nX\ni\n∥J(xi; p)∆p −ri∥2,\n(A.46)\nwhere the Jacobians J(xi; p) and residual vectors ri play the same role in forming the normal\nequations as ai and bi in (A.28).\nBecause the above approximation only holds near a local minimum or for small values\nof ∆p, the update p ←p + ∆p may not always decrease the summed square residual error\n(A.45). One way to mitigate this problem is to take a smaller step,\np ←p + α∆p,\n0 < α ≤1.\n(A.47)\nA simple way to determine a reasonable value of α is to start with 1 and successively halve\nthe value, which is a simple form of line search (Al-Baali and Fletcher. 1986; Bj¨orck 1996;\nNocedal and Wright 2006).\nAnother approach to ensuring a downhill step in error is to add a diagonal damping term\nto the approximate Hessian\nC =\nX\ni\nJT (xi)J(xi),\n(A.48)\ni.e., to solve\n[C + λ diag(C)]∆p = d,\n(A.49)\nwhere\nd =\nX\ni\nJT (xi)ri,\n(A.50)",
  "image_path": "page_767.jpg",
  "pages": [
    766,
    767,
    768
  ]
}