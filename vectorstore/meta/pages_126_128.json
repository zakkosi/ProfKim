{
  "doc_id": "pages_126_128",
  "text": "104\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nbias and gain parameters can also be spatially varying,\ng(x) = a(x)f(x) + b(x),\n(3.4)\ne.g., when simulating the graded density ﬁlter used by photographers to selectively darken\nthe sky or when modeling vignetting in an optical system.\nMultiplicative gain (both global and spatially varying) is a linear operation, since it obeys\nthe superposition principle,\nh(f0 + f1) = h(f0) + h(f1).\n(3.5)\n(We will have more to say about linear shift invariant operators in Section 3.2.) Operators\nsuch as image squaring (which is often used to get a local estimate of the energy in a band-\npass ﬁltered signal, see Section 3.5) are not linear.\nAnother commonly used dyadic (two-input) operator is the linear blend operator,\ng(x) = (1 −α)f0(x) + αf1(x).\n(3.6)\nBy varying α from 0 →1, this operator can be used to perform a temporal cross-dissolve\nbetween two images or videos, as seen in slide shows and ﬁlm production, or as a component\nof image morphing algorithms (Section 3.6.3).\nOne highly used non-linear transform that is often applied to images before further pro-\ncessing is gamma correction, which is used to remove the non-linear mapping between input\nradiance and quantized pixel values (Section 2.3.2).\nTo invert the gamma mapping applied\nby the sensor, we can use\ng(x) = [f(x)]1/γ ,\n(3.7)\nwhere a gamma value of γ ≈2.2 is a reasonable ﬁt for most digital cameras.\n3.1.2 Color transforms\nWhile color images can be treated as arbitrary vector-valued functions or collections of inde-\npendent bands, it usually makes sense to think about them as highly correlated signals with\nstrong connections to the image formation process (Section 2.2), sensor design (Section 2.3),\nand human perception (Section 2.3.2). Consider, for example, brightening a picture by adding\na constant value to all three channels, as shown in Figure 3.2b. Can you tell if this achieves the\ndesired effect of making the image look brighter? Can you see any undesirable side-effects\nor artifacts?\nIn fact, adding the same value to each color channel not only increases the apparent in-\ntensity of each pixel, it can also affect the pixel’s hue and saturation. How can we deﬁne and\nmanipulate such quantities in order to achieve the desired perceptual effects?\n3.1 Point operators\n105\n(a)\n(b)\n(c)\n(d)\nFigure 3.4 Image matting and compositing (Chuang, Curless, Salesin et al. 2001) c⃝2001\nIEEE: (a) source image; (b) extracted foreground object F; (c) alpha matte α shown in\ngrayscale; (d) new composite C.\nAs discussed in Section 2.3.2, chromaticity coordinates (2.104) or even simpler color ra-\ntios (2.116) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figure\n2.32g–i shows some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 × 3 color twist transform matrix.\nExercises 2.9 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn\n1996), while the process of inserting it into another image (without visible artifacts) is called\ncompositing (Porter and Duff 1984; Blinn 1994a).\nThe intermediate representation used for the foreground object between these two stages\nis called an alpha-matted color image (Figure 3.4b–c). In addition to the three color RGB\nchannels, an alpha-matted image contains a fourth alpha channel α (or A) that describes the\nrelative amount of opacity or fractional coverage at each pixel (Figures 3.4c and 3.5b). The\nopacity is the opposite of the transparency. Pixels within the object are fully opaque (α = 1),\nwhile pixels fully outside the object are transparent (α = 0). Pixels on the boundary of the\nobject vary smoothly between these two extremes, which hides the perceptual visible jaggies\n106\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n×\n(1−\n)\n+\n=\nB\nα\nαF\nC\n(a)\n(b)\n(c)\n(d)\nFigure 3.5\nCompositing equation C = (1 −α)B + αF. The images are taken from a\nclose-up of the region of the hair in the upper right part of the lion in Figure 3.4.\nthat occur if only binary opacities are used.\nTo composite a new (or foreground) image on top of an old (background) image, the over\noperator, ﬁrst proposed by Porter and Duff (1984) and then studied extensively by Blinn\n(1994a; 1994b), is used,\nC = (1 −α)B + αF.\n(3.8)\nThis operator attenuates the inﬂuence of the background image B by a factor (1 −α) and\nthen adds in the color (and opacity) values corresponding to the foreground layer F, as shown\nin Figure 3.5.\nIn many situations, it is convenient to represent the foreground colors in pre-multiplied\nform, i.e., to store (and manipulate) the αF values directly. As Blinn (1994b) shows, the\npre-multiplied RGBA representation is preferred for several reasons, including the ability\nto blur or resample (e.g., rotate) alpha-matted images without any additional complications\n(just treating each RGBA band independently). However, when matting using local color\nconsistency (Ruzon and Tomasi 2000; Chuang, Curless, Salesin et al. 2001), the pure un-\nmultiplied foreground colors F are used, since these remain constant (or vary slowly) in the\nvicinity of the object edge.\nThe over operation is not the only kind of compositing operation that can be used. Porter\nand Duff (1984) describe a number of additional operations that can be useful in photo editing\nand visual effects applications. In this book, we concern ourselves with only one additional,\ncommonly occurring case (but see Exercise 3.2).\nWhen light reﬂects off clean transparent glass, the light passing through the glass and\nthe light reﬂecting off the glass are simply added together (Figure 3.6). This model is use-\nful in the analysis of transparent motion (Black and Anandan 1996; Szeliski, Avidan, and\nAnandan 2000), which occurs when such scenes are observed from a moving camera (see\nSection 8.5.2).\nThe actual process of matting, i.e., recovering the foreground, background, and alpha\nmatte values from one or more images, has a rich history, which we study in Section 10.4.",
  "image_path": "page_127.jpg",
  "pages": [
    126,
    127,
    128
  ]
}