{
  "doc_id": "pages_713_715",
  "text": "14.3 Instance recognition\n691\nsimple metric to a dozen other metrics and conclude that it performs just about as well as\nmore complicated metrics. Because the number of non-zero ti terms in a typical query or\ndocument is small (M ≈200) compared to the number of visual words (V ≈20, 000), the\ndistance between pairs of (sparse) tf-idf vectors can be computed quite quickly.\nAfter retrieving the top Ns = 500 documents based on word frequencies, Sivic and Zis-\nserman (2009) re-rank these results using spatial consistency. This step involves taking every\nmatching feature and counting the number of k = 15 nearest adjacent features that also match\nbetween the two documents. (This latter process is accelerated using inverted ﬁles, which we\ndiscuss in more detail below.) As shown in Figure 14.29, this step helps remove spurious false\npositive matches and produces a better estimate of which frames and regions in the video are\nactually true matches. Algorithm 14.2 summarizes the processing steps involved in image\nretrieval using visual words.\nWhile this approach works well for tens of thousand of visual words and thousands of\nkeyframes, as the size of the database continues to increase, both the time to quantize each\nfeature and to ﬁnd potential matching frames or images can become prohibitive. Nist´er and\nStew´enius (2006) address this problem by constructing a hierarchical vocabulary tree, where\nfeature vectors are hierarchically clustered into a k-way tree of prototypes. (This technique is\nalso known as tree-structured vector quantization (Gersho and Gray 1991).) At both database\nconstruction time and query time, each descriptor vector is compared to several prototypes\nat a given level in the vocabulary tree and the branch with the closest prototype is selected\nfor further reﬁnement (Figure 14.30). In this way, vocabularies with millions (106) of words\ncan be supported, which enables individual words to be far more discriminative, while only\nrequiring 10 · 6 comparisons for quantizing each descriptor.\nAt query time, each node in the vocabulary tree keeps its own inverted ﬁle index, so that\nfeatures that match a particular node in the tree can be rapidly mapped to potential matching\nimages. (Interior leaf nodes just use the inverted indices of their corresponding leaf-node\ndescendants.) To score a particular query tf-idf vector tq against all document vectors {tj}\nusing an Lp metric,18 the non-zero tiq entries in tq are used to fetch corresponding non-zero\ntij entries, and the Lp norm is efﬁciently computed as\n∥tq −tj∥p\np = 2 +\nX\ni|tiq>0∧tij>0\n(|tiq −tij|p −|tiq|p −|tij|p).\n(14.35)\nIn order to mitigate quantization errors due to noise in the descriptor vectors, Nist´er and\nStew´enius (2006) not only score leaf nodes in the vocabulary tree (corresponding to visual\nwords), but also score interior nodes in the tree, which correspond to clusters of similar visual\nwords.\n18 In their actual implementation, Nist´er and Stew´enius (2006) use an L1 metric.\n692\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.30 Scalable recognition using a vocabulary tree (Nist´er and Stew´enius 2006) c⃝\n2006 IEEE. (a) Each MSER elliptical region is converted into a SIFT descriptor, which is\nthen quantized by comparing it hierarchically to some prototype descriptors in a vocabulary\ntree. Each leaf node stores its own inverted index (sparse list of non-zero tf-idf counts) into\nimages that contain that feature. (b) A recognition result, showing a query image (top row)\nbeing indexed into a database of 6000 test images and correctly ﬁnding the corresponding\nfour images.\nBecause of the high efﬁciency in both quantizing and scoring features, their vocabulary-\ntree-based recognition system is able to process incoming images in real time against a\ndatabase of 40,000 CD covers and at 1Hz when matching a database of one million frames\ntaken from six feature-length movies. Figure 14.30b shows some typical images from the\ndatabase of objects taken under varying viewpoints and illumination that was used to train\nand test the vocabulary tree recognition system.\nThe state of the art in instance recognition continues to improve rapidly. Philbin, Chum,\nIsard et al. (2007) have shown that randomized forest of k-d trees perform better than vocabu-\nlary trees on a large location recognition task (Figure 14.31). They also compare the effects of\nusing different 2D motion models (Section 2.1.2) in the veriﬁcation stage. In follow-on work,\nChum, Philbin, Sivic et al. (2007) apply another idea from information retrieval, namely\n14.3 Instance recognition\n693\nFigure 14.31 Location or building recognition using randomized trees (Philbin, Chum, Isard\net al. 2007) c⃝2007 IEEE. The left image is the query, the other images are the highest-ranked\nresults.\nquery expansion, which involves re-submitting top-ranked images from the initial query as\nadditional queries to generate additional candidate results, to further improve recognition\nrates for difﬁcult (occluded or oblique) examples. Philbin, Chum, Sivic et al. (2008) show\nhow to mitigate quantization problems in visual words selection using soft assignment, where\neach feature descriptor is mapped to a number of visual words based on its distance from the\ncluster prototypes. The soft weights derived from these distances are used, in turn, to weight\nthe counts used in the tf-idf vectors and to retrieve additional images for later veriﬁcation.\nTaken together, these recent advances hold the promise of extending current instance recog-\nnition algorithms to performing Web-scale retrieval and matching tasks (Agarwal, Snavely,\nSimon et al. 2009; Agarwal, Furukawa, Snavely et al. 2010; Snavely, Simon, Goesele et al.\n2010).\n14.3.3 Application: Location recognition\nOne of the most exciting applications of instance recognition today is in the area of location\nrecognition, which can be used both in desktop applications (where did I take this holiday\nsnap?) and in mobile (cell-phone) applications. The latter case includes not only ﬁnding out\nyour current location based on a cell-phone image but also providing you with navigation\ndirections or annotating your images with useful information, such as building names and\nrestaurant reviews (i.e., a portable form of augmented reality).\nSome approaches to location recognition assume that the photos consist of architectural\nscenes for which vanishing directions can be used to pre-rectify the images for easier match-\ning (Robertson and Cipolla 2004). Other approaches use general afﬁne covariant interest\npoints to perform wide baseline matching (Schaffalitzky and Zisserman 2002). The Photo\nTourism system of Snavely, Seitz, and Szeliski (2006) (Section 13.1.2) was the ﬁrst to apply\nthese kinds of ideas to large-scale image matching and (implicit) location recognition from",
  "image_path": "page_714.jpg",
  "pages": [
    713,
    714,
    715
  ]
}