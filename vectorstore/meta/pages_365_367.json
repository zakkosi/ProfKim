{
  "doc_id": "pages_365_367",
  "text": "Chapter 7\nStructure from motion\n7.1\nTriangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\n7.2\nTwo-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . . 347\n7.2.1\nProjective (uncalibrated) reconstruction . . . . . . . . . . . . . . . . 353\n7.2.2\nSelf-calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n7.2.3\nApplication: View morphing . . . . . . . . . . . . . . . . . . . . . . 357\n7.3\nFactorization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\n7.3.1\nPerspective and projective factorization . . . . . . . . . . . . . . . . 360\n7.3.2\nApplication: Sparse 3D model extraction\n. . . . . . . . . . . . . . . 362\n7.4\nBundle adjustment\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\n7.4.1\nExploiting sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n7.4.2\nApplication: Match move and augmented reality\n. . . . . . . . . . . 368\n7.4.3\nUncertainty and ambiguities . . . . . . . . . . . . . . . . . . . . . . 370\n7.4.4\nApplication: Reconstruction from Internet photos . . . . . . . . . . . 371\n7.5\nConstrained structure and motion . . . . . . . . . . . . . . . . . . . . . . . . 374\n7.5.1\nLine-based techniques . . . . . . . . . . . . . . . . . . . . . . . . . 374\n7.5.2\nPlane-based techniques . . . . . . . . . . . . . . . . . . . . . . . . . 376\n7.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n7.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n344\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n(l)\n(m)\n(n)\nFigure 7.1\nStructure from motion systems: (a–d) orthographic factorization (Tomasi and\nKanade 1992) c⃝1992 Springer; (e–f) line matching (Schmid and Zisserman 1997) c⃝1997\nIEEE; (g–k) incremental structure from motion (Snavely, Seitz, and Szeliski 2006); (l) 3D\nreconstruction of Trafalgar Square (Snavely, Seitz, and Szeliski 2006); (m) 3D reconstruction\nof the Great Wall of China (Snavely, Seitz, and Szeliski 2006); (n) 3D reconstruction of the\nOld Town Square, Prague (Snavely, Seitz, and Szeliski 2006) c⃝2006 ACM.\n7.1 Triangulation\n345\nIn the previous chapter, we saw how 2D and 3D point sets could be aligned and how such\nalignments could be used to estimate both a camera’s pose and its internal calibration parame-\nters. In this chapter, we look at the converse problem of estimating the locations of 3D points\nfrom multiple images given only a sparse set of correspondences between image features.\nWhile this process often involves simultaneously estimating both 3D geometry (structure)\nand camera pose (motion), it is commonly known as structure from motion (Ullman 1979).\nThe topics of projective geometry and structure from motion are extremely rich and\nsome excellent textbooks and surveys have been written on them (Faugeras and Luong 2001;\nHartley and Zisserman 2004; Moons, Van Gool, and Vergauwen 2010). This chapter skips\nover a lot of the richer material available in these books, such as the trifocal tensor and al-\ngebraic techniques for full self-calibration, and concentrates instead on the basics that we\nhave found useful in large-scale, image-based reconstruction problems (Snavely, Seitz, and\nSzeliski 2006).\nWe begin with a brief discussion of triangulation (Section 7.1), which is the problem of\nestimating a point’s 3D location when it is seen from multiple cameras. Next, we look at the\ntwo-frame structure from motion problem (Section 7.2), which involves the determination of\nthe epipolar geometry between two cameras and which can also be used to recover certain\ninformation about the camera intrinsics using self-calibration (Section 7.2.2). Section 7.3\nlooks at factorization approaches to simultaneously estimating structure and motion from\nlarge numbers of point tracks using orthographic approximations to the projection model.\nWe then develop a more general and useful approach to structure from motion, namely the\nsimultaneous bundle adjustment of all the camera and 3D structure parameters (Section 7.4).\nWe also look at special cases that arise when there are higher-level structures, such as lines\nand planes, in the scene (Section 7.5).\n7.1 Triangulation\nThe problem of determining a point’s 3D position from a set of corresponding image locations\nand known camera positions is known as triangulation. This problem is the converse of the\npose estimation problem we studied in Section 6.2.\nOne of the simplest ways to solve this problem is to ﬁnd the 3D point p that lies closest to\nall of the 3D rays corresponding to the 2D matching feature locations {xj} observed by cam-\neras {P j = Kj[Rj|tj]}, where tj = −Rjcj and cj is the jth camera center (2.55–2.56).\nAs you can see in Figure 7.2, these rays originate at cj in a direction ˆvj = N(R−1\nj K−1\nj xj).\nThe nearest point to p on this ray, which we denote as qj, minimizes the distance\n∥cj + djˆvj −p∥2,\n(7.1)",
  "image_path": "page_366.jpg",
  "pages": [
    365,
    366,
    367
  ]
}