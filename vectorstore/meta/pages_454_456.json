{
  "doc_id": "pages_454_456",
  "text": "432\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMore recent stitching algorithms ﬁrst extract features and then match them up, often using\nrobust techniques such as RANSAC (Section 6.1.4) to compute a good set of inliers. The ﬁnal\ncomputation of the homography (9.2), i.e., the solution of the least squares ﬁtting problem\ngiven pairs of corresponding features,\nx1 = (1 + h00)x0 + h01y0 + h02\nh20x0 + h21y0 + 1\nand y1 = h10x0 + (1 + h11)y0 + h12\nh20x0 + h21y0 + 1\n,\n(9.3)\nuses iterative least squares, as described in Section 6.1.3 and Equations (6.21–6.23).\n9.1.2 Application: Whiteboard and document scanning\nThe simplest image-stitching application is to stitch together a number of image scans taken\non a ﬂatbed scanner. Say you have a large map, or a piece of child’s artwork, that is too large\nto ﬁt on your scanner. Simply take multiple scans of the document, making sure to overlap\nthe scans by a large enough amount to ensure that there are enough common features. Next,\ntake successive pairs of images that you know overlap, extract features, match them up, and\nestimate the 2D rigid transform (2.16),\nxk+1 = Rkxk + tk,\n(9.4)\nthat best matches the features, using two-point RANSAC, if necessary, to ﬁnd a good set\nof inliers. Then, on a ﬁnal compositing surface (aligned with the ﬁrst scan, for example),\nresample your images (Section 3.6.1) and average them together. Can you see any potential\nproblems with this scheme?\nOne complication is that a 2D rigid transformation is non-linear in the rotation angle θ,\nso you will have to either use non-linear least squares or constrain R to be orthonormal, as\ndescribed in Section 6.1.3.\nA bigger problem lies in the pairwise alignment process. As you align more and more\npairs, the solution may drift so that it is no longer globally consistent. In this case, a global op-\ntimization procedure, as described in Section 9.2, may be required. Such global optimization\noften requires a large system of non-linear equations to be solved, although in some cases,\nsuch as linearized homographies (Section 9.1.3) or similarity transforms (Section 6.1.2), reg-\nular least squares may be an option.\nA slightly more complex scenario is when you take multiple overlapping handheld pic-\ntures of a whiteboard or other large planar object (He and Zhang 2005; Zhang and He 2007).\nHere, the natural motion model to use is a homography, although a more complex model that\nestimates the 3D rigid motion relative to the plane (plus the focal length, if unknown), could\nin principle be used.\n9.1 Motion models\n433\nΠ∞:\n(0,0,0,1)·p= 0\nR10\nx1 = (x1,y1,f1)\n~\nx0 = (x0,y0,f0)\n~\nFigure 9.3 Pure 3D camera rotation. The form of the homography (mapping) is particularly\nsimple and depends only on the 3D rotation matrix and focal lengths.\n9.1.3 Rotational panoramas\nThe most typical case for panoramic image stitching is when the camera undergoes a pure ro-\ntation. Think of standing at the rim of the Grand Canyon. Relative to the distant geometry in\nthe scene, as you snap away, the camera is undergoing a pure rotation, which is equivalent to\nassuming that all points are very far from the camera, i.e., on the plane at inﬁnity (Figure 9.3).\nSetting t0 = t1 = 0, we get the simpliﬁed 3 × 3 homography\n˜\nH10 = K1R1R−1\n0 K−1\n0\n= K1R10K−1\n0 ,\n(9.5)\nwhere Kk = diag(fk, fk, 1) is the simpliﬁed camera intrinsic matrix (2.59), assuming that\ncx = cy = 0, i.e., we are indexing the pixels starting from the optical center (Szeliski 1996).\nThis can also be re-written as\n\n\nx1\ny1\n1\n\n∼\n\n\nf1\nf1\n1\n\nR10\n\n\nf −1\n0\nf −1\n0\n1\n\n\n\n\nx0\ny0\n1\n\n\n(9.6)\nor\n\n\nx1\ny1\nf1\n\n∼R10\n\n\nx0\ny0\nf0\n\n,\n(9.7)\nwhich reveals the simplicity of the mapping equations and makes all of the motion parameters\nexplicit. Thus, instead of the general eight-parameter homography relating a pair of images,\nwe get the three-, four-, or ﬁve-parameter 3D rotation motion models corresponding to the\ncases where the focal length f is known, ﬁxed, or variable (Szeliski and Shum 1997).3 Es-\ntimating the 3D rotation matrix (and, optionally, focal length) associated with each image is\n3 An initial estimate of the focal lengths can be obtained using the intrinsic calibration techniques described in\nSection 6.3.4 or from EXIF tags.\n434\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nintrinsically more stable than estimating a homography with a full eight degrees of freedom,\nwhich makes this the method of choice for large-scale image stitching algorithms (Szeliski\nand Shum 1997; Shum and Szeliski 2000; Brown and Lowe 2007).\nGiven this representation, how do we update the rotation matrices to best align two over-\nlapping images? Given a current estimate for the homography ˜\nH10 in (9.5), the best way to\nupdate R10 is to prepend an incremental rotation matrix R(ω) to the current estimate R10\n(Szeliski and Shum 1997; Shum and Szeliski 2000),\n˜\nH(ω) = K1R(ω)R10K−1\n0\n= [K1R(ω)K−1\n1 ][K1R10K−1\n0 ] = D ˜\nH10.\n(9.8)\nNote that here we have written the update rule in the compositional form, where the in-\ncremental update D is prepended to the current homography ˜\nH10. Using the small-angle\napproximation to R(ω) given in (2.35), we can write the incremental update matrix as\nD = K1R(ω)K−1\n1\n≈K1(I + [ω]×)K−1\n1\n=\n\n\n1\n−ωz\nf1ωy\nωz\n1\n−f1ωx\n−ωy/f1\nωx/f1\n1\n\n.\n(9.9)\nNotice how there is now a nice one-to-one correspondence between the entries in the D\nmatrix and the h00, . . . , h21 parameters used in Table 6.1 and Equation (6.19), i.e.,\n(h00, h01, h02, h00, h11, h12, h20, h21) = (0, −ωz, f1ωy, ωz, 0, −f1ωx, −ωy/f1, ωx/f1).\n(9.10)\nWe can therefore apply the chain rule to Equations (6.24 and 9.10) to obtain\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\n−xy/f1\nf1 + x2/f1\n−y\n−(f1 + y2/f1)\nxy/f1\nx\n# \n\nωx\nωy\nωz\n\n,\n(9.11)\nwhich give us the linearized update equations needed to estimate ω = (ωx, ωy, ωz).4 Notice\nthat this update rule depends on the focal length f1 of the target view and is independent\nof the focal length f0 of the template view. This is because the compositional algorithm\nessentially makes small perturbations to the target. Once the incremental rotation vector ω\nhas been computed, the R1 rotation matrix can be updated using R1 ←R(ω)R1.\nThe formulas for updating the focal length estimates are a little more involved and are\ngiven in (Shum and Szeliski 2000). We will not repeat them here, since an alternative up-\ndate rule, based on minimizing the difference between back-projected 3D rays, is given in\nSection 9.2.1. Figure 9.4 shows the alignment of four images under the 3D rotation motion\nmodel.\n4 This is the same as the rotational component of instantaneous rigid ﬂow (Bergen, Anandan, Hanna et al. 1992)\nand the update equations given by Szeliski and Shum (1997) and Shum and Szeliski (2000).",
  "image_path": "page_455.jpg",
  "pages": [
    454,
    455,
    456
  ]
}