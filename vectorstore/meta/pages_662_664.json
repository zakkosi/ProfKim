{
  "doc_id": "pages_662_664",
  "text": "640\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Figures 12.18–12.19). While traditional performance-driven animation systems use marker-\nbased motion capture (Williams 1990; Litwinowicz and Williams 1994; Ma, Jones, Chiang\net al. 2008), video footage can now often be used directly to control the animation (Buck,\nFinkelstein, Jacobs et al. 2000; Pighin, Szeliski, and Salesin 2002; Zhang, Snavely, Curless\net al. 2004; Vlasic, Brand, Pﬁster et al. 2005; Roble and Zafar 2009).\nIn addition to its most common application to facial animation, video-based animation\ncan also be applied to whole body motion (Section 12.6.4), e.g., by matching the ﬂow ﬁelds\nbetween two different source videos and using one to drive the other (Efros, Berg, Mori et al.\n2003). Another approach to video-based rendering is to use ﬂow or 3D modeling to unwrap\nsurface textures into stabilized images, which can then be manipulated and re-rendered onto\nthe original video (Pighin, Szeliski, and Salesin 2002; Rav-Acha, Kohli, Fitzgibbon et al.\n2008).\n13.5.2 Video textures\nVideo-based animation is a powerful means of creating photo-realistic videos by re-purposing\nexisting video footage to match some other desired activity or script. What if instead of\nconstructing a special animation or narrative, we simply want the video to continue playing\nin a plausible manner? For example, many Web sites use images or videos to highlight their\ndestinations, e.g., to portray attractive beaches with surf and palm trees waving in the wind.\nInstead of using a static image or a video clip that has a discontinuity when it loops, can we\ntransform the video clip into an inﬁnite-length animation that plays forever?\nThis idea is the basis of video textures, in which a short video clip can be arbitrarily\nextended by re-arranging video frames while preserving visual continuity (Sch¨odl, Szeliski,\nSalesin et al. 2000). The basic problem in creating video textures is how to perform this\nre-arrangement without introducing visual artifacts. Can you think of how you might do this?\nThe simplest approach is to match frames by visual similarity (e.g., L2 distance) and to\njump between frames that appear similar. Unfortunately, if the motions in the two frames\nare different, a dramatic visual artifact will occur (the video will appear to “stutter”). For\nexample, if we fail to match the motions of the clock pendulum in Figure 13.13a, it can\nsuddenly change direction in mid-swing.\nHow can we extend our basic frame matching to also match motion? In principle, we\ncould compute optic ﬂow at each frame and match this. However, ﬂow estimates are often\nunreliable (especially in textureless regions) and it is not clear how to weight the visual and\nmotion similarities relative to each other. As an alternative, Sch¨odl, Szeliski, Salesin et al.\n(2000) suggest matching triplets or larger neighborhoods of adjacent video frames, much\nin the same way as Video Rewrite matches triphones. Once we have constructed an n ×\nn similarity matrix between all video frames (where n is the number of frames), a simple\n13.5 Video-based rendering\n641\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 13.13\nVideo textures (Sch¨odl, Szeliski, Salesin et al. 2000) c⃝2000 ACM: (a) a\nclock pendulum, with correctly matched direction of motion; (b) a candle ﬂame, showing\ntemporal transition arcs; (c) the ﬂag is generated using morphing at jumps; (d) a bonﬁre\nuses longer cross-dissolves; (e) a waterfall cross-dissolves several sequences at once; (f) a\nsmiling animated face; (g) two swinging children are animated separately; (h) the balloons\nare automatically segmented into separate moving regions; (i) a synthetic ﬁsh tank consisting\nof bubbles, plants, and ﬁsh. Videos corresponding to these images can be found at http:\n//www.cc.gatech.edu/gvu/perception/projects/videotexture/.\n642\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nﬁnite impulse response (FIR) ﬁltering of each match sequence can be used to emphasize\nsubsequences that match well.\nThe results of this match computation gives us a jump table or, equivalently, a transition\nprobability between any two frames in the original video. This is shown schematically as\nred arcs in Figure 13.13b, where the red bar indicates which video frame is currently be-\ning displayed, and arcs light up as a forward or backward transition is taken. We can view\nthese transition probabilities as encoding the hidden Markov model (HMM) that underlies a\nstochastic video generation process.\nSometimes, it is not possible to ﬁnd exactly matching subsequences in the original video.\nIn this case, morphing, i.e., warping and blending frames during transitions (Section 3.6.3)\ncan be used to hide the visual differences (Figure 13.13c). If the motion is chaotic enough,\nas in a bonﬁre or a waterfall (Figures 13.13d–e), simple blending (extended cross-dissolves)\nmay be sufﬁcient. Improved transitions can also be obtained by performing 3D graph cuts on\nthe spatio-temporal volume around a transition (Kwatra, Sch¨odl, Essa et al. 2003).\nVideo textures need not be restricted to chaotic random phenomena such as ﬁre, wind,\nand water. Pleasing video textures can be created of people, e.g., a smiling face (as in Fig-\nure 13.13f) or someone running on a treadmill (Sch¨odl, Szeliski, Salesin et al. 2000). When\nmultiple people or objects are moving independently, as in Figures 13.13g–h, we must ﬁrst\nsegment the video into independently moving regions and animate each region separately.\nIt is also possible to create large panoramic video textures from a slowly panning camera\n(Agarwala, Zheng, Pal et al. 2005).\nInstead of just playing back the original frames in a stochastic (random) manner, video\ntextures can also be used to create scripted or interactive animations. If we extract individual\nelements, such as ﬁsh in a ﬁshtank (Figure 13.13i) into separate video sprites, we can animate\nthem along pre-speciﬁed paths (by matching the path direction with the original sprite motion)\nto make our video elements move in a desired fashion (Sch¨odl and Essa 2002). In fact, work\non video textures inspired research on systems that re-synthesize new motion sequences from\nmotion capture data, which some people refer to as “mocap soup” (Arikan and Forsyth 2002;\nKovar, Gleicher, and Pighin 2002; Lee, Chai, Reitsma et al. 2002; Li, Wang, and Shum 2002;\nPullen and Bregler 2002).\nWhile video textures primarily analyze the video as a sequence of frames (or regions) that\ncan be re-arranged in time, temporal textures (Szummer and Picard 1996; Bar-Joseph, El-\nYaniv, Lischinski et al. 2001) and dynamic textures (Doretto, Chiuso, Wu et al. 2003; Yuan,\nWen, Liu et al. 2004; Doretto and Soatto 2006) treat the video as a 3D spatio-temporal volume\nwith textural properties, which can be described using auto-regressive temporal models.",
  "image_path": "page_663.jpg",
  "pages": [
    662,
    663,
    664
  ]
}