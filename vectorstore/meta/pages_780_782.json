{
  "doc_id": "pages_780_782",
  "text": "758\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen each noise vector ni is a multivariate Gaussian with covariance Σi,\nni ∼N(0, Σi),\n(B.7)\nwe can write this likelihood as\nL\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2(yi −f i(x))T Σ−1\ni (yi −f i(x))\n\u0013\n(B.8)\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2∥yi −f i(x)∥2\nΣ\n−1\ni\n\u0013\n,\nwhere the matrix norm ∥x∥2\nA is a shorthand notation for xT Ax.\nThe norm ∥yi −yi∥Σ\n−1\ni\nis often called the Mahalanobis distance (5.26 and 14.14) and is\nused to measure the distance between a measurement and the mean of a multivariate Gaussian\ndistribution. Contours of equal Mahalanobis distance are equi-probability contours. Note\nthat when the measurement covariance is isotropic (the same in all directions), i.e., when\nΣi = σ2\ni I, the likelihood can be written as\nL =\nY\ni\n(2πσ2\ni )−Ni/2 exp\n\u0012\n−1\n2σ2\ni\n∥yi −f i(x)∥2\n\u0013\n,\n(B.9)\nwhere Ni is the length of the ith measurement vector yi.\nWe can more easily visualize the structure of the covariance matrix and the correspond-\ning Mahalanobis distance if we ﬁrst perform an eigenvalue or principal component analysis\n(PCA) of the covariance matrix (A.6),\nΣ = Φ diag(λ0 . . . λN−1) ΦT .\n(B.10)\nEqual-probability contours of the corresponding multi-variate Gaussian, which are also equi-\ndistance contours in the Mahalanobis distance (Figure 14.14), are multi-dimensional ellip-\nsoids whose axis directions are given by the columns of Φ (the eigenvectors) and whose\nlengths are given by the σj =\np\nλj (Figure A.1).\nIt is usually more convenient to work with the negative log likelihood, which we can think\nof as a cost or energy\nE = −log L\n=\n1\n2\nX\ni\n(yi −f i(x))T Σ−1\ni (yi −f i(x)) + k\n(B.11)\n=\n1\n2\nX\ni\n∥yi −f i(x)∥2\nΣ\n−1\ni\n+ k,\n(B.12)\nwhere k = P\ni log |2πΣi| is a constant that depends on the measurement variances, but is\nindependent of x.\nB.2 Maximum likelihood estimation and least squares\n759\nNotice that the inverse covariance Ci = Σ−1\ni\nplays the role of a weight on each of the\nmeasurement error residuals, i.e., the difference between the contaminated measurement yi\nand its uncontaminated (predicted) value f i(x). In fact, the inverse covariance is often called\nthe (Fisher) information matrix (Bishop 2006), since it tells us how much information is\ncontained in a given measurement, i.e., how well it constrains the ﬁnal estimate. We can also\nthink of this matrix as denoting the amount of conﬁdence to associate with each measurement\n(hence the letter C).\nIn this formulation, it is quite acceptable for some information matrices to be singular\n(of degenerate rank) or even zero (if the measurement is missing altogether). Rank-deﬁcient\nmeasurements often occur, for example, when using a line feature or edge to measure a 3D\nedge-like feature, since its exact position along the edge is unknown (of inﬁnite or extremely\nlarge variance) §8.1.3.\nIn order to make the distinction between the noise contaminated measurement and its\nexpected value for a particular setting of x more explicit, we adopt the notation ˜y for the\nformer (think of the tilde as the approximate or noisy value) and ˆy = f i(x) for the latter\n(think of the hat as the predicted or expected value). We can then write the negative log\nlikelihood as\nE = −log L =\nX\ni\n∥˜yi −ˆyi∥Σ\n−1\ni\n+ k.\n(B.13)\nB.2 Maximum likelihood estimation and least squares\nNow that we have presented the likelihood and log likelihood functions, how can we ﬁnd the\noptimal value for our state estimate x? One plausible choice might be to select the value of x\nthat maximizes L = p(y|x). In fact, in the absence of any prior model for x (Appendix B.4),\nwe have\nL = p(y|x) = p(y, x) = p(x|y).\nTherefore, choosing the value of x that maximizes the likelihood is equivalent to choosing\nthe maximum of our probability density estimate for x.\nWhen might this be a good idea? If the data (measurements) constrain the possible values\nof x so that they all cluster tightly around one value (e.g., if the distribution p(x|y) is a\nunimodal Gaussian), the maximum likelihood estimate is the optimal one in that it is both\nunbiased and has the least possible variance. In many other cases, e.g., if a single estimate\nis all that is required, it is still often the best estimate.3 However, if the probability is multi-\nmodal, i.e., it has several local minima in the log likelihood (Figure 5.7), much more care\n3 According to the Gauss-Markov theorem, least squares produces the best linear unbiased estimator (BLUE) for\na linear measurement model regardless of the actual noise distribution, assuming that the noise is zero mean and\nuncorrelated.\n760\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmay be required. In particular, it might be necessary to defer certain decisions (such as the\nultimate position of an object being tracked) until more measurements have been taken. The\nCONDENSATION algorithm presented in Section 5.1.2 is one possible method for modeling\nand updating such multi-modal distributions but is just one example of more general particle\nﬁltering and Markov Chain Monte Carlo (MCMC) techniques (Andrieu, de Freitas, Doucet\net al. 2003; Bishop 2006; Koller and Friedman 2009).\nAnother possible way to choose the best estimate is to maximize the expected utility\n(or, conversely, to minimize the expected risk or loss) associated with obtaining the correct\nestimate, i.e., by minimizing\nEloss(x, y) =\nZ\nl(x −z)p(z|y)dz.\n(B.14)\nFor example, if a robot wants to avoid hitting a wall at all costs, the loss function will be\nhigh whenever the estimate underestimates the true distance to the wall. When l(x −y) =\nδ(x −y), we obtain the maximum likelihood estimate, whereas when l(x −y) = ∥x −y∥2,\nwe obtain the mean square error (MSE) or expected value estimate. The explicit modeling of\na utility or loss function is what characterizes statistical decision theory (Berger 1993; Hastie,\nTibshirani, and Friedman 2001; Bishop 2006; Robert 2007).\nHow do we ﬁnd the maximum likelihood estimate? If the measurement noise is Gaussian,\nwe can minimize the quadratic objective function (B.13). This becomes even simpler if the\nmeasurement equations are linear, i.e.,\nf i(x) = Hix,\n(B.15)\nwhere H is the measurement matrix relating unknown state variables x to measurements ˜y.\nIn this case, (B.13) becomes\nE =\nX\ni\n∥˜yi −Hix∥Σ\n−1\ni\n=\nX\ni\n(˜yi −Hix)T Ci(˜yi −Hix),\n(B.16)\nwhich is a simple quadratic form in x, which can be solved using linear least squares (Ap-\npendix A.2). When the measurements are non-linear, the system must be solved iteratively\nusing non-linear least squares (Appendix A.3).\nB.3 Robust statistics\nIn Appendix B.1.1, we assumed that the noise being added to each measurement (B.5) was\nmultivariate Gaussian (B.7). This is an appropriate model if the noise is the result of lots of\ntiny errors being added together, e.g., from thermal noise in a silicon imager. In most cases,\nhowever, measurements can be contaminated with larger outliers, i.e., gross failures in the",
  "image_path": "page_781.jpg",
  "pages": [
    780,
    781,
    782
  ]
}