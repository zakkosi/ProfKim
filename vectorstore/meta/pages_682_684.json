{
  "doc_id": "pages_682_684",
  "text": "660\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 14.3\nPre-processing stages for face detector training (Rowley, Baluja, and Kanade\n1998a) c⃝1998 IEEE: (a) artiﬁcially mirroring, rotating, scaling, and translating training\nimages for greater variability; (b) using images without faces (looking up at a tree) to generate\nnon-face examples; (c) pre-processing the patches by subtracting a best ﬁt linear function\n(constant gradient) and histogram equalizing.\nAppearance-based approaches scan over small overlapping rectangular patches of the im-\nage searching for likely face candidates, which can then be reﬁned using a cascade of more\nexpensive but selective detection algorithms (Sung and Poggio 1998; Rowley, Baluja, and\nKanade 1998a; Romdhani, Torr, Sch¨olkopf et al. 2001; Fleuret and Geman 2001; Viola and\nJones 2004). In order to deal with scale variation, the image is usually converted into a\nsub-octave pyramid and a separate scan is performed on each level. Most appearance-based\napproaches today rely heavily on training classiﬁers using sets of labeled face and non-face\npatches.\nSung and Poggio (1998) and Rowley, Baluja, and Kanade (1998a) present two of the ear-\nliest appearance-based face detectors and introduce a number of innovations that are widely\nused in later work by others.\nTo start with, both systems collect a set of labeled face patches (Figure 14.2) as well as a\nset of patches taken from images that are known not to contain faces, such as aerial images or\nvegetation (Figure 14.3b). The collected face images are augmented by artiﬁcially mirroring,\nrotating, scaling, and translating the images by small amounts to make the face detectors less\nsensitive to such effects (Figure 14.3a).\nAfter an initial set of training images has been collected, some optional pre-processing\ncan be performed, such as subtracting an average gradient (linear function) from the image\nto compensate for global shading effects and using histogram equalization to compensate for\nvarying camera contrast (Figure 14.3c).\n14.1 Object detection\n661\nFigure 14.4\nLearning a mixture of Gaussians model for face detection (Sung and Poggio\n1998) c⃝1998 IEEE. The face and non-face images (192-long vectors) are ﬁrst clustered into\nsix separate clusters (each) using k-means and then analyzed using PCA. The cluster centers\nare shown in the right-hand columns.\nClustering and PCA.\nOnce the face and non-face patterns have been pre-processed, Sung\nand Poggio (1998) cluster each of these datasets into six separate clusters using k-means\nand then ﬁt PCA subspaces to each of the resulting 12 clusters (Figure 14.4). At detection\ntime, the DIFS and DFFS metrics ﬁrst developed by Moghaddam and Pentland (1997) (see\nFigure 14.14 and (14.14)) are used to produce 24 Mahalanobis distance measurements (two\nper cluster). The resulting 24 measurements are input to a multi-layer perceptron (MLP),\nwhich is a neural network with alternating layers of weighted summations and sigmoidal non-\nlinearities trained using the “backpropagation” algorithm (Rumelhart, Hinton, and Williams\n1986).\nNeural networks.\nInstead of ﬁrst clustering the data and computing Mahalanobis distances\nto the cluster centers, Rowley, Baluja, and Kanade (1998a) apply a neural network (MLP) di-\nrectly to the 20×20 pixel patches of gray-level intensities, using a variety of differently sized\nhand-crafted “receptive ﬁelds” to capture both large-scale and smaller scale structure (Fig-\nure 14.5). The resulting neural network directly outputs the likelihood of a face at the center\n662\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.5\nA neural network for face detection (Rowley, Baluja, and Kanade 1998a) c⃝\n1998 IEEE. Overlapping patches are extracted from different levels of a pyramid and then\npre-processed as shown in Figure 14.3b. A three-layer neural network is then used to detect\nlikely face locations.\nof every overlapping patch in a multi-resolution pyramid. Since several overlapping patches\n(in both space and resolution) may ﬁre near a face, an additional merging network is used\nto merge overlapping detections. The authors also experiment with training several networks\nand merging their outputs. Figure 14.2 shows a sample result from their face detector.\nTo make the detector run faster, a separate network operating on 30×30 patches is trained\nto detect both faces and faces shifted by ±5 pixels. This network is evaluated at every 10th\npixel in the image (horizontally and vertically) and the results of this “coarse” or “sloppy”\ndetector are used to select regions on which to run the slower single-pixel overlap technique.\nTo deal with in-plane rotations of faces, Rowley, Baluja, and Kanade (1998b) train a router\nnetwork to estimate likely rotation angles from input patches and then apply the estimated\nrotation to each patch before running the result through their upright face detector.\nSupport vector machines.\nInstead of using a neural network to classify patches, Osuna,\nFreund, and Girosi (1997) use a support vector machine (SVM) (Hastie, Tibshirani, and\nFriedman 2001; Sch¨olkopf and Smola 2002; Bishop 2006; Lampert 2008) to classify the same\npreprocessed patches as Sung and Poggio (1998). An SVM searches for a series of maximum\nmargin separating planes in feature space between different classes (in this case, face and\nnon-face patches). In those cases where linear classiﬁcation boundaries are insufﬁcient, the\nfeature space can be lifted into higher-dimensional features using kernels (Hastie, Tibshirani,\nand Friedman 2001; Sch¨olkopf and Smola 2002; Bishop 2006). SVMs have been used by\nother researchers for both face detection and face recognition (Heisele, Ho, Wu et al. 2003;",
  "image_path": "page_683.jpg",
  "pages": [
    682,
    683,
    684
  ]
}