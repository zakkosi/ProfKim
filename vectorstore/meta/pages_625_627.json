{
  "doc_id": "pages_625_627",
  "text": "12.6 Model-based reconstruction\n603\nuse such models for a variety of animation and visual effects (Blanz and Vetter 1999). It is\nalso possible to design stereo matching algorithms that optimize directly for the head model\nparameters (Shan, Liu, and Zhang 2001; Kang and Jones 2002) or to use the output of real-\ntime stereo with active illumination (Zhang, Snavely, Curless et al. 2004) (Figures 12.7 and\n12.18b).\nAs the sophistication of 3D facial capture systems evolves, so does the detail and realism\nin the reconstructed models. Newer systems can capture (in real-time) not only surface details\nsuch as wrinkles and creases, but also accurate models of skin reﬂection, translucency, and\nsub-surface scattering (Weyrich, Matusik, Pﬁster et al. 2006; Golovinskiy, Matusik, ster et al.\n2006; Bickel, Botsch, Angst et al. 2007; Igarashi, Nishino, and Nayar 2007).\nOnce a 3D head model has been constructed, it can be used in a variety of applications,\nsuch as head tracking (Toyama 1998; Lepetit, Pilet, and Fua 2004; Matthews, Xiao, and Baker\n2007), as shown in Figures 4.29 and 14.24, and face transfer, i.e., replacing one person’s\nface with another in a video (Bregler, Covell, and Slaney 1997; Vlasic, Brand, Pﬁster et al.\n2005). Additional applications include face beautiﬁcation by warping face images toward a\nmore attractive “standard” (Leyvand, Cohen-Or, Dror et al. 2008), face de-identiﬁcation for\nprivacy protection (Gross, Sweeney, De la Torre et al. 2008), and face swapping (Bitouk,\nKumar, Dhillon et al. 2008).\n12.6.3 Application: Facial animation\nPerhaps the most widely used application of 3D head modeling is facial animation. Once\na parameterized 3D model of shape and appearance (surface texture) has been constructed,\nit can be used directly to track a person’s facial motions (Figure 12.18a) and to animate a\ndifferent character with these same motions and expressions (Pighin, Szeliski, and Salesin\n2002).\nAn improved version of such a system can be constructed by ﬁrst applying principal com-\nponent analysis (PCA) to the space of possible head shapes and facial appearances. Blanz\nand Vetter (1999) describe a system where they ﬁrst capture a set of 200 colored range scans\nof faces (Figure 12.19a), which can be represented as a large collection of (X, Y, Z, R, G, B)\nsamples (vertices).9 In order for 3D morphing to be meaningful, corresponding vertices in\ndifferent people’s scans must ﬁrst be put into correspondence (Pighin, Hecker, Lischinski et\nal. 1998). Once this is done, PCA can be applied to more naturally parameterize the 3D mor-\nphable model. The ﬂexibility of this model can be increased by performing separate analyses\nin different subregions, such as the eyes, nose, and mouth, just as in modular eigenspaces\n(Moghaddam and Pentland 1997).\n9 A cylindrical coordinate system provides a natural two-dimensional embedding for this collection, but such an\nembedding is not necessary to perform PCA.\n604\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.19\n3D morphable face model (Blanz and Vetter 1999) c⃝1999 ACM: (a) orig-\ninal 3D face model with the addition of shape and texture variations in speciﬁc directions:\ndeviation from the mean (caricature), gender, expression, weight, and nose shape; (b) a 3D\nmorphable model is ﬁt to a single image, after which its weight or expression can be manip-\nulated; (c) another example of a 3D reconstruction along with a different set of 3D manipula-\ntions such as lighting and pose change.\n12.6 Model-based reconstruction\n605\nAfter computing a subspace representation, different directions in this space can be as-\nsociated with different characteristics such as gender, facial expressions, or facial features\n(Figure 12.19a). As in the work of Rowland and Perrett (1995), faces can be turned into\ncaricatures by exaggerating their displacement from the mean image.\n3D morphable models can be ﬁtted to a single image using gradient descent on the error\nbetween the input image and the re-synthesized model image, after an initial manual place-\nment of the model in an approximately correct pose, scale, and location (Figures 12.19b–c).\nThe efﬁciency of this ﬁtting process can be increased using inverse compositional image\nalignment (8.64–8.65), as described by Romdhani and Vetter (2003).\nThe resulting texture-mapped 3D model can then be modiﬁed to produce a variety of vi-\nsual effects, including changing a person’s weight or expression, or three-dimensional effects\nsuch as re-lighting or 3D video-based animation (Section 13.5.1). Such models can also be\nused for video compression, e.g., by only transmitting a small number of facial expression\nand pose parameters to drive a synthetic avatar (Eisert, Wiegand, and Girod 2000; Gao, Chen,\nWang et al. 2003).\n3D facial animation is often matched to the performance of an actor, in what is known\nas performance-driven animation (Section 4.1.5) (Williams 1990). Traditional performance-\ndriven animation systems use marker-based motion capture (Ma, Jones, Chiang et al. 2008),\nwhile some newer systems use video footage to control the animation (Buck, Finkelstein,\nJacobs et al. 2000; Pighin, Szeliski, and Salesin 2002; Zhang, Snavely, Curless et al. 2004;\nVlasic, Brand, Pﬁster et al. 2005).\nAn example of the latter approach is the system developed for the ﬁlm Benjamin Button,\nin which Digital Domain used the CONTOUR system from Mova10 to capture actor Brad\nPitt’s facial motions and expressions (Roble and Zafar 2009). CONTOUR uses a combina-\ntion of phosphorescent paint and multiple high-resolution video cameras to capture real-time\n3D range scans of the actor. These 3D models were then translated into Facial Action Cod-\ning System (FACS) shape and expression parameters (Ekman and Friesen 1978) to drive a\ndifferent (older) synthetically animated computer-generated imagery (CGI) character.\n12.6.4 Whole body modeling and tracking\nThe topics of tracking humans, modeling their shape and appearance, and recognizing their\nactivities, are some of the most actively studied areas of computer vision. Annual confer-\nences11 and special journal issues (Hilton, Fua, and Ronfard 2006) are devoted to this sub-\nject, and two recent surveys (Forsyth, Arikan, Ikemoto et al. 2006; Moeslund, Hilton, and\n10 http://www.mova.com.\n11 International Conference on Automatic Face and Gesture Recognition (FG), IEEE Workshop on Analysis and\nModeling of Faces and Gestures, and International Workshop on Tracking Humans for the Evaluation of their Motion\nin Image Sequences (THEMIS).",
  "image_path": "page_626.jpg",
  "pages": [
    625,
    626,
    627
  ]
}