{
  "doc_id": "pages_520_522",
  "text": "498\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nformation equations (3.75) used for image deblurring (Section 3.4.3), which we also used\nin (10.2) for blur kernel (PSF) estimation (Section 10.1.4). In this case, we have several ob-\nserved images {ok(x)}, as well as an image warping function ˆhk(x) for each observed image\n(Figure 3.47). Combining all of these elements, we get the (noisy) observation equations17\nok(x) = D{b(x) ∗s(ˆhk(x))} + nk(x),\n(10.26)\nwhere D is the downsampling operator, which operates after the super-resolved (sharp)\nwarped image s(ˆhk(x)) has been convolved with the blur kernel b(x). The above image\nformation equations lead to the following least squares problem,\nX\nk\n∥ok(x) −D{bk(x) ∗s(ˆhk(x))}∥2.\n(10.27)\nIn most super-resolution algorithms, the alignment (warping) ˆhk is estimated using one of\nthe input frames as the reference frame; either feature-based (Section 6.1.3) or direct (image-\nbased) (Section 8.2) parametric alignment techniques can be used. (A few algorithms, such\nas those described by Schultz and Stevenson (1996) or Capel (2004) use dense (per-pixel\nﬂow) estimates.) A better approach is to re-compute the alignment by directly minimizing\n(10.27) once an initial estimate of s(x) has been computed (Hardie, Barnard, and Armstrong\n1997) or to marginalize out the motion parameters altogether (Pickup, Capel, Roberts et al.\n2007)—see also the work of Protter and Elad (2009) for some related video super-resolution\nwork.\nThe point spread function (blur kernel) bk is either inferred from knowledge of the image\nformation process (e.g., the amount of motion or defocus blur and the camera sensor optics)\nor calibrated from a test image or the observed images {ok} using one of the techniques\ndescribed in Section 10.1.4. The problem of simultaneously inferring the blur kernel and the\nsharp image is known as blind image deconvolution (Kundur and Hatzinakos 1996; Levin\n2006).18\nGiven an estimate of ˆhk and bk(x), (10.27) can be re-written using matrix/vector notation\nas a large sparse least squares problem in the unknown values of the super-resolved pixels s,\nX\nk\n∥ok −DBkW ks∥2.\n(10.28)\n17 It is also possible to add an unknown bias–gain term to each observation (Capel 2004), as was done for motion\nestimation in (8.8).\n18 Notice that there is a chicken-and-egg problem if both the blur kernel and the super-resolved image are un-\nknown. This can be “broken” either using structural assumptions about the sharp image, e.g., the presence of edges\n(Joshi, Szeliski, and Kriegman 2008) or prior models for the image, such as edge sparsity (Fergus, Singh, Hertzmann\net al. 2006).\n10.3 Super-resolution and blur removal\n499\n(Recall from (3.89) that once the warping function ˆhk is known, values of s(ˆhk(x)) depend\nlinearly on those in s(x).) An efﬁcient way to solve this least squares problem is to use\npreconditioned conjugate gradient descent (Capel 2004), although some earlier algorithms,\nsuch as the one developed by Irani and Peleg (1991), used regular gradient descent (also\nknown as iterative back projection (IBP), in the computed tomography literature).\nThe above formulation assumes that warping can be expressed as a simple (sinc or bicu-\nbic) interpolated resampling of the super-resolved sharp image, followed by a stationary\n(spatially invariant) blurring (PSF) and area integration process. However, if the surface is\nseverely foreshortened, we have to take into account the spatially varying ﬁltering that occurs\nduring the image warping (Section 3.6.1), before we can then model the PSF induced by the\noptics and camera sensor (Wang, Kang, Szeliski et al. 2001; Capel 2004).\nHow well does this least squares (MLE) approach to super-resolution work? In practice,\nthis depends a lot on the amount of blur and aliasing in the camera optics, as well as the accu-\nracy in the motion and PSF estimates (Baker and Kanade 2002; Jiang, Wong, and Bao 2003;\nCapel 2004). Less blurring and more aliasing means that there is more (aliased) high fre-\nquency information available to be recovered. However, because the least squares (maximum\nlikelihood) formulation uses no image prior, a lot of high-frequency noise can be introduced\ninto the solution (Figure 10.31c).\nFor this reason, most super-resolution algorithms assume some form of image prior. The\nsimplest of these is to place a penalty on the image derivatives similar to Equations (3.105\nand 3.113), e.g.,\nX\n(i,j)\nρp(s(i, j) −s(i + 1, j)) + ρp(s(i, j) −s(i, j + 1)).\n(10.29)\nAs discussed in Section 3.7.2, when ρp is quadratic, this is a form of Tikhonov regulariza-\ntion (Section 3.7.1), and the overall problem is still linear least squares. The resulting prior\nimage model is a Gaussian Markov random ﬁeld (GMRF), which can be extended to other\n(e.g., diagonal) differences, as in (Capel 2004) (Figure 10.31).\nUnfortunately, GMRFs tend to produce solutions with visible ripples, which can also\nbe interpreted as increased noise sensitivity in middle frequencies (Exercise 3.17). A bet-\nter image prior is a robust prior that encourages piecewise continuous solutions (Black and\nRangarajan 1996), see Appendix B.3. Examples of such priors include the Huber potential\n(Schultz and Stevenson 1996; Capel and Zisserman 2003), which is a blend of a Gaussian\nwith a longer-tailed Laplacian, and the even sparser (heavier-tailed) hyper-Laplacians used\nby Levin, Fergus, Durand et al. (2007) and Krishnan and Fergus (2009). It is also possible to\nlearn the parameters for such priors using cross-validation (Capel 2004; Pickup 2007).\nWhile sparse (robust) derivative priors can reduce rippling effects and increase edge\nsharpness, they cannot hallucinate higher-frequency texture or details. To do this, a train-\n500\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 10.31 Super-resolution results using a variety of image priors (Capel 2001): (a) Low-\nres ROI (bicubic 3× zoom); (b) average image; (c) MLE @ 1.25× pixel-zoom; (d) simple\n∥x∥2 prior (λ = 0.004); (e) GMRF (λ = 0.003); (f) HMRF (λ = 0.01, α = 0.04). 10\nimages are used as input and a 3× super-resolved image is produced in each case, except for\nthe MLE result in (c).\n(a)\n(b)\n(c)\nFigure 10.32 Example-based super-resolution: (a) original 32 × 32 low-resolution image;\n(b) example-based super-resolved 256 × 256 image (Freeman, Jones, and Pasztor 2002) c⃝\n2002 IEEE; (c) upsampling via imposed edge statistics (Fattal 2007) c⃝2007 ACM.",
  "image_path": "page_521.jpg",
  "pages": [
    520,
    521,
    522
  ]
}