{
  "doc_id": "pages_246_248",
  "text": "224\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.18 A schematic representation of Lowe’s (2004) scale invariant feature transform\n(SIFT): (a) Gradient orientations and magnitudes are computed at each pixel and weighted\nby a Gaussian fall-off function (blue circle). (b) A weighted gradient orientation histogram\nis then computed in each subregion, using trilinear interpolation. While this ﬁgure shows an\n8 × 8 pixel patch and a 2 × 2 descriptor array, Lowe’s actual implementation uses 16 × 16\npatches and a 4 × 4 array of eight-bin histograms.\n11, and 15, with eight angular bins (except for the central region), for a total of 17 spa-\ntial bins and 16 orientation bins. The 272-dimensional histogram is then projected onto\na 128-dimensional descriptor using PCA trained on a large database. In their evaluation,\nMikolajczyk and Schmid (2005) found that GLOH, which has the best performance overall,\noutperforms SIFT by a small margin.\nSteerable ﬁlters.\nSteerable ﬁlters (Section 3.2.3) are combinations of derivative of Gaus-\nsian ﬁlters that permit the rapid computation of even and odd (symmetric and anti-symmetric)\nedge-like and corner-like features at all possible orientations (Freeman and Adelson 1991).\nBecause they use reasonably broad Gaussians, they too are somewhat insensitive to localiza-\ntion and orientation errors.\nPerformance of local descriptors.\nAmong the local descriptors that Mikolajczyk and Schmid\n(2005) compared, they found that GLOH performed best, followed closely by SIFT (see Fig-\nure 4.25). They also present results for many other descriptors not covered in this book.\nThe ﬁeld of feature descriptors continues to evolve rapidly, with some of the newer tech-\nniques looking at local color information (van de Weijer and Schmid 2006; Abdel-Hakim\nand Farag 2006). Winder and Brown (2007) develop a multi-stage framework for feature\ndescriptor computation that subsumes both SIFT and GLOH (Figure 4.20a) and also allows\nthem to learn optimal parameters for newer descriptors that outperform previous hand-tuned\n4.1 Points and patches\n225\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.19 The gradient location-orientation histogram (GLOH) descriptor uses log-polar\nbins instead of square bins to compute orientation histograms (Mikolajczyk and Schmid\n2005).\ndescriptors. Hua, Brown, and Winder (2007) extend this work by learning lower-dimensional\nprojections of higher-dimensional descriptors that have the best discriminative power. Both\nof these papers use a database of real-world image patches (Figure 4.20b) obtained by sam-\npling images at locations that were reliably matched using a robust structure-from-motion\nalgorithm applied to Internet photo collections (Snavely, Seitz, and Szeliski 2006; Goesele,\nSnavely, Curless et al. 2007). In concurrent work, Tola, Lepetit, and Fua (2010) developed a\nsimilar DAISY descriptor for dense stereo matching and optimized its parameters based on\nground truth stereo data.\nWhile these techniques construct feature detectors that optimize for repeatability across\nall object classes, it is also possible to develop class- or instance-speciﬁc feature detectors that\nmaximize discriminability from other classes (Ferencz, Learned-Miller, and Malik 2008).\n4.1.3 Feature matching\nOnce we have extracted features and their descriptors from two or more images, the next step\nis to establish some preliminary feature matches between these images. In this section, we\ndivide this problem into two separate components. The ﬁrst is to select a matching strategy,\nwhich determines which correspondences are passed on to the next stage for further process-\ning. The second is to devise efﬁcient data structures and algorithms to perform this matching\nas quickly as possible. (See the discussion of related techniques in Section 14.3.2.)\n226\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 4.20 Spatial summation blocks for SIFT, GLOH, and some newly developed feature\ndescriptors (Winder and Brown 2007) c⃝2007 IEEE: (a) The parameters for the new features,\ne.g., their Gaussian weights, are learned from a training database of (b) matched real-world\nimage patches obtained from robust structure from motion applied to Internet photo collec-\ntions (Hua, Brown, and Winder 2007).\nMatching strategy and error rates\nDetermining which feature matches are reasonable to process further depends on the context\nin which the matching is being performed. Say we are given two images that overlap to a fair\namount (e.g., for image stitching, as in Figure 4.16, or for tracking objects in a video). We\nknow that most features in one image are likely to match the other image, although some may\nnot match because they are occluded or their appearance has changed too much.\nOn the other hand, if we are trying to recognize how many known objects appear in a clut-\ntered scene (Figure 4.21), most of the features may not match. Furthermore, a large number\nof potentially matching objects must be searched, which requires more efﬁcient strategies, as\ndescribed below.\nTo begin with, we assume that the feature descriptors have been designed so that Eu-\nclidean (vector magnitude) distances in feature space can be directly used for ranking poten-\ntial matches. If it turns out that certain parameters (axes) in a descriptor are more reliable\nthan others, it is usually preferable to re-scale these axes ahead of time, e.g., by determin-\ning how much they vary when compared against other known good matches (Hua, Brown,\nand Winder 2007). A more general process, which involves transforming feature vectors\ninto a new scaled basis, is called whitening and is discussed in more detail in the context of\neigenface-based face recognition (Section 14.2.1).\nGiven a Euclidean distance metric, the simplest matching strategy is to set a threshold\n(maximum distance) and to return all matches from other images within this threshold. Set-\nting the threshold too high results in too many false positives, i.e., incorrect matches being\nreturned. Setting the threshold too low results in too many false negatives, i.e., too many\ncorrect matches being missed (Figure 4.22).\nWe can quantify the performance of a matching algorithm at a particular threshold by",
  "image_path": "page_247.jpg",
  "pages": [
    246,
    247,
    248
  ]
}