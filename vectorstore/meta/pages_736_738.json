{
  "doc_id": "pages_736_738",
  "text": "714\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Figures 14.44–14.45), where the arrangements of various objects in a scene are used as part\nof the labeling process. Torralba, Murphy, and Freeman (2004) describe a conditional random\nﬁeld where the estimated locations of building and roads inﬂuence the detection of cars, and\nwhere boosting is used to learn the structure of the CRF. Rabinovich, Vedaldi, Galleguillos\net al. (2007) use context to improve the results of CRF segmentation by noting that certain\nadjacencies (relationships) are more likely than others, e.g., a person is more likely to be on\na horse than on a dog.\nContext also plays an important role in 3D inference from single images (Figure 14.47),\nusing computer vision techniques for labeling pixels as belonging to the ground, vertical\nsurfaces, or sky (Hoiem, Efros, and Hebert 2005a,b). This line of work has been extended to\na more holistic approach that simultaneously reasons about object identity, location, surface\norientations, occlusions, and camera viewing parameters (Hoiem, Efros, and Hebert 2008a,b).\nA number of approaches use the gist of a scene (Torralba 2003; Torralba, Murphy, Free-\nman et al. 2003) to determine where instances of particular objects are likely to occur. For\nexample, Murphy, Torralba, and Freeman (2003) train a regressor to predict the vertical loca-\ntions of objects such as pedestrians, cars, and buildings (or screens and keyboard for indoor\nofﬁce scenes) based on the gist of an image. These location distributions are then used with\nclassic object detectors to improve the performance of the detectors. Gists can also be used to\ndirectly match complete images, as we saw in the scene completion work of Hays and Efros\n(2007).\nFinally, some of the most recent work in scene understanding exploits the existence of\nlarge numbers of labeled (or even unlabeled) images to perform matching directly against\nwhole images, where the images themselves implicitly encode the expected relationships\nbetween objects (Figure 14.51) (Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros\n2008). We discuss such techniques in the next section, where we look at the inﬂuence that\nlarge image databases have had on object recognition and scene understanding.\n14.5.1 Learning and large image collections\nGiven how learning techniques are widely used in recognition algorithms, you may wonder\nwhether the topic of learning deserves its own section (or even chapter), or whether it is just\npart of the basic fabric of all recognition tasks. In fact, trying to build a recognition system\nwithout lots of training data for anything other than a basic pattern such as a UPC code has\nproven to be a dismal failure.\nIn this chapter, we have already seen lots of techniques borrowed from the machine learn-\ning, statistics, and pattern recognition communities. These include principal component, sub-\nspace, and discriminant analysis (Section 14.2.1) and more sophisticated discriminative clas-\nsiﬁcation algorithms such as neural networks, support vector machines, and boosting (Sec-\n14.5 Context and scene understanding\n715\n(a)\n(b)\n(c)\nFigure 14.51 Recognition by scene alignment (Russell, Torralba, Liu et al. 2007): (a) input\nimage; (b) matched images with similar scene conﬁgurations; (c) ﬁnal labeling of the input\nimage.\ntion 14.1.1). Some of the best-performing techniques on challenging recognition benchmarks\n(Varma and Ray 2007; Felzenszwalb, McAllester, and Ramanan 2008; Fritz and Schiele 2008;\nVedaldi, Gulshan, Varma et al. 2009) rely heavily on the latest machine learning techniques,\nwhose development is often being driven by challenging vision problems (Freeman, Perona,\nand Sch¨olkopf 2008).\nA distinction sometimes made in the recognition community is between problems where\nmost of the variables of interest (say, parts) are already (partially) labeled and systems that\nlearn more of the problem structure with less supervision (Fergus, Perona, and Zisserman\n2007; Fei-Fei, Fergus, and Perona 2006). In fact, recent work by Sivic, Russell, Zisserman et\nal. (2008) has demonstrated the ability to learn visual hierarchies (hierarchies of object parts\nwith related visual appearance) and scene segmentations in a totally unsupervised framework.\nPerhaps the most dramatic change in the recognition community has been the appearance\nof very large databases of training images.20 Early learning-based algorithms, such as those\nfor face and pedestrian detection (Section 14.1), used relatively few (in the hundreds) labeled\nexamples to train recognition algorithm parameters (say, the thresholds used in boosting). To-\nday, some recognition algorithms use databases such as LabelMe (Russell, Torralba, Murphy\net al. 2008), which contain tens of thousands of labeled examples.\nThe existence of such large databases opens up the possibility of matching directly against\nthe training images rather than using them to learn the parameters of recognition algorithms.\nRussell, Torralba, Liu et al. (2007) describe a system where a new image is matched against\neach of the training images, from which a consensus labeling for the unknown objects in\nthe scene can be inferred, as shown in Figure 14.51. Malisiewicz and Efros (2008) start\nby over-segmenting each image and then use the LabelMe database to search for similar\nimages and conﬁgurations in order to obtain per-pixel category labelings. It is also possible\nto combine feature-based correspondence algorithms with large labeled databases to perform\n20 We have already seen some computational photography applications of such databases in Section 14.4.4.\n716\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.52 Recognition using tiny images (Torralba, Freeman, and Fergus 2008) c⃝2008\nIEEE: columns (a) and (c) show sample input images and columns (b) and (d) show the\ncorresponding 16 nearest neighbors in the database of 80 million tiny images.\nsimultaneous recognition and segmentation (Liu, Yuen, and Torralba 2009).\nWhen the database of images becomes large enough, it is even possible to directly match\ncomplete images with the expectation of ﬁnding a good match. Torralba, Freeman, and Fergus\n(2008) start with a database of 80 million tiny (32 × 32) images and compensate for the poor\naccuracy in their image labels, which are collected automatically from the Internet, by using\na semantic taxonomy (Wordnet) to infer the most likely labels for a new image. Somewhere\nin the 80 million images, there are enough examples to associate some set of images with\neach of the 75,000 non-abstract nouns in Wordnet that they use in their system. Some sample\nrecognition results are shown in Figure 14.52.\nAnother example of a large labeled database of images is ImageNet (Deng, Dong, Socher\net al. 2009), which is collecting images for the 80,000 nouns (synonym sets) in WordNet\n(Fellbaum 1998). As of April 2010, about 500–1000 carefully vetted examples for 14841",
  "image_path": "page_737.jpg",
  "pages": [
    736,
    737,
    738
  ]
}