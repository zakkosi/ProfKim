{
  "doc_id": "pages_033_035",
  "text": "1.2 A brief history\n11\nDigital image processing\nBlocks world, line labeling\nGeneralized cylinders\n197\nGeneralized cylinders\nPictorial structures\nStereo correspondence\nIntrinsic images\nOptical flow\nStructure from motion\n70\nImage pyramids\nScale-space processing\nShape from shading, \ntexture, and focus\nPhysically-based  modeling\n1980\nRegularization\nMarkov Random Fields\nKalman filters\n3D range data processing\nProjective invariants\nFactorization\n1\nFactorization\nPhysics-based vision\nGraph cuts\nParticle filtering\nEnergy-based segmentation\nFace recognition and detection\n1990\nFace recognition and detection\nSubspace methods\nImage-based modeling \nand rendering\nTexture synthesis and inpainting\nComputational photography\n2000\nFeature-based  recognition\nMRF inference algorithms\nCategory recognition\nLearning\nFigure 1.6\nA rough timeline of some of the most active topics of research in computer\nvision.\n1970s.\nWhen computer vision ﬁrst started out in the early 1970s, it was viewed as the\nvisual perception component of an ambitious agenda to mimic human intelligence and to\nendow robots with intelligent behavior. At the time, it was believed by some of the early\npioneers of artiﬁcial intelligence and robotics (at places such as MIT, Stanford, and CMU)\nthat solving the “visual input” problem would be an easy step along the path to solving more\ndifﬁcult problems such as higher-level reasoning and planning. According to one well-known\nstory, in 1966, Marvin Minsky at MIT asked his undergraduate student Gerald Jay Sussman\nto “spend the summer linking a camera to a computer and getting the computer to describe\nwhat it saw” (Boden 2006, p. 781).5 We now know that the problem is slightly more difﬁcult\nthan that.6\nWhat distinguished computer vision from the already existing ﬁeld of digital image pro-\ncessing (Rosenfeld and Pfaltz 1966; Rosenfeld and Kak 1976) was a desire to recover the\nthree-dimensional structure of the world from images and to use this as a stepping stone to-\nwards full scene understanding. Winston (1975) and Hanson and Riseman (1978) provide\ntwo nice collections of classic papers from this early period.\nEarly attempts at scene understanding involved extracting edges and then inferring the\n3D structure of an object or a “blocks world” from the topological structure of the 2D lines\n(Roberts 1965). Several line labeling algorithms (Figure 1.7a) were developed at that time\n(Huffman 1971; Clowes 1971; Waltz 1975; Rosenfeld, Hummel, and Zucker 1976; Kanade\n1980). Nalwa (1993) gives a nice review of this area. The topic of edge detection was also\n5 Boden (2006) cites (Crevier 1993) as the original source. The actual Vision Memo was authored by Seymour\nPapert (1966) and involved a whole cohort of students.\n6 To see how far robotic vision has come in the last four decades, have a look at the towel-folding robot at\nhttp://rll.eecs.berkeley.edu/pr/icra10/ (Maitin-Shepard, Cusumano-Towner, Lei et al. 2010).\n12\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.7\nSome early (1970s) examples of computer vision algorithms: (a) line label-\ning (Nalwa 1993) c⃝1993 Addison-Wesley, (b) pictorial structures (Fischler and Elschlager\n1973) c⃝1973 IEEE, (c) articulated body model (Marr 1982) c⃝1982 David Marr, (d) intrin-\nsic images (Barrow and Tenenbaum 1981) c⃝1973 IEEE, (e) stereo correspondence (Marr\n1982) c⃝1982 David Marr, (f) optical ﬂow (Nagel and Enkelmann 1986) c⃝1986 IEEE.\nan active area of research; a nice survey of contemporaneous work can be found in (Davis\n1975).\nThree-dimensional modeling of non-polyhedral objects was also being studied (Baum-\ngart 1974; Baker 1977). One popular approach used generalized cylinders, i.e., solids of\nrevolution and swept closed curves (Agin and Binford 1976; Nevatia and Binford 1977), of-\nten arranged into parts relationships7 (Hinton 1977; Marr 1982) (Figure 1.7c). Fischler and\nElschlager (1973) called such elastic arrangements of parts pictorial structures (Figure 1.7b).\nThis is currently one of the favored approaches being used in object recognition (see Sec-\ntion 14.4 and Felzenszwalb and Huttenlocher 2005).\nA qualitative approach to understanding intensities and shading variations and explaining\nthem by the effects of image formation phenomena, such as surface orientation and shadows,\nwas championed by Barrow and Tenenbaum (1981) in their paper on intrinsic images (Fig-\nure 1.7d), along with the related 2 1/2 -D sketch ideas of Marr (1982). This approach is again\nseeing a bit of a revival in the work of Tappen, Freeman, and Adelson (2005).\nMore quantitative approaches to computer vision were also developed at the time, in-\ncluding the ﬁrst of many feature-based stereo correspondence algorithms (Figure 1.7e) (Dev\n7 In robotics and computer animation, these linked-part graphs are often called kinematic chains.\n1.2 A brief history\n13\n1974; Marr and Poggio 1976; Moravec 1977; Marr and Poggio 1979; Mayhew and Frisby\n1981; Baker 1982; Barnard and Fischler 1982; Ohta and Kanade 1985; Grimson 1985; Pol-\nlard, Mayhew, and Frisby 1985; Prazdny 1985) and intensity-based optical ﬂow algorithms\n(Figure 1.7f) (Horn and Schunck 1981; Huang 1981; Lucas and Kanade 1981; Nagel 1986).\nThe early work in simultaneously recovering 3D structure and camera motion (see Chapter 7)\nalso began around this time (Ullman 1979; Longuet-Higgins 1981).\nA lot of the philosophy of how vision was believed to work at the time is summarized\nin David Marr’s (1982) book.8 In particular, Marr introduced his notion of the three levels\nof description of a (visual) information processing system. These three levels, very loosely\nparaphrased according to my own interpretation, are:\n• Computational theory: What is the goal of the computation (task) and what are the\nconstraints that are known or can be brought to bear on the problem?\n• Representations and algorithms: How are the input, output, and intermediate infor-\nmation represented and which algorithms are used to calculate the desired result?\n• Hardware implementation: How are the representations and algorithms mapped onto\nactual hardware, e.g., a biological vision system or a specialized piece of silicon? Con-\nversely, how can hardware constraints be used to guide the choice of representation\nand algorithm? With the increasing use of graphics chips (GPUs) and many-core ar-\nchitectures for computer vision (see Section C.2), this question is again becoming quite\nrelevant.\nAs I mentioned earlier in this introduction, it is my conviction that a careful analysis of the\nproblem speciﬁcation and known constraints from image formation and priors (the scientiﬁc\nand statistical approaches) must be married with efﬁcient and robust algorithms (the engineer-\ning approach) to design successful vision algorithms. Thus, it seems that Marr’s philosophy\nis as good a guide to framing and solving problems in our ﬁeld today as it was 25 years ago.\n1980s.\nIn the 1980s, a lot of attention was focused on more sophisticated mathematical\ntechniques for performing quantitative image and scene analysis.\nImage pyramids (see Section 3.5) started being widely used to perform tasks such as im-\nage blending (Figure 1.8a) and coarse-to-ﬁne correspondence search (Rosenfeld 1980; Burt\nand Adelson 1983a,b; Rosenfeld 1984; Quam 1984; Anandan 1989). Continuous versions\nof pyramids using the concept of scale-space processing were also developed (Witkin 1983;\nWitkin, Terzopoulos, and Kass 1986; Lindeberg 1990). In the late 1980s, wavelets (see Sec-\ntion 3.5.4) started displacing or augmenting regular image pyramids in some applications\n8 More recent developments in visual perception theory are covered in (Palmer 1999; Livingstone 2008).",
  "image_path": "page_034.jpg",
  "pages": [
    33,
    34,
    35
  ]
}