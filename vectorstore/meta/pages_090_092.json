{
  "doc_id": "pages_090_092",
  "text": "68\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nCombinations of the two techniques have also been developed (Wallace, Cohen, and Green-\nberg 1987), as well as more general light transport techniques for simulating effects such as\nthe caustics cast by rippling water.\nThe basic ray tracing algorithm associates a light ray with each pixel in the camera im-\nage and ﬁnds its intersection with the nearest surface. A primary contribution can then be\ncomputed using the simple shading equations presented previously (e.g., Equation (2.93))\nfor all light sources that are visible for that surface element. (An alternative technique for\ncomputing which surfaces are illuminated by a light source is to compute a shadow map,\nor shadow buffer, i.e., a rendering of the scene from the light source’s perspective, and then\ncompare the depth of pixels being rendered with the map (Williams 1983; Akenine-M¨oller\nand Haines 2002).) Additional secondary rays can then be cast along the specular direction\ntowards other objects in the scene, keeping track of any attenuation or color change that the\nspecular reﬂection induces.\nRadiosity works by associating lightness values with rectangular surface areas in the scene\n(including area light sources). The amount of light interchanged between any two (mutually\nvisible) areas in the scene can be captured as a form factor, which depends on their relative\norientation and surface reﬂectance properties, as well as the 1/r2 fall-off as light is distributed\nover a larger effective sphere the further away it is (Cohen and Wallace 1993; Sillion and\nPuech 1994; Glassner 1995). A large linear system can then be set up to solve for the ﬁnal\nlightness of each area patch, using the light sources as the forcing function (right hand side).\nOnce the system has been solved, the scene can be rendered from any desired point of view.\nUnder certain circumstances, it is possible to recover the global illumination in a scene from\nphotographs using computer vision techniques (Yu, Debevec, Malik et al. 1999).\nThe basic radiosity algorithm does not take into account certain near ﬁeld effects, such\nas the darkening inside corners and scratches, or the limited ambient illumination caused\nby partial shadowing from other surfaces. Such effects have been exploited in a number of\ncomputer vision algorithms (Nayar, Ikeuchi, and Kanade 1991; Langer and Zucker 1994).\nWhile all of these global illumination effects can have a strong effect on the appearance\nof a scene, and hence its 3D interpretation, they are not covered in more detail in this book.\n(But see Section 12.7.1 for a discussion of recovering BRDFs from real scenes and objects.)\n2.2.3 Optics\nOnce the light from a scene reaches the camera, it must still pass through the lens before\nreaching the sensor (analog ﬁlm or digital silicon). For many applications, it sufﬁces to\ntreat the lens as an ideal pinhole that simply projects all rays through a common center of\nprojection (Figures 2.8 and 2.9).\nHowever, if we want to deal with issues such as focus, exposure, vignetting, and aber-\n2.2 Photometric image formation\n69\nzi=102 mm\nf = 100 mm\nW=35mm\nzo=5 m\nf.o.v.\nc\nΔzi\nP\nd\nFigure 2.19\nA thin lens of focal length f focuses the light from a plane a distance zo in front\nof the lens at a distance zi behind the lens, where\n1\nzo + 1\nzi = 1\nf . If the focal plane (vertical\ngray line next to c) is moved forward, the images are no longer in focus and the circle of\nconfusion c (small thick line segments) depends on the distance of the image plane motion\n∆zi relative to the lens aperture diameter d. The ﬁeld of view (f.o.v.) depends on the ratio\nbetween the sensor width W and the focal length f (or, more precisely, the focusing distance\nzi, which is usually quite close to f).\nration, we need to develop a more sophisticated model, which is where the study of optics\ncomes in (M¨oller 1988; Hecht 2001; Ray 2002).\nFigure 2.19 shows a diagram of the most basic lens model, i.e., the thin lens composed\nof a single piece of glass with very low, equal curvature on both sides. According to the\nlens law (which can be derived using simple geometric arguments on light ray refraction), the\nrelationship between the distance to an object zo and the distance behind the lens at which a\nfocused image is formed zi can be expressed as\n1\nzo\n+ 1\nzi\n= 1\nf ,\n(2.96)\nwhere f is called the focal length of the lens. If we let zo →∞, i.e., we adjust the lens (move\nthe image plane) so that objects at inﬁnity are in focus, we get zi = f, which is why we can\nthink of a lens of focal length f as being equivalent (to a ﬁrst approximation) to a pinhole a\ndistance f from the focal plane (Figure 2.10), whose ﬁeld of view is given by (2.60).\nIf the focal plane is moved away from its proper in-focus setting of zi (e.g., by twisting\nthe focus ring on the lens), objects at zo are no longer in focus, as shown by the gray plane in\nFigure 2.19. The amount of mis-focus is measured by the circle of confusion c (shown as short\nthick blue line segments on the gray plane).7 The equation for the circle of confusion can be\nderived using similar triangles; it depends on the distance of travel in the focal plane ∆zi\nrelative to the original focus distance zi and the diameter of the aperture d (see Exercise 2.4).\n7 If the aperture is not completely circular, e.g., if it is caused by a hexagonal diaphragm, it is sometimes possible\nto see this effect in the actual blur function (Levin, Fergus, Durand et al. 2007; Joshi, Szeliski, and Kriegman 2008)\nor in the “glints” that are seen when shooting into the sun.\n70\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 2.20 Regular and zoom lens depth of ﬁeld indicators.\nThe allowable depth variation in the scene that limits the circle of confusion to an accept-\nable number is commonly called the depth of ﬁeld and is a function of both the focus distance\nand the aperture, as shown diagrammatically by many lens markings (Figure 2.20). Since this\ndepth of ﬁeld depends on the aperture diameter d, we also have to know how this varies with\nthe commonly displayed f-number, which is usually denoted as f/# or N and is deﬁned as\nf/# = N = f\nd ,\n(2.97)\nwhere the focal length f and the aperture diameter d are measured in the same unit (say,\nmillimeters).\nThe usual way to write the f-number is to replace the # in f/# with the actual number,\ni.e., f/1.4, f/2, f/2.8, . . . , f/22. (Alternatively, we can say N = 1.4, etc.) An easy way to\ninterpret these numbers is to notice that dividing the focal length by the f-number gives us the\ndiameter d, so these are just formulas for the aperture diameter.8\nNotice that the usual progression for f-numbers is in full stops, which are multiples of\n√\n2,\nsince this corresponds to doubling the area of the entrance pupil each time a smaller f-number\nis selected. (This doubling is also called changing the exposure by one exposure value or EV.\nIt has the same effect on the amount of light reaching the sensor as doubling the exposure\nduration, e.g., from 1/125 to 1/250, see Exercise 2.5.)\nNow that you know how to convert between f-numbers and aperture diameters, you can\nconstruct your own plots for the depth of ﬁeld as a function of focal length f, circle of\nconfusion c, and focus distance zo, as explained in Exercise 2.4 and see how well these match\nwhat you observe on actual lenses, such as those shown in Figure 2.20.\nOf course, real lenses are not inﬁnitely thin and therefore suffer from geometric aber-\nrations, unless compound elements are used to correct for them. The classic ﬁve Seidel\naberrations, which arise when using third-order optics, include spherical aberration, coma,\nastigmatism, curvature of ﬁeld, and distortion (M¨oller 1988; Hecht 2001; Ray 2002).\n8 This also explains why, with zoom lenses, the f-number varies with the current zoom (focal length) setting.",
  "image_path": "page_091.jpg",
  "pages": [
    90,
    91,
    92
  ]
}