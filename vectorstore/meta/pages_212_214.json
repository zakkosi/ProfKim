{
  "doc_id": "pages_212_214",
  "text": "190\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nd (i, j+1)\nFigure 3.63\nGraphical model for a discriminative random ﬁeld (DRF). The additional green\nedges show how combinations of sensed data, e.g., d(i, j + 1), inﬂuence the data term for\nf(i, j). The generative model is therefore more complex, i.e., we cannot just apply a simple\nfunction to the unknown variables and add noise.\nBrown et al. 2004; Rother, Kolmogorov, and Blake 2004), denoising (Tappen, Liu, Freeman\net al. 2007), and object recognition (Section 14.4.3) (Winn and Shotton 2006; Shotton, Winn,\nRother et al. 2009).\nIn stereo matching, the idea of encouraging disparity discontinuities to coincide with\nintensity edges goes back even further to the early days of optimization and MRF-based\nalgorithms (Poggio, Gamble, and Little 1988; Fua 1993; Bobick and Intille 1999; Boykov,\nVeksler, and Zabih 2001) and is discussed in more detail in (Section 11.5).\nIn addition to using smoothness terms that adapt to the input data, Kumar and Hebert\n(2003) also compute a neighborhood function over the input data for each Vp(xp, y) term,\nas illustrated in Figure 3.63, instead of using the classic unary MRF data term Vp(xp, yp)\nshown in Figure 3.56.26 Because such neighborhood functions can be thought of as dis-\ncriminant functions (a term widely used in machine learning (Bishop 2006)), they call the\nresulting graphical model a discriminative random ﬁeld (DRF). In their paper, Kumar and\nHebert (2006) show that DRFs outperform similar CRFs on a number of applications, such\nas structure detection (Figure 3.64) and binary image denoising.\nHere again, one could argue that previous stereo correspondence algorithms also look at\na neighborhood of input data, either explicitly, because they compute correlation measures\n(Criminisi, Cross, Blake et al. 2006) as data terms, or implicitly, because even pixel-wise\ndisparity costs look at several pixels in either the left or right image (Barnard 1989; Boykov,\nVeksler, and Zabih 2001).\n26 Kumar and Hebert (2006) call the unary potentials Vp(xp, y) association potentials and the pairwise potentials\nVp,q(xp, yq, y) interaction potentials.\n3.7 Global optimization\n191\nFigure 3.64\nStructure detection results using an MRF (left) and a DRF (right) (Kumar and\nHebert 2006) c⃝2006 Springer.\nWhat, then are the advantages and disadvantages of using conditional or discriminative\nrandom ﬁelds instead of MRFs?\nClassic Bayesian inference (MRF) assumes that the prior distribution of the data is in-\ndependent of the measurements. This makes a lot of sense: if you see a pair of sixes when\nyou ﬁrst throw a pair of dice, it would be unwise to assume that they will always show up\nthereafter. However, if after playing for a long time you detect a statistically signiﬁcant bias,\nyou may want to adjust your prior. What CRFs do, in essence, is to select or modify the prior\nmodel based on observed data. This can be viewed as making a partial inference over addi-\ntional hidden variables or correlations between the unknowns (say, a label, depth, or clean\nimage) and the knowns (observed images).\nIn some cases, the CRF approach makes a lot of sense and is, in fact, the only plausi-\nble way to proceed. For example, in grayscale image colorization (Section 10.3.2) (Levin,\nLischinski, and Weiss 2004), the best way to transfer the continuity information from the\ninput grayscale image to the unknown color image is to modify local smoothness constraints.\nSimilarly, for simultaneous segmentation and recognition (Winn and Shotton 2006; Shotton,\nWinn, Rother et al. 2009), it makes a lot of sense to permit strong color edges to inﬂuence\nthe semantic image label continuities.\nIn other cases, such as image denoising, the situation is more subtle.\nUsing a non-\nquadratic (robust) smoothness term as in (3.113) plays a qualitatively similar role to setting\nthe smoothness based on local gradient information in a Gaussian MRF (GMRF) (Tappen,\nLiu, Freeman et al. 2007). (In more recent work, Tanaka and Okutomi (2008) use a larger\nneighborhood and full covariance matrix on a related Gaussian MRF.) The advantage of Gaus-\nsian MRFs, when the smoothness can be correctly inferred, is that the resulting quadratic\nenergy can be minimized in a single step. However, for situations where the discontinuities\nare not self-evident in the input data, such as for piecewise-smooth sparse data interpolation\n(Blake and Zisserman 1987; Terzopoulos 1988), classic robust smoothness energy minimiza-\n192\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntion may be preferable. Thus, as with most computer vision algorithms, a careful analysis of\nthe problem at hand and desired robustness and computation constraints may be required to\nchoose the best technique.\nPerhaps the biggest advantage of CRFs and DRFs, as argued by Kumar and Hebert (2006),\nTappen, Liu, Freeman et al. (2007) and Blake, Rother, Brown et al. (2004), is that learning the\nmodel parameters is sometimes easier. While learning parameters in MRFs and their variants\nis not a topic that we cover in this book, interested readers can ﬁnd more details in recently\npublished articles (Kumar and Hebert 2006; Roth and Black 2007a; Tappen, Liu, Freeman et\nal. 2007; Tappen 2007; Li and Huttenlocher 2008).\n3.7.3 Application: Image restoration\nIn Section 3.4.4, we saw how two-dimensional linear and non-linear ﬁlters can be used to\nremove noise or enhance sharpness in images. Sometimes, however, images are degraded by\nlarger problems, such as scratches and blotches (Kokaram 2004). In this case, Bayesian meth-\nods such as MRFs, which can model spatially varying per-pixel measurement noise, can be\nused instead. An alternative is to use hole ﬁlling or inpainting techniques (Bertalmio, Sapiro,\nCaselles et al. 2000; Bertalmio, Vese, Sapiro et al. 2003; Criminisi, P´erez, and Toyama 2004),\nas discussed in Sections 5.1.4 and 10.5.1.\nFigure 3.57 shows an example of image denoising and inpainting (hole ﬁlling) using a\nMarkov random ﬁeld. The original image has been corrupted by noise and a portion of the\ndata has been removed. In this case, the loopy belief propagation algorithm computes a\nslightly lower energy and also a smoother image than the alpha-expansion graph cut algo-\nrithm.\n3.8 Additional reading\nIf you are interested in exploring the topic of image processing in more depth, some popular\ntextbooks have been written by Lim (1990); Crane (1997); Gomes and Velho (1997); J¨ahne\n(1997); Pratt (2007); Russ (2007); Burger and Burge (2008); Gonzales and Woods (2008).\nThe pre-eminent conference and journal in this ﬁeld are the IEEE Conference on Image Pro-\ncesssing and the IEEE Transactions on Image Processing.\nFor image compositing operators, the seminal reference is by Porter and Duff (1984)\nwhile Blinn (1994a,b) provides a more detailed tutorial. For image compositing, Smith and\nBlinn (1996) were the ﬁrst to bring this topic to the attention of the graphics community,\nwhile Wang and Cohen (2007a) provide a recent in-depth survey.\nIn the realm of linear ﬁltering, Freeman and Adelson (1991) provide a great introduc-\ntion to separable and steerable oriented band-pass ﬁlters, while Perona (1995) shows how to",
  "image_path": "page_213.jpg",
  "pages": [
    212,
    213,
    214
  ]
}