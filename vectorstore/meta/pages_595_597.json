{
  "doc_id": "pages_595_597",
  "text": "11.8 Exercises\n573\nEx 11.5: Aggregation and window-based stereo\nImplement one or more of the matching\ncost aggregation strategies described in Section 11.4:\n• convolution with a box or Gaussian kernel;\n• shifting window locations by applying a min ﬁlter (Scharstein and Szeliski 2002);\n• picking a window that maximizes some match-reliability metric (Veksler 2001, 2003);\n• weighting pixels by their similarity to the central pixel (Yoon and Kweon 2006).\nOnce you have aggregated the costs in the DSI, pick the winner at each pixel (winner-take-\nall), and then optionally perform one or more of the following post-processing steps:\n1. compute matches both ways and pick only the reliable matches (draw the others in\nanother color);\n2. tag matches that are unsure (whose conﬁdence is too low);\n3. ﬁll in the matches that are unsure from neighboring values;\n4. reﬁne your matches to sub-pixel disparity by either ﬁtting a parabola to the DSI values\naround the winner or by using an iteration of Lukas–Kanade.\nEx 11.6: Optimization-based stereo\nCompute the disparity space image (DSI) volume us-\ning one of the techniques you implemented in Exercise 11.4 and then implement one (or more)\nof the global optimization techniques described in Section 11.5 to compute the depth map.\nPotential choices include:\n• dynamic programming or scanline optimization (relatively easy);\n• semi-global optimization (Hirschm¨uller 2008), which is a simple extension of scanline\noptimization and performs well;\n• graph cuts using alpha expansions (Boykov, Veksler, and Zabih 2001), for which you\nwill need to ﬁnd a max-ﬂow or min-cut algorithm (http://vision.middlebury.edu/stereo);\n• loopy belief propagation (Appendix B.5.3).\nEvaluate your algorithm by running it on the Middlebury stereo data sets.\nHow well does your algorithm do against local aggregation (Yoon and Kweon 2006)?\nCan you think of some extensions or modiﬁcations to make it even better?\n574\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 11.7: View interpolation, revisited\nCompute a dense depth map using one of the tech-\nniques you developed above and use it (or, better yet, a depth map for each source image) to\ngenerate smooth in-between views from a stereo data set.\nCompare your results against using the ground truth depth data (if available).\nWhat kinds of artifacts do you see? Can you think of ways to reduce them?\nMore details on implementing such algorithms can be found in Section 13.1 and Exercises\n13.1–13.4.\nEx 11.8: Multi-frame stereo\nExtend one of your previous techniques to use multiple input\nframes (Section 11.6) and try to improve the results you obtained with just two views.\nIf helpful, try using temporal selection (Kang and Szeliski 2004) to deal with the increased\nnumber of occlusions in multi-frame data sets.\nYou can also try to simultaneously estimate multiple depth maps and make them consis-\ntent (Kolmogorov and Zabih 2002; Kang and Szeliski 2004).\nTest your algorithms out on some standard multi-view data sets.\nEx 11.9: Volumetric stereo\nImplement voxel coloring (Seitz and Dyer 1999) as a simple\nextension to the plane sweep algorithm you implemented in Exercise 11.4.\n1. Instead of computing the complete DSI all at once, evaluate each plane one at a time\nfrom front to back.\n2. Tag every voxel whose photoconsistency is below a certain threshold as being part of\nthe object and remember its average (or robust) color (Seitz and Dyer 1999; Eisert,\nSteinbach, and Girod 2000; Kutulakos 2000; Slabaugh, Culbertson, Slabaugh et al.\n2004).\n3. Erase the input pixels corresponding to tagged voxels in the input images, e.g., by\nsetting their alpha value to 0 (or to some reduced number, depending on occupancy).\n4. As you evaluate the next plane, use the source image alpha values to modify your\nphotoconsistency score, e.g., only consider pixels that have full alpha or weight pixels\nby their alpha values.\n5. If the cameras are not all on the same side of your plane sweeps, use space carving\n(Kutulakos and Seitz 2000) to cycle through different subsets of source images while\ncarving away the volume from different directions.\nEx 11.10: Depth map merging\nUse the technique you developed for multi-frame stereo in\nExercise 11.8 or a different technique, such as the one described by Goesele, Snavely, Curless\net al. (2007), to compute a depth map for every input image.\n11.8 Exercises\n575\nMerge these depth maps into a coherent 3D model, e.g., using Poisson surface reconstruc-\ntion (Kazhdan, Bolitho, and Hoppe 2006).\nEx 11.11: Shape from silhouettes\nBuild a silhouette-based volume reconstruction algo-\nrithm (Section 11.6.2). Use an octree or some other representation of your choosing.",
  "image_path": "page_596.jpg",
  "pages": [
    595,
    596,
    597
  ]
}