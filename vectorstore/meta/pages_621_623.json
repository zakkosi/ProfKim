{
  "doc_id": "pages_621_623",
  "text": "12.6 Model-based reconstruction\n599\nFigure 12.14 Interactive architectural modeling using the Fac¸ade system (Debevec, Taylor,\nand Malik 1996) c⃝1996 ACM: (a) input image with user-drawn edges shown in green;\n(b) shaded 3D solid model; (c) geometric primitives overlaid onto the input image; (d) ﬁnal\nview-dependent, texture-mapped 3D model.\n(Figure 12.14b–c). This approach is intrinsically more reliable than general feature-based\nstructure from motion, because it exploits the strong geometry available in the block primi-\ntives. Related work by Becker and Bove (1995), Horry, Anjyo, and Arai (1997), and Crimin-\nisi, Reid, and Zisserman (2000) exploits similar information available from vanishing points.\nIn the interactive, image-based modeling system of Sinha, Steedly, Szeliski et al. (2008),\nvanishing point directions are used to guide the user drawing of polygons, which are then\nautomatically ﬁtted to sparse 3D points recovered using structure from motion.\nOnce the rough geometry has been estimated, more detailed offset maps can be com-\nputed for each planar face using a local plane sweep, which Debevec, Taylor, and Malik\n(1996) call model-based stereo. Finally, during rendering, images from different viewpoints\nare warped and blended together as the camera moves around the scene, using a process (re-\nlated to light ﬁeld and Lumigraph rendering, see Section 13.3) called view-dependent texture\nmapping (Figure 12.14d).\nFor interior modeling, instead of working with single pictures, it is more useful to work\nwith panoramas, since you can see larger extents of walls and other structures. The 3D mod-\neling system developed by Shum, Han, and Szeliski (1998) ﬁrst constructs calibrated panora-\nmas from multiple images (Section 7.4) and then has the user draw vertical and horizontal\nlines in the image to demarcate the boundaries of planar regions. The lines are initially used\nto establish an absolute rotation for each panorama and are later used (along with the inferred\nvertices and planes) to optimize the 3D structure, which can be recovered up to scale from\none or more images (Figure 12.15). 360◦high dynamic range panoramas can also be used for\noutdoor modeling, since they provide highly reliable estimates of relative camera orientations\nas well as vanishing point directions (Antone and Teller 2002; Teller, Antone, Bodnar et al.\n600\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 12.15\nInteractive 3D modeling from panoramas (Shum, Han, and Szeliski 1998)\nc⃝1998 IEEE: (a) wide-angle view of a panorama with user-drawn vertical and horizontal\n(axis-aligned) lines; (b) single-view reconstruction of the corridors.\n2003).\nWhile earlier image-based modeling systems required some user authoring, Werner and\nZisserman (2002) present a fully automated line-based reconstruction system. As described\nin Section 7.5.1, they ﬁrst detect lines and vanishing points and use them to calibrate the\ncamera; then they establish line correspondences using both appearance matching and tri-\nfocal tensors, which enables them to reconstruct families of 3D line segments, as shown in\nFigure 12.16a. They then generate plane hypotheses, using both co-planar 3D lines and a\nplane sweep (Section 11.1.2) based on cross-correlation scores evaluated at interest points.\nIntersections of planes are used to determine the extent of each plane, i.e., an initial coarse ge-\nometry, which is then reﬁned with the addition of rectangular or wedge-shaped indentations\nand extrusions (Figure 12.16c). Note that when top-down maps of the buildings being mod-\neled are available, these can be used to further constrain the 3D modeling process (Robertson\nand Cipolla 2002, 2009). The idea of using matched 3D lines for estimating vanishing point\ndirections and dominant planes continues to be used in a number of recent fully automated\nimage-based architectural modeling systems (Zebedin, Bauer, Karner et al. 2008; Miˇcuˇs´ık\nand Koˇseck´a 2009; Furukawa, Curless, Seitz et al. 2009b; Sinha, Steedly, and Szeliski 2009).\nAnother common characteristic of architecture is the repeated use of primitives such as\nwindows, doors, and colonnades. Architectural modeling systems can be designed to search\nfor such repeated elements and to use them as part of the structure inference process (Dick,\nTorr, and Cipolla 2004; Mueller, Zeng, Wonka et al. 2007; Schindler, Krishnamurthy, Lublin-\nerman et al. 2008; Sinha, Steedly, Szeliski et al. 2008).\nThe combination of all these techniques now makes it possible to reconstruct the structure\nof large 3D scenes (Zhu and Kanade 2008). For example, the Urbanscan system of Polle-\nfeys, Nist´er, Frahm et al. (2008) reconstructs texture-mapped 3D models of city streets from\nvideos acquired with a GPS-equipped vehicle. To obtain real-time performance, they use\nboth optimized on-line structure-from-motion algorithms, as well as GPU implementations\n12.6 Model-based reconstruction\n601\n(a)\n(b)\n(c)\n(d)\nFigure 12.16\nAutomated architectural reconstruction using 3D lines and planes (Werner\nand Zisserman 2002) c⃝2002 Springer: (a) reconstructed 3D lines, color coded by their van-\nishing directions; (b) wire-frame model superimposed onto an input image; (c) triangulated\npiecewise-planar model with windows; (d) ﬁnal texture-mapped model.\nof plane-sweep stereo aligned to dominant planes and depth map fusion. Cornelis, Leibe,\nCornelis et al. (2008) present a related system that also uses plane-sweep stereo (aligned to\nvertical building fac¸ades) combined with object recognition and segmentation for vehicles.\nMiˇcuˇs´ık and Koˇseck´a (2009) build on these results using omni-directional images and super-\npixel-based stereo matching along dominant plane orientations. Reconstruction directly from\nactive range scanning data combined with color imagery that has been compensated for ex-\nposure and lighting variations is also possible (Chen and Chen 2008; Stamos, Liu, Chen et\nal. 2008; Troccoli and Allen 2008).\n12.6.2 Heads and faces\nAnother area in which specialized shape and appearance models are extremely helpful is in\nthe modeling of heads and faces. Even though the appearance of people seems at ﬁrst glance\nto be inﬁnitely variable, the actual shape of a person’s head and face can be described rea-\nsonably well using a few dozen parameters (Pighin, Hecker, Lischinski et al. 1998; Guenter,\nGrimm, Wood et al. 1998; DeCarlo, Metaxas, and Stone 1998; Blanz and Vetter 1999; Shan,\nLiu, and Zhang 2001).\nFigure 12.17 shows an example of an image-based modeling system, where user-speciﬁed\nkeypoints in several images are used to ﬁt a generic head model to a person’s face. As you\ncan see in Figure 12.17c, after specifying just over 100 keypoints, the shape of the face has\nbecome quite adapted and recognizable. Extracting a texture map from the original images\nand then applying it to the head model results in an animatable model with striking visual\nﬁdelity (Figure 12.18a).\nA more powerful system can be built by applying principal component analysis (PCA) to\na collection of 3D scanned faces, which is a topic we discuss in Section 12.6.3. As you can\nsee in Figure 12.19, it is then possible to ﬁt morphable 3D models to single images and to",
  "image_path": "page_622.jpg",
  "pages": [
    621,
    622,
    623
  ]
}