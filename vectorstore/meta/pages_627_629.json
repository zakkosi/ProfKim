{
  "doc_id": "pages_627_629",
  "text": "12.6 Model-based reconstruction\n605\nAfter computing a subspace representation, different directions in this space can be as-\nsociated with different characteristics such as gender, facial expressions, or facial features\n(Figure 12.19a). As in the work of Rowland and Perrett (1995), faces can be turned into\ncaricatures by exaggerating their displacement from the mean image.\n3D morphable models can be ﬁtted to a single image using gradient descent on the error\nbetween the input image and the re-synthesized model image, after an initial manual place-\nment of the model in an approximately correct pose, scale, and location (Figures 12.19b–c).\nThe efﬁciency of this ﬁtting process can be increased using inverse compositional image\nalignment (8.64–8.65), as described by Romdhani and Vetter (2003).\nThe resulting texture-mapped 3D model can then be modiﬁed to produce a variety of vi-\nsual effects, including changing a person’s weight or expression, or three-dimensional effects\nsuch as re-lighting or 3D video-based animation (Section 13.5.1). Such models can also be\nused for video compression, e.g., by only transmitting a small number of facial expression\nand pose parameters to drive a synthetic avatar (Eisert, Wiegand, and Girod 2000; Gao, Chen,\nWang et al. 2003).\n3D facial animation is often matched to the performance of an actor, in what is known\nas performance-driven animation (Section 4.1.5) (Williams 1990). Traditional performance-\ndriven animation systems use marker-based motion capture (Ma, Jones, Chiang et al. 2008),\nwhile some newer systems use video footage to control the animation (Buck, Finkelstein,\nJacobs et al. 2000; Pighin, Szeliski, and Salesin 2002; Zhang, Snavely, Curless et al. 2004;\nVlasic, Brand, Pﬁster et al. 2005).\nAn example of the latter approach is the system developed for the ﬁlm Benjamin Button,\nin which Digital Domain used the CONTOUR system from Mova10 to capture actor Brad\nPitt’s facial motions and expressions (Roble and Zafar 2009). CONTOUR uses a combina-\ntion of phosphorescent paint and multiple high-resolution video cameras to capture real-time\n3D range scans of the actor. These 3D models were then translated into Facial Action Cod-\ning System (FACS) shape and expression parameters (Ekman and Friesen 1978) to drive a\ndifferent (older) synthetically animated computer-generated imagery (CGI) character.\n12.6.4 Whole body modeling and tracking\nThe topics of tracking humans, modeling their shape and appearance, and recognizing their\nactivities, are some of the most actively studied areas of computer vision. Annual confer-\nences11 and special journal issues (Hilton, Fua, and Ronfard 2006) are devoted to this sub-\nject, and two recent surveys (Forsyth, Arikan, Ikemoto et al. 2006; Moeslund, Hilton, and\n10 http://www.mova.com.\n11 International Conference on Automatic Face and Gesture Recognition (FG), IEEE Workshop on Analysis and\nModeling of Faces and Gestures, and International Workshop on Tracking Humans for the Evaluation of their Motion\nin Image Sequences (THEMIS).\n606\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKr¨uger 2006) each list over 400 papers devoted to these topics.12 The HumanEva database\nof articulated human motions13 contains multi-view video sequences of human actions along\nwith corresponding motion capture data, evaluation code, and a reference 3D tracker based on\nparticle ﬁltering. The companion paper by Sigal, Balan, and Black (2010) not only describes\nthe database and evaluation but also has a nice survey of important work in this ﬁeld.\nGiven the breadth of this area, it is difﬁcult to categorize all of this research, especially\nsince different techniques usually build on each other. Moeslund, Hilton, and Kr¨uger (2006)\ndivide their survey into initialization, tracking (which includes background modeling and\nsegmentation), pose estimation, and action (activity) recognition. Forsyth, Arikan, Ikemoto et\nal. (2006) divide their survey into sections on tracking (background subtraction, deformable\ntemplates, ﬂow, and probabilistic models), recovering 3D pose from 2D observations, and\ndata association and body parts. They also include a section on motion synthesis, which is\nmore widely studied in computer graphics (Arikan and Forsyth 2002; Kovar, Gleicher, and\nPighin 2002; Lee, Chai, Reitsma et al. 2002; Li, Wang, and Shum 2002; Pullen and Bregler\n2002), see Section 13.5.2. Another potential taxonomy for work in this ﬁeld would be along\nthe lines of whether 2D or 3D (or multi-view) images are used as input and whether 2D or\n3D kinematic models are used.\nIn this section, we brieﬂy review some of the more seminal and widely cited papers in the\nareas of background subtraction, initialization and detection, tracking with ﬂow, 3D kinematic\nmodels, probabilistic models, adaptive shape modeling, and activity recognition. We refer the\nreader to the previously mentioned surveys for other topics and more details.\nBackground subtraction.\nOne of the ﬁrst steps in many (but certainly not all) human track-\ning systems is to model the background in order to extract the moving foreground objects\n(silhouettes) corresponding to people. Toyama, Krumm, Brumitt et al. (1999) review several\ndifference matting and background maintenance (modeling) techniques and provide a good\nintroduction to this topic. Stauffer and Grimson (1999) describe some techniques based on\nmixture models, while Sidenbladh and Black (2003) develop a more comprehensive treat-\nment, which models not only the background image statistics but also the appearance of the\nforeground objects, e.g., their edge and motion (frame difference) statistics.\nOnce silhouettes have been extracted from one or more cameras, they can then be mod-\neled using deformable templates or other contour models (Baumberg and Hogg 1996; Wren,\nAzarbayejani, Darrell et al. 1997). Tracking such silhouettes over time supports the analysis\nof multiple people moving around a scene, including building shape and appearance models\n12 Older surveys include those by Gavrila (1999) and Moeslund and Granum (2001). Some surveys on gesture\nrecognition, which we do not cover in this book, include those by Pavlovi´c, Sharma, and Huang (1997) and Yang,\nAhuja, and Tabb (2002).\n13 http://vision.cs.brown.edu/humaneva/.\n12.6 Model-based reconstruction\n607\nand detecting if they are carrying objects (Haritaoglu, Harwood, and Davis 2000; Mittal and\nDavis 2003; Dimitrijevic, Lepetit, and Fua 2006).\nInitialization and detection.\nIn order to track people in a fully automated manner, it is\nnecessary to ﬁrst detect (or re-acquire) their presence in individual video frames. This topic\nis closely related to pedestrian detection, which is often considered as a kind of object recog-\nnition (Mori, Ren, Efros et al. 2004; Felzenszwalb and Huttenlocher 2005; Felzenszwalb,\nMcAllester, and Ramanan 2008), and is therefore treated in more depth in Section 14.1.2.\nAdditional techniques for initializing 3D trackers based on 2D images include those described\nby Howe, Leventon, and Freeman (2000), Rosales and Sclaroff (2000), Shakhnarovich, Viola,\nand Darrell (2003), Sminchisescu, Kanaujia, Li et al. (2005), Agarwal and Triggs (2006), Lee\nand Cohen (2006), Sigal and Black (2006), and Stenger, Thayananthan, Torr et al. (2006).\nSingle-frame human detection and pose estimation algorithms can sometimes be used by\nthemselves to perform tracking (Ramanan, Forsyth, and Zisserman 2005; Rogez, Rihan, Ra-\nmalingam et al. 2008; Bourdev and Malik 2009), as described in Section 4.1.4. More often,\nhowever, they are combined with frame-to-frame tracking techniques to provide better relia-\nbility (Fossati, Dimitrijevic, Lepetit et al. 2007; Andriluka, Roth, and Schiele 2008; Ferrari,\nMarin-Jimenez, and Zisserman 2008).\nTracking with ﬂow.\nThe tracking of people and their pose from frame to frame can be en-\nhanced by computing optic ﬂow or matching the appearance of their limbs from one frame\nto another. For example, the cardboard people model of Ju, Black, and Yacoob (1996) mod-\nels the appearance of each leg portion (upper and lower) as a moving rectangle, and uses\noptic ﬂow to estimate their location in each subsequent frame. Cham and Rehg (1999) and\nSidenbladh, Black, and Fleet (2000) track limbs using optical ﬂow and templates, along with\ntechniques for dealing with multiple hypotheses and uncertainty. Bregler, Malik, and Pullen\n(2004) use a full 3D model of limb and body motion, as described below. It is also possible to\nmatch the estimated motion ﬁeld itself to some prototypes in order to identify the particular\nphase of a running motion or to match two low-resolution video portions in order to perform\nvideo replacement (Efros, Berg, Mori et al. 2003).\n3D kinematic models.\nThe effectiveness of human modeling and tracking can be greatly\nenhanced using a more accurate 3D model of a person’s shape and motion. Underlying such\nrepresentations, which are ubiquitous in 3D computer animation in games and special effects,\nis a kinematic model or kinematic chain, which speciﬁes the length of each limb in a skeleton\nas well as the 2D or 3D rotation angles between the limbs or segments (Figure 12.20a–b).\nInferring the values of the joint angles from the locations of the visible surface points is\ncalled inverse kinematics (IK) and is widely studied in computer graphics.",
  "image_path": "page_628.jpg",
  "pages": [
    627,
    628,
    629
  ]
}