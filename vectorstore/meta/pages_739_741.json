{
  "doc_id": "pages_739_741",
  "text": "14.5 Context and scene understanding\n717\nFigure 14.53\nImageNet (Deng, Dong, Socher et al. 2009) c⃝2009 IEEE. This database\ncontains over 500 carefully vetted images for each of 14,841 (as of April, 2010) nouns from\nthe WordNet hierarchy.\nsynsets have been collected (Figure 14.53). The paper by Deng, Dong, Socher et al. (2009)\nalso has a nice review of related databases.\nAs we mentioned in Section 14.4.3, the existence of large databases of partially labeled\nInternet imagery has given rise to a new sub-ﬁeld of Internet computer vision, with its own\nworkshops21 and a special journal issue (Avidan, Baker, and Shan 2010).\n14.5.2 Application: Image search\nEven though visual recognition algorithms are by some measures still in their infancy, they\nare already starting to have some impact on image search, i.e., the retrieval of images from the\nWeb using combinations of keywords and visual similarity. Today, most image search engines\nrely mostly on textual keywords found in captions, nearby text, and ﬁlenames, augmented by\nuser click-through data (Craswell and Szummer 2007). As recognition algorithms continue\nto improve, however, visual features and visual similarity will start being used to recognize\nimages with missing or erroneous keywords.\nThe topic of searching by visual similarity has a long history and goes by a variety of\nnames, including content-based image retrieval (CBIR) (Smeulders, Worring, Santini et al.\n2000; Lew, Sebe, Djeraba et al. 2006; Vasconcelos 2007; Datta, Joshi, Li et al. 2008) and\nquery by image content (QBIC) (Flickner, Sawhney, Niblack et al. 1995). Original publica-\ntions in these ﬁelds were based primarily on simple whole-image similarity metrics, such as\ncolor and texture (Swain and Ballard 1991; Jacobs, Finkelstein, and Salesin 1995; Manjunathi\nand Ma 1996).\n21 http://www.internetvisioner.org/.\n718\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn more recent work, Fergus, Perona, and Zisserman (2004) use a feature-based learning\nand recognition algorithm to re-rank the outputs from a traditional keyword-based image\nsearch engine. In follow-on work, Fergus, Fei-Fei, Perona et al. (2005) cluster the results\nreturned by image search using an extension of probabilistic latest semantic analysis (PLSA)\n(Hofmann 1999) and then select the clusters associated with the highest ranked results as the\nrepresentative images for that category.\nEven more recent work relies on carefully annotated image databases such as LabelMe\n(Russell, Torralba, Murphy et al. 2008). For example, Malisiewicz and Efros (2008) describe\na system that, given a query image, can ﬁnd similar LabelMe images, whereas Liu, Yuen, and\nTorralba (2009) combine feature-based correspondence algorithms with the labeled database\nto perform simultaneous recognition and segmentation.\n14.6 Recognition databases and test sets\nIn addition to rapid advances in machine learning and statistical modeling techniques, one\nof the key ingredients in the continued improvement of recognition algorithms has been the\nincreased availability and quality of image recognition databases.\nTables 14.1 and 14.2, which are based on similar tables in Fei-Fei, Fergus, and Torralba\n(2009), updated with more recent entries and URLs, show some of the mostly widely used\nrecognition databases. Some of these databases, such as the ones for face recognition and\nlocalization, date back over a decade. The most recent ones, such as the PASCAL database,\nare refreshed annually with ever more challenging problems. Table 14.1 shows examples of\ndatabases used primarily for (whole image) recognition while Table 14.2 shows databases\nwhere more accurate localization or segmentation information is available and expected.\nPonce, Berg, Everingham et al. (2006) discuss some of the problems with earlier datasets\nand describe how the latest PASCAL Visual Object Classes Challenge aims to overcome\nthese. Some examples of the 20 visual classes in the 2008 challenge are shown in Fig-\nure 14.54. The slides from the VOC workshops,22 are a great source for pointers to the\nbest recognition techniques currently available.\nTwo of the most recent trends in recognition databases are the emergence of Web-based\nannotation and data collection tools, and the use of search and recognition algorithms to build\nup databases (Ponce, Berg, Everingham et al. 2006). Some of the most interesting work in\nhuman annotation of images comes from a series of interactive multi-person games such as\nESP (von Ahn and Dabbish 2004) and Peekaboom (von Ahn, Liu, and Blum 2006). In these\ngames, people help each other guess the identity of a hidden image by giving textual clues\nas to its contents, which implicitly labels either the whole image or just regions. A more\n22 http://pascallin.ecs.soton.ac.uk/challenges/VOC/.\n14.6 Recognition databases and test sets\n719\nName / URL\nExtents\nContents / Reference\nFace and person recognition\nYale face database\nCentered face images\nFrontal faces\nhttp://www1.cs.columbia.edu/∼belhumeur/\nBelhumeur, Hespanha, and Kriegman (1997)\nResources for face detection\nVarious databases\nFaces in various poses\nhttp://vision.ai.uiuc.edu/mhyang/face-detection-survey.html\nYang, Kriegman, and Ahuja (2002)\nFERET\nCentered face images\nFrontal faces\nhttp://www.frvt.org/FERET\nPhillips, Moon, Rizvi et al. (2000)\nFRVT\nCentered face images\nFaces in various poses\nhttp://www.frvt.org/\nPhillips, Scruggs, O’Toole et al. (2010)\nCMU PIE database\nCentered face image\nFaces in various poses\nhttp://www.ri.cmu.edu/projects/project 418.html\nSim, Baker, and Bsat (2003)\nCMU Multi-PIE database\nCentered face image\nFaces in various poses\nhttp://multipie.org\nGross, Matthews, Cohn et al. (2010)\nFaces in the Wild\nInternet images\nFaces in various poses\nhttp://vis-www.cs.umass.edu/lfw/\nHuang, Ramesh, Berg et al. (2007)\nConsumer image person DB\nComplete images\nPeople\nhttp://chenlab.ece.cornell.edu/people/Andy/GallagherDataset.html\nGallagher and Chen (2008)\nObject recognition\nCaltech 101\nSegmentation masks\n101 categories\nhttp://www.vision.caltech.edu/Image Datasets/Caltech101/\nFei-Fei, Fergus, and Perona (2006)\nCaltech 256\nCentered objects\n256 categories and clutter\nhttp://www.vision.caltech.edu/Image Datasets/Caltech256/\nGrifﬁn, Holub, and Perona (2007)\nCOIL-100\nCentered objects\n100 instances\nhttp://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php\nNene, Nayar, and Murase (1996)\nETH-80\nCentered objects\n8 instances, 10 views\nhttp://www.mis.tu-darmstadt.de/datasets\nLeibe and Schiele (2003)\nInstance recognition benchmark\nObjects in various poses\n2550 objects\nhttp://vis.uky.edu/∼stewe/ukbench/\nNist´er and Stew´enius (2006)\nOxford buildings dataset\nPictures of buildings\n5062 images\nhttp://www.robots.ox.ac.uk/∼vgg/data/oxbuildings/\nPhilbin, Chum, Isard et al. (2007)\nNORB\nBounding box\n50 toys\nhttp://www.cs.nyu.edu/∼ylclab/data/norb-v1.0/\nLeCun, Huang, and Bottou (2004)\nTiny images\nComplete images\n75,000 (Wordnet) things\nhttp://people.csail.mit.edu/torralba/tinyimages/\nTorralba, Freeman, and Fergus (2008)\nImageNet\nComplete images\n14,000 (Wordnet) things\nhttp://www.image-net.org/\nDeng, Dong, Socher et al. (2009)\nTable 14.1\nImage databases for recognition, adapted and expanded from Fei-Fei, Fergus,\nand Torralba (2009).",
  "image_path": "page_740.jpg",
  "pages": [
    739,
    740,
    741
  ]
}