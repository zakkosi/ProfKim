{
  "doc_id": "pages_746_748",
  "text": "724\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLazebnik et al. (2007). Additional and more recent papers in this area include Sivic, Russell,\nEfros et al. (2005), Serre, Wolf, and Poggio (2005), Opelt, Pinz, Fussenegger et al. (2006),\nGrauman and Darrell (2007a), Torralba, Murphy, and Freeman (2007), Boiman, Shechtman,\nand Irani (2008), Ferencz, Learned-Miller, and Malik (2008), and Mutch and Lowe (2008).\nIt is also possible to recognize objects based on their contours, e.g., using shape contexts\n(Belongie, Malik, and Puzicha 2002) or other techniques (Jurie and Schmid 2004; Shotton,\nBlake, and Cipolla 2005; Opelt, Pinz, and Zisserman 2006; Ferrari, Tuytelaars, and Van Gool\n2006a).\nMany object recognition algorithms use part-based decompositions to provide greater in-\nvariance to articulation and pose. Early algorithms focused on the relative positions of the\nparts (Fischler and Elschlager 1973; Kanade 1977; Yuille 1991) while newer algorithms use\nmore sophisticated models of appearance (Felzenszwalb and Huttenlocher 2005; Fergus, Per-\nona, and Zisserman 2007; Felzenszwalb, McAllester, and Ramanan 2008). Good overviews\non part-based models for recognition can be found in the course notes of Fergus 2007b; 2009.\nCarneiro and Lowe (2006) discuss a number of graphical models used for part-based\nrecognition, which include trees and stars (Felzenszwalb and Huttenlocher 2005; Fergus, Per-\nona, and Zisserman 2005; Felzenszwalb, McAllester, and Ramanan 2008), k-fans (Crandall,\nFelzenszwalb, and Huttenlocher 2005; Crandall and Huttenlocher 2006), and constellations\n(Burl, Weber, and Perona 1998; Weber, Welling, and Perona 2000; Fergus, Perona, and Zis-\nserman 2007). Other techniques that use part-based recognition include those developed by\nDork´o and Schmid (2003) and Bar-Hillel, Hertz, and Weinshall (2005).\nCombining object recognition with scene segmentation can yield strong beneﬁts. One\napproach is to pre-segment the image into pieces and then match the pieces to portions of\nthe model (Mori, Ren, Efros et al. 2004; Mori 2005; He, Zemel, and Ray 2006; Russell,\nEfros, Sivic et al. 2006; Borenstein and Ullman 2008; Csurka and Perronnin 2008; Gu, Lim,\nArbelaez et al. 2009). Another is to vote for potential object locations and scales based on\nobject detection (Leibe, Leonardis, and Schiele 2008). One of the currently most popular\napproaches is to use conditional random ﬁelds (Kumar and Hebert 2006; He, Zemel, and\nCarreira-Perpi˜n´an 2004; He, Zemel, and Ray 2006; Levin and Weiss 2006; Winn and Shotton\n2006; Hoiem, Rother, and Winn 2007; Rabinovich, Vedaldi, Galleguillos et al. 2007; Verbeek\nand Triggs 2007; Yang, Meer, and Foran 2007; Batra, Sukthankar, and Chen 2008; Larlus\nand Jurie 2008; He and Zemel 2008; Shotton, Winn, Rother et al. 2009; Kumar, Torr, and\nZisserman 2010), which produce some of the best results on the difﬁcult PASCAL VOC seg-\nmentation challenge (Shotton, Johnson, and Cipolla 2008; Kohli, Ladick´y, and Torr 2009).\nMore and more recognition algorithms are starting to use scene context as part of their\nrecognition strategy. Representative papers in this area include those by Torralba (2003),\nTorralba, Murphy, Freeman et al. (2003), Murphy, Torralba, and Freeman (2003), Torralba,\nMurphy, and Freeman (2004), Crandall and Huttenlocher (2007), Rabinovich, Vedaldi, Gal-\n14.8 Exercises\n725\nleguillos et al. (2007), Russell, Torralba, Liu et al. (2007), Hoiem, Efros, and Hebert (2008a),\nHoiem, Efros, and Hebert (2008b), Sudderth, Torralba, Freeman et al. (2008), and Divvala,\nHoiem, Hays et al. (2009).\nSophisticated machine learning techniques are also becoming a key component of suc-\ncessful object detection and recognition algorithms (Varma and Ray 2007; Felzenszwalb,\nMcAllester, and Ramanan 2008; Fritz and Schiele 2008; Sivic, Russell, Zisserman et al.\n2008; Vedaldi, Gulshan, Varma et al. 2009), as is exploiting large human-labeled databases\n(Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros 2008; Torralba, Freeman, and Fer-\ngus 2008; Liu, Yuen, and Torralba 2009). Rough three-dimensional models are also making\na comeback for recognition, as evidenced in some recent papers (Savarese and Fei-Fei 2007,\n2008; Sun, Su, Savarese et al. 2009; Su, Sun, Fei-Fei et al. 2009). As always, the latest con-\nferences on computer vision are your best reference for the newest algorithms in this rapidly\nevolving ﬁeld.\n14.8 Exercises\nEx 14.1: Face detection\nBuild and test one of the face detectors presented in Section 14.1.1.\n1. Download one or more of the labeled face detection databases in Table 14.2.\n2. Generate your own negative examples by ﬁnding photographs that do not contain any\npeople.\n3. Implement one of the following face detectors (or devise one of your own):\n• boosting (Algorithm 14.1) based on simple area features, with an optional cascade\nof detectors (Viola and Jones 2004);\n• PCA face subspace (Moghaddam and Pentland 1997);\n• distances to clustered face and non-face prototypes, followed by a neural network\n(Sung and Poggio 1998) or SVM (Osuna, Freund, and Girosi 1997) classiﬁer;\n• a multi-resolution neural network trained directly on normalized gray-level patches\n(Rowley, Baluja, and Kanade 1998a).\n4. Test the performance of your detector on the database by evaluating the detector at ev-\nery location in a sub-octave pyramid. Optionally retrain your detector on false positive\nexamples you get on non-face images.\nEx 14.2: Determining the threshold for AdaBoost\nGiven a set of function evaluations on\nthe training examples xi, fi = f(xi) ∈±1, training labels yi ∈±1, and weights wi ∈(0, 1),\n726\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nas explained in Algorithm 14.1, devise an efﬁcient algorithm to ﬁnd values of θ and s = ±1\nthat maximize\nX\ni\nwiyih(sfi, θ),\n(14.43)\nwhere h(x, θ) = sign(x −θ).\nEx 14.3: Face recognition using eigenfaces\nCollect a set of facial photographs and then\nbuild a recognition system to re-recognize the same people.\n1. Take several photos of each of your classmates and store them.\n2. Align the images by automatically or manually detecting the corners of the eyes and\nusing a similarity transform to stretch and rotate each image to a canonical position.\n3. Compute the average image and a PCA subspace for the face images\n4. Take a new set of photographs a week later and use them as your test set.\n5. Compare each new image to each database image and select the nearest one as the\nrecognized identity. Verify that the distance in PCA space is close to the distance\ncomputed with a full SSD (sum of squared difference) measure.\n6. (Optional) Compute different principal components for identity and expression, and\nuse them to improve your recognition results.\nEx 14.4: Bayesian face recognition\nMoghaddam, Jebara, and Pentland (2000) compute\nseparate covariance matrices ΣI and ΣE by looking at differences between all pairs of im-\nages. At run time, they select the nearest image to determine the facial identity. Does it make\nsense to estimate statistics for all pairs of images and use them for testing the distance to the\nnearest exemplar? Discuss whether this is statistically correct.\nHow is the all-pair intrapersonal covariance matrix ΣI related to the within-class scatter\nmatrix SW? Does a similar relationship hold between ΣE and SB?\nEx 14.5: Modular eigenfaces\nExtend your face recognition system to separately match the\neye, nose, and mouth regions, as shown in Figure 14.18.\n1. After normalizing face images to a canonical scale and location, manually segment out\nsome of the eye, nose, and face regions.\n2. Build separate detectors for these three (or four) kinds of region, either using a subspace\n(PCA) approach or one of the techniques presented in Section 14.1.1.\n3. For each new image to be recognized, ﬁrst detect the locations of the facial features.",
  "image_path": "page_747.jpg",
  "pages": [
    746,
    747,
    748
  ]
}