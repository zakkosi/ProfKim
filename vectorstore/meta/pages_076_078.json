{
  "doc_id": "pages_076_078",
  "text": "54\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\naccomplished using modiﬁed normalized device coordinates,\nx′\ns = (2xs −W)/S and y′\ns = (2ys −H)/S,\nwhere\nS = max(W, H).\n(2.61)\nThis has the advantage that the focal length f and optical center (cx, cy) become independent\nof the image resolution, which can be useful when using multi-resolution, image-processing\nalgorithms, such as image pyramids (Section 3.5).2 The use of S instead of W also makes the\nfocal length the same for landscape (horizontal) and portrait (vertical) pictures, as is the case\nin 35mm photography. (In some computer graphics textbooks and systems, normalized device\ncoordinates go from [−1, 1] × [−1, 1], which requires the use of two different focal lengths\nto describe the camera intrinsics (Watt 1995; OpenGL-ARB 1997).) Setting S = W = 2 in\n(2.60), we obtain the simpler (unitless) relationship\nf −1 = tan θ\n2.\n(2.62)\nThe conversion between the various focal length representations is straightforward, e.g.,\nto go from a unitless f to one expressed in pixels, multiply by W/2, while to convert from an\nf expressed in pixels to the equivalent 35mm focal length, multiply by 35/W.\nCamera matrix\nNow that we have shown how to parameterize the calibration matrix K, we can put the\ncamera intrinsics and extrinsics together to obtain a single 3 × 4 camera matrix\nP = K\nh\nR\nt\ni\n.\n(2.63)\nIt is sometimes preferable to use an invertible 4 × 4 matrix, which can be obtained by not\ndropping the last row in the P matrix,\n˜\nP =\n\"\nK\n0\n0T\n1\n# \"\nR\nt\n0T\n1\n#\n= ˜\nKE,\n(2.64)\nwhere E is a 3D rigid-body (Euclidean) transformation and ˜\nK is the full-rank calibration\nmatrix. The 4 × 4 camera matrix ˜\nP can be used to map directly from 3D world coordinates\n¯pw = (xw, yw, zw, 1) to screen coordinates (plus disparity), xs = (xs, ys, 1, d),\nxs ∼˜\nP ¯pw,\n(2.65)\nwhere ∼indicates equality up to scale. Note that after multiplication by ˜\nP , the vector is\ndivided by the third element of the vector to obtain the normalized form xs = (xs, ys, 1, d).\n2 To make the conversion truly accurate after a downsampling step in a pyramid, ﬂoating point values of W and\nH would have to be maintained since they can become non-integral if they are ever odd at a larger resolution in the\npyramid.\n2.1 Geometric primitives and transformations\n55\nC\n(xs,ys,d)\nZ\nimage plane\nd=1.0 d=0.67 d=0.5\nd = inverse depth\n(xw,yw,zw)\nd\nz\nC\n(xs,ys,d)\nZ\nimage plane\nd=0.5\nd=0\nd=-0.25\nd = projective depth\n(xw,yw,zw)\nz\nplane\nparallax\nFigure 2.11 Regular disparity (inverse depth) and projective depth (parallax from a reference\nplane).\nPlane plus parallax (projective depth)\nIn general, when using the 4 × 4 matrix ˜\nP , we have the freedom to remap the last row to\nwhatever suits our purpose (rather than just being the “standard” interpretation of disparity as\ninverse depth). Let us re-write the last row of ˜\nP as p3 = s3[ˆn0|c0], where ∥ˆn0∥= 1. We\nthen have the equation\nd = s3\nz (ˆn0 · pw + c0),\n(2.66)\nwhere z = p2 · ¯pw = rz · (pw −c) is the distance of pw from the camera center C (2.25)\nalong the optical axis Z (Figure 2.11). Thus, we can interpret d as the projective disparity\nor projective depth of a 3D scene point pw from the reference plane ˆn0 · pw + c0 = 0\n(Szeliski and Coughlan 1997; Szeliski and Golland 1999; Shade, Gortler, He et al. 1998;\nBaker, Szeliski, and Anandan 1998). (The projective depth is also sometimes called parallax\nin reconstruction algorithms that use the term plane plus parallax (Kumar, Anandan, and\nHanna 1994; Sawhney 1994).) Setting ˆn0 = 0 and c0 = 1, i.e., putting the reference plane\nat inﬁnity, results in the more standard d = 1/z version of disparity (Okutomi and Kanade\n1993).\nAnother way to see this is to invert the ˜\nP matrix so that we can map pixels plus disparity\ndirectly back to 3D points,\n˜pw = ˜\nP\n−1xs.\n(2.67)\nIn general, we can choose ˜\nP to have whatever form is convenient, i.e., to sample space us-\ning an arbitrary projection. This can come in particularly handy when setting up multi-view\nstereo reconstruction algorithms, since it allows us to sweep a series of planes (Section 11.1.2)\nthrough space with a variable (projective) sampling that best matches the sensed image mo-\ntions (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).\n56\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z,1)\nx1 = (x1,y1,1,d1)\nx0 = (x0,y0,1,d0)\n~\n~\nM10\nn0·p+c0= 0\nx1 = (x1,y1,1)\nx0 = (x0,y0,1)\n~\n~\nH10\n^\n.\n(a)\n(b)\nFigure 2.12 A point is projected into two images: (a) relationship between the 3D point co-\nordinate (X, Y, Z, 1) and the 2D projected point (x, y, 1, d); (b) planar homography induced\nby points all lying on a common plane ˆn0 · p + c0 = 0.\nMapping from one camera to another\nWhat happens when we take two images of a 3D scene from different camera positions or\norientations (Figure 2.12a)? Using the full rank 4 × 4 camera matrix ˜\nP = ˜\nKE from (2.64),\nwe can write the projection from world to screen coordinates as\n˜x0 ∼˜\nK0E0p = ˜\nP 0p.\n(2.68)\nAssuming that we know the z-buffer or disparity value d0 for a pixel in one image, we can\ncompute the 3D point location p using\np ∼E−1\n0\n˜\nK\n−1\n0 ˜x0\n(2.69)\nand then project it into another image yielding\n˜x1 ∼˜\nK1E1p = ˜\nK1E1E−1\n0\n˜\nK\n−1\n0 ˜x0 = ˜\nP 1 ˜\nP\n−1\n0 ˜x0 = M 10˜x0.\n(2.70)\nUnfortunately, we do not usually have access to the depth coordinates of pixels in a regular\nphotographic image. However, for a planar scene, as discussed above in (2.66), we can\nreplace the last row of P 0 in (2.64) with a general plane equation, ˆn0 · p + c0 that maps\npoints on the plane to d0 = 0 values (Figure 2.12b). Thus, if we set d0 = 0, we can ignore\nthe last column of M 10 in (2.70) and also its last row, since we do not care about the ﬁnal\nz-buffer depth. The mapping equation (2.70) thus reduces to\n˜x1 ∼˜\nH10˜x0,\n(2.71)\nwhere ˜\nH10 is a general 3 × 3 homography matrix and ˜x1 and ˜x0 are now 2D homogeneous\ncoordinates (i.e., 3-vectors) (Szeliski 1996).This justiﬁes the use of the 8-parameter homog-\nraphy as a general alignment model for mosaics of planar scenes (Mann and Picard 1994;\nSzeliski 1996).",
  "image_path": "page_077.jpg",
  "pages": [
    76,
    77,
    78
  ]
}