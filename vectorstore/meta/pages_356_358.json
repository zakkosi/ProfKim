{
  "doc_id": "pages_356_358",
  "text": "334\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis provides us with some homogeneous linear constraints on the entries in A = KKT ,\nwhich is known as the dual of the image of the absolute conic (Hartley 1997b; Hartley and\nZisserman 2004). (Recall that when we estimate a homography, we can only recover it up to\nan unknown scale.) Given a sufﬁcient number of independent homography estimates ˜\nHij,\nwe can recover A (up to a scale) using either SVD or eigenvalue analysis and then recover\nK through Cholesky decomposition (Appendix A.1.4). Extensions to the cases of temporally\nvarying calibration parameters and non-stationary cameras are discussed by Hartley, Hayman,\nde Agapito et al. (2000) and de Agapito, Hayman, and Reid (2001).\nThe quality of the intrinsic camera parameters can be greatly increased by constructing a\nfull 360◦panorama, since mis-estimating the focal length will result in a gap (or excessive\noverlap) when the ﬁrst image in the sequence is stitched to itself (Figure 9.5). The resulting\nmis-alignment can be used to improve the estimate of the focal length and to re-adjust the\nrotation estimates, as described in Section 9.1.4. Rotating the camera by 90◦around its optic\naxis and re-shooting the panorama is a good way to check for aspect ratio and skew pixel\nproblems, as is generating a full hemi-spherical panorama when there is sufﬁcient texture.\nUltimately, however, the most accurate estimate of the calibration parameters (including\nradial distortion) can be obtained using a full simultaneous non-linear minimization of the\nintrinsic and extrinsic (rotation) parameters, as described in Section 9.2.\n6.3.5 Radial distortion\nWhen images are taken with wide-angle lenses, it is often necessary to model lens distor-\ntions such as radial distortion. As discussed in Section 2.1.6, the radial distortion model\nsays that coordinates in the observed images are displaced away from (barrel distortion) or\ntowards (pincushion distortion) the image center by an amount proportional to their radial\ndistance (Figure 2.13a–b). The simplest radial distortion models use low-order polynomials\n(c.f. Equation (2.78)),\nˆx\n=\nx(1 + κ1r2 + κ2r4)\nˆy\n=\ny(1 + κ1r2 + κ2r4),\n(6.59)\nwhere r2 = x2 + y2 and κ1 and κ2 are called the radial distortion parameters (Brown 1971;\nSlama 1980).13\nA variety of techniques can be used to estimate the radial distortion parameters for a\ngiven lens.14 One of the simplest and most useful is to take an image of a scene with a lot\n13 Sometimes the relationship between x and ˆx is expressed the other way around, i.e., using primed (ﬁnal)\ncoordinates on the right-hand side, x = ˆx(1 + κ1ˆr2 + κ2ˆr4). This is convenient if we map image pixels into\n(warped) rays and then undistort the rays to obtain 3D rays in space, i.e., if we are using inverse warping.\n14 Some of today’s digital cameras are starting to remove radial distortion using software in the camera itself.\n6.4 Additional reading\n335\nof straight lines, especially lines aligned with and near the edges of the image. The radial\ndistortion parameters can then be adjusted until all of the lines in the image are straight,\nwhich is commonly called the plumb-line method (Brown 1971; Kang 2001; El-Melegy and\nFarag 2003). Exercise 6.10 gives some more details on how to implement such a technique.\nAnother approach is to use several overlapping images and to combine the estimation\nof the radial distortion parameters with the image alignment process, i.e., by extending the\npipeline used for stitching in Section 9.2.1. Sawhney and Kumar (1999) use a hierarchy\nof motion models (translation, afﬁne, projective) in a coarse-to-ﬁne strategy coupled with\na quadratic radial distortion correction term. They use direct (intensity-based) minimiza-\ntion to compute the alignment. Stein (1997) uses a feature-based approach combined with\na general 3D motion model (and quadratic radial distortion), which requires more matches\nthan a parallax-free rotational panorama but is potentially more general. More recent ap-\nproaches sometimes simultaneously compute both the unknown intrinsic parameters and the\nradial distortion coefﬁcients, which may include higher-order terms or more complex rational\nor non-parametric forms (Claus and Fitzgibbon 2005; Sturm 2005; Thirthala and Pollefeys\n2005; Barreto and Daniilidis 2005; Hartley and Kang 2005; Steele and Jaynes 2006; Tardif,\nSturm, Trudeau et al. 2009).\nWhen a known calibration target is being used (Figure 6.8), the radial distortion estima-\ntion can be folded into the estimation of the other intrinsic and extrinsic parameters (Zhang\n2000; Hartley and Kang 2007; Tardif, Sturm, Trudeau et al. 2009). This can be viewed as\nadding another stage to the general non-linear minimization pipeline shown in Figure 6.5\nbetween the intrinsic parameter multiplication box f C and the perspective division box f P.\n(See Exercise 6.11 on more details for the case of a planar calibration target.)\nOf course, as discussed in Section 2.1.6, more general models of lens distortion, such as\nﬁsheye and non-central projection, may sometimes be required. While the parameterization\nof such lenses may be more complicated (Section 2.1.6), the general approach of either us-\ning calibration rigs with known 3D positions or self-calibration through the use of multiple\noverlapping images of a scene can both be used (Hartley and Kang 2007; Tardif, Sturm, and\nRoy 2007). The same techniques used to calibrate for radial distortion can also be used to\nreduce the amount of chromatic aberration by separately calibrating each color channel and\nthen warping the channels to put them back into alignment (Exercise 6.12).\n6.4 Additional reading\nHartley and Zisserman (2004) provide a wonderful introduction to the topics of feature-based\nalignment and optimal motion estimation, as well as an in-depth discussion of camera cali-\nbration and pose estimation techniques.\n336\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTechniques for robust estimation are discussed in more detail in Appendix B.3 and in\nmonographs and review articles on this topic (Huber 1981; Hampel, Ronchetti, Rousseeuw et\nal. 1986; Rousseeuw and Leroy 1987; Black and Rangarajan 1996; Stewart 1999). The most\ncommonly used robust initialization technique in computer vision is RANdom SAmple Con-\nsensus (RANSAC) (Fischler and Bolles 1981), which has spawned a series of more efﬁcient\nvariants (Nist´er 2003; Chum and Matas 2005).\nThe topic of registering 3D point data sets is called absolute orientation (Horn 1987) and\n3D pose estimation (Lorusso, Eggert, and Fisher 1995). A variety of techniques has been\ndeveloped for simultaneously computing 3D point correspondences and their corresponding\nrigid transformations (Besl and McKay 1992; Zhang 1994; Szeliski and Lavall´ee 1996; Gold,\nRangarajan, Lu et al. 1998; David, DeMenthon, Duraiswami et al. 2004; Li and Hartley 2007;\nEnqvist, Josephson, and Kahl 2009).\nCamera calibration was ﬁrst studied in photogrammetry (Brown 1971; Slama 1980; Atkin-\nson 1996; Kraus 1997) but it has also been widely studied in computer vision (Tsai 1987;\nGremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Szeliski et al. 1992; Zhang\n2000; Grossberg and Nayar 2001). Vanishing points observed either from rectahedral cali-\nbration objects or man-made architecture are often used to perform rudimentary calibration\n(Caprile and Torre 1990; Becker and Bove 1995; Liebowitz and Zisserman 1998; Cipolla,\nDrummond, and Robertson 1999; Antone and Teller 2002; Criminisi, Reid, and Zisserman\n2000; Hartley and Zisserman 2004; Pﬂugfelder 2008). Performing camera calibration without\nusing known targets is known as self-calibration and is discussed in textbooks and surveys on\nstructure from motion (Faugeras, Luong, and Maybank 1992; Hartley and Zisserman 2004;\nMoons, Van Gool, and Vergauwen 2010). One popular subset of such techniques uses pure\nrotational motion (Stein 1995; Hartley 1997b; Hartley, Hayman, de Agapito et al. 2000; de\nAgapito, Hayman, and Reid 2001; Kang and Weiss 1999; Shum and Szeliski 2000; Frahm\nand Koch 2003).\n6.5 Exercises\nEx 6.1: Feature-based image alignment for ﬂip-book animations\nTake a set of photos of\nan action scene or portrait (preferably in motor-drive—continuous shooting—mode) and\nalign them to make a composite or ﬂip-book animation.\n1. Extract features and feature descriptors using some of the techniques described in Sec-\ntions 4.1.1–4.1.2.\n2. Match your features using nearest neighbor matching with a nearest neighbor distance\nratio test (4.18).",
  "image_path": "page_357.jpg",
  "pages": [
    356,
    357,
    358
  ]
}