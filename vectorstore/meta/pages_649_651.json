{
  "doc_id": "pages_649_651",
  "text": "13.2 Layered depth images\n627\nFigure 13.5\nA variety of image-based rendering primitives, which can be used depending\non the distance between the camera and the object of interest (Shade, Gortler, He et al. 1998)\nc⃝1998 ACM. Closer objects may require more detailed polygonal representations, while\nmid-level objects can use a layered depth image (LDI), and far-away objects can use sprites\n(potentially with depth) and environment maps.\n(a)\n(b)\n(c)\n(d)\nFigure 13.6\nSprites with depth (Shade, Gortler, He et al. 1998) c⃝1998 ACM: (a) alpha-\nmatted color sprite; (b) corresponding relative depth or parallax; (c) rendering without relative\ndepth; (d) rendering with depth (note the curved object boundaries).\n628\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nShum, Sun, Yamazaki et al. 2004) or to infer their 3D structure from monocular image cues\n(Section 14.4.4) (Hoiem, Efros, and Hebert 2005b; Saxena, Sun, and Ng 2009).\nHow can we render a sprite with depth from a novel viewpoint? One possibility, as with\na regular depth map, is to just forward warp each pixel to its new location, which can cause\naliasing and cracks. A better way, which we already mentioned in Section 3.6.2, is to ﬁrst\nwarp the depth (or (u, v) displacement) map to the novel view, ﬁll in the cracks, and then use\nhigher-quality inverse warping to resample the color image (Shade, Gortler, He et al. 1998).\nFigure 13.6d shows the results of applying such a two-pass rendering algorithm. From this\nstill image, you can appreciate that the foreground sprites look more rounded; however, to\nfully appreciate the improvement in realism, you would have to look at the actual animated\nsequence.\nSprites with depth can also be rendered using conventional graphics hardware, as de-\nscribed in (Zitnick, Kang, Uyttendaele et al. 2004). Rogmans, Lu, Bekaert et al. (2009)\ndescribe GPU implementations of both real-time stereo matching and real-time forward and\ninverse rendering algorithms.\n13.3 Light ﬁelds and Lumigraphs\nWhile image-based rendering approaches can synthesize scene renderings from novel view-\npoints, they raise the following more general question:\nIs is possible to capture and render the appearance of a scene from all possible\nviewpoints and, if so, what is the complexity of the resulting structure?\nLet us assume that we are looking at a static scene, i.e., one where the objects and illu-\nminants are ﬁxed, and only the observer is moving around. Under these conditions, we can\ndescribe each image by the location and orientation of the virtual camera (6 dof) as well as\nits intrinsics (e.g., its focal length). However, if we capture a two-dimensional spherical im-\nage around each possible camera location, we can re-render any view from this information.4\nThus, taking the cross-product of the three-dimensional space of camera positions with the\n2D space of spherical images, we obtain the 5D plenoptic function of Adelson and Bergen\n(1991), which forms the basis of the image-based rendering system of McMillan and Bishop\n(1995).\nNotice, however, that when there is no light dispersion in the scene, i.e., no smoke or fog,\nall the coincident rays along a portion of free space (between solid or refractive objects) have\nthe same color value. Under these conditions, we can reduce the 5D plenoptic function to\n4 Since we are counting dimensions, we ignore for now any sampling or resolution issues.\n13.3 Light ﬁelds and Lumigraphs\n629\ns\nt\nu\nv\n(s,t)\n(u,v)\nCamera center\nImage plane\n pixel\n(a)\n(b)\nFigure 13.7 The Lumigraph (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996 ACM: (a) a\nray is represented by its 4D two-plane parameters (s, t) and (u, v); (b) a slice through the 3D\nlight ﬁeld subset (u, v, s).\nthe 4D light ﬁeld of all possible rays (Gortler, Grzeszczuk, Szeliski et al. 1996; Levoy and\nHanrahan 1996; Levoy 2006).5\nTo make the parameterization of this 4D function simpler, let us put two planes in the\n3D scene roughly bounding the area of interest, as shown in Figure 13.7a. Any light ray\nterminating at a camera that lives in front of the st plane (assuming that this space is empty)\npasses through the two planes at (s, t) and (u, v) and can be described by its 4D coordinate\n(s, t, u, v). This diagram (and parameterization) can be interpreted as describing a family of\ncameras living on the st plane with their image planes being the uv plane. The uv plane\ncan be placed at inﬁnity, which corresponds to all the virtual cameras looking in the same\ndirection.\nIn practice, if the planes are of ﬁnite extent, the ﬁnite light slab L(s, t, u, v) can be used to\ngenerate any synthetic view that a camera would see through a (ﬁnite) viewport in the st plane\nwith a view frustum that wholly intersects the far uv plane. To enable the camera to move\nall the way around an object, the 3D space surrounding the object can be split into multiple\ndomains, each with its own light slab parameterization. Conversely, if the camera is moving\ninside a bounded volume of free space looking outward, multiple cube faces surrounding the\ncamera can be used as (s, t) planes.\n5 Levoy and Hanrahan (1996) borrowed the term light ﬁeld from a paper by Gershun (1939). Another name for\nthis representation is the photic ﬁeld (Moon and Spencer 1981).",
  "image_path": "page_650.jpg",
  "pages": [
    649,
    650,
    651
  ]
}