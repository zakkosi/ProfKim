{
  "doc_id": "pages_342_344",
  "text": "320\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nk\np\nS\n3\n0.5\n35\n6\n0.6\n97\n6\n0.5\n293\nTable 6.2 Number of trials S to attain a 99% probability of success (Stewart 1999).\nby inverting the Hessian matrix (6.9) and multiplying it by the feature position noise (if these\nhave not already been used to weight the individual measurements, as in Equations (6.10)\nand 6.11)). In statistics, the Hessian, which is the inverse covariance, is sometimes called the\n(Fisher) information matrix (Appendix B.1.1).\nWhen the problem involves non-linear least squares, the inverse of the Hessian matrix\nprovides the Cramer–Rao lower bound on the covariance matrix, i.e., it provides the minimum\namount of covariance in a given solution, which can actually have a wider spread (“longer\ntails”) if the energy ﬂattens out away from the local minimum where the optimal solution is\nfound.\n6.1.5 3D alignment\nInstead of aligning 2D sets of image features, many computer vision applications require the\nalignment of 3D points. In the case where the 3D transformations are linear in the motion\nparameters, e.g., for translation, similarity, and afﬁne, regular least squares (6.5) can be used.\nThe case of rigid (Euclidean) motion,\nER3D =\nX\ni\n∥x′\ni −Rxi −t∥2,\n(6.31)\nwhich arises more frequently and is often called the absolute orientation problem (Horn\n1987), requires slightly different techniques. If only scalar weightings are being used (as\nopposed to full 3D per-point anisotropic covariance estimates), the weighted centroids of the\ntwo point clouds c and c′ can be used to estimate the translation t = c′ −Rc.8 We are then\nleft with the problem of estimating the rotation between two sets of points {ˆxi = xi −c}\nand {ˆx′\ni = x′\ni −c′} that are both centered at the origin.\nOne commonly used technique is called the orthogonal Procrustes algorithm (Golub and\nVan Loan 1996, p. 601) and involves computing the singular value decomposition (SVD) of\n8 When full covariances are used, they are transformed by the rotation and so a closed-form solution for transla-\ntion is not possible.\n6.2 Pose estimation\n321\nthe 3 × 3 correlation matrix\nC =\nX\ni\nˆx′ˆxT = UΣV T .\n(6.32)\nThe rotation matrix is then obtained as R = UV T . (Verify this for yourself when ˆx′ = Rˆx.)\nAnother technique is the absolute orientation algorithm (Horn 1987) for estimating the\nunit quaternion corresponding to the rotation matrix R, which involves forming a 4×4 matrix\nfrom the entries in C and then ﬁnding the eigenvector associated with its largest positive\neigenvalue.\nLorusso, Eggert, and Fisher (1995) experimentally compare these two techniques to two\nadditional techniques proposed in the literature, but ﬁnd that the difference in accuracy is\nnegligible (well below the effects of measurement noise).\nIn situations where these closed-form algorithms are not applicable, e.g., when full 3D\ncovariances are being used or when the 3D alignment is part of some larger optimization, the\nincremental rotation update introduced in Section 2.1.4 (2.35–2.36), which is parameterized\nby an instantaneous rotation vector ω, can be used (See Section 9.1.3 for an application to\nimage stitching.)\nIn some situations, e.g., when merging range data maps, the correspondence between\ndata points is not known a priori. In this case, iterative algorithms that start by matching\nnearby points and then update the most likely correspondence can be used (Besl and McKay\n1992; Zhang 1994; Szeliski and Lavall´ee 1996; Gold, Rangarajan, Lu et al. 1998; David,\nDeMenthon, Duraiswami et al. 2004; Li and Hartley 2007; Enqvist, Josephson, and Kahl\n2009). These techniques are discussed in more detail in Section 12.2.1.\n6.2 Pose estimation\nA particular instance of feature-based alignment, which occurs very often, is estimating an\nobject’s 3D pose from a set of 2D point projections. This pose estimation problem is also\nknown as extrinsic calibration, as opposed to the intrinsic calibration of internal camera pa-\nrameters such as focal length, which we discuss in Section 6.3. The problem of recovering\npose from three correspondences, which is the minimal amount of information necessary,\nis known as the perspective-3-point-problem (P3P), with extensions to larger numbers of\npoints collectively known as PnP (Haralick, Lee, Ottenberg et al. 1994; Quan and Lan 1999;\nMoreno-Noguer, Lepetit, and Fua 2007).\nIn this section, we look at some of the techniques that have been developed to solve such\nproblems, starting with the direct linear transform (DLT), which recovers a 3×4 camera ma-\ntrix, followed by other “linear” algorithms, and then looking at statistically optimal iterative\nalgorithms.\n322\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n6.2.1 Linear algorithms\nThe simplest way to recover the pose of the camera is to form a set of linear equations analo-\ngous to those used for 2D motion estimation (6.19) from the camera matrix form of perspec-\ntive projection (2.55–2.56),\nxi\n=\np00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(6.33)\nyi\n=\np10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n,\n(6.34)\nwhere (xi, yi) are the measured 2D feature locations and (Xi, Yi, Zi) are the known 3D\nfeature locations (Figure 6.4). As with (6.21), this system of equations can be solved in a\nlinear fashion for the unknowns in the camera matrix P by multiplying the denominator on\nboth sides of the equation.9 The resulting algorithm is called the direct linear transform\n(DLT) and is commonly attributed to Sutherland (1974). (For a more in-depth discussion,\nrefer to the work of Hartley and Zisserman (2004).) In order to compute the 12 (or 11)\nunknowns in P , at least six correspondences between 3D and 2D locations must be known.\nAs with the case of estimating homographies (6.21–6.23), more accurate results for the\nentries in P can be obtained by directly minimizing the set of Equations (6.33–6.34) using\nnon-linear least squares with a small number of iterations.\nOnce the entries in P have been recovered, it is possible to recover both the intrinsic\ncalibration matrix K and the rigid transformation (R, t) by observing from Equation (2.56)\nthat\nP = K[R|t].\n(6.35)\nSince K is by convention upper-triangular (see the discussion in Section 2.1.5), both K and\nR can be obtained from the front 3 × 3 sub-matrix of P using RQ factorization (Golub and\nVan Loan 1996).10\nIn most applications, however, we have some prior knowledge about the intrinsic cali-\nbration matrix K, e.g., that the pixels are square, the skew is very small, and the optical\ncenter is near the center of the image (2.57–2.59). Such constraints can be incorporated into\na non-linear minimization of the parameters in K and (R, t), as described in Section 6.2.2.\nIn the case where the camera is already calibrated, i.e., the matrix K is known (Sec-\ntion 6.3), we can perform pose estimation using as few as three points (Fischler and Bolles\n1981; Haralick, Lee, Ottenberg et al. 1994; Quan and Lan 1999). The basic observation that\nthese linear PnP (perspective n-point) algorithms employ is that the visual angle between any\n9 Because P is unknown up to a scale, we can either ﬁx one of the entries, e.g., p23 = 1, or ﬁnd the smallest\nsingular vector of the set of linear equations.\n10 Note the unfortunate clash of terminologies: In matrix algebra textbooks, R represents an upper-triangular\nmatrix; in computer vision, R is an orthogonal rotation.",
  "image_path": "page_343.jpg",
  "pages": [
    342,
    343,
    344
  ]
}