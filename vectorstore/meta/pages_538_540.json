{
  "doc_id": "pages_538_540",
  "text": "516\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.45\nComparative matting results with scribble-based inputs. Wang and Cohen\n(2007a) describe the individual techniques being compared.\nFigure 10.46\nStroke-based segmentation result (Rhemann, Rother, Rav-Acha et al. 2008)\nc⃝2008 IEEE.\nto be hidden, in which case Poisson blending (P´erez, Gangnet, and Blake 2003) can be used\n(Section 9.3.4).\nIn the latter case, it is helpful if the matte boundary passes through regions that either\nhave little texture or look similar in the old and new images. Papers by Jia, Sun, Tang et al.\n(2006) and Wang and Cohen (2007c) explain how to do this.\n10.4.4 Smoke, shadow, and ﬂash matting\nIn addition to matting out solid objects with fractional boundaries, it is also possible to matte\nout translucent media such as smoke (Chuang, Agarwala, Curless et al. 2002). Starting with\na video sequence, each pixel is modeled as a linear combination of its (unknown) background\ncolor and a constant foreground (smoke) color that is common to all pixels. Voting in color\n10.4 Image matting and compositing\n517\n(a)\n(b)\n(c)\n(d)\nFigure 10.47\nSmoke matting (Chuang, Agarwala, Curless et al. 2002) c⃝2002 ACM: (a)\ninput video frame; (b) after removing the foreground object; (c) estimated alpha matte; (d)\ninsertion of new objects into the background.\nFigure 10.48 Shadow matting (Chuang, Goldman, Curless et al. 2003) c⃝2003 ACM. In-\nstead of simply darkening the new scene with the shadow (c), shadow matting correctly dims\nthe lit scene with the new shadow and drapes the shadow over 3D geometry (d).\nspace is used to estimate this foreground color and the distance along each color line is used\nto estimate the per-pixel temporally varying alpha (Figure 10.47).\nExtracting and re-inserting shadows is also possible using a related technique (Chuang,\nGoldman, Curless et al. 2003). Here, instead of assuming a constant foreground color, each\npixel is assumed to vary between its fully lit and fully shadowed colors, which can be esti-\nmated by taking (robust) minimum and maximum values over time as a shadow passes over\nthe scene (Exercise 10.9). The resulting fractional shadow matte can be used to re-project\nthe shadow into a new scene. If the destination scene has a non-planar geometry, it can be\nscanned by waving a straight stick shadow across the scene. The new shadow matte can then\nbe warped with the computed deformation ﬁeld to have it drape correctly over the new scene\n(Figure 10.48).\nThe quality and reliability of matting algorithms can also be enhanced using more sophis-\nticated acquisition systems. For example, taking a ﬂash and non-ﬂash image pair supports\nthe reliable extraction of foreground mattes, which show up as regions of large illumination\nchange between the two images (Sun, Li, Kang et al. 2006). Taking simultaneous video\nstreams focused at different distances (McGuire, Matusik, Pﬁster et al. 2005) or using multi-\ncamera arrays (Joshi, Matusik, and Avidan 2006) are also good approaches to producing\n518\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nhigh-quality mattes. These techniques are described in more detail in (Wang and Cohen\n2007a).\nLastly, photographing a refractive object in front of a number of patterned backgrounds\nallows the object to be placed in novel 3D environments. These environment matting tech-\nniques (Zongker, Werner, Curless et al. 1999; Chuang, Zongker, Hindorff et al. 2000) are\ndiscussed in Section 13.4.\n10.4.5 Video matting\nWhile regular single-frame matting techniques such as blue or green screen matting (Smith\nand Blinn 1996; Wright 2006; Brinkmann 2008) can be applied to video sequences, the pres-\nence of moving objects can sometimes make the matting process easier, as portions of the\nbackground may get revealed in preceding or subsequent frames.\nChuang, Agarwala, Curless et al. (2002) describe a nice approach to this video matting\nproblem, where foreground objects are ﬁrst removed using a conservative garbage matte and\nthe resulting background plates are aligned and composited to yield a high-quality back-\nground estimate. They also describe how trimaps drawn at sparse keyframes can be inter-\npolated to in-between frames using bi-direction optic ﬂow. Alternative approaches to video\nmatting, such as rotoscoping, which involves drawing and tracking curves in video sequences\n(Agarwala, Hertzmann, Seitz et al. 2004), are discussed in the matting survey paper by Wang\nand Cohen (2007a).\n10.5 Texture analysis and synthesis\nWhile texture analysis and synthesis may not at ﬁrst seem like computational photography\ntechniques, they are, in fact, widely used to repair defects, such as small holes, in images or\nto create non-photorealistic painterly renderings from regular photographs.\nThe problem of texture synthesis can be formulated as follows: given a small sample of\na “texture” (Figure 10.49a), generate a larger similar-looking image (Figure 10.49b). As you\ncan imagine, for certain sample textures, this problem can be quite challenging.\nTraditional approaches to texture analysis and synthesis try to match the spectrum of the\nsource image while generating shaped noise. Matching the frequency characteristics, which\nis equivalent to matching spatial correlations, is in itself not sufﬁcient. The distributions of\nthe responses at different frequencies must also match. Heeger and Bergen (1995) develop an\nalgorithm that alternates between matching the histograms of multi-scale (steerable pyramid)\nresponses and matching the ﬁnal image histogram. Portilla and Simoncelli (2000) improve\non this technique by also matching pairwise statistics across scale and orientations. De Bonet\n(1997) uses a coarse-to-ﬁne strategy to ﬁnd locations in the source texture with a similar",
  "image_path": "page_539.jpg",
  "pages": [
    538,
    539,
    540
  ]
}