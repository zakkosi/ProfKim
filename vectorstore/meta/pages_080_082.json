{
  "doc_id": "pages_080_082",
  "text": "58\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nare known). The inverse distance ηz is now mostly decoupled from the estimates of s and\ncan be estimated from the amount of foreshortening as the object rotates. Furthermore, as\nthe lens becomes longer, i.e., the projection model becomes orthographic, there is no need to\nreplace a perspective imaging model with an orthographic one, since the same equation can\nbe used, with ηz →0 (as opposed to f and tz both going to inﬁnity). This allows us to form\na natural link between orthographic reconstruction techniques such as factorization and their\nprojective/perspective counterparts (Section 7.3).\n2.1.6 Lens distortions\nThe above imaging models all assume that cameras obey a linear projection model where\nstraight lines in the world result in straight lines in the image. (This follows as a natural\nconsequence of linear matrix operations being applied to homogeneous coordinates.) Unfor-\ntunately, many wide-angle lenses have noticeable radial distortion, which manifests itself as\na visible curvature in the projection of straight lines. (See Section 2.2.3 for a more detailed\ndiscussion of lens optics, including chromatic aberration.) Unless this distortion is taken into\naccount, it becomes impossible to create highly accurate photorealistic reconstructions. For\nexample, image mosaics constructed without taking radial distortion into account will often\nexhibit blurring due to the mis-registration of corresponding features before pixel blending\n(Chapter 9).\nFortunately, compensating for radial distortion is not that difﬁcult in practice. For most\nlenses, a simple quartic model of distortion can produce good results. Let (xc, yc) be the\npixel coordinates obtained after perspective division but before scaling by focal length f and\nshifting by the optical center (cx, cy), i.e.,\nxc\n=\nrx · p + tx\nrz · p + tz\nyc\n=\nry · p + ty\nrz · p + tz\n.\n(2.77)\nThe radial distortion model says that coordinates in the observed images are displaced away\n(barrel distortion) or towards (pincushion distortion) the image center by an amount propor-\ntional to their radial distance (Figure 2.13a–b).3\nThe simplest radial distortion models use\nlow-order polynomials, e.g.,\nˆxc\n=\nxc(1 + κ1r2\nc + κ2r4\nc)\nˆyc\n=\nyc(1 + κ1r2\nc + κ2r4\nc),\n(2.78)\n3 Anamorphic lenses, which are widely used in feature ﬁlm production, do not follow this radial distortion model.\nInstead, they can be thought of, to a ﬁrst approximation, as inducing different vertical and horizontal scalings, i.e.,\nnon-square pixels.\n2.1 Geometric primitives and transformations\n59\n(a)\n(b)\n(c)\nFigure 2.13 Radial lens distortions: (a) barrel, (b) pincushion, and (c) ﬁsheye. The ﬁsheye\nimage spans almost 180◦from side-to-side.\nwhere r2\nc = x2\nc + y2\nc and κ1 and κ2 are called the radial distortion parameters.4 After the\nradial distortion step, the ﬁnal pixel coordinates can be computed using\nxs\n=\nfx′\nc + cx\nys\n=\nfy′\nc + cy.\n(2.79)\nA variety of techniques can be used to estimate the radial distortion parameters for a given\nlens, as discussed in Section 6.3.5.\nSometimes the above simpliﬁed model does not model the true distortions produced by\ncomplex lenses accurately enough (especially at very wide angles). A more complete ana-\nlytic model also includes tangential distortions and decentering distortions (Slama 1980), but\nthese distortions are not covered in this book.\nFisheye lenses (Figure 2.13c) require a model that differs from traditional polynomial\nmodels of radial distortion. Fisheye lenses behave, to a ﬁrst approximation, as equi-distance\nprojectors of angles away from the optical axis (Xiong and Turkowski 1997), which is the\nsame as the polar projection described by Equations (9.22–9.24). Xiong and Turkowski\n(1997) describe how this model can be extended with the addition of an extra quadratic cor-\nrection in φ and how the unknown parameters (center of projection, scaling factor s, etc.)\ncan be estimated from a set of overlapping ﬁsheye images using a direct (intensity-based)\nnon-linear minimization algorithm.\nFor even larger, less regular distortions, a parametric distortion model using splines may\nbe necessary (Goshtasby 1989). If the lens does not have a single center of projection, it\n4 Sometimes the relationship between xc and ˆxc is expressed the other way around, i.e., xc = ˆxc(1 + κ1ˆr2\nc +\nκ2ˆr4\nc). This is convenient if we map image pixels into (warped) rays by dividing through by f. We can then undistort\nthe rays and have true 3D rays in space.\n60\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmay become necessary to model the 3D line (as opposed to direction) corresponding to each\npixel separately (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Sautot et al.\n1992; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm, Trudeau et\nal. 2009). Some of these techniques are described in more detail in Section 6.3.5, which\ndiscusses how to calibrate lens distortions.\nThere is one subtle issue associated with the simple radial distortion model that is often\nglossed over. We have introduced a non-linearity between the perspective projection and ﬁnal\nsensor array projection steps. Therefore, we cannot, in general, post-multiply an arbitrary 3×\n3 matrix K with a rotation to put it into upper-triangular form and absorb this into the global\nrotation. However, this situation is not as bad as it may at ﬁrst appear. For many applications,\nkeeping the simpliﬁed diagonal form of (2.59) is still an adequate model. Furthermore, if we\ncorrect radial and other distortions to an accuracy where straight lines are preserved, we have\nessentially converted the sensor back into a linear imager and the previous decomposition still\napplies.\n2.2 Photometric image formation\nIn modeling the image formation process, we have described how 3D geometric features in\nthe world are projected into 2D features in an image. However, images are not composed of\n2D features. Instead, they are made up of discrete color or intensity values. Where do these\nvalues come from? How do they relate to the lighting in the environment, surface properties\nand geometry, camera optics, and sensor properties (Figure 2.14)? In this section, we develop\na set of models to describe these interactions and formulate a generative process of image\nformation. A more detailed treatment of these topics can be found in other textbooks on\ncomputer graphics and image synthesis (Glassner 1995; Weyrich, Lawrence, Lensch et al.\n2008; Foley, van Dam, Feiner et al. 1995; Watt 1995; Cohen and Wallace 1993; Sillion and\nPuech 1994).\n2.2.1 Lighting\nImages cannot exist without light. To produce an image, the scene must be illuminated with\none or more light sources. (Certain modalities such as ﬂuorescent microscopy and X-ray\ntomography do not ﬁt this model, but we do not deal with them in this book.) Light sources\ncan generally be divided into point and area light sources.\nA point light source originates at a single location in space (e.g., a small light bulb),\npotentially at inﬁnity (e.g., the sun). (Note that for some applications such as modeling soft\nshadows (penumbras), the sun may have to be treated as an area light source.) In addition to\nits location, a point light source has an intensity and a color spectrum, i.e., a distribution over",
  "image_path": "page_081.jpg",
  "pages": [
    80,
    81,
    82
  ]
}