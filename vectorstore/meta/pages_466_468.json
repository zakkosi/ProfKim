{
  "doc_id": "pages_466_468",
  "text": "444\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis results in the simplest set of update equations (Shum and Szeliski 2000), since the fk can\nbe folded into the creation of the homogeneous coordinate vector as in Equation (9.7). Thus,\neven though this formula over-weights features that occur more frequently, it is the method\nused by Shum and Szeliski (2000) and Brown, Szeliski, and Winder (2005). In order to reduce\nthe bias towards longer focal lengths, we multiply each residual (3D error) by\np\nfjfk, which\nis similar to projecting the 3D rays into a “virtual camera” of intermediate focal length.\nUp vector selection.\nAs mentioned above, there exists a global ambiguity in the pose of the\n3D cameras computed by the above methods. While this may not appear to matter, people\nprefer that the ﬁnal stitched image is “upright” rather than twisted or tilted. More concretely,\npeople are used to seeing photographs displayed so that the vertical (gravity) axis points\nstraight up in the image. Consider how you usually shoot photographs: while you may pan\nand tilt the camera any which way, you usually keep the horizontal edge of your camera (its\nx-axis) parallel to the ground plane (perpendicular to the world gravity direction).\nMathematically, this constraint on the rotation matrices can be expressed as follows. Re-\ncall from Equation (9.26) that the 3D to 2D projection is given by\n˜xik ∼KkRkxi.\n(9.32)\nWe wish to post-multiply each rotation matrix Rk by a global rotation Rg such that the pro-\njection of the global y-axis, ˆ= (0, 1, 0) is perpendicular to the image x-axis, ˆı = (1, 0, 0).10\nThis constraint can be written as\nˆıT RkRgˆ= 0\n(9.33)\n(note that the scaling by the calibration matrix is irrelevant here). This is equivalent to re-\nquiring that the ﬁrst row of Rk, rk0 = ˆıT Rk be perpendicular to the second column of Rg,\nrg1 = Rgˆ. This set of constraints (one per input image) can be written as a least squares\nproblem,\nrg1 = arg min\nr\nX\nk\n(rT rk0)2 = arg min\nr rT\n\"X\nk\nrk0rT\nk0\n#\nr.\n(9.34)\nThus, rg1 is the smallest eigenvector of the scatter or moment matrix spanned by the indi-\nvidual camera rotation x-vectors, which should generally be of the form (c, 0, s) when the\ncameras are upright.\nTo fully specify the Rg global rotation, we need to specify one additional constraint. This\nis related to the view selection problem discussed in Section 9.3.1. One simple heuristic is to\n10 Note that here we use the convention common in computer graphics that the vertical world axis corresponds to\ny. This is a natural choice if we wish the rotation matrix associated with a “regular” image taken horizontally to be\nthe identity, rather than a 90◦rotation around the x-axis.\n9.2 Global alignment\n445\nprefer the average z-axis of the individual rotation matrices, k = P\nk ˆk\nT Rk to be close to\nthe world z-axis, rg2 = Rgˆk. We can therefore compute the full rotation matrix Rg in three\nsteps:\n1. rg1 = min eigenvector (P\nk rk0rT\nk0);\n2. rg0 = N((P\nk rk2) × rg1);\n3. rg2 = rg0 × rg1,\nwhere N(v) = v/∥v∥normalizes a vector v.\n9.2.2 Parallax removal\nOnce we have optimized the global orientations and focal lengths of our cameras, we may ﬁnd\nthat the images are still not perfectly aligned, i.e., the resulting stitched image looks blurry\nor ghosted in some places. This can be caused by a variety of factors, including unmodeled\nradial distortion, 3D parallax (failure to rotate the camera around its optical center), small\nscene motions such as waving tree branches, and large-scale scene motions such as people\nmoving in and out of pictures.\nEach of these problems can be treated with a different approach. Radial distortion can be\nestimated (potentially ahead of time) using one of the techniques discussed in Section 2.1.6.\nFor example, the plumb-line method (Brown 1971; Kang 2001; El-Melegy and Farag 2003)\nadjusts radial distortion parameters until slightly curved lines become straight, while mosaic-\nbased approaches adjust them until mis-registration is reduced in image overlap areas (Stein\n1997; Sawhney and Kumar 1999).\n3D parallax can be handled by doing a full 3D bundle adjustment, i.e., by replacing the\nprojection equation (9.26) used in Equation (9.29) with Equation (2.68), which models cam-\nera translations. The 3D positions of the matched feature points and cameras can then be si-\nmultaneously recovered, although this can be signiﬁcantly more expensive than parallax-free\nimage registration. Once the 3D structure has been recovered, the scene could (in theory) be\nprojected to a single (central) viewpoint that contains no parallax. However, in order to do\nthis, dense stereo correspondence needs to be performed (Section 11.3) (Li, Shum, Tang et al.\n2004; Zheng, Kang, Cohen et al. 2007), which may not be possible if the images contain only\npartial overlap. In that case, it may be necessary to correct for parallax only in the overlap\nareas, which can be accomplished using a multi-perspective plane sweep (MPPS) algorithm\n(Kang, Szeliski, and Uyttendaele 2004; Uyttendaele, Criminisi, Kang et al. 2004).\nWhen the motion in the scene is very large, i.e., when objects appear and disappear com-\npletely, a sensible solution is to simply select pixels from only one image at a time as the\nsource for the ﬁnal composite (Milgram 1977; Davis 1998; Agarwala, Dontcheva, Agrawala\n446\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\net al. 2004), as discussed in Section 9.3.2. However, when the motion is reasonably small (on\nthe order of a few pixels), general 2D motion estimation (optical ﬂow) can be used to perform\nan appropriate correction before blending using a process called local alignment (Shum and\nSzeliski 2000; Kang, Uyttendaele, Winder et al. 2003). This same process can also be used\nto compensate for radial distortion and 3D parallax, although it uses a weaker motion model\nthan explicitly modeling the source of error and may, therefore, fail more often or introduce\nunwanted distortions.\nThe local alignment technique introduced by Shum and Szeliski (2000) starts with the\nglobal bundle adjustment (9.31) used to optimize the camera poses. Once these have been\nestimated, the desired location of a 3D point xi can be estimated as the average of the back-\nprojected 3D locations,\n¯xi ∼\nX\nj\ncij ˜xi(ˆxij; Rj, fj)\n,X\nj\ncij ,\n(9.35)\nwhich can be projected into each image j to obtain a target location ¯xij. The difference\nbetween the target locations ¯xij and the original features xij provide a set of local motion\nestimates\nuij = ¯xij −xij,\n(9.36)\nwhich can be interpolated to form a dense correction ﬁeld uj(xj). In their system, Shum and\nSzeliski (2000) use an inverse warping algorithm where the sparse −uij values are placed at\nthe new target locations ¯xij, interpolated using bilinear kernel functions (Nielson 1993) and\nthen added to the original pixel coordinates when computing the warped (corrected) image.\nIn order to get a reasonably dense set of features to interpolate, Shum and Szeliski (2000)\nplace a feature point at the center of each patch (the patch size controls the smoothness in\nthe local alignment stage), rather than relying of features extracted using an interest operator\n(Figure 9.10).\nAn alternative approach to motion-based de-ghosting was proposed by Kang, Uytten-\ndaele, Winder et al. (2003), who estimate dense optical ﬂow between each input image and a\ncentral reference image. The accuracy of the ﬂow vector is checked using a photo-consistency\nmeasure before a given warped pixel is considered valid and is used to compute a high dy-\nnamic range radiance estimate, which is the goal of their overall algorithm. The requirement\nfor a reference image makes their approach less applicable to general image mosaicing, al-\nthough an extension to this case could certainly be envisaged.\n9.2.3 Recognizing panoramas\nThe ﬁnal piece needed to perform fully automated image stitching is a technique to recognize\nwhich images actually go together, which Brown and Lowe (2007) call recognizing panora-",
  "image_path": "page_467.jpg",
  "pages": [
    466,
    467,
    468
  ]
}