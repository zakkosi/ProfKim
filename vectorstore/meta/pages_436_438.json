{
  "doc_id": "pages_436_438",
  "text": "414\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 8.13\nSlice through a spatio-temporal volume (Szeliski 1999) c⃝1999 IEEE: (a–b)\ntwo frames from the ﬂower garden sequence; (c) a horizontal slice through the complete\nspatio-temporal volume, with the arrows indicating locations of potential key frames where\nﬂow is estimated. Note that the colors for the ﬂower garden sequence are incorrect; the correct\ncolors (yellow ﬂowers) are shown in Figure 8.15.\n2D window-based motion estimators.) An alternative to full spatio-temporal ﬁltering is to\nestimate more local spatio-temporal derivatives and use them inside a global optimization\nframework to ﬁll in textureless regions (Bruhn, Weickert, and Schn¨orr 2005; Govindu 2006).\nAnother alternative is to simultaneously estimate multiple motion estimates, while also\noptionally reasoning about occlusion relationships (Szeliski 1999). Figure 8.13c shows schemat-\nically one potential approach to this problem. The horizontal arrows show the locations of\nkeyframes s where motion is estimated, while other slices indicate video frames t whose\ncolors are matched with those predicted by interpolating between the keyframes. Motion es-\ntimation can be cast as a global energy minimization problem that simultaneously minimizes\nbrightness compatibility and ﬂow compatibility terms between keyframes and other frames,\nin addition to using robust smoothness terms.\nThe multi-view framework is potentially even more appropriate for rigid scene motion\n(multi-view stereo) (Section 11.6), where the unknowns at each pixel are disparities and\nocclusion relationships can be determined directly from pixel depths (Szeliski 1999; Kol-\nmogorov and Zabih 2002). However, it may also be applicable to general motion, with the\naddition of models for object accelerations and occlusion relationships.\n8.4.2 Application: Video denoising\nVideo denoising is the process of removing noise and other artifacts such as scratches from\nﬁlm and video (Kokaram 2004). Unlike single image denoising, where the only information\navailable is in the current picture, video denoisers can average or borrow information from\nadjacent frames. However, in order to do this without introducing blur or jitter (irregular\nmotion), they need accurate per-pixel motion estimates.\nExercise 8.7 lists some of the steps required, which include the ability to determine if the\n8.5 Layered motion\n415\ncurrent motion estimate is accurate enough to permit averaging with other frames. Gai and\nKang (2009) describe their recently developed restoration process, which involves a series of\nadditional steps to deal with the special characteristics of vintage ﬁlm.\n8.4.3 Application: De-interlacing\nAnother commonly used application of per-pixel motion estimation is video de-interlacing,\nwhich is the process of converting a video taken with alternating ﬁelds of even and odd\nlines to a non-interlaced signal that contains both ﬁelds in each frame (de Haan and Bellers\n1998). Two simple de-interlacing techniques are bob, which copies the line above or below\nthe missing line from the same ﬁeld, and weave, which copies the corresponding line from\nthe ﬁeld before or after. The names come from the visual artifacts generated by these two\nsimple techniques: bob introduces an up-and-down bobbing motion along strong horizontal\nlines; weave can lead to a “zippering” effect along horizontally translating edges. Replacing\nthese copy operators with averages can help but does not completely remove these artifacts.\nA wide variety of improved techniques have been developed for this process, which is\noften embedded in specialized DSP chips found inside video digitization boards in computers\n(since broadcast video is often interlaced, while computer monitors are not). A large class\nof these techniques estimates local per-pixel motions and interpolates the missing data from\nthe information available in spatially and temporally adjacent ﬁelds. Dai, Baker, and Kang\n(2009) review this literature and propose their own algorithm, which selects among seven\ndifferent interpolation functions at each pixel using an MRF framework.\n8.5 Layered motion\nIn many situation, visual motion is caused by the movement of a small number of objects\nat different depths in the scene. In such situations, the pixel motions can be described more\nsuccinctly (and estimated more reliably) if pixels are grouped into appropriate objects or\nlayers (Wang and Adelson 1994).\nFigure 8.14 shows this approach schematically. The motion in this sequence is caused by\nthe translational motion of the checkered background and the rotation of the foreground hand.\nThe complete motion sequence can be reconstructed from the appearance of the foreground\nand background elements, which can be represented as alpha-matted images (sprites or video\nobjects) and the parametric motion corresponding to each layer. Displacing and compositing\nthese layers in back to front order (Section 3.1.3) recreates the original video sequence.\nLayered motion representations not only lead to compact representations (Wang and\nAdelson 1994; Lee, ge Chen, lung Bruce Lin et al. 1997), but they also exploit the infor-\nmation available in multiple video frames, as well as accurately modeling the appearance of\n416\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIntensity map\nAlpha map\nVelocity map\nIntensity map\nAlpha map\nVelocity map\nFrame 1\nFrame 2\nFrame 3\nFigure 8.14\nLayered motion estimation framework (Wang and Adelson 1994) c⃝1994\nIEEE: The top two rows describe the two layers, each of which consists of an intensity (color)\nimage, an alpha mask (black=transparent), and a parametric motion ﬁeld. The layers are com-\nposited with different amounts of motion to recreate the video sequence.\npixels near motion discontinuities. This makes them particularly suited as a representation\nfor image-based rendering (Section 13.2.1) (Shade, Gortler, He et al. 1998; Zitnick, Kang,\nUyttendaele et al. 2004) as well as object-level video editing.\nTo compute a layered representation of a video sequence, Wang and Adelson (1994) ﬁrst\nestimate afﬁne motion models over a collection of non-overlapping patches and then cluster\nthese estimates using k-means. They then alternate between assigning pixels to layers and\nrecomputing motion estimates for each layer using the assigned pixels, using a technique\nﬁrst proposed by Darrell and Pentland (1991). Once the parametric motions and pixel-wise\nlayer assignments have been computed for each frame independently, layers are constructed\nby warping and merging the various layer pieces from all of the frames together. Median\nﬁltering is used to produce sharp composite layers that are robust to small intensity variations,\nas well as to infer occlusion relationships between the layers. Figure 8.15 shows the results\nof this process on the ﬂower garden sequence. You can see both the initial and ﬁnal layer\nassignments for one of the frames, as well as the composite ﬂow and the alpha-matted layers\nwith their corresponding ﬂow vectors overlaid.\nIn follow-on work, Weiss and Adelson (1996) use a formal probabilistic mixture model\nto infer both the optimal number of layers and the per-pixel layer assignments. Weiss (1997)",
  "image_path": "page_437.jpg",
  "pages": [
    436,
    437,
    438
  ]
}