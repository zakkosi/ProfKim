{
  "doc_id": "pages_313_315",
  "text": "5.3 Mean shift and mode ﬁnding\n291\nbased on their statistics, and for accelerating the process of ﬁnding the nearest mean center\n(Bishop 2006).\nIn mixtures of Gaussians, each cluster center is augmented by a covariance matrix whose\nvalues are re-estimated from the corresponding samples. Instead of using nearest neighbors\nto associate input samples with cluster centers, a Mahalanobis distance (Appendix B.1.1) is\nused:\nd(xi, µk; Σk) = ∥xi −µk∥Σ\n−1\nk\n= (xi −µk)T Σ−1\nk (xi −µk)\n(5.26)\nwhere xi are the input samples, µk are the cluster centers, and Σk are their covariance es-\ntimates. Samples can be associated with the nearest cluster center (a hard assignment of\nmembership) or can be softly assigned to several nearby clusters.\nThis latter, more commonly used, approach corresponds to iteratively re-estimating the\nparameters for a mixture of Gaussians density function,\np(x|{πk, µk, Σk}) =\nX\nk\nπk N(x|µk, Σk),\n(5.27)\nwhere πk are the mixing coefﬁcients, µk and Σk are the Gaussian means and covariances,\nand\nN(x|µk, Σk) =\n1\n|Σk|e−d(x,µk;Σk)\n(5.28)\nis the normal (Gaussian) distribution (Bishop 2006).\nTo iteratively compute (a local) maximum likely estimate for the unknown mixture pa-\nrameters {πk, µk, Σk}, the expectation maximization (EM) algorithm (Dempster, Laird, and\nRubin 1977) proceeds in two alternating stages:\n1. The expectation stage (E step) estimates the responsibilities\nzik = 1\nZi\nπk N(x|µk, Σk)\nwith\nX\nk\nzik = 1,\n(5.29)\nwhich are the estimates of how likely a sample xi was generated from the kth Gaussian\ncluster.\n2. The maximization stage (M step) updates the parameter values\nµk\n=\n1\nNk\nX\ni\nzikxi,\n(5.30)\nΣk\n=\n1\nNk\nX\ni\nzik(xi −µk)(xi −µk)T ,\n(5.31)\nπk\n=\nNk\nN ,\n(5.32)\n292\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhere\nNk =\nX\ni\nzik.\n(5.33)\nis an estimate of the number of sample points assigned to each cluster.\nBishop (2006) has a wonderful exposition of both mixture of Gaussians estimation and the\nmore general topic of expectation maximization.\nIn the context of image segmentation, Ma, Derksen, Hong et al. (2007) present a nice\nreview of segmentation using mixtures of Gaussians and develop their own extension based\non Minimum Description Length (MDL) coding, which they show produces good results on\nthe Berkeley segmentation database.\n5.3.2 Mean shift\nWhile k-means and mixtures of Gaussians use a parametric form to model the probability den-\nsity function being segmented, mean shift implicitly models this distribution using a smooth\ncontinuous non-parametric model. The key to mean shift is a technique for efﬁciently ﬁnd-\ning peaks in this high-dimensional data distribution without ever computing the complete\nfunction explicitly (Fukunaga and Hostetler 1975; Cheng 1995; Comaniciu and Meer 2002).\nConsider once again the data points shown in Figure 5.16c, which can be thought of as\nhaving been drawn from some probability density function. If we could compute this density\nfunction, as visualized in Figure 5.16e, we could ﬁnd its major peaks (modes) and identify\nregions of the input space that climb to the same peak as being part of the same region. This\nis the inverse of the watershed algorithm described in Section 5.2.1, which climbs downhill\nto ﬁnd basins of attraction.\nThe ﬁrst question, then, is how to estimate the density function given a sparse set of\nsamples. One of the simplest approaches is to just smooth the data, e.g., by convolving it\nwith a ﬁxed kernel of width h,\nf(x) =\nX\ni\nK(x −xi) =\nX\ni\nk\n\u0012∥x −xi∥2\nh2\n\u0013\n,\n(5.34)\nwhere xi are the input samples and k(r) is the kernel function (or Parzen window).9 This\napproach is known as kernel density estimation or the Parzen window technique (Duda, Hart,\nand Stork 2001, Section 4.3; Bishop 2006, Section 2.5.1). Once we have computed f(x), as\nshown in Figures 5.16e and 5.17, we can ﬁnd its local maxima using gradient ascent or some\nother optimization technique.\n9 In this simpliﬁed formula, a Euclidean metric is used. We discuss a little later (5.42) how to generalize this\nto non-uniform (scaled or oriented) metrics. Note also that this distribution may not be proper, i.e., integrate to 1.\nSince we are looking for maxima in the density, this does not matter.\n5.3 Mean shift and mode ﬁnding\n293\nx\nf (x)\nxi\nK(x)\nG(x)\nf '(xk)\nxk\nm(xk)\nFigure 5.17 One-dimensional visualization of the kernel density estimate, its derivative, and\na mean shift. The kernel density estimate f(x) is obtained by convolving the sparse set of\ninput samples xi with the kernel function K(x). The derivative of this function, f ′(x), can\nbe obtained by convolving the inputs with the derivative kernel G(x). Estimating the local\ndisplacement vectors around a current estimate xk results in the mean-shift vector m(xk),\nwhich, in a multi-dimensional setting, point in the same direction as the function gradient\n∇f(xk). The red dots indicate local maxima in f(x) to which the mean shifts converge.\nThe problem with this “brute force” approach is that, for higher dimensions, it becomes\ncomputationally prohibitive to evaluate f(x) over the complete search space.10 Instead, mean\nshift uses a variant of what is known in the optimization literature as multiple restart gradient\ndescent. Starting at some guess for a local maximum, yk, which can be a random input data\npoint xi, mean shift computes the gradient of the density estimate f(x) at yk and takes an\nuphill step in that direction (Figure 5.17). The gradient of f(x) is given by\n∇f(x) =\nX\ni\n(xi −x)G(x −xi) =\nX\ni\n(xi −x)g\n\u0012∥x −xi∥2\nh2\n\u0013\n,\n(5.35)\nwhere\ng(r) = −k′(r),\n(5.36)\nand k′(r) is the ﬁrst derivative of k(r). We can re-write the gradient of the density function\nas\n∇f(x) =\n\"X\ni\nG(x −xi)\n#\nm(x),\n(5.37)\nwhere the vector\nm(x) =\nP\ni xiG(x −xi)\nP\ni G(x −xi)\n−x\n(5.38)\nis called the mean shift, since it is the difference between the weighted mean of the neighbors\nxi around x and the current value of x.\n10 Even for one dimension, if the space is extremely sparse, it may be inefﬁcient.",
  "image_path": "page_314.jpg",
  "pages": [
    313,
    314,
    315
  ]
}