{
  "doc_id": "pages_354_356",
  "text": "332\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 6.11\nFour images taken with a hand-held camera registered using a 3D rotation\nmotion model, which can be used to estimate the focal length of the camera (Szeliski and\nShum 1997) c⃝2000 ACM.\npoints above the ground plane, when paired with their ground plane projections, can also be\nrecovered. A fully metric reconstruction of the scene then becomes possible.\nExercise 6.9 has you implement such a system and then use it to model some simple\n3D scenes. Section 12.6.1 describes other, potentially multi-view, approaches to architectural\nreconstruction, including an interactive piecewise-planar modeling system that uses vanishing\npoints to establish 3D line directions and plane normals (Sinha, Steedly, Szeliski et al. 2008).\n6.3.4 Rotational motion\nWhen no calibration targets or known structures are available but you can rotate the camera\naround its front nodal point (or, equivalently, work in a large open environment where all ob-\njects are distant), the camera can be calibrated from a set of overlapping images by assuming\nthat it is undergoing pure rotational motion, as shown in Figure 6.11 (Stein 1995; Hartley\n1997b; Hartley, Hayman, de Agapito et al. 2000; de Agapito, Hayman, and Reid 2001; Kang\nand Weiss 1999; Shum and Szeliski 2000; Frahm and Koch 2003). When a full 360◦mo-\ntion is used to perform this calibration, a very accurate estimate of the focal length f can be\nobtained, as the accuracy in this estimate is proportional to the total number of pixels in the\nresulting cylindrical panorama (Section 9.1.6) (Stein 1995; Shum and Szeliski 2000).\nTo use this technique, we ﬁrst compute the homographies ˜\nHij between all overlapping\npairs of images, as explained in Equations (6.19–6.23). Then, we use the observation, ﬁrst\nmade in Equation (2.72) and explored in more detail in Section 9.1.3 (9.5), that each homog-\nraphy is related to the inter-camera rotation Rij through the (unknown) calibration matrices\n6.3 Geometric intrinsic calibration\n333\nKi and Kj,\n˜\nHij = KiRiR−1\nj K−1\nj\n= KiRijK−1\nj .\n(6.52)\nThe simplest way to obtain the calibration is to use the simpliﬁed form of the calibra-\ntion matrix (2.59), where we assume that the pixels are square and the optical center lies at\nthe center of the image, i.e., Kk = diag(fk, fk, 1). (We number the pixel coordinates ac-\ncordingly, i.e., place pixel (x, y) = (0, 0) at the center of the image.) We can then rewrite\nEquation (6.52) as\nR10 ∼K−1\n1\n˜\nH10K0 ∼\n\n\nh00\nh01\nf −1\n0 h02\nh10\nh11\nf −1\n0 h12\nf1h20\nf1h21\nf −1\n0 f1h22\n\n,\n(6.53)\nwhere hij are the elements of ˜\nH10.\nUsing the orthonormality properties of the rotation matrix R10 and the fact that the right\nhand side of (6.53) is known only up to a scale, we obtain\nh2\n00 + h2\n01 + f −2\n0 h2\n02 = h2\n10 + h2\n11 + f −2\n0 h2\n12\n(6.54)\nand\nh00h10 + h01h11 + f −2\n0 h02h12 = 0.\n(6.55)\nFrom this, we can compute estimates for f0 of\nf 2\n0 =\nh2\n12 −h2\n02\nh2\n00 + h2\n01 −h2\n10 −h2\n11\nif h2\n00 + h2\n01 ̸= h2\n10 + h2\n11\n(6.56)\nor\nf 2\n0 = −\nh02h12\nh00h10 + h01h11\nif h00h10 ̸= −h01h11.\n(6.57)\n(Note that the equations originally given by Szeliski and Shum (1997) are erroneous; the\ncorrect equations are given by Shum and Szeliski (2000).) If neither of these conditions\nholds, we can also take the dot products between the ﬁrst (or second) row and the third one.\nSimilar results can be obtained for f1 as well, by analyzing the columns of ˜\nH10. If the focal\nlength is the same for both images, we can take the geometric mean of f0 and f1 as the\nestimated focal length f = √f1f0. When multiple estimates of f are available, e.g., from\ndifferent homographies, the median value can be used as the ﬁnal estimate.\nA more general (upper-triangular) estimate of K can be obtained in the case of a ﬁxed-\nparameter camera Ki = K using the technique of Hartley (1997b). Observe from (6.52)\nthat Rij ∼K−1 ˜\nHijK and R−T\nij\n∼KT ˜\nH\n−T\nij K−T . Equating Rij = R−T\nij\nwe obtain\nK−1 ˜\nHijK ∼KT ˜\nH\n−T\nij K−T , from which we get\n˜\nHij(KKT ) ∼(KKT ) ˜\nH\n−T\nij .\n(6.58)\n334\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis provides us with some homogeneous linear constraints on the entries in A = KKT ,\nwhich is known as the dual of the image of the absolute conic (Hartley 1997b; Hartley and\nZisserman 2004). (Recall that when we estimate a homography, we can only recover it up to\nan unknown scale.) Given a sufﬁcient number of independent homography estimates ˜\nHij,\nwe can recover A (up to a scale) using either SVD or eigenvalue analysis and then recover\nK through Cholesky decomposition (Appendix A.1.4). Extensions to the cases of temporally\nvarying calibration parameters and non-stationary cameras are discussed by Hartley, Hayman,\nde Agapito et al. (2000) and de Agapito, Hayman, and Reid (2001).\nThe quality of the intrinsic camera parameters can be greatly increased by constructing a\nfull 360◦panorama, since mis-estimating the focal length will result in a gap (or excessive\noverlap) when the ﬁrst image in the sequence is stitched to itself (Figure 9.5). The resulting\nmis-alignment can be used to improve the estimate of the focal length and to re-adjust the\nrotation estimates, as described in Section 9.1.4. Rotating the camera by 90◦around its optic\naxis and re-shooting the panorama is a good way to check for aspect ratio and skew pixel\nproblems, as is generating a full hemi-spherical panorama when there is sufﬁcient texture.\nUltimately, however, the most accurate estimate of the calibration parameters (including\nradial distortion) can be obtained using a full simultaneous non-linear minimization of the\nintrinsic and extrinsic (rotation) parameters, as described in Section 9.2.\n6.3.5 Radial distortion\nWhen images are taken with wide-angle lenses, it is often necessary to model lens distor-\ntions such as radial distortion. As discussed in Section 2.1.6, the radial distortion model\nsays that coordinates in the observed images are displaced away from (barrel distortion) or\ntowards (pincushion distortion) the image center by an amount proportional to their radial\ndistance (Figure 2.13a–b). The simplest radial distortion models use low-order polynomials\n(c.f. Equation (2.78)),\nˆx\n=\nx(1 + κ1r2 + κ2r4)\nˆy\n=\ny(1 + κ1r2 + κ2r4),\n(6.59)\nwhere r2 = x2 + y2 and κ1 and κ2 are called the radial distortion parameters (Brown 1971;\nSlama 1980).13\nA variety of techniques can be used to estimate the radial distortion parameters for a\ngiven lens.14 One of the simplest and most useful is to take an image of a scene with a lot\n13 Sometimes the relationship between x and ˆx is expressed the other way around, i.e., using primed (ﬁnal)\ncoordinates on the right-hand side, x = ˆx(1 + κ1ˆr2 + κ2ˆr4). This is convenient if we map image pixels into\n(warped) rays and then undistort the rays to obtain 3D rays in space, i.e., if we are using inverse warping.\n14 Some of today’s digital cameras are starting to remove radial distortion using software in the camera itself.",
  "image_path": "page_355.jpg",
  "pages": [
    354,
    355,
    356
  ]
}