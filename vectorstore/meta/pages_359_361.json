{
  "doc_id": "pages_359_361",
  "text": "6.5 Exercises\n337\n3. Compute an optimal 2D translation and rotation between the ﬁrst image and all subse-\nquent images, using least squares (Section 6.1.1) with optional RANSAC for robustness\n(Section 6.1.4).\n4. Resample all of the images onto the ﬁrst image’s coordinate frame (Section 3.6.1) using\neither bilinear or bicubic resampling and optionally crop them to their common area.\n5. Convert the resulting images into an animated GIF (using software available from the\nWeb) or optionally implement cross-dissolves to turn them into a “slo-mo” video.\n6. (Optional) Combine this technique with feature-based (Exercise 3.25) morphing.\nEx 6.2: Panography\nCreate the kind of panograph discussed in Section 6.1.2 and com-\nmonly found on the Web.\n1. Take a series of interesting overlapping photos.\n2. Use the feature detector, descriptor, and matcher developed in Exercises 4.1–4.4 (or\nexisting software) to match features among the images.\n3. Turn each connected component of matching features into a track, i.e., assign a unique\nindex i to each track, discarding any tracks that are inconsistent (contain two different\nfeatures in the same image).\n4. Compute a global translation for each image using Equation (6.12).\n5. Since your matches probably contain errors, turn the above least square metric into a\nrobust metric (6.25) and re-solve your system using iteratively reweighted least squares.\n6. Compute the size of the resulting composite canvas and resample each image into its\nﬁnal position on the canvas. (Keeping track of bounding boxes will make this more\nefﬁcient.)\n7. Average all of the images, or choose some kind of ordering and implement translucent\nover compositing (3.8).\n8. (Optional) Extend your parametric motion model to include rotations and scale, i.e.,\nthe similarity transform given in Table 6.1. Discuss how you could handle the case of\ntranslations and rotations only (no scale).\n9. (Optional) Write a simple tool to let the user adjust the ordering and opacity, and add\nor remove images.\n338\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n10. (Optional) Write down a different least squares problem that involves pairwise match-\ning of images. Discuss why this might be better or worse than the global matching\nformula given in (6.12).\nEx 6.3: 2D rigid/Euclidean matching\nSeveral alternative approaches are given in Section 6.1.3\nfor estimating a 2D rigid (Euclidean) alignment.\n1. Implement the various alternatives and compare their accuracy on synthetic data, i.e.,\nrandom 2D point clouds with noisy feature positions.\n2. One approach is to estimate the translations from the centroids and then estimate ro-\ntation in polar coordinates. Do you need to weight the angles obtained from a polar\ndecomposition in some way to get the statistically correct estimate?\n3. How can you modify your techniques to take into account either scalar (6.10) or full\ntwo-dimensional point covariance weightings (6.11)? Do all of the previously devel-\noped “shortcuts” still work or does full weighting require iterative optimization?\nEx 6.4: 2D match move/augmented reality\nReplace a picture in a magazine or a book\nwith a different image or video.\n1. With a webcam, take a picture of a magazine or book page.\n2. Outline a ﬁgure or picture on the page with a rectangle, i.e., draw over the four sides as\nthey appear in the image.\n3. Match features in this area with each new image frame.\n4. Replace the original image with an “advertising” insert, warping the new image with\nthe appropriate homography.\n5. Try your approach on a clip from a sporting event (e.g., indoor or outdoor soccer) to\nimplement a billboard replacement.\nEx 6.5: 3D joystick\nTrack a Rubik’s cube to implement a 3D joystick/mouse control.\n1. Get out an old Rubik’s cube (or get one from your parents).\n2. Write a program to detect the center of each colored square.\n3. Group these centers into lines and then ﬁnd the vanishing points for each face.\n4. Estimate the rotation angle and focal length from the vanishing points.\n6.5 Exercises\n339\n5. Estimate the full 3D pose (including translation) by ﬁnding one or more 3×3 grids and\nrecovering the plane’s full equation from this known homography using the technique\ndeveloped by Zhang (2000).\n6. Alternatively, since you already know the rotation, simply estimate the unknown trans-\nlation from the known 3D corner points on the cube and their measured 2D locations\nusing either linear or non-linear least squares.\n7. Use the 3D rotation and position to control a VRML or 3D game viewer.\nEx 6.6: Rotation-based calibration\nTake an outdoor or indoor sequence from a rotating\ncamera with very little parallax and use it to calibrate the focal length of your camera using\nthe techniques described in Section 6.3.4 or Sections 9.1.3–9.2.1.\n1. Take out any radial distortion in the images using one of the techniques from Exer-\ncises 6.10–6.11 or using parameters supplied for a given camera by your instructor.\n2. Detect and match feature points across neighboring frames and chain them into feature\ntracks.\n3. Compute homographies between overlapping frames and use Equations (6.56–6.57) to\nget an estimate of the focal length.\n4. Compute a full 360◦panorama and update your focal length estimate to close the gap\n(Section 9.1.4).\n5. (Optional) Perform a complete bundle adjustment in the rotation matrices and focal\nlength to obtain the highest quality estimate (Section 9.2.1).\nEx 6.7: Target-based calibration\nUse a three-dimensional target to calibrate your camera.\n1. Construct a three-dimensional calibration pattern with known 3D locations. It is not\neasy to get high accuracy unless you use a machine shop, but you can get close using\nheavy plywood and printed patterns.\n2. Find the corners, e.g, using a line ﬁnder and intersecting the lines.\n3. Implement one of the iterative calibration and pose estimation algorithms described\nin Tsai (1987); Bogart (1991); Gleicher and Witkin (1992) or the system described in\nSection 6.2.2.\n4. Take many pictures at different distances and orientations relative to the calibration\ntarget and report on both your re-projection errors and accuracy. (To do the latter, you\nmay need to use simulated data.)",
  "image_path": "page_360.jpg",
  "pages": [
    359,
    360,
    361
  ]
}