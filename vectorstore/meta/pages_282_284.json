{
  "doc_id": "pages_282_284",
  "text": "260\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n• oriented/steerable ﬁlter, looking for either second-order high second response or two\nedges in a window (Koethe 2003), as discussed in Section 4.1.1.\nOther detectors are described by Mikolajczyk, Tuytelaars, Schmid et al. (2005); Tuytelaars\nand Mikolajczyk (2007). Additional optional steps could include:\n1. Compute the detections on a sub-octave pyramid and ﬁnd 3D maxima.\n2. Find local orientation estimates using steerable ﬁlter responses or a gradient histogram-\nming method.\n3. Implement non-maximal suppression, such as the adaptive technique of Brown, Szeliski,\nand Winder (2005).\n4. Vary the window shape and size (pre-ﬁlter and aggregation).\nTo test for repeatability, download the code from http://www.robots.ox.ac.uk/∼vgg/research/\nafﬁne/ (Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuytelaars and Mikolajczyk 2007) or\nsimply rotate or shear your own test images. (Pick a domain you may want to use later, e.g.,\nfor outdoor stitching.)\nBe sure to measure and report the stability of your scale and orientation estimates.\nEx 4.2: Interest point descriptor\nImplement one or more descriptors (steered to local scale\nand orientation) and compare their performance (with your own or with a classmate’s detec-\ntor).\nSome possible descriptors include\n• contrast-normalized patches (Brown, Szeliski, and Winder 2005);\n• SIFT (Lowe 2004);\n• GLOH (Mikolajczyk and Schmid 2005);\n• DAISY (Winder and Brown 2007; Tola, Lepetit, and Fua 2010).\nOther detectors are described by Mikolajczyk and Schmid (2005).\nEx 4.3: ROC curve computation\nGiven a pair of curves (histograms) plotting the number\nof matching and non-matching features as a function of Euclidean distance d as shown in\nFigure 4.23b, derive an algorithm for plotting a ROC curve (Figure 4.23a). In particular, let\nt(d) be the distribution of true matches and f(d) be the distribution of (false) non-matches.\nWrite down the equations for the ROC, i.e., TPR(FPR), and the AUC.\n(Hint: Plot the cumulative distributions T(d) =\nR\nt(d) and F(d) =\nR\nf(d) and see if\nthese help you derive the TPR and FPR at a given threshold θ.)\n4.5 Exercises\n261\nEx 4.4: Feature matcher\nAfter extracting features from a collection of overlapping or dis-\ntorted images,10 match them up by their descriptors either using nearest neighbor matching\nor a more efﬁcient matching strategy such as a k-d tree.\nSee whether you can improve the accuracy of your matches using techniques such as the\nnearest neighbor distance ratio.\nEx 4.5: Feature tracker\nInstead of ﬁnding feature points independently in multiple images\nand then matching them, ﬁnd features in the ﬁrst image of a video or image sequence and\nthen re-locate the corresponding points in the next frames using either search and gradient\ndescent (Shi and Tomasi 1994) or learned feature detectors (Lepetit, Pilet, and Fua 2006;\nFossati, Dimitrijevic, Lepetit et al. 2007). When the number of tracked points drops below a\nthreshold or new regions in the image become visible, ﬁnd additional points to track.\n(Optional) Winnow out incorrect matches by estimating a homography (6.19–6.23) or\nfundamental matrix (Section 7.2.1).\n(Optional) Reﬁne the accuracy of your matches using the iterative registration algorithm\ndescribed in Section 8.2 and Exercise 8.2.\nEx 4.6: Facial feature tracker\nApply your feature tracker to tracking points on a person’s\nface, either manually initialized to interesting locations such as eye corners or automatically\ninitialized at interest points.\n(Optional) Match features between two people and use these features to perform image\nmorphing (Exercise 3.25).\nEx 4.7: Edge detector\nImplement an edge detector of your choice. Compare its perfor-\nmance to that of your classmates’ detectors or code downloaded from the Internet.\nA simple but well-performing sub-pixel edge detector can be created as follows:\n1. Blur the input image a little,\nBσ(x) = Gσ(x) ∗I(x).\n2. Construct a Gaussian pyramid (Exercise 3.19),\nP = Pyramid{Bσ(x)}\n3. Subtract an interpolated coarser-level pyramid image from the original resolution blurred\nimage,\nS(x) = Bσ(x) −P.InterpolatedLevel(L).\n10 http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/.\n262\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nstruct SEdgel {\nfloat e[2][2];\n// edgel endpoints (zero crossing)\nfloat x, y;\n// sub-pixel edge position (midpoint)\nfloat n_x, n_y;\n// orientation, as normal vector\nfloat theta;\n// orientation, as angle (degrees)\nfloat length;\n// length of edgel\nfloat strength;\n// strength of edgel (gradient magnitude)\n};\nstruct SLine : public SEdgel {\nfloat line_length;\n// length of line (est. from ellipsoid)\nfloat sigma;\n// estimated std. dev. of edgel noise\nfloat r;\n// line equation: x * n_y - y * n_x = r\n};\nFigure 4.48 A potential C++ structure for edgel and line elements.\n4. For each quad of pixels, {(i, j), (i + 1, j), (i, j + 1), (i + 1, j + 1)}, count the number\nof zero crossings along the four edges.\n5. When there are exactly two zero crossings, compute their locations using (4.25) and\nstore these edgel endpoints along with the midpoint in the edgel structure (Figure 4.48).\n6. For each edgel, compute the local gradient by taking the horizontal and vertical differ-\nences between the values of S along the zero crossing edges.\n7. Store the magnitude of this gradient as the edge strength and either its orientation or\nthat of the segment joining the edgel endpoints as the edge orientation.\n8. Add the edgel to a list of edgels or store it in a 2D array of edgels (addressed by pixel\ncoordinates).\nFigure 4.48 shows a possible representation for each computed edgel.\nEx 4.8: Edge linking and thresholding\nLink up the edges computed in the previous exer-\ncise into chains and optionally perform thresholding with hysteresis.\nThe steps may include:\n1. Store the edgels either in a 2D array (say, an integer image with indices into the edgel\nlist) or pre-sort the edgel list ﬁrst by (integer) x coordinates and then y coordinates, for\nfaster neighbor ﬁnding.",
  "image_path": "page_283.jpg",
  "pages": [
    282,
    283,
    284
  ]
}