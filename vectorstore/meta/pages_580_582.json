{
  "doc_id": "pages_580_582",
  "text": "558\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nof neighboring plane ﬁt parameters. Another highly ranked algorithm, by Yang, Wang, Yang\net al. (2009), uses the color correlation approach of Yoon and Kweon (2006) and hierarchical\nbelief propagation to obtain an initial set of disparity estimates. After left–right consistency\nchecking to detect occluded pixels, the data terms for low-conﬁdence and occluded pixels\nare recomputed using segmentation-based plane ﬁts and one or more rounds of hierarchical\nbelief propagation are used to obtain the ﬁnal disparity estimates.\nAnother important ability of segmentation-based stereo algorithms, which they share with\nalgorithms that use explicit layers (Baker, Szeliski, and Anandan 1998; Szeliski and Golland\n1999) or boundary extraction (Hasinoff, Kang, and Szeliski 2006), is the ability to extract\nfractional pixel alpha mattes at depth discontinuities (Bleyer, Gelautz, Rother et al. 2009).\nThis ability is crucial when attempting to create virtual view interpolation without clinging\nboundary or tearing artifacts (Zitnick, Kang, Uyttendaele et al. 2004) and also to seamlessly\ninsert virtual objects (Taguchi, Wilburn, and Zitnick 2008), as shown in Figure 11.13b.\nSince new stereo matching algorithms continue to be introduced every year, it is a good\nidea to periodically check the Middlebury evaluation site at http://vision.middlebury.edu/\nstereo for a listing of the most recent algorithms to be evaluated.\n11.5.3 Application: Z-keying and background replacement\nAnother application of real-time stereo matching is z-keying, which is the process of seg-\nmenting a foreground actor from the background using depth information, usually for the\npurpose of replacing the background with some computer-generated imagery, as shown in\nFigure 11.2g.\nOriginally, z-keying systems required expensive custom-built hardware to produce the\ndesired depth maps in real time and were, therefore, restricted to broadcast studio applica-\ntions (Kanade, Yoshida, Oda et al. 1996; Iddan and Yahav 2001). Off-line systems were also\ndeveloped for estimating 3D multi-viewpoint geometry from video streams (Section 13.5.4)\n(Kanade, Rander, and Narayanan 1997; Carranza, Theobalt, Magnor et al. 2003; Zitnick,\nKang, Uyttendaele et al. 2004; Vedula, Baker, and Kanade 2005). Recent advances in highly\naccurate real-time stereo matching, however, now make it possible to perform z-keying on\nregular PCs, enabling desktop videoconferencing applications such as those shown in Fig-\nure 11.14 (Kolmogorov, Criminisi, Blake et al. 2006).\n11.6 Multi-view stereo\nWhile matching pairs of images is a useful way of obtaining depth information, matching\nmore images can lead to even better results. In this section, we review not only techniques for\n11.6 Multi-view stereo\n559\nFigure 11.14\nBackground replacement using z-keying with a bi-layer segmentation algo-\nrithm (Kolmogorov, Criminisi, Blake et al. 2006) c⃝2006 IEEE.\ncreating complete 3D object models, but also simpler techniques for improving the quality of\ndepth maps using multiple source images.\nAs we saw in our discussion of plane sweep (Section 11.1.2), it is possible to resample\nall neighboring k images at each disparity hypothesis d into a generalized disparity space\nvolume ˜I(x, y, d, k). The simplest way to take advantage of these additional images is to sum\nup their differences from the reference image Ir as in (11.4),\nC(x, y, d) =\nX\nk\nρ(˜I(x, y, d, k) −Ir(x, y)).\n(11.15)\nThis is the basis of the well-known sum of summed-squared-difference (SSSD) and SSAD\napproaches (Okutomi and Kanade 1993; Kang, Webb, Zitnick et al. 1995), which can be ex-\ntended to reason about likely patterns of occlusion (Nakamura, Matsuura, Satoh et al. 1996).\nMore recent work by Gallup, Frahm, Mordohai et al. (2008) show how to adapt the base-\nlines used to the expected depth in order to get the best tradeoff between geometric accuracy\n(wide baseline) and robustness to occlusion (narrow baseline). Alternative multi-view cost\nmetrics include measures such as synthetic focus sharpness and the entropy of the pixel color\ndistribution (Vaish, Szeliski, Zitnick et al. 2006).\nA useful way to visualize the multi-frame stereo estimation problem is to examine the\nepipolar plane image (EPI) formed by stacking corresponding scanlines from all the images,\nas shown in Figures 8.13c and 11.15 (Bolles, Baker, and Marimont 1987; Baker and Bolles\n1989; Baker 1989). As you can see in Figure 11.15, as a camera translates horizontally (in a\nstandard horizontally rectiﬁed geometry), objects at different depths move sideways at a rate\ninversely proportional to their depth (11.1).6 Foreground objects occlude background objects,\nwhich can be seen as EPI-strips (Criminisi, Kang, Swaminathan et al. 2005) occluding other\n6 The four-dimensional generalization of the EPI is the light ﬁeld, which we study in Section 13.3. In principle,\n560\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nA\nE\nD\nC\nB\nleft\nmiddle\nright\nF\nx\nt\n(a)\n(b)\nFigure 11.15 Epipolar plane image (EPI) (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996\nACM and a schematic EPI (Kang, Szeliski, and Chai 2001) c⃝2001 IEEE. (a) The Lumigraph\n(light ﬁeld) (Section 13.3) is the 4D space of all light rays passing through a volume of space.\nTaking a 2D slice results in all of the light rays embedded in a plane and is equivalent to a\nscanline taken from a stacked EPI volume. Objects at different depths move sideways with\nvelocities (slopes) proportional to their inverse depth. Occlusion (and translucency) effects\ncan easily be seen in this representation. (b) The EPI corresponding to Figure 11.16 showing\nthe three images (middle, left, and right) as slices through the EPI volume. The spatially and\ntemporally shifted window around the black pixel is indicated by the rectangle, showing the\nright image is not being used in matching.\nstrips in the EPI. If we are given a dense enough set of images, we can ﬁnd such strips and\nreason about their relationships in order to both reconstruct the 3D scene and make inferences\nabout translucent objects (Tsin, Kang, and Szeliski 2006) and specular reﬂections (Swami-\nnathan, Kang, Szeliski et al. 2002; Criminisi, Kang, Swaminathan et al. 2005). Alternatively,\nwe can treat the series of images as a set of sequential observations and merge them using\nKalman ﬁltering (Matthies, Kanade, and Szeliski 1989) or maximum likelihood inference\n(Cox 1994).\nWhen fewer images are available, it becomes necessary to fall back on aggregation tech-\nniques such as sliding windows or global optimization. With additional input images, how-\never, the likelihood of occlusions increases. It is therefore prudent to adjust not only the best\nwindow locations using a shiftable window approach, as shown in Figure 11.16a, but also to\noptionally select a subset of neighboring frames in order to discount those images where the\nregion of interest is occluded, as shown in Figure 11.16b (Kang, Szeliski, and Chai 2001).\nthere is enough information in a light ﬁeld to recover both the shape and the BRDF of objects (Soatto, Yezzi, and Jin\n2003), although relatively little progress has been made to date on this topic.",
  "image_path": "page_581.jpg",
  "pages": [
    580,
    581,
    582
  ]
}