{
  "doc_id": "pages_558_560",
  "text": "536\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\nFigure 11.2 Applications of stereo vision: (a) input image, (b) computed depth map, and (c)\nnew view generation from multi-view stereo (Matthies, Kanade, and Szeliski 1989) c⃝1989\nSpringer; (d) view morphing between two images (Seitz and Dyer 1996) c⃝1996 ACM; (e–f)\n3D face modeling (images courtesy of Fr´ed´eric Devernay); (g) z-keying live and computer-\ngenerated imagery (Kanade, Yoshida, Oda et al. 1996) c⃝1996 IEEE; (h–j) building 3D\nsurface models from multiple video streams in Virtualized Reality (Kanade, Rander, and\nNarayanan 1997).\n11.1 Epipolar geometry\n537\nthe general taxonomy proposed by Scharstein and Szeliski (2002). We begin in Section 11.1\nwith a review of the geometry of stereo image matching, i.e., how to compute for a given\npixel in one image the range of possible locations the pixel might appear at in the other\nimage, i.e., its epipolar line. We describe how to pre-warp images so that corresponding\nepipolar lines are coincident (rectiﬁcation). We also describe a general resampling algorithm\ncalled plane sweep that can be used to perform multi-image stereo matching with arbitrary\ncamera conﬁgurations.\nNext, we brieﬂy survey techniques for the sparse stereo matching of interest points and\nedge-like features (Section 11.2). We then turn to the main topic of this chapter, namely the\nestimation of a dense set of pixel-wise correspondences in the form of a disparity map (Fig-\nure 11.1c). This involves ﬁrst selecting a pixel matching criterion (Section 11.3) and then\nusing either local area-based aggregation (Section 11.4) or global optimization (Section 11.5)\nto help disambiguate potential matches. In Section 11.6, we discuss multi-view stereo meth-\nods that aim to reconstruct a complete 3D model instead of just a single disparity image\n(Figure 11.1d–f).\n11.1 Epipolar geometry\nGiven a pixel in one image, how can we compute its correspondence in the other image? In\nChapter 8, we saw that a variety of search techniques can be used to match pixels based on\ntheir local appearance as well as the motions of neighboring pixels. In the case of stereo\nmatching, however, we have some additional information available, namely the positions and\ncalibration data for the cameras that took the pictures of the same static scene (Section 7.2).\nHow can we exploit this information to reduce the number of potential correspondences,\nand hence both speed up the matching and increase its reliability? Figure 11.3a shows how a\npixel in one image x0 projects to an epipolar line segment in the other image. The segment\nis bounded at one end by the projection of the original viewing ray at inﬁnity p∞and at the\nother end by the projection of the original camera center c0 into the second camera, which\nis known as the epipole e1. If we project the epipolar line in the second image back into the\nﬁrst, we get another line (segment), this time bounded by the other corresponding epipole\ne0. Extending both line segments to inﬁnity, we get a pair of corresponding epipolar lines\n(Figure 11.3b), which are the intersection of the two image planes with the epipolar plane\nthat passes through both camera centers c0 and c1 as well as the point of interest p (Faugeras\nand Luong 2001; Hartley and Zisserman 2004).\n538\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np\nx1\nx0\n(R,t)\np∞\ne1\ne0\nc0\nc1\n       epipolar plane\np∞\np\n(R,t)\nc0\nc1\nepipolar\nlines\nx\n0\ne0\ne1\nx1\nl1\nl0\n(a)\n(b)\nFigure 11.3\nEpipolar geometry: (a) epipolar line segment corresponding to one ray; (b)\ncorresponding set of epipolar lines and their epipolar plane.\n11.1.1 Rectiﬁcation\nAs we saw in Section 7.2, the epipolar geometry for a pair of cameras is implicit in the\nrelative pose and calibrations of the cameras, and can easily be computed from seven or more\npoint matches using the fundamental matrix (or ﬁve or more points for the calibrated essential\nmatrix) (Zhang 1998a,b; Faugeras and Luong 2001; Hartley and Zisserman 2004). Once this\ngeometry has been computed, we can use the epipolar line corresponding to a pixel in one\nimage to constrain the search for corresponding pixels in the other image. One way to do this\nis to use a general correspondence algorithm, such as optical ﬂow (Section 8.4), but to only\nconsider locations along the epipolar line (or to project any ﬂow vectors that fall off back onto\nthe line).\nA more efﬁcient algorithm can be obtained by ﬁrst rectifying (i.e, warping) the input\nimages so that corresponding horizontal scanlines are epipolar lines (Loop and Zhang 1999;\nFaugeras and Luong 2001; Hartley and Zisserman 2004).2 Afterwards, it is possible to match\nhorizontal scanlines independently or to shift images horizontally while computing matching\nscores (Figure 11.4).\nA simple way to rectify the two images is to ﬁrst rotate both cameras so that they are\nlooking perpendicular to the line joining the camera centers c0 and c1. Since there is a de-\ngree of freedom in the tilt, the smallest rotations that achieve this should be used. Next, to\ndetermine the desired twist around the optical axes, make the up vector (the camera y axis)\n2 This makes most sense if the cameras are next to each other, although by rotating the cameras, rectiﬁcation can\nbe performed on any pair that is not verged too much or has too much of a scale change. In those latter cases, using\nplane sweep (below) or hypothesizing small planar patch locations in 3D (Goesele, Snavely, Curless et al. 2007) may\nbe preferable.",
  "image_path": "page_559.jpg",
  "pages": [
    558,
    559,
    560
  ]
}