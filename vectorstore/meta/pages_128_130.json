{
  "doc_id": "pages_128_130",
  "text": "106\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n×\n(1−\n)\n+\n=\nB\nα\nαF\nC\n(a)\n(b)\n(c)\n(d)\nFigure 3.5\nCompositing equation C = (1 −α)B + αF. The images are taken from a\nclose-up of the region of the hair in the upper right part of the lion in Figure 3.4.\nthat occur if only binary opacities are used.\nTo composite a new (or foreground) image on top of an old (background) image, the over\noperator, ﬁrst proposed by Porter and Duff (1984) and then studied extensively by Blinn\n(1994a; 1994b), is used,\nC = (1 −α)B + αF.\n(3.8)\nThis operator attenuates the inﬂuence of the background image B by a factor (1 −α) and\nthen adds in the color (and opacity) values corresponding to the foreground layer F, as shown\nin Figure 3.5.\nIn many situations, it is convenient to represent the foreground colors in pre-multiplied\nform, i.e., to store (and manipulate) the αF values directly. As Blinn (1994b) shows, the\npre-multiplied RGBA representation is preferred for several reasons, including the ability\nto blur or resample (e.g., rotate) alpha-matted images without any additional complications\n(just treating each RGBA band independently). However, when matting using local color\nconsistency (Ruzon and Tomasi 2000; Chuang, Curless, Salesin et al. 2001), the pure un-\nmultiplied foreground colors F are used, since these remain constant (or vary slowly) in the\nvicinity of the object edge.\nThe over operation is not the only kind of compositing operation that can be used. Porter\nand Duff (1984) describe a number of additional operations that can be useful in photo editing\nand visual effects applications. In this book, we concern ourselves with only one additional,\ncommonly occurring case (but see Exercise 3.2).\nWhen light reﬂects off clean transparent glass, the light passing through the glass and\nthe light reﬂecting off the glass are simply added together (Figure 3.6). This model is use-\nful in the analysis of transparent motion (Black and Anandan 1996; Szeliski, Avidan, and\nAnandan 2000), which occurs when such scenes are observed from a moving camera (see\nSection 8.5.2).\nThe actual process of matting, i.e., recovering the foreground, background, and alpha\nmatte values from one or more images, has a rich history, which we study in Section 10.4.\n3.1 Point operators\n107\nFigure 3.6 An example of light reﬂecting off the transparent glass of a picture frame (Black\nand Anandan 1996) c⃝1996 Elsevier. You can clearly see the woman’s portrait inside the\npicture frame superimposed with the reﬂection of a man’s face off the glass.\nSmith and Blinn (1996) have a nice survey of traditional blue-screen matting techniques,\nwhile Toyama, Krumm, Brumitt et al. (1999) review difference matting. More recently, there\nhas been a lot of activity in computational photography relating to natural image matting\n(Ruzon and Tomasi 2000; Chuang, Curless, Salesin et al. 2001; Wang and Cohen 2007a),\nwhich attempts to extract the mattes from a single natural image (Figure 3.4a) or from ex-\ntended video sequences (Chuang, Agarwala, Curless et al. 2002). All of these techniques are\ndescribed in more detail in Section 10.4.\n3.1.4 Histogram equalization\nWhile the brightness and gain controls described in Section 3.1.1 can improve the appearance\nof an image, how can we automatically determine their best values? One approach might\nbe to look at the darkest and brightest pixel values in an image and map them to pure black\nand pure white. Another approach might be to ﬁnd the average value in the image, push it\ntowards middle gray, and expand the range so that it more closely ﬁlls the displayable values\n(Kopf, Uyttendaele, Deussen et al. 2007).\nHow can we visualize the set of lightness values in an image in order to test some of\nthese heuristics? The answer is to plot the histogram of the individual color channels and\nluminance values, as shown in Figure 3.7b.2 From this distribution, we can compute relevant\nstatistics such as the minimum, maximum, and average intensity values. Notice that the image\nin Figure 3.7a has both an excess of dark values and light values, but that the mid-range values\nare largely under-populated. Would it not be better if we could simultaneously brighten some\n2 The histogram is simply the count of the number of pixels at each gray level value. For an eight-bit image, an\naccumulation table with 256 entries is needed. For higher bit depths, a table with the appropriate number of entries\n(probably fewer than the full number of gray levels) should be used.\n108\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n0\n50\n100\n150\n200\n250\nB\nG\nR\nY\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\n0\n50\n100\n150\n200\n250\nB\nG\nR\nY\n(a)\n(b)\n(c)\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\nB\nG\nR\nY\n(d)\n(e)\n(f)\nFigure 3.7 Histogram analysis and equalization: (a) original image (b) color channel and in-\ntensity (luminance) histograms; (c) cumulative distribution functions; (d) equalization (trans-\nfer) functions; (e) full histogram equalization; (f) partial histogram equalization.\ndark values and darken some light values, while still using the full extent of the available\ndynamic range? Can you think of a mapping that might do this?\nOne popular answer to this question is to perform histogram equalization, i.e., to ﬁnd\nan intensity mapping function f(I) such that the resulting histogram is ﬂat. The trick to\nﬁnding such a mapping is the same one that people use to generate random samples from\na probability density function, which is to ﬁrst compute the cumulative distribution function\nshown in Figure 3.7c.\nThink of the original histogram h(I) as the distribution of grades in a class after some\nexam. How can we map a particular grade to its corresponding percentile, so that students at\nthe 75% percentile range scored better than 3/4 of their classmates? The answer is to integrate\nthe distribution h(I) to obtain the cumulative distribution c(I),\nc(I) = 1\nN\nI\nX\ni=0\nh(i) = c(I −1) + 1\nN h(I),\n(3.9)\nwhere N is the number of pixels in the image or students in the class. For any given grade or\nintensity, we can look up its corresponding percentile c(I) and determine the ﬁnal value that\npixel should take. When working with eight-bit pixel values, the I and c axes are rescaled\nfrom [0, 255].",
  "image_path": "page_129.jpg",
  "pages": [
    128,
    129,
    130
  ]
}