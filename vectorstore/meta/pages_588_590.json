{
  "doc_id": "pages_588_590",
  "text": "566\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncan be found.\nSome approaches use global optimization deﬁned over a three-dimensional photoconsis-\ntency volume to recover a complete surface. Approaches based on graph cuts use polynomial\ncomplexity binary segmentation algorithms to recover the object model deﬁned on the voxel\ngrid (Sinha and Pollefeys 2005; Vogiatzis, Hernandez, Torr et al. 2007; Hiep, Keriven, Pons\net al. 2009). Level set approaches use a continuous surface evolution to ﬁnd a good mini-\nmum in the conﬁguration space of potential surfaces and therefore require a reasonably good\ninitialization (Faugeras and Keriven 1998; Pons, Keriven, and Faugeras 2007). In order for\nthe photoconsistency volume to be meaningful, matching costs need to be computed in some\nrobust fashion, e.g., using sets of limited views or by aggregating multiple depth maps.\nAn alternative approach to global optimization is to sweep through the 3D volume while\ncomputing both photoconsistency and visibility simultaneously. The voxel coloring algorithm\nof Seitz and Dyer (1999) performs a front-to-back plane sweep. On every plane, any voxels\nthat are sufﬁciently photoconsistent are labeled as part of the object. The corresponding\npixels in the source images can then be “erased”, since they are already accounted for, and\ntherefore do not contribute to further photoconsistency computations. (A similar approach,\nalbeit without the front-to-back sweep order, is used by Szeliski and Golland (1999).) The\nresulting 3D volume, under noise- and resampling-free conditions, is guaranteed to produce\nboth a photoconsistent 3D model and to enclose whatever true 3D object model generated the\nimages.\nUnfortunately, voxel coloring is only guaranteed to work if all of the cameras lie on the\nsame side of the sweep planes, which is not possible in general ring conﬁgurations of cameras.\nKutulakos and Seitz (2000) generalize voxel coloring to space carving, where subsets of\ncameras that satisfy the voxel coloring constraint are iteratively selected and the 3D voxel\ngrid is alternately carved away along different axes.\nAnother popular approach to multi-view stereo is to ﬁrst independently compute multiple\ndepth maps and then merge these partial maps into a complete 3D model. Approaches to\ndepth map merging, which are discussed in more detail in Section 12.2.1, include signed\ndistance functions (Curless and Levoy 1996), used by Goesele, Curless, and Seitz (2006),\nand Poisson surface reconstruction (Kazhdan, Bolitho, and Hoppe 2006), used by Goesele,\nSnavely, Curless et al. (2007). It is also possible to reconstruct sparser representations, such\nas 3D points and lines, and to interpolate them to full 3D surfaces (Section 12.3.1) (Taylor\n2003).\nInitialization requirements.\nOne ﬁnal element discussed by Seitz, Curless, Diebel et al.\n(2006) is the varying degrees of initialization required by different algorithms. Because some\nalgorithms reﬁne or evolve a rough 3D model, they require a reasonably accurate (or over-\ncomplete) initial model, which can often be obtained by reconstructing a volume from object\n11.6 Multi-view stereo\n567\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 11.20 The multi-view stereo data sets captured by Seitz, Curless, Diebel et al. (2006)\nc⃝2006 Springer. Only (a) and (b) are currently used for evaluation.\nsilhouettes, as discussed in Section 11.6.2. However, if the algorithm performs a global op-\ntimization (Kolev, Klodt, Brox et al. 2009; Kolev and Cremers 2009), this dependence on\ninitialization is not an issue.\nEmpirical evaluation.\nIn order to evaluate the large number of design alternatives in multi-\nview stereo, Seitz, Curless, Diebel et al. (2006) collected a dataset of calibrated images using\na spherical gantry. A representative image from each of the six datasets is shown in Fig-\nure 11.20, although only the ﬁrst two datasets have as yet been fully processed and used for\nevaluation. Figure 11.21 shows the results of running seven different algorithms on the tem-\nple dataset. As you can see, most of the techniques do an impressive job of capturing the ﬁne\ndetails in the columns, although it is also clear that the techniques employ differing amounts\nof smoothing to achieve these results.\nSince the publication of the survey by Seitz, Curless, Diebel et al. (2006), the ﬁeld of\nmulti-view stereo has continued to advance at a rapid pace (Strecha, Fransens, and Van\nGool 2006; Hernandez, Vogiatzis, and Cipolla 2007; Habbecke and Kobbelt 2007; Furukawa\nand Ponce 2007; Vogiatzis, Hernandez, Torr et al. 2007; Goesele, Snavely, Curless et al.\n2007; Sinha, Mordohai, and Pollefeys 2007; Gargallo, Prados, and Sturm 2007; Merrell, Ak-\nbarzadeh, Wang et al. 2007; Zach, Pock, and Bischof 2007b; Furukawa and Ponce 2008;\nHornung, Zeng, and Kobbelt 2008; Bradley, Boubekeur, and Heidrich 2008; Zach 2008;\nCampbell, Vogiatzis, Hern´andez et al. 2008; Kolev, Klodt, Brox et al. 2009; Hiep, Keriven,\nPons et al. 2009; Furukawa, Curless, Seitz et al. 2010). The multi-view stereo evaluation site,\nhttp://vision.middlebury.edu/mview/, provides quantitative results for these algorithms along\nwith pointers to where to ﬁnd these papers.\n11.6.2 Shape from silhouettes\nIn many situations, performing a foreground–background segmentation of the object of in-\nterest is a good way to initialize or ﬁt a 3D model (Grauman, Shakhnarovich, and Darrell\n568\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 11.21 Reconstruction results (details) for seven algorithms (Hernandez and Schmitt\n2004; Furukawa and Ponce 2009; Pons, Keriven, and Faugeras 2005; Goesele, Curless, and\nSeitz 2006; Vogiatzis, Torr, and Cipolla 2005; Tran and Davis 2002; Kolmogorov and Zabih\n2002) evaluated by Seitz, Curless, Diebel et al. (2006) on the 47-image Temple Ring dataset.\nThe numbers underneath each detail image are the accuracy of each of these techniques mea-\nsured in millimeters.\n2003; Vlasic, Baran, Matusik et al. 2008) or to impose a convex set of constraints on multi-\nview stereo (Kolev and Cremers 2008). Over the years, a number of techniques have been\ndeveloped to reconstruct a 3D volumetric model from the intersection of the binary silhou-\nettes projected into 3D. The resulting model is called a visual hull (or sometimes a line hull),\nanalogous with the convex hull of a set of points, since the volume is maximal with respect\nto the visual silhouettes and surface elements are tangent to the viewing rays (lines) along\nthe silhouette boundaries (Laurentini 1994). It is also possible to carve away a more accu-\nrate reconstruction using multi-view stereo (Sinha and Pollefeys 2005) or by analyzing cast\nshadows (Savarese, Andreetto, Rushmeier et al. 2007).\nSome techniques ﬁrst approximate each silhouette with a polygonal representation and\nthen intersect the resulting faceted conical regions in three-space to produce polyhedral mod-\nels (Baumgart 1974; Martin and Aggarwal 1983; Matusik, Buehler, and McMillan 2001),\nwhich can later be reﬁned using triangular splines (Sullivan and Ponce 1998). Other ap-\nproaches use voxel-based representations, usually encoded as octrees (Samet 1989), because\nof the resulting space–time efﬁciency. Figures 11.22a–b show an example of a 3D octree\nmodel and its associated colored tree, where black nodes are interior to the model, white",
  "image_path": "page_589.jpg",
  "pages": [
    588,
    589,
    590
  ]
}