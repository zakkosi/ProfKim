{
  "doc_id": "pages_467_469",
  "text": "9.2 Global alignment\n445\nprefer the average z-axis of the individual rotation matrices, k = P\nk ˆk\nT Rk to be close to\nthe world z-axis, rg2 = Rgˆk. We can therefore compute the full rotation matrix Rg in three\nsteps:\n1. rg1 = min eigenvector (P\nk rk0rT\nk0);\n2. rg0 = N((P\nk rk2) × rg1);\n3. rg2 = rg0 × rg1,\nwhere N(v) = v/∥v∥normalizes a vector v.\n9.2.2 Parallax removal\nOnce we have optimized the global orientations and focal lengths of our cameras, we may ﬁnd\nthat the images are still not perfectly aligned, i.e., the resulting stitched image looks blurry\nor ghosted in some places. This can be caused by a variety of factors, including unmodeled\nradial distortion, 3D parallax (failure to rotate the camera around its optical center), small\nscene motions such as waving tree branches, and large-scale scene motions such as people\nmoving in and out of pictures.\nEach of these problems can be treated with a different approach. Radial distortion can be\nestimated (potentially ahead of time) using one of the techniques discussed in Section 2.1.6.\nFor example, the plumb-line method (Brown 1971; Kang 2001; El-Melegy and Farag 2003)\nadjusts radial distortion parameters until slightly curved lines become straight, while mosaic-\nbased approaches adjust them until mis-registration is reduced in image overlap areas (Stein\n1997; Sawhney and Kumar 1999).\n3D parallax can be handled by doing a full 3D bundle adjustment, i.e., by replacing the\nprojection equation (9.26) used in Equation (9.29) with Equation (2.68), which models cam-\nera translations. The 3D positions of the matched feature points and cameras can then be si-\nmultaneously recovered, although this can be signiﬁcantly more expensive than parallax-free\nimage registration. Once the 3D structure has been recovered, the scene could (in theory) be\nprojected to a single (central) viewpoint that contains no parallax. However, in order to do\nthis, dense stereo correspondence needs to be performed (Section 11.3) (Li, Shum, Tang et al.\n2004; Zheng, Kang, Cohen et al. 2007), which may not be possible if the images contain only\npartial overlap. In that case, it may be necessary to correct for parallax only in the overlap\nareas, which can be accomplished using a multi-perspective plane sweep (MPPS) algorithm\n(Kang, Szeliski, and Uyttendaele 2004; Uyttendaele, Criminisi, Kang et al. 2004).\nWhen the motion in the scene is very large, i.e., when objects appear and disappear com-\npletely, a sensible solution is to simply select pixels from only one image at a time as the\nsource for the ﬁnal composite (Milgram 1977; Davis 1998; Agarwala, Dontcheva, Agrawala\n446\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\net al. 2004), as discussed in Section 9.3.2. However, when the motion is reasonably small (on\nthe order of a few pixels), general 2D motion estimation (optical ﬂow) can be used to perform\nan appropriate correction before blending using a process called local alignment (Shum and\nSzeliski 2000; Kang, Uyttendaele, Winder et al. 2003). This same process can also be used\nto compensate for radial distortion and 3D parallax, although it uses a weaker motion model\nthan explicitly modeling the source of error and may, therefore, fail more often or introduce\nunwanted distortions.\nThe local alignment technique introduced by Shum and Szeliski (2000) starts with the\nglobal bundle adjustment (9.31) used to optimize the camera poses. Once these have been\nestimated, the desired location of a 3D point xi can be estimated as the average of the back-\nprojected 3D locations,\n¯xi ∼\nX\nj\ncij ˜xi(ˆxij; Rj, fj)\n,X\nj\ncij ,\n(9.35)\nwhich can be projected into each image j to obtain a target location ¯xij. The difference\nbetween the target locations ¯xij and the original features xij provide a set of local motion\nestimates\nuij = ¯xij −xij,\n(9.36)\nwhich can be interpolated to form a dense correction ﬁeld uj(xj). In their system, Shum and\nSzeliski (2000) use an inverse warping algorithm where the sparse −uij values are placed at\nthe new target locations ¯xij, interpolated using bilinear kernel functions (Nielson 1993) and\nthen added to the original pixel coordinates when computing the warped (corrected) image.\nIn order to get a reasonably dense set of features to interpolate, Shum and Szeliski (2000)\nplace a feature point at the center of each patch (the patch size controls the smoothness in\nthe local alignment stage), rather than relying of features extracted using an interest operator\n(Figure 9.10).\nAn alternative approach to motion-based de-ghosting was proposed by Kang, Uytten-\ndaele, Winder et al. (2003), who estimate dense optical ﬂow between each input image and a\ncentral reference image. The accuracy of the ﬂow vector is checked using a photo-consistency\nmeasure before a given warped pixel is considered valid and is used to compute a high dy-\nnamic range radiance estimate, which is the goal of their overall algorithm. The requirement\nfor a reference image makes their approach less applicable to general image mosaicing, al-\nthough an extension to this case could certainly be envisaged.\n9.2.3 Recognizing panoramas\nThe ﬁnal piece needed to perform fully automated image stitching is a technique to recognize\nwhich images actually go together, which Brown and Lowe (2007) call recognizing panora-\n9.2 Global alignment\n447\n(a)\n(b)\n(c)\nFigure 9.10\nDeghosting a mosaic with motion parallax (Shum and Szeliski 2000) c⃝2000\nIEEE: (a) composite with parallax; (b) after a single deghosting step (patch size 32); (c) after\nmultiple steps (sizes 32, 16 and 8).\nmas. If the user takes images in sequence so that each image overlaps its predecessor and\nalso speciﬁes the ﬁrst and last images to be stitched, bundle adjustment combined with the\nprocess of topology inference can be used to automatically assemble a panorama (Sawhney\nand Kumar 1999). However, users often jump around when taking panoramas, e.g., they\nmay start a new row on top of a previous one, jump back to take a repeat shot, or create\n360◦panoramas where end-to-end overlaps need to be discovered. Furthermore, the ability\nto discover multiple panoramas taken by a user over an extended period of time can be a big\nconvenience.\nTo recognize panoramas, Brown and Lowe (2007) ﬁrst ﬁnd all pairwise image overlaps\nusing a feature-based method and then ﬁnd connected components in the overlap graph to\n“recognize” individual panoramas (Figure 9.11). The feature-based matching stage ﬁrst ex-\ntracts scale invariant feature transform (SIFT) feature locations and feature descriptors (Lowe\n2004) from all the input images and places them in an indexing structure, as described in Sec-\ntion 4.1.3. For each image pair under consideration, the nearest matching neighbor is found\nfor each feature in the ﬁrst image, using the indexing structure to rapidly ﬁnd candidates and\nthen comparing feature descriptors to ﬁnd the best match. RANSAC is used to ﬁnd a set of in-\nlier matches; pairs of matches are used to hypothesize similarity motion models that are then\nused to count the number of inliers. (A more recent RANSAC algorithm tailored speciﬁcally\nfor rotational panoramas is described by Brown, Hartley, and Nist´er (2007).)\nIn practice, the most difﬁcult part of getting a fully automated stitching algorithm to\nwork is deciding which pairs of images actually correspond to the same parts of the scene.\nRepeated structures such as windows (Figure 9.12) can lead to false matches when using\na feature-based approach. One way to mitigate this problem is to perform a direct pixel-\nbased comparison between the registered images to determine if they actually are different\nviews of the same scene. Unfortunately, this heuristic may fail if there are moving objects\nin the scene (Figure 9.13). While there is no magic bullet for this problem, short of full\nscene understanding, further improvements can likely be made by applying domain-speciﬁc",
  "image_path": "page_468.jpg",
  "pages": [
    467,
    468,
    469
  ]
}