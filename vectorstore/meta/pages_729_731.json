{
  "doc_id": "pages_729_731",
  "text": "14.4 Category recognition\n707\n(a)\n(b)\nFigure 14.44\nSimultaneous recognition and segmentation using TextonBoost (Shotton,\nWinn, Rother et al. 2009) c⃝2009 Springer: (a) successful recognition results; (b) less suc-\ncessful results.\n708\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.45 Layout consistent random ﬁeld (Winn and Shotton 2006) c⃝2006 IEEE. The\nnumbers indicate the kind of neighborhood relations that can exist between pixels assigned\nto the same or different classes. Each pairwise relationship carries its own likelihood (energy\npenalty).\nlem using energy minimization or Bayesian inference techniques, i.e., conditional random\nﬁelds (Section 3.7.2, (3.118)) (Kumar and Hebert 2006; He, Zemel, and Carreira-Perpi˜n´an\n2004). The TextonBoost system of Shotton, Winn, Rother et al. (2009) uses unary (pixel-\nwise) potentials based on image-speciﬁc color distributions (Section 5.5) (Boykov and Jolly\n2001; Rother, Kolmogorov, and Blake 2004), location information (e.g., foreground objects\nare more likely to be in the middle of the image, sky is likely to be higher, and road is likely\nto be lower), and novel texture-layout classiﬁers trained using shared boosting. It also uses\ntraditional pairwise potentials that look at image color gradients (Veksler 2001; Boykov and\nJolly 2001; Rother, Kolmogorov, and Blake 2004). The texton-layout features ﬁrst ﬁlter the\nimage with a series of 17 oriented ﬁlter banks and then cluster the responses to classify each\npixel into 30 different texton classes (Malik, Belongie, Leung et al. 2001). The responses\nare then ﬁltered using offset rectangular regions trained with joint boosting (Viola and Jones\n2004) to produce the texton-layout features used as unary potentials.\nFigure 14.44a shows some examples of images successfully labeled and segmented using\nTextonBoost, while Figure 14.44b shows examples where it does not do as well. As you can\nsee, this kind of semantic labeling can be extremely challenging.\nThe TextonBoost conditional random ﬁeld framework has been extended to LayoutCRFs\nby Winn and Shotton (2006), who incorporate additional constraints to recognize multiple\nobject instances and deal with occlusions (Figure 14.45), and even more recently by Hoiem,\nRother, and Winn (2007) to incorporate full 3D models.\nConditional random ﬁelds continue to be widely used and extended for simultaneous\nrecognition and segmentation applications (Kumar and Hebert 2006; He, Zemel, and Ray\n2006; Levin and Weiss 2006; Verbeek and Triggs 2007; Yang, Meer, and Foran 2007; Rabi-\nnovich, Vedaldi, Galleguillos et al. 2007; Batra, Sukthankar, and Chen 2008; Larlus and Jurie\n14.4 Category recognition\n709\n(a)\n(b)\n(c)\n(d)\nFigure 14.46\nScene completion using millions of photographs (Hays and Efros 2007) c⃝\n2007 ACM: (a) original image; (b) after unwanted foreground removal; (c) plausible scene\nmatches, with the one the user selected highlighted in red; (d) output image after replacement\nand blending.\n2008; He and Zemel 2008; Kumar, Torr, and Zisserman 2010), producing some of the best\nresults on the difﬁcult PASCAL VOC segmentation challenge (Shotton, Johnson, and Cipolla\n2008; Kohli, Ladick´y, and Torr 2009). Approaches that ﬁrst segment the image into unique\nor multiple segmentations (Borenstein and Ullman 2008; He, Zemel, and Ray 2006; Russell,\nEfros, Sivic et al. 2006) (potentially combined with CRF models) also do quite well: Csurka\nand Perronnin (2008) have one of the top algorithms in the VOC segmentation challenge.\nHierarchical (multi-scale) and grammar (parsing) models are also sometimes used (Tu, Chen,\nYuille et al. 2005; Zhu, Chen, Lin et al. 2008).\n14.4.4 Application: Intelligent photo editing\nRecent advances in object recognition and scene understanding have greatly increased the\npower of intelligent (semi-automated) photo editing applications. One example is the Photo\nClip Art system of Lalonde, Hoiem, Efros et al. (2007), which recognizes and segments\nobjects of interest, such as pedestrians, in Internet photo collections and then allows users to\npaste them into their own photos. Another is the scene completion system of Hays and Efros\n(2007), which tackles the same inpainting problem we studied in Section 10.5. Given an\nimage in which we wish to erase and ﬁll in a large section (Figure 14.46a–b), where do you\nget the pixels to ﬁll in the gaps in the edited image? Traditional approaches either use smooth\ncontinuation (Bertalmio, Sapiro, Caselles et al. 2000) or borrowing pixels from other parts of\nthe image (Efros and Leung 1999; Criminisi, P´erez, and Toyama 2004; Efros and Freeman\n2001). With the advent of huge repositories of images on the Web (a topic we return to in\nSection 14.5.1), it often makes more sense to ﬁnd a different image to serve as the source of\nthe missing pixels.\nIn their system, Hays and Efros (2007) compute the gist of each image (Oliva and Tor-",
  "image_path": "page_730.jpg",
  "pages": [
    729,
    730,
    731
  ]
}