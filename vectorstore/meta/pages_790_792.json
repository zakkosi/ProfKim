{
  "doc_id": "pages_790_792",
  "text": "768\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nDynamic programming is not restricted to trees with pairwise potentials. Figure B.2b\nshows an example of a three-way potential Vijk(xi, xj, xk) inside a tree. To compute the\noptimum value of ˆEk(xk), the recursion formula in (B.28) now has to evaluate the mini-\nmum over all combinations of possible state values leading into a factor node (gray box).\nFor this reason, dynamic programming is normally exponential in complexity in the order\nof the clique size, i.e., a clique of size n with l labels at each node requires the evaluation\nof ln−1 possible states (Potetz and Lee 2008; Kohli, Kumar, and Torr 2009). However, for\ncertain kinds of potential functions Vi,k(xi, xk), including the Potts model (delta function),\nabsolute values (total variation), and quadratic (Gaussian MRF), Felzenszwalb and Hutten-\nlocher (2006) show how to reduce the complexity of the min-ﬁnding step (B.28) from O(l2)\nto O(l). In Appendix B.5.3, we also discuss how Potetz and Lee (2008) reduce the complexity\nfor special kinds of higher-order clique, i.e., linear summations followed by non-linearities.\nFigure B.2a also shows what happens if we add an extra factor between nodes i and j.\nIn this case, the graph is no longer a tree, i.e., it contains a cycle. It is no longer possible\nto use the recursion formula (B.28), since ˆEi(xi) now appears in two different terms inside\nthe summation, i.e., as a child of both nodes j and k, and the same setting for xi may not\nminimize both. In other words, when loops exist, there is no ordering of the variables that\nallows the recursion (elimination) in (B.28) to be well-founded.\nIt is, however, possible to convert small loops into higher-order factors and to solve these\nas shown in Figure B.2b. However, graphs with long loops or meshes result in extremely\nlarge clique sizes and hence an amount of computation potentially exponential in the size of\nthe graph.\nB.5.3 Belief propagation\nBelief propagation is an inference technique originally developed for trees (Pearl 1988) but\nmore recently extended to “loopy” (cyclic) graphs such as MRFs (Frey and MacKay 1997;\nFreeman, Pasztor, and Carmichael 2000; Yedidia, Freeman, and Weiss 2001; Weiss and Free-\nman 2001a,b; Yuille 2002; Sun, Zheng, and Shum 2003; Felzenszwalb and Huttenlocher\n2006). It is closely related to dynamic programming, in that both techniques pass messages\nforward and backward over a tree or graph. In fact, one of the two variants of belief prop-\nagation, the max-product rule, performs the exact same computation (inference) as dynamic\nprogramming, albeit using probabilities instead of energies.\nRecall that the energy we are minimizing in MAP estimation (B.26) is the negative log\nlikelihood (B.12, B.13, and B.22) of a factored Gibbs posterior distribution,\np(x) =\nY\n(i,j)∈N\nφi,j(xi, xj),\n(B.29)\nB.5 Markov random ﬁelds\n769\nwhere\nφi,j(xi, xj) = e−Vi,j(xi,xj)\n(B.30)\nare the pairwise interaction potentials. We can rewrite (B.27) as\n˜pk(x) =\nY\ni<k, j≤k\nφi,j(xi, xj) =\nY\ni∈Ck\n˜pi,k(x),\n(B.31)\nwhere\n˜pi,k(x) = φi,k(xi, xk)˜pi(x).\n(B.32)\nWe can therefore rewrite (B.28) as\nˆpk(xk) =\nmax\n{xi, i<k} ˜pk(x) =\nY\ni∈Ck\nˆpi,k(x),\n(B.33)\nwith\nˆpi,k(x) = max\nxi φi,k(xi, xk)ˆpi(x).\n(B.34)\nEquation (B.34) is the max update rule evaluated at all square box factors in Figure B.2a,\nwhile (B.33) is the product rule evaluated at the nodes. The probability distribution ˆpi,k(x)\nis often interpreted as a message passing information about child i to parent k and is hence\nwritten as mi,k(xk) (Yedidia, Freeman, and Weiss 2001) or µi→k(xk) (Bishop 2006).\nThe max-product rule can be used to compute the MAP estimate in a tree using the same\nkind of forward and backward sweep as in dynamic programming (which is sometimes called\nthe max-sum algorithm (Bishop 2006)). An alternative rule, known as the sum–product, sums\nover all possible values in (B.34) rather than taking the maximum, in essence computing\nthe expected distribution rather than the maximum likelihood distribution. This produces a\nset of probability estimates that can be used to compute the marginal distributions bi(xi) =\nP\nx\\xi p(x) (Pearl 1988; Yedidia, Freeman, and Weiss 2001; Bishop 2006).\nBelief propagation may not produce optimal estimates for cyclic graphs for the same\nreason that dynamic programming fails to work, i.e., because a node with multiple parents\nmay take on different optimal values for each of the parents, i.e., there is no unique elim-\nination ordering. Early algorithms for extending belief propagation to graphs with cycles,\ndubbed loopy belief propagation, performed the updates in parallel over the graph, i.e., us-\ning synchronous updates (Frey and MacKay 1997; Freeman, Pasztor, and Carmichael 2000;\nYedidia, Freeman, and Weiss 2001; Weiss and Freeman 2001a,b; Yuille 2002; Sun, Zheng,\nand Shum 2003; Felzenszwalb and Huttenlocher 2006).\nFor example, Felzenszwalb and Huttenlocher (2006) split an N4 graph into its red and\nblack (checkerboard) components and alternate between sending messages from the red nodes\nto the black and vice versa. They also use multi-grid (coarser level) updates to speed up the\nconvergence. As discussed previously, to reduce the complexity of the basic max-product\n770\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nupdate rule (B.28) from O(l2) to O(l), they develop specialized update algorithms for sev-\neral cost functions Vi,k(xi, xk), including the Potts model (delta function), absolute values\n(total variation), and quadratic (Gaussian MRF). A related algorithm, mean ﬁeld diffusion\n(Scharstein and Szeliski 1998), also uses synchronous updates between nodes to compute\nmarginal distributions. Yuille (2010) discusses the relationships between mean ﬁeld theory\nand loopy belief propagation.\nMore recent loopy belief propagation algorithms and their variants use sequential scans\nthrough the graph (Szeliski, Zabih, Scharstein et al. 2008). For example, Tappen and Free-\nman (2003) pass messages from left to right along each row and then reverse the direction\nonce they reach the end. This is similar to treating each row as an independent tree (chain),\nexcept that messages from nodes above and below the row are also incorporated. They then\nperform similar computations along columns. These sequential updates allow the information\nto propagate much more quickly across the image than synchronous updates.\nThe other belief propagation variant tested by Szeliski, Zabih, Scharstein et al. (2008),\nwhich they call BP-S or TRW-S, is based on Kolmogorov’s (2006) sequential extension of\nthe tree-reweighted message passing of Wainwright, Jaakkola, and Willsky (2005). TRW\nﬁrst selects a set of trees from the neighborhood graph and computes a set of probability\ndistributions over each tree. These are then used to reweight the messages being passed\nduring loopy belief propagation. The sequential version of TRW, called TRW-S, processed\nnodes in scan-line order, with a forward and backward pass. In the forward pass, each node\nsends messages to its right and bottom neighbors. In the backward pass, messages are sent\nto the left and upper neighbors. TRW-S also computes a lower bound on the energy, which\nis used by Szeliski, Zabih, Scharstein et al. (2008) to estimate how close to the best possible\nsolution all of the MRF inference algorithms being evaluated get.\nAs with dynamic programming, belief propagation techniques also become less efﬁcient\nas the order of each factor clique increases. Potetz and Lee (2008) shows how this complex-\nity can be reduced back to linear in the clique order for continuous-valued problems where\nthe factors involve linear summations followed by a non-linearity, which is typical of more\nsophisticated MRF models such as ﬁelds of experts (Roth and Black 2009) and steerable ran-\ndom ﬁelds (Roth and Black 2007b). Kohli, Kumar, and Torr (2009) and Alahari, Kohli, and\nTorr (2011) develop alternative ways for dealing with higher-order cliques in the context of\ngraph cut algorithms.\nB.5.4 Graph cuts\nThe computer vision community has adopted “graph cuts” as an informal name to describe\na large family of MRF inference algorithms based on solving one or more min-cut or max-\nﬂow problems (Boykov, Veksler, and Zabih 2001; Boykov and Kolmogorov 2010; Boykov,",
  "image_path": "page_791.jpg",
  "pages": [
    790,
    791,
    792
  ]
}