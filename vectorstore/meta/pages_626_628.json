{
  "doc_id": "pages_626_628",
  "text": "604\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.19\n3D morphable face model (Blanz and Vetter 1999) c⃝1999 ACM: (a) orig-\ninal 3D face model with the addition of shape and texture variations in speciﬁc directions:\ndeviation from the mean (caricature), gender, expression, weight, and nose shape; (b) a 3D\nmorphable model is ﬁt to a single image, after which its weight or expression can be manip-\nulated; (c) another example of a 3D reconstruction along with a different set of 3D manipula-\ntions such as lighting and pose change.\n12.6 Model-based reconstruction\n605\nAfter computing a subspace representation, different directions in this space can be as-\nsociated with different characteristics such as gender, facial expressions, or facial features\n(Figure 12.19a). As in the work of Rowland and Perrett (1995), faces can be turned into\ncaricatures by exaggerating their displacement from the mean image.\n3D morphable models can be ﬁtted to a single image using gradient descent on the error\nbetween the input image and the re-synthesized model image, after an initial manual place-\nment of the model in an approximately correct pose, scale, and location (Figures 12.19b–c).\nThe efﬁciency of this ﬁtting process can be increased using inverse compositional image\nalignment (8.64–8.65), as described by Romdhani and Vetter (2003).\nThe resulting texture-mapped 3D model can then be modiﬁed to produce a variety of vi-\nsual effects, including changing a person’s weight or expression, or three-dimensional effects\nsuch as re-lighting or 3D video-based animation (Section 13.5.1). Such models can also be\nused for video compression, e.g., by only transmitting a small number of facial expression\nand pose parameters to drive a synthetic avatar (Eisert, Wiegand, and Girod 2000; Gao, Chen,\nWang et al. 2003).\n3D facial animation is often matched to the performance of an actor, in what is known\nas performance-driven animation (Section 4.1.5) (Williams 1990). Traditional performance-\ndriven animation systems use marker-based motion capture (Ma, Jones, Chiang et al. 2008),\nwhile some newer systems use video footage to control the animation (Buck, Finkelstein,\nJacobs et al. 2000; Pighin, Szeliski, and Salesin 2002; Zhang, Snavely, Curless et al. 2004;\nVlasic, Brand, Pﬁster et al. 2005).\nAn example of the latter approach is the system developed for the ﬁlm Benjamin Button,\nin which Digital Domain used the CONTOUR system from Mova10 to capture actor Brad\nPitt’s facial motions and expressions (Roble and Zafar 2009). CONTOUR uses a combina-\ntion of phosphorescent paint and multiple high-resolution video cameras to capture real-time\n3D range scans of the actor. These 3D models were then translated into Facial Action Cod-\ning System (FACS) shape and expression parameters (Ekman and Friesen 1978) to drive a\ndifferent (older) synthetically animated computer-generated imagery (CGI) character.\n12.6.4 Whole body modeling and tracking\nThe topics of tracking humans, modeling their shape and appearance, and recognizing their\nactivities, are some of the most actively studied areas of computer vision. Annual confer-\nences11 and special journal issues (Hilton, Fua, and Ronfard 2006) are devoted to this sub-\nject, and two recent surveys (Forsyth, Arikan, Ikemoto et al. 2006; Moeslund, Hilton, and\n10 http://www.mova.com.\n11 International Conference on Automatic Face and Gesture Recognition (FG), IEEE Workshop on Analysis and\nModeling of Faces and Gestures, and International Workshop on Tracking Humans for the Evaluation of their Motion\nin Image Sequences (THEMIS).\n606\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKr¨uger 2006) each list over 400 papers devoted to these topics.12 The HumanEva database\nof articulated human motions13 contains multi-view video sequences of human actions along\nwith corresponding motion capture data, evaluation code, and a reference 3D tracker based on\nparticle ﬁltering. The companion paper by Sigal, Balan, and Black (2010) not only describes\nthe database and evaluation but also has a nice survey of important work in this ﬁeld.\nGiven the breadth of this area, it is difﬁcult to categorize all of this research, especially\nsince different techniques usually build on each other. Moeslund, Hilton, and Kr¨uger (2006)\ndivide their survey into initialization, tracking (which includes background modeling and\nsegmentation), pose estimation, and action (activity) recognition. Forsyth, Arikan, Ikemoto et\nal. (2006) divide their survey into sections on tracking (background subtraction, deformable\ntemplates, ﬂow, and probabilistic models), recovering 3D pose from 2D observations, and\ndata association and body parts. They also include a section on motion synthesis, which is\nmore widely studied in computer graphics (Arikan and Forsyth 2002; Kovar, Gleicher, and\nPighin 2002; Lee, Chai, Reitsma et al. 2002; Li, Wang, and Shum 2002; Pullen and Bregler\n2002), see Section 13.5.2. Another potential taxonomy for work in this ﬁeld would be along\nthe lines of whether 2D or 3D (or multi-view) images are used as input and whether 2D or\n3D kinematic models are used.\nIn this section, we brieﬂy review some of the more seminal and widely cited papers in the\nareas of background subtraction, initialization and detection, tracking with ﬂow, 3D kinematic\nmodels, probabilistic models, adaptive shape modeling, and activity recognition. We refer the\nreader to the previously mentioned surveys for other topics and more details.\nBackground subtraction.\nOne of the ﬁrst steps in many (but certainly not all) human track-\ning systems is to model the background in order to extract the moving foreground objects\n(silhouettes) corresponding to people. Toyama, Krumm, Brumitt et al. (1999) review several\ndifference matting and background maintenance (modeling) techniques and provide a good\nintroduction to this topic. Stauffer and Grimson (1999) describe some techniques based on\nmixture models, while Sidenbladh and Black (2003) develop a more comprehensive treat-\nment, which models not only the background image statistics but also the appearance of the\nforeground objects, e.g., their edge and motion (frame difference) statistics.\nOnce silhouettes have been extracted from one or more cameras, they can then be mod-\neled using deformable templates or other contour models (Baumberg and Hogg 1996; Wren,\nAzarbayejani, Darrell et al. 1997). Tracking such silhouettes over time supports the analysis\nof multiple people moving around a scene, including building shape and appearance models\n12 Older surveys include those by Gavrila (1999) and Moeslund and Granum (2001). Some surveys on gesture\nrecognition, which we do not cover in this book, include those by Pavlovi´c, Sharma, and Huang (1997) and Yang,\nAhuja, and Tabb (2002).\n13 http://vision.cs.brown.edu/humaneva/.",
  "image_path": "page_627.jpg",
  "pages": [
    626,
    627,
    628
  ]
}