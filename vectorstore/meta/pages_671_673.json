{
  "doc_id": "pages_671_673",
  "text": "13.6 Additional reading\n649\nEarly work on planar impostors and layers was carried out by Shade, Lischinski, Salesin\net al. (1996), Lengyel and Snyder (1997), and Torborg and Kajiya (1996), while newer work\nbased on sprites with depth is described by Shade, Gortler, He et al. (1998).\nThe two foundational papers in image-based rendering are Light ﬁeld rendering by Levoy\nand Hanrahan (1996) and The Lumigraph by Gortler, Grzeszczuk, Szeliski et al. (1996).\nBuehler, Bosse, McMillan et al. (2001) generalize the Lumigraph approach to irregularly\nspaced collections of images, while Levoy (2006) provides a survey and more gentle intro-\nduction to the topic of light ﬁeld and image-based rendering.\nSurface light ﬁelds (Wood, Azuma, Aldinger et al. 2000) provide an alternative param-\neterization for light ﬁelds with accurately known surface geometry and support both better\ncompression and the possibility of editing surface properties. Concentric mosaics (Shum and\nHe 1999; Shum, Wang, Chai et al. 2002) and panoramas with depth (Peleg, Ben-Ezra, and\nPritch 2001; Li, Shum, Tang et al. 2004; Zheng, Kang, Cohen et al. 2007), provide useful\nparameterizations for light ﬁelds captured with panning cameras. Multi-perspective images\n(Rademacher and Bishop 1998) and manifold projections (Peleg and Herman 1997), although\nnot true light ﬁelds, are also closely related to these ideas.\nAmong the possible extensions of light ﬁelds to higher-dimensional structures, environ-\nment mattes (Zongker, Werner, Curless et al. 1999; Chuang, Zongker, Hindorff et al. 2000)\nare the most useful, especially for placing captured objects into new scenes.\nVideo-based rendering, i.e., the re-use of video to create new animations or virtual ex-\nperiences, started with the seminal work of Szummer and Picard (1996), Bregler, Covell,\nand Slaney (1997), and Sch¨odl, Szeliski, Salesin et al. (2000). Important follow-on work\nto these basic re-targeting approaches was carried out by Sch¨odl and Essa (2002), Kwatra,\nSch¨odl, Essa et al. (2003), Doretto, Chiuso, Wu et al. (2003), Wang and Zhu (2003), Zhong\nand Sclaroff (2003), Yuan, Wen, Liu et al. (2004), Doretto and Soatto (2006), Zhao and\nPietik¨ainen (2007), and Chan and Vasconcelos (2009).\nSystems that allow users to change their 3D viewpoint based on multiple synchronized\nvideo streams include those by Moezzi, Katkere, Kuramura et al. (1996), Kanade, Ran-\nder, and Narayanan (1997), Matusik, Buehler, Raskar et al. (2000), Matusik, Buehler, and\nMcMillan (2001), Carranza, Theobalt, Magnor et al. (2003), Zitnick, Kang, Uyttendaele et\nal. (2004), Magnor (2005), and Vedula, Baker, and Kanade (2005). 3D (multiview) video\ncoding and compression is also an active area of research (Smolic and Kauff 2005; Gotchev\nand Rosenhahn 2009), with 3D Blu-Ray discs, encoded using the multiview video coding\n(MVC) extension to H.264/MPEG-4 AVC, expected by the end of 2010.\n650\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n13.7 Exercises\nEx 13.1: Depth image rendering\nDevelop a “view extrapolation” algorithm to re-render a\npreviously computed stereo depth map coupled with its corresponding reference color image.\n1. Use a 3D graphics mesh rendering system such as OpenGL or Direct3D, with two\ntriangles per pixel quad and perspective (projective) texture mapping (Debevec, Yu,\nand Borshukov 1998).\n2. Alternatively, use the one- or two-pass forward warper you constructed in Exercise 3.24,\nextended using (2.68–2.70) to convert from disparities or depths into displacements.\n3. (Optional) Kinks in straight lines introduced during view interpolation or extrapola-\ntion are visually noticeable, which is one reason why image morphing systems let you\nspecify line correspondences (Beier and Neely 1992). Modify your depth estimation\nalgorithm to match and estimate the geometry of straight lines and incorporate it into\nyour image-based rendering algorithm.\nEx 13.2: View interpolation\nExtend the system you created in the previous exercise to ren-\nder two reference views and then blend the images using a combination of z-buffering, hole\nﬁling, and blending (morphing) to create the ﬁnal image (Section 13.1).\n1. (Optional) If the two source images have very different exposures, the hole-ﬁlled re-\ngions and the blended regions will have different exposures. Can you extend your\nalgorithm to mitigate this?\n2. (Optional) Extend your algorithm to perform three-way (trilinear) interpolation be-\ntween neighboring views. You can triangulate the reference camera poses and use\nbarycentric coordinates for the virtual camera in order to determine the blending weights.\nEx 13.3: View morphing\nModify your view interpolation algorithm to perform morphs be-\ntween views of a non-rigid object, such as a person changing expressions.\n1. Instead of using a pure stereo algorithm, use a general ﬂow algorithm to compute dis-\nplacements, but separate them into a rigid displacement due to camera motion and a\nnon-rigid deformation.\n2. At render time, use the rigid geometry to determine the new pixel location but then add\na fraction of the non-rigid displacement as well.\n3. Alternatively, compute a stereo depth map but let the user specify additional correspon-\ndences or use a feature-based matching algorithm to provide them automatically.\n13.7 Exercises\n651\n4. (Optional) Take a single image, such as the Mona Lisa or a friend’s picture, and create\nan animated 3D view morph (Seitz and Dyer 1996).\n(a) Find the vertical axis of symmetry in the image and reﬂect your reference image\nto provide a virtual pair (assuming the person’s hairstyle is somewhat symmetric).\n(b) Use structure from motion to determine the relative camera pose of the pair.\n(c) Use dense stereo matching to estimate the 3D shape.\n(d) Use view morphing to create a 3D animation.\nEx 13.4: View dependent texture mapping\nUse a 3D model you created along with the\noriginal images to implement a view-dependent texture mapping system.\n1. Use one of the 3D reconstruction techniques you developed in Exercises 7.3, 11.9,\n11.10, or 12.8 to build a triangulated 3D image-based model from multiple photographs.\n2. Extract textures for each model face from your photographs, either by performing the\nappropriate resampling or by ﬁguring out how to use the texture mapping software to\ndirectly access the source images.\n3. At run time, for each new camera view, select the best source image for each visible\nmodel face.\n4. Extend this to blend between the top two or three textures. This is trickier, since it\ninvolves the use of texture blending or pixel shading (Debevec, Taylor, and Malik 1996;\nDebevec, Yu, and Borshukov 1998; Pighin, Hecker, Lischinski et al. 1998).\nEx 13.5: Layered depth images\nExtend your view interpolation algorithm (Exercise 13.2)\nto store more than one depth or color value per pixel (Shade, Gortler, He et al. 1998), i.e., a\nlayered depth image (LDI). Modify your rendering algorithm accordingly. For your data, you\ncan use synthetic ray tracing, a layered reconstructed model, or a volumetric reconstruction.\nEx 13.6: Rendering from sprites or layers\nExtend your view interpolation algorithm to\nhandle multiple planes or sprites (Section 13.2.1) (Shade, Gortler, He et al. 1998).\n1. Extract your layers using the technique you developed in Exercise 8.9.\n2. Alternatively, use an interactive painting and 3D placement system to extract your lay-\ners (Kang 1998; Oh, Chen, Dorsey et al. 2001; Shum, Sun, Yamazaki et al. 2004).\n3. Determine a back-to-front order based on expected visibility or add a z-buffer to your\nrendering algorithm to handle occlusions.",
  "image_path": "page_672.jpg",
  "pages": [
    671,
    672,
    673
  ]
}