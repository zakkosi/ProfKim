{
  "doc_id": "pages_589_591",
  "text": "11.6 Multi-view stereo\n567\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 11.20 The multi-view stereo data sets captured by Seitz, Curless, Diebel et al. (2006)\nc⃝2006 Springer. Only (a) and (b) are currently used for evaluation.\nsilhouettes, as discussed in Section 11.6.2. However, if the algorithm performs a global op-\ntimization (Kolev, Klodt, Brox et al. 2009; Kolev and Cremers 2009), this dependence on\ninitialization is not an issue.\nEmpirical evaluation.\nIn order to evaluate the large number of design alternatives in multi-\nview stereo, Seitz, Curless, Diebel et al. (2006) collected a dataset of calibrated images using\na spherical gantry. A representative image from each of the six datasets is shown in Fig-\nure 11.20, although only the ﬁrst two datasets have as yet been fully processed and used for\nevaluation. Figure 11.21 shows the results of running seven different algorithms on the tem-\nple dataset. As you can see, most of the techniques do an impressive job of capturing the ﬁne\ndetails in the columns, although it is also clear that the techniques employ differing amounts\nof smoothing to achieve these results.\nSince the publication of the survey by Seitz, Curless, Diebel et al. (2006), the ﬁeld of\nmulti-view stereo has continued to advance at a rapid pace (Strecha, Fransens, and Van\nGool 2006; Hernandez, Vogiatzis, and Cipolla 2007; Habbecke and Kobbelt 2007; Furukawa\nand Ponce 2007; Vogiatzis, Hernandez, Torr et al. 2007; Goesele, Snavely, Curless et al.\n2007; Sinha, Mordohai, and Pollefeys 2007; Gargallo, Prados, and Sturm 2007; Merrell, Ak-\nbarzadeh, Wang et al. 2007; Zach, Pock, and Bischof 2007b; Furukawa and Ponce 2008;\nHornung, Zeng, and Kobbelt 2008; Bradley, Boubekeur, and Heidrich 2008; Zach 2008;\nCampbell, Vogiatzis, Hern´andez et al. 2008; Kolev, Klodt, Brox et al. 2009; Hiep, Keriven,\nPons et al. 2009; Furukawa, Curless, Seitz et al. 2010). The multi-view stereo evaluation site,\nhttp://vision.middlebury.edu/mview/, provides quantitative results for these algorithms along\nwith pointers to where to ﬁnd these papers.\n11.6.2 Shape from silhouettes\nIn many situations, performing a foreground–background segmentation of the object of in-\nterest is a good way to initialize or ﬁt a 3D model (Grauman, Shakhnarovich, and Darrell\n568\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 11.21 Reconstruction results (details) for seven algorithms (Hernandez and Schmitt\n2004; Furukawa and Ponce 2009; Pons, Keriven, and Faugeras 2005; Goesele, Curless, and\nSeitz 2006; Vogiatzis, Torr, and Cipolla 2005; Tran and Davis 2002; Kolmogorov and Zabih\n2002) evaluated by Seitz, Curless, Diebel et al. (2006) on the 47-image Temple Ring dataset.\nThe numbers underneath each detail image are the accuracy of each of these techniques mea-\nsured in millimeters.\n2003; Vlasic, Baran, Matusik et al. 2008) or to impose a convex set of constraints on multi-\nview stereo (Kolev and Cremers 2008). Over the years, a number of techniques have been\ndeveloped to reconstruct a 3D volumetric model from the intersection of the binary silhou-\nettes projected into 3D. The resulting model is called a visual hull (or sometimes a line hull),\nanalogous with the convex hull of a set of points, since the volume is maximal with respect\nto the visual silhouettes and surface elements are tangent to the viewing rays (lines) along\nthe silhouette boundaries (Laurentini 1994). It is also possible to carve away a more accu-\nrate reconstruction using multi-view stereo (Sinha and Pollefeys 2005) or by analyzing cast\nshadows (Savarese, Andreetto, Rushmeier et al. 2007).\nSome techniques ﬁrst approximate each silhouette with a polygonal representation and\nthen intersect the resulting faceted conical regions in three-space to produce polyhedral mod-\nels (Baumgart 1974; Martin and Aggarwal 1983; Matusik, Buehler, and McMillan 2001),\nwhich can later be reﬁned using triangular splines (Sullivan and Ponce 1998). Other ap-\nproaches use voxel-based representations, usually encoded as octrees (Samet 1989), because\nof the resulting space–time efﬁciency. Figures 11.22a–b show an example of a 3D octree\nmodel and its associated colored tree, where black nodes are interior to the model, white\n11.6 Multi-view stereo\n569\n(a)\n(b)\n(c)\n(d)\nFigure 11.22\nVolumetric octree reconstruction from binary silhouettes (Szeliski 1993) c⃝\n1993 Elsevier: (a) octree representation and its corresponding (b) tree structure; (c) input\nimage of an object on a turntable; (d) computed 3D volumetric octree model.\nnodes are exterior, and gray nodes are of mixed occupancy. Examples of octree-based re-\nconstruction approaches include those by Potmesil (1987), Noborio, Fukada, and Arimoto\n(1988), Srivasan, Liang, and Hackwood (1990), and Szeliski (1993).\nThe approach of Szeliski (1993) ﬁrst converts each binary silhouette into a one-sided\nvariant of a distance map, where each pixel in the map indicates the largest square that is\ncompletely inside (or outside) the silhouette. This makes it fast to project an octree cell\ninto the silhouette to conﬁrm whether it is completely inside or outside the object, so that\nit can be colored black, white, or left as gray (mixed) for further reﬁnement on a smaller\ngrid. The octree construction algorithm proceeds in a coarse-to-ﬁne manner, ﬁrst building an\noctree at a relatively coarse resolution, and then reﬁning it by revisiting and subdividing all\nthe input images for the gray (mixed) cells whose occupancy has not yet been determined.\nFigure 11.22d shows the resulting octree model computed from a coffee cup rotating on a\nturntable.\nMore recent work on visual hull computation borrows ideas from image-based rendering,\nand is hence called an image-based visual hull (Matusik, Buehler, Raskar et al. 2000). Instead\nof precomputing a global 3D model, an image-based visual hull is recomputed for each new",
  "image_path": "page_590.jpg",
  "pages": [
    589,
    590,
    591
  ]
}