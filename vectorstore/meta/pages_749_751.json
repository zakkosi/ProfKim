{
  "doc_id": "pages_749_751",
  "text": "14.8 Exercises\n727\n4. Then, match the individual features against your database and note the locations of\nthese features.\n5. Train and test a classiﬁer that uses the individual feature matching IDs as well as (op-\ntionally) the feature locations to perform face recognition.\nEx 14.6: Recognition-based color balancing\nBuild a system that recognizes the most im-\nportant color areas in common photographs (sky, grass, skin) and color balances the image\naccordingly. Some references and ideas for skin detection are given in Exercise 2.8 and\nby Forsyth and Fleck (1999), Jones and Rehg (2001), Vezhnevets, Sazonov, and Andreeva\n(2003), and Kakumanu, Makrogiannis, and Bourbakis (2007). These may give you ideas\nfor how to detect other regions or you can try more sophisticated MRF-based approaches\n(Shotton, Winn, Rother et al. 2009).\nEx 14.7: Pedestrian detection\nBuild and test one of the pedestrian detectors presented in\nSection 14.1.2.\nEx 14.8: Simple instance recognition\nUse the feature detection, matching, and alignment\nalgorithms you developed in Exercises 4.1–4.4 and 9.2 to ﬁnd matching images given a query\nimage or region (Figure 14.26).\nEvaluate several feature detectors, descriptors, and robust geometric veriﬁcation strate-\ngies, either on your own or by comparing your results with those of classmates.\nEx 14.9: Large databases and location recognition\nExtend the previous exercise to larger\ndatabases using quantized visual words and information retrieval techniques, as described in\nAlgorithm 14.2.\nTest your algorithm on a large database, such as the one used by Nist´er and Stew´enius\n(2006) or Philbin, Chum, Sivic et al. (2008), which are listed in Table 14.1. Alternatively,\nuse keyword search on the Web or in a photo sharing site (e.g., for a city) to create your own\ndatabase.\nEx 14.10: Bag of words\nAdapt the feature extraction and matching pipeline developed in\nExercise 14.8 to category (class) recognition, using some of the techniques described in Sec-\ntion 14.4.1.\n1. Download the training and test images from one or more of the databases listed in\nTables 14.1 and 14.2, e.g., Caltech 101, Caltech 256, or PASCAL VOC.\n2. Extract features from each of the training images, quantize them, and compute the tf-idf\nvectors (bag of words histograms).\n728\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. As an option, consider not quantizing the features and using pyramid matching (14.40–\n14.41) (Grauman and Darrell 2007b) or using a spatial pyramid for greater selectivity\n(Lazebnik, Schmid, and Ponce 2006).\n4. Choose a classiﬁcation algorithm (e.g., nearest neighbor classiﬁcation or support vector\nmachine) and “train” your recognizer, i.e., build up the appropriate data structures (e.g.,\nk-d trees) or set the appropriate classiﬁer parameters.\n5. Test your algorithm on the test data set using the same pipeline you developed in steps\n2–4 and compare your results to the best reported results.\n6. Explain why your results differ from the previously reported ones and give some ideas\nfor how you could improve your system.\nYou can ﬁnd a good synopsis of the best-performing classiﬁcation algorithms and their ap-\nproaches in the report of the PASCAL Visual Object Classes Challenge found on their Web\nsite (http://pascallin.ecs.soton.ac.uk/challenges/VOC/).\nEx 14.11: Object detection and localization\nExtend the classiﬁcation algorithm developed\nin the previous exercise to localize the objects in an image by reporting a bounding box around\neach detected object. The easiest way to do this is to use a sliding window approach. Some\npointers to recent techniques in this area can be found in the workshop associated with the\nPASCAL VOC 2008 Challenge.\nEx 14.12: Part-based recognition\nChoose one or more of the techniques described in Sec-\ntion 14.4.2 and implement a part-based recognition system. Since these techniques are fairly\ninvolved, you will need to read several of the research papers in this area, select which gen-\neral approach you want to follow, and then implement your algorithm. A good starting point\ncould be the paper by Felzenszwalb, McAllester, and Ramanan (2008), since it performed\nwell in the PASCAL VOC 2008 detection challenge.\nEx 14.13: Recognition and segmentation\nChoose one or more of the techniques described\nin Section 14.4.3 and implement a simultaneous recognition and segmentation system. Since\nthese techniques are fairly involved, you will need to read several of the research papers in this\narea, select which general approach you want to follow, and then implement your algorithm.\nTest your algorithm on one or more of the segmentation databases in Table 14.2.\nEx 14.14: Context\nImplement one or more of the context and scene understanding sys-\ntems described in Section 14.5 and report on your experience. Does context or whole scene\nunderstanding perform better at naming objects than stand-alone systems?\n14.8 Exercises\n729\nEx 14.15: Tiny images\nDownload the tiny images database from http://people.csail.mit.\nedu/torralba/tinyimages/ and build a classiﬁer based on comparing your test images directly\nagainst all of the labeled training images. Does this seem like a promising approach?",
  "image_path": "page_750.jpg",
  "pages": [
    749,
    750,
    751
  ]
}