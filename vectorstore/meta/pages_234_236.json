{
  "doc_id": "pages_234_236",
  "text": "212\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nUsing a Taylor Series expansion of the image function I0(xi+∆u) ≈I0(xi)+∇I0(xi)·\n∆u (Lucas and Kanade 1981; Shi and Tomasi 1994), we can approximate the auto-correlation\nsurface as\nEAC(∆u)\n=\nX\ni\nw(xi)[I0(xi + ∆u) −I0(xi)]2\n(4.3)\n≈\nX\ni\nw(xi)[I0(xi) + ∇I0(xi) · ∆u −I0(xi)]2\n(4.4)\n=\nX\ni\nw(xi)[∇I0(xi) · ∆u]2\n(4.5)\n=\n∆uT A∆u,\n(4.6)\nwhere\n∇I0(xi) = (∂I0\n∂x , ∂I0\n∂y )(xi)\n(4.7)\nis the image gradient at xi. This gradient can be computed using a variety of techniques\n(Schmid, Mohr, and Bauckhage 2000). The classic “Harris” detector (Harris and Stephens\n1988) uses a [-2 -1 0 1 2] ﬁlter, but more modern variants (Schmid, Mohr, and Bauckhage\n2000; Triggs 2004) convolve the image with horizontal and vertical derivatives of a Gaussian\n(typically with σ = 1).\nThe auto-correlation matrix A can be written as\nA = w ∗\n\"\nI2\nx\nIxIy\nIxIy\nI2\ny\n#\n,\n(4.8)\nwhere we have replaced the weighted summations with discrete convolutions with the weight-\ning kernel w. This matrix can be interpreted as a tensor (multiband) image, where the outer\nproducts of the gradients ∇I are convolved with a weighting function w to provide a per-pixel\nestimate of the local (quadratic) shape of the auto-correlation function.\nAs ﬁrst shown by Anandan (1984; 1989) and further discussed in Section 8.1.3 and (8.44),\nthe inverse of the matrix A provides a lower bound on the uncertainty in the location of a\nmatching patch. It is therefore a useful indicator of which patches can be reliably matched.\nThe easiest way to visualize and reason about this uncertainty is to perform an eigenvalue\nanalysis of the auto-correlation matrix A, which produces two eigenvalues (λ0, λ1) and two\neigenvector directions (Figure 4.6). Since the larger uncertainty depends on the smaller eigen-\nvalue, i.e., λ−1/2\n0\n, it makes sense to ﬁnd maxima in the smaller eigenvalue to locate good\nfeatures to track (Shi and Tomasi 1994).\nF¨orstner–Harris.\nWhile Anandan and Lucas and Kanade (1981) were the ﬁrst to analyze\nthe uncertainty structure of the auto-correlation matrix, they did so in the context of asso-\nciating certainties with optic ﬂow measurements. F¨orstner (1986) and Harris and Stephens\n4.1 Points and patches\n213\nFigure 4.6\nUncertainty ellipse corresponding to an eigenvalue analysis of the auto-\ncorrelation matrix A.\n(1988) were the ﬁrst to propose using local maxima in rotationally invariant scalar measures\nderived from the auto-correlation matrix to locate keypoints for the purpose of sparse feature\nmatching. (Schmid, Mohr, and Bauckhage (2000); Triggs (2004) give more detailed histori-\ncal reviews of feature detection algorithms.) Both of these techniques also proposed using a\nGaussian weighting window instead of the previously used square patches, which makes the\ndetector response insensitive to in-plane image rotations.\nThe minimum eigenvalue λ0 (Shi and Tomasi 1994) is not the only quantity that can be\nused to ﬁnd keypoints. A simpler quantity, proposed by Harris and Stephens (1988), is\ndet(A) −α trace(A)2 = λ0λ1 −α(λ0 + λ1)2\n(4.9)\nwith α = 0.06. Unlike eigenvalue analysis, this quantity does not require the use of square\nroots and yet is still rotationally invariant and also downweights edge-like features where\nλ1 ≫λ0. Triggs (2004) suggests using the quantity\nλ0 −αλ1\n(4.10)\n(say, with α = 0.05), which also reduces the response at 1D edges, where aliasing errors\nsometimes inﬂate the smaller eigenvalue. He also shows how the basic 2 × 2 Hessian can be\nextended to parametric motions to detect points that are also accurately localizable in scale\nand rotation. Brown, Szeliski, and Winder (2005), on the other hand, use the harmonic mean,\ndet A\ntr A\n=\nλ0λ1\nλ0 + λ1\n,\n(4.11)\nwhich is a smoother function in the region where λ0 ≈λ1. Figure 4.7 shows isocontours\nof the various interest point operators, from which we can see how the two eigenvalues are\nblended to determine the ﬁnal interest value.\n214\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.7\nIsocontours of popular keypoint detection functions (Brown, Szeliski, and\nWinder 2004).\nEach detector looks for points where the eigenvalues λ0, λ1 of A =\nw ∗∇I∇IT are both large.\n1. Compute the horizontal and vertical derivatives of the image Ix and Iy by con-\nvolving the original image with derivatives of Gaussians (Section 3.2.3).\n2. Compute the three images corresponding to the outer products of these gradients.\n(The matrix A is symmetric, so only three entries are needed.)\n3. Convolve each of these images with a larger Gaussian.\n4. Compute a scalar interest measure using one of the formulas discussed above.\n5. Find local maxima above a certain threshold and report them as detected feature\npoint locations.\nAlgorithm 4.1 Outline of a basic feature detection algorithm.",
  "image_path": "page_235.jpg",
  "pages": [
    234,
    235,
    236
  ]
}