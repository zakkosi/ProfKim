{
  "doc_id": "pages_794_796",
  "text": "772\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ndistances (Bai and Sapiro 2009; Criminisi, Sharp, and Blake 2008) and then thresholding the\nresults. More details on these techniques are provided in Section 5.5 and a nice review can\nbe found in the work of Singaraju, Grady, Sinop et al. (2010). A different connection to\ncontinuous segmentation techniques, this time to the literature on level sets (Section 5.1.4),\nis made by Boykov, Kolmogorov, Cremers et al. (2006), who develop an approach to solving\nsurface propagation PDEs based on combinatorial graph cut algorithms—Boykov and Funka-\nLea (2006) discuss this and related techniques.\nMulti-valued MRF inference problems usually require solving a series of related binary\nMRF problems (Boykov, Veksler, and Zabih 2001), although for special cases, such as some\nconvex functions, a single graph cut may sufﬁce (Ishikawa 2003; Schlesinger and Flach\n2006). The seminal work in this area is that of Boykov, Veksler, and Zabih (2001), who intro-\nduced two algorithms, called the swap move and the expansion move, which are sketched in\nFigure B.4. The α–β-swap move selects two labels (usually by cycling through all possible\npairings) and then formulates a binary MRF problem that allows any pixels currently labeled\nas either α or β to optionally switch their values to the other label. The α-expansion move\nallows any pixel in the MRF to take on the α label or to keep its current identity. It is easy\nto see by inspection that both of these moves result in binary MRFs with well-deﬁned energy\nfunctions.\nBecause these algorithms use a binary MRF optimization inside their inner loop, they\nare subject to the constraints on the energy functions that occur in the binary labeling case\n(Kolmogorov and Zabih 2004). However, more recent algorithms such as those developed by\nKomodakis, Tziritas, and Paragios (2008) and Rother, Kolmogorov, Lempitsky et al. (2007)\ncan be used to provide approximate solutions for more general energy functions. Efﬁcient\nalgorithms for re-using previous solutions (ﬂow- and cut-recycling) have been developed for\non-line applications such as dynamic MRFs (Kohli and Torr 2005; Juan and Boykov 2006;\nAlahari, Kohli, and Torr 2011) and coarse-to-ﬁne banded graph cuts (Agarwala, Zheng, Pal et\nal. 2005; Lombaert, Sun, Grady et al. 2005; Juan and Boykov 2006). It is also now possible to\nminimize the number of labels used as part of the alpha-expansion process (Delong, Osokin,\nIsack et al. 2010).\nIn experimental comparisons, α-expansions usually converge faster to a good solution\nthan α–β-swaps (Szeliski, Zabih, Scharstein et al. 2008), especially for problems that in-\nvolve large regions of identical labels, such as the labeling of source imagery in image stitch-\ning (Figure 3.60). For truncated convex energy functions deﬁned over ordinal values, more\naccurate algorithms that consider complete ranges of labels inside each min-cut and often\nproduce lower energies have been developed (Veksler 2007; Kumar and Torr 2008; Kumar,\nVeksler, and Torr 2010). The whole ﬁeld of efﬁcient MRF inference algorithms is rapidly\ndeveloping, as witnessed by a recent special journal issue (Kohli and Torr 2008; Komodakis,\nTziritas, and Paragios 2008; Olsson, Eriksson, and Kahl 2008; Potetz and Lee 2008), articles\nB.5 Markov random ﬁelds\n773\n(a) initial labeling\n(b) standard move\n(c) α-β-swap\n(d) α-expansion\nFigure B.4 Multi-level graph optimization from (Boykov, Veksler, and Zabih 2001) c⃝2001\nIEEE: (a) initial problem conﬁguration; (b) the standard move changes only one pixel; (c)\nthe α–β-swap optimally exchanges all α- and β-labeled pixels; (d) the α-expansion move\noptimally selects among current pixel values and the α label.\n(Alahari, Kohli, and Torr 2011), and a forthcoming book (Blake, Kohli, and Rother 2010).\nB.5.5 Linear programming\n8 Many successful algorithms for MRF optimization are based on the linear programming\n(LP) relaxation of the energy function (Weiss, Yanover, and Meltzer 2010). For some prac-\ntical MRF problems, LP-based techniques can produce globally minimal solutions (Meltzer,\nYanover, and Weiss 2005), even though MRF inference is in general NP-hard. In order to\ndescribe this relaxation, let us ﬁrst rewrite the energy function (B.26) as\nE(x)\n=\nX\n(i,j)∈N\nVi,j(xi, xj) +\nX\ni\nVi(xi)\n(B.35)\n=\nX\ni,j,α,β\nVi,j(α, β)xi,j;α,β +\nX\ni,α\nVi(α)xi;α\n(B.36)\nsubject to\nxi;α\n=\nX\nβ\nxi,j;α,β\n∀(i, j) ∈N, α,\n(B.37)\nxj;β\n=\nX\nα\nxi,j;α,β\n∀(i, j) ∈N, β,\nand\n(B.38)\nxi,α, xi,j;α,β\n∈\n{0, 1}.\n(B.39)\nHere, α and β range over label values and xi;α = δ(xi −α) and xij;αβ = δ(xi −α)δ(xj −β)\nare indicator variables of assignments xi = α and (xi, xj) = (α, β), respectively. The LP\nrelaxation is obtained by replacing the discreteness constraints (B.39) with linear constraints\nxij;αβ ∈[0, 1]. It is easy to show that the optimal value of (B.36) is a lower bound on (B.26).\n8 This section was contributed by Vladimir Kolmogorov. Thanks!\n774\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis relaxation has been extensively studied in the literature, starting with the work of\nSchlesinger (1976). An important question is how to solve this LP efﬁciently. Unfortunately,\ngeneral-purpose LP solvers cannot handle large problems in vision (Yanover, Meltzer, and\nWeiss 2006). A large number of customized iterative techniques have been proposed. Most\nof these solve the dual problem, i.e., they formulate a lower bound on (B.36) and then try to\nmaximize this bound. The bound is often formulated using a convex combination of trees, as\nproposed in (Wainwright, Jaakkola, and Willsky 2005).\nThe LP lower bound can be maximized via a number of techniques, such as max-sum dif-\nfusion (Werner 2007), tree-reweighted message passing (TRW) (Wainwright, Jaakkola, and\nWillsky 2005; Kolmogorov 2006), subgradient methods (Schlesinger and Giginyak 2007a,b;\nKomodakis, Paragios, and Tziritas 2007), and Bregman projections (Ravikumar, Agarwal,\nand Wainwright 2008). Note that the max-sum diffusion and TRW algorithms are not guar-\nanteed to converge to a global maximum of LP—they may get stuck at a suboptimal point\n(Kolmogorov 2006; Werner 2007). However, in practice, this does not appear to be a problem\n(Kolmogorov 2006).\nFor some vision applications, algorithms based on relaxation (B.36) produce excellent\nresults. However, this is not guaranteed in all cases—after all, the problem is NP-hard.\nRecently, researchers have investigated alternative linear programming relaxations (Sontag\nand Jaakkola 2007; Sontag, Meltzer, Globerson et al. 2008; Komodakis and Paragios 2008;\nSchraudolph 2010). These algorithms are capable of producing tighter bounds compared to\n(B.36) at the expense of additional computational cost.\nLP relaxation and alpha expansion.\nSolving a linear program produces primal and dual\nsolutions that satisfy complementary slackness conditions. In general, the primal solution\nof (B.36) does not have to be integer-valued so, in practice, we may have to round it to\nobtain a valid labeling x. An alternative proposed by Komodakis and Tziritas (2007a); Ko-\nmodakis, Tziritas, and Paragios (2007) is to search for primal and dual solutions such that\nthey satisfy approximate complementary slackness conditions and the primal solution is al-\nready integer-valued. Several max-ﬂow-based algorithms are proposed by (Komodakis and\nTziritas 2007a; Komodakis, Tziritas, and Paragios 2007) for this purpose and the Fast-PD\nmethod (Komodakis, Tziritas, and Paragios 2007) is shown to perform best. In the case of\nmetric interactions, the default version of Fast-PD produces the same primal solution as the\nalpha-expansion algorithm (Boykov, Veksler, and Zabih 2001). This provides an interesting\ninterpretation of the alpha expansion algorithm as trying to approximately solve relaxation\n(B.36).\nUnlike the standard alpha expansion algorithm, Fast-PD also maintains a dual solution\nand thus runs faster in practice. Fast-PD can be extended to the case of semi-metric interac-\ntions (Komodakis, Tziritas, and Paragios 2007). The primal version of such extension was",
  "image_path": "page_795.jpg",
  "pages": [
    794,
    795,
    796
  ]
}