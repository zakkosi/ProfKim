{
  "doc_id": "pages_108_110",
  "text": "86\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nrgB\nrGb\nrgB\nrGb\nrGb\nRgb\nrGb\nRgb\nrgB\nrGb\nrgB\nrGb\nrGb\nRgb\nrGb\nRgb\nB\nG\nB\nG\nG\nR\nG\nR\nG\nB\nG\nR\nG\nR\nB\nG\nFigure 2.30 Bayer RGB pattern: (a) color ﬁlter array layout; (b) interpolated pixel values,\nwith unknown (guessed) values shown as lower case.\nThe most commonly used pattern in color cameras today is the Bayer pattern (Bayer\n1976), which places green ﬁlters over half of the sensors (in a checkerboard pattern), and red\nand blue ﬁlters over the remaining ones (Figure 2.30). The reason that there are twice as many\ngreen ﬁlters as red and blue is because the luminance signal is mostly determined by green\nvalues and the visual system is much more sensitive to high frequency detail in luminance\nthan in chrominance (a fact that is exploited in color image compression—see Section 2.3.3).\nThe process of interpolating the missing color values so that we have valid RGB values for\nall the pixels is known as demosaicing and is covered in detail in Section 10.3.1.\nSimilarly, color LCD monitors typically use alternating stripes of red, green, and blue\nﬁlters placed in front of each liquid crystal active area to simulate the experience of a full color\ndisplay. As before, because the visual system has higher resolution (acuity) in luminance than\nchrominance, it is possible to digitally pre-ﬁlter RGB (and monochrome) images to enhance\nthe perception of crispness (Betrisey, Blinn, Dresevic et al. 2000; Platt 2000).\nColor balance\nBefore encoding the sensed RGB values, most cameras perform some kind of color balancing\noperation in an attempt to move the white point of a given image closer to pure white (equal\nRGB values). If the color system and the illumination are the same (the BT.709 system uses\nthe daylight illuminant D65 as its reference white), the change may be minimal. However,\nif the illuminant is strongly colored, such as incandescent indoor lighting (which generally\nresults in a yellow or orange hue), the compensation can be quite signiﬁcant.\nA simple way to perform color correction is to multiply each of the RGB values by a\ndifferent factor (i.e., to apply a diagonal matrix transform to the RGB color space). More\ncomplicated transforms, which are sometimes the result of mapping to XYZ space and back,\n2.3 The digital camera\n87\nY\nY’\nY’ = Y1/γ\nY’\nY\nY = Y’γ\nquantization \nnoise\nvisible \nnoise\nFigure 2.31 Gamma compression: (a) The relationship between the input signal luminance\nY and the transmitted signal Y ′ is given by Y ′ = Y 1/γ. (b) At the receiver, the signal Y ′ is\nexponentiated by the factor γ, ˆY = Y ′γ. Noise introduced during transmission is squashed in\nthe dark regions, which corresponds to the more noise-sensitive region of the visual system.\nactually perform a color twist, i.e., they use a general 3 × 3 color transform matrix.21 Exer-\ncise 2.9 has you explore some of these issues.\nGamma\nIn the early days of black and white television, the phosphors in the CRT used to display\nthe TV signal responded non-linearly to their input voltage. The relationship between the\nvoltage and the resulting brightness was characterized by a number called gamma (γ), since\nthe formula was roughly\nB = V γ,\n(2.110)\nwith a γ of about 2.2. To compensate for this effect, the electronics in the TV camera would\npre-map the sensed luminance Y through an inverse gamma,\nY ′ = Y\n1\nγ ,\n(2.111)\nwith a typical value of 1\nγ = 0.45.\nThe mapping of the signal through this non-linearity before transmission had a beneﬁcial\nside effect: noise added during transmission (remember, these were analog days!) would be\nreduced (after applying the gamma at the receiver) in the darker regions of the signal where\nit was more visible (Figure 2.31).22 (Remember that our visual system is roughly sensitive to\nrelative differences in luminance.)\n21 Those of you old enough to remember the early days of color television will naturally think of the hue adjustment\nknob on the television set, which could produce truly bizarre results.\n22 A related technique called companding was the basis of the Dolby noise reduction systems used with audio\ntapes.\n88\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen color television was invented, it was decided to separately pass the red, green, and\nblue signals through the same gamma non-linearity before combining them for encoding.\nToday, even though we no longer have analog noise in our transmission systems, signals are\nstill quantized during compression (see Section 2.3.3), so applying inverse gamma to sensed\nvalues is still useful.\nUnfortunately, for both computer vision and computer graphics, the presence of gamma\nin images is often problematic. For example, the proper simulation of radiometric phenomena\nsuch as shading (see Section 2.2 and Equation (2.87)) occurs in a linear radiance space. Once\nall of the computations have been performed, the appropriate gamma should be applied before\ndisplay. Unfortunately, many computer graphics systems (such as shading models) operate\ndirectly on RGB values and display these values directly. (Fortunately, newer color imaging\nstandards such as the 16-bit scRGB use a linear space, which makes this less of a problem\n(Glassner 1995).)\nIn computer vision, the situation can be even more daunting. The accurate determination\nof surface normals, using a technique such as photometric stereo (Section 12.1.1) or even a\nsimpler operation such as accurate image deblurring, require that the measurements be in a\nlinear space of intensities. Therefore, it is imperative when performing detailed quantitative\ncomputations such as these to ﬁrst undo the gamma and the per-image color re-balancing\nin the sensed color values. Chakrabarti, Scharstein, and Zickler (2009) develop a sophisti-\ncated 24-parameter model that is a good match to the processing performed by today’s digital\ncameras; they also provide a database of color images you can use for your own testing.23\nFor other vision applications, however, such as feature detection or the matching of sig-\nnals in stereo and motion estimation, this linearization step is often not necessary. In fact,\ndetermining whether it is necessary to undo gamma can take some careful thinking, e.g., in\nthe case of compensating for exposure variations in image stitching (see Exercise 2.7).\nIf all of these processing steps sound confusing to model, they are. Exercise 2.10 has you\ntry to tease apart some of these phenomena using empirical investigation, i.e., taking pictures\nof color charts and comparing the RAW and JPEG compressed color values.\nOther color spaces\nWhile RGB and XYZ are the primary color spaces used to describe the spectral content (and\nhence tri-stimulus response) of color signals, a variety of other representations have been\ndeveloped both in video and still image coding and in computer graphics.\nThe earliest color representation developed for video transmission was the YIQ standard\ndeveloped for NTSC video in North America and the closely related YUV standard developed\nfor PAL in Europe. In both of these cases, it was desired to have a luma channel Y (so called\n23 http://vision.middlebury.edu/color/.",
  "image_path": "page_109.jpg",
  "pages": [
    108,
    109,
    110
  ]
}