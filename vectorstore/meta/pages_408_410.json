{
  "doc_id": "pages_408_410",
  "text": "386\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBias and gain (exposure differences).\nOften, the two images being aligned were not taken\nwith the same exposure. A simple model of linear (afﬁne) intensity variation between the two\nimages is the bias and gain model,\nI1(x + u) = (1 + α)I0(x) + β,\n(8.8)\nwhere β is the bias and α is the gain (Lucas and Kanade 1981; Gennert 1988; Fuh and\nMaragos 1991; Baker, Gross, and Matthews 2003; Evangelidis and Psarakis 2008). The least\nsquares formulation then becomes\nEBG(u) =\nX\ni\n[I1(xi + u) −(1 + α)I0(xi) −β]2 =\nX\ni\n[αI0(xi) + β −ei]2.\n(8.9)\nRather than taking a simple squared difference between corresponding patches, it becomes\nnecessary to perform a linear regression (Appendix A.2), which is somewhat more costly.\nNote that for color images, it may be necessary to estimate a different bias and gain for each\ncolor channel to compensate for the automatic color correction performed by some digital\ncameras (Section 2.3.2). Bias and gain compensation is also used in video codecs, where it is\nknown as weighted prediction (Richardson 2003).\nA more general (spatially varying, non-parametric) model of intensity variation, which is\ncomputed as part of the registration process, is used in (Negahdaripour 1998; Jia and Tang\n2003; Seitz and Baker 2009). This can be useful for dealing with local variations such as\nthe vignetting caused by wide-angle lenses, wide apertures, or lens housings. It is also pos-\nsible to pre-process the images before comparing their values, e.g., using band-pass ﬁltered\nimages (Anandan 1989; Bergen, Anandan, Hanna et al. 1992), gradients (Scharstein 1994;\nPapenberg, Bruhn, Brox et al. 2006), or using other local transformations such as histograms\nor rank transforms (Cox, Roy, and Hingorani 1995; Zabih and Woodﬁll 1994), or to max-\nimize mutual information (Viola and Wells III 1997; Kim, Kolmogorov, and Zabih 2003).\nHirschm¨uller and Scharstein (2009) compare a number of these approaches and report on\ntheir relative performance in scenes with exposure differences.\nCorrelation.\nAn alternative to taking intensity differences is to perform correlation, i.e., to\nmaximize the product (or cross-correlation) of the two aligned images,\nECC(u) =\nX\ni\nI0(xi)I1(xi + u).\n(8.10)\nAt ﬁrst glance, this may appear to make bias and gain modeling unnecessary, since the images\nwill prefer to line up regardless of their relative scales and offsets. However, this is actually\nnot true. If a very bright patch exists in I1(x), the maximum product may actually lie in that\narea.\n8.1 Translational alignment\n387\nFor this reason, normalized cross-correlation is more commonly used,\nENCC(u) =\nP\ni[I0(xi) −I0] [I1(xi + u) −I1]\nqP\ni[I0(xi) −I0]2\nqP\ni[I1(xi + u) −I1]2\n,\n(8.11)\nwhere\nI0\n=\n1\nN\nX\ni\nI0(xi)\nand\n(8.12)\nI1\n=\n1\nN\nX\ni\nI1(xi + u)\n(8.13)\nare the mean images of the corresponding patches and N is the number of pixels in the patch.\nThe normalized cross-correlation score is always guaranteed to be in the range [−1, 1], which\nmakes it easier to handle in some higher-level applications, such as deciding which patches\ntruly match. Normalized correlation works well when matching images taken with different\nexposures, e.g., when creating high dynamic range images (Section 10.2). Note, however,\nthat the NCC score is undeﬁned if either of the two patches has zero variance (and, in fact, its\nperformance degrades for noisy low-contrast regions).\nA variant on NCC, which is related to the bias–gain regression implicit in the matching\nscore (8.9), is the normalized SSD score\nENSSD(u) = 1\n2\nP\ni\n\u0002\n[I0(xi) −I0] −[I1(xi + u) −I1]\n\u00032\nqP\ni[I0(xi) −I0]2 + [I1(xi + u) −I1]2\n(8.14)\nrecently proposed by Criminisi, Shotton, Blake et al. (2007). In their experiments, they ﬁnd\nthat it produces comparable results to NCC, but is more efﬁcient when applied to a large\nnumber of overlapping patches using a moving average technique (Section 3.2.2).\n8.1.1 Hierarchical motion estimation\nNow that we have a well-deﬁned alignment cost function to optimize, how can we ﬁnd its\nminimum? The simplest solution is to do a full search over some range of shifts, using ei-\nther integer or sub-pixel steps. This is often the approach used for block matching in motion\ncompensated video compression, where a range of possible motions (say, ±16 pixels) is ex-\nplored.4\nTo accelerate this search process, hierarchical motion estimation is often used: an image\npyramid (Section 3.5) is constructed and a search over a smaller number of discrete pixels\n4 In stereo matching (Section 11.1.2), an explicit search over all possible disparities (i.e., a plane sweep) is almost\nalways performed, since the number of search hypotheses is much smaller due to the 1D nature of the potential\ndisplacements.\n388\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(corresponding to the same range of motion) is ﬁrst performed at coarser levels (Quam 1984;\nAnandan 1989; Bergen, Anandan, Hanna et al. 1992). The motion estimate from one level\nof the pyramid is then used to initialize a smaller local search at the next ﬁner level. Al-\nternatively, several seeds (good solutions) from the coarse level can be used to initialize the\nﬁne-level search. While this is not guaranteed to produce the same result as a full search, it\nusually works almost as well and is much faster.\nMore formally, let\nI(l)\nk (xj) ←˜I(l−1)\nk\n(2xj)\n(8.15)\nbe the decimated image at level l obtained by subsampling (downsampling) a smoothed ver-\nsion of the image at level l−1. See Section 3.5 for how to perform the required downsampling\n(pyramid construction) without introducing too much aliasing.\nAt the coarsest level, we search for the best displacement u(l) that minimizes the dif-\nference between images I(l)\n0\nand I(l)\n1 . This is usually done using a full search over some\nrange of displacements u(l) ∈2−l[−S, S]2, where S is the desired search range at the ﬁnest\n(original) resolution level, optionally followed by the incremental reﬁnement step described\nin Section 8.1.3.\nOnce a suitable motion vector has been estimated, it is used to predict a likely displace-\nment\nˆu(l−1) ←2u(l)\n(8.16)\nfor the next ﬁner level.5 The search over displacements is then repeated at the ﬁner level over\na much narrower range of displacements, say ˆu(l−1) ± 1, again optionally combined with an\nincremental reﬁnement step (Anandan 1989). Alternatively, one of the images can be warped\n(resampled) by the current motion estimate, in which case only small incremental motions\nneed to be computed at the ﬁner level. A nice description of the whole process, extended to\nparametric motion estimation (Section 8.2), is provided by Bergen, Anandan, Hanna et al.\n(1992).\n8.1.2 Fourier-based alignment\nWhen the search range corresponds to a signiﬁcant fraction of the larger image (as is the case\nin image stitching, see Chapter 9), the hierarchical approach may not work that well, since\nit is often not possible to coarsen the representation too much before signiﬁcant features are\nblurred away. In this case, a Fourier-based approach may be preferable.\n5 This doubling of displacements is only necessary if displacements are deﬁned in integer pixel coordinates,\nwhich is the usual case in the literature (Bergen, Anandan, Hanna et al. 1992). If normalized device coordinates\n(Section 2.1.5) are used instead, the displacements (and search ranges) need not change from level to level, although\nthe step sizes will need to be adjusted, to keep search steps of roughly one pixel.",
  "image_path": "page_409.jpg",
  "pages": [
    408,
    409,
    410
  ]
}