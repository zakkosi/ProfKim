{
  "doc_id": "pages_406_408",
  "text": "384\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n8.1 Translational alignment\nThe simplest way to establish an alignment between two images or image patches is to shift\none image relative to the other. Given a template image I0(x) sampled at discrete pixel\nlocations {xi = (xi, yi)}, we wish to ﬁnd where it is located in image I1(x). A least squares\nsolution to this problem is to ﬁnd the minimum of the sum of squared differences (SSD)\nfunction\nESSD(u) =\nX\ni\n[I1(xi + u) −I0(xi)]2 =\nX\ni\ne2\ni ,\n(8.1)\nwhere u = (u, v) is the displacement and ei = I1(xi + u) −I0(xi) is called the residual\nerror (or the displaced frame difference in the video coding literature).1 (We ignore for the\nmoment the possibility that parts of I0 may lie outside the boundaries of I1 or be otherwise\nnot visible.) The assumption that corresponding pixel values remain the same in the two\nimages is often called the brightness constancy constraint.2\nIn general, the displacement u can be fractional, so a suitable interpolation function must\nbe applied to image I1(x). In practice, a bilinear interpolant is often used but bicubic inter-\npolation can yield slightly better results (Szeliski and Scharstein 2004). Color images can be\nprocessed by summing differences across all three color channels, although it is also possible\nto ﬁrst transform the images into a different color space or to only use the luminance (which\nis often done in video encoders).\nRobust error metrics.\nWe can make the above error metric more robust to outliers by re-\nplacing the squared error terms with a robust function ρ(ei) (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Black and Anandan 1996; Stewart 1999) to obtain\nESRD(u) =\nX\ni\nρ(I1(xi + u) −I0(xi)) =\nX\ni\nρ(ei).\n(8.2)\nThe robust norm ρ(e) is a function that grows less quickly than the quadratic penalty associ-\nated with least squares. One such function, sometimes used in motion estimation for video\ncoding because of its speed, is the sum of absolute differences (SAD) metric3 or L1 norm,\ni.e.,\nESAD(u) =\nX\ni\n|I1(xi + u) −I0(xi)| =\nX\ni\n|ei|.\n(8.3)\n1 The usual justiﬁcation for using least squares is that it is the optimal estimate with respect to Gaussian noise.\nSee the discussion below on robust error metrics as well as Appendix B.3.\n2 Brightness constancy (Horn 1974) is the tendency for objects to maintain their perceived brightness under\nvarying illumination conditions.\n3 In video compression, e.g., the H.264 standard (http://www.itu.int/rec/T-REC-H.264), the sum of absolute trans-\nformed differences (SATD), which measures the differences in a frequency transform space, e.g., using a Hadamard\ntransform, is often used since it more accurately predicts quality (Richardson 2003).\n8.1 Translational alignment\n385\nHowever, since this function is not differentiable at the origin, it is not well suited to gradient-\ndescent approaches such as the ones presented in Section 8.1.3.\nInstead, a smoothly varying function that is quadratic for small values but grows more\nslowly away from the origin is often used. Black and Rangarajan (1996) discuss a variety of\nsuch functions, including the Geman–McClure function,\nρGM(x) =\nx2\n1 + x2/a2 ,\n(8.4)\nwhere a is a constant that can be thought of as an outlier threshold. An appropriate value for\nthe threshold can itself be derived using robust statistics (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Rousseeuw and Leroy 1987), e.g., by computing the median absolute\ndeviation, MAD = medi|ei|, and multiplying it by 1.4 to obtain a robust estimate of the\nstandard deviation of the inlier noise process (Stewart 1999).\nSpatially varying weights.\nThe error metrics above ignore that fact that for a given align-\nment, some of the pixels being compared may lie outside the original image boundaries.\nFurthermore, we may want to partially or completely downweight the contributions of cer-\ntain pixels. For example, we may want to selectively “erase” some parts of an image from\nconsideration when stitching a mosaic where unwanted foreground objects have been cut out.\nFor applications such as background stabilization, we may want to downweight the middle\npart of the image, which often contains independently moving objects being tracked by the\ncamera.\nAll of these tasks can be accomplished by associating a spatially varying per-pixel weight\nvalue with each of the two images being matched.\nThe error metric then becomes the\nweighted (or windowed) SSD function,\nEWSSD(u) =\nX\ni\nw0(xi)w1(xi + u)[I1(xi + u) −I0(xi)]2,\n(8.5)\nwhere the weighting functions w0 and w1 are zero outside the image boundaries.\nIf a large range of potential motions is allowed, the above metric can have a bias towards\nsmaller overlap solutions. To counteract this bias, the windowed SSD score can be divided\nby the overlap area\nA =\nX\ni\nw0(xi)w1(xi + u)\n(8.6)\nto compute a per-pixel (or mean) squared pixel error EWSSD/A. The square root of this\nquantity is the root mean square intensity error\nRMS =\np\nEWSSD/A\n(8.7)\noften reported in comparative studies.\n386\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBias and gain (exposure differences).\nOften, the two images being aligned were not taken\nwith the same exposure. A simple model of linear (afﬁne) intensity variation between the two\nimages is the bias and gain model,\nI1(x + u) = (1 + α)I0(x) + β,\n(8.8)\nwhere β is the bias and α is the gain (Lucas and Kanade 1981; Gennert 1988; Fuh and\nMaragos 1991; Baker, Gross, and Matthews 2003; Evangelidis and Psarakis 2008). The least\nsquares formulation then becomes\nEBG(u) =\nX\ni\n[I1(xi + u) −(1 + α)I0(xi) −β]2 =\nX\ni\n[αI0(xi) + β −ei]2.\n(8.9)\nRather than taking a simple squared difference between corresponding patches, it becomes\nnecessary to perform a linear regression (Appendix A.2), which is somewhat more costly.\nNote that for color images, it may be necessary to estimate a different bias and gain for each\ncolor channel to compensate for the automatic color correction performed by some digital\ncameras (Section 2.3.2). Bias and gain compensation is also used in video codecs, where it is\nknown as weighted prediction (Richardson 2003).\nA more general (spatially varying, non-parametric) model of intensity variation, which is\ncomputed as part of the registration process, is used in (Negahdaripour 1998; Jia and Tang\n2003; Seitz and Baker 2009). This can be useful for dealing with local variations such as\nthe vignetting caused by wide-angle lenses, wide apertures, or lens housings. It is also pos-\nsible to pre-process the images before comparing their values, e.g., using band-pass ﬁltered\nimages (Anandan 1989; Bergen, Anandan, Hanna et al. 1992), gradients (Scharstein 1994;\nPapenberg, Bruhn, Brox et al. 2006), or using other local transformations such as histograms\nor rank transforms (Cox, Roy, and Hingorani 1995; Zabih and Woodﬁll 1994), or to max-\nimize mutual information (Viola and Wells III 1997; Kim, Kolmogorov, and Zabih 2003).\nHirschm¨uller and Scharstein (2009) compare a number of these approaches and report on\ntheir relative performance in scenes with exposure differences.\nCorrelation.\nAn alternative to taking intensity differences is to perform correlation, i.e., to\nmaximize the product (or cross-correlation) of the two aligned images,\nECC(u) =\nX\ni\nI0(xi)I1(xi + u).\n(8.10)\nAt ﬁrst glance, this may appear to make bias and gain modeling unnecessary, since the images\nwill prefer to line up regardless of their relative scales and offsets. However, this is actually\nnot true. If a very bright patch exists in I1(x), the maximum product may actually lie in that\narea.",
  "image_path": "page_407.jpg",
  "pages": [
    406,
    407,
    408
  ]
}