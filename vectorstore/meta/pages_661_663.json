{
  "doc_id": "pages_661_663",
  "text": "13.5 Video-based rendering\n639\nFigure 13.12\nVideo Rewrite (Bregler, Covell, and Slaney 1997) c⃝1997 ACM: the video\nframes are composed from bits and pieces of old video footage matched to a new audio track.\ncreate virtual camera paths between the source cameras as part of a real-time viewing expe-\nrience. Finally, we discuss capturing environments by driving or walking through them with\npanoramic video cameras in order to create interactive video-based walkthrough experiences\n(Section 13.5.5).\n13.5.1 Video-based animation\nAs we mentioned above, an early example of video-based animation is Video Rewrite, in\nwhich frames from original video footage are rearranged in order to match them to novel\nspoken utterances, e.g., for movie dubbing (Figure 13.12). This is similar in spirit to the way\nthat concatenative speech synthesis systems work (Taylor 2009).\nIn their system, Bregler, Covell, and Slaney (1997) ﬁrst use speech recognition to extract\nphonemes from both the source video material and the novel audio stream. Phonemes are\ngrouped into triphones (triplets of phonemes), since these better model the coarticulation\neffect present when people speak. Matching triphones are then found in the source footage\nand audio track. The mouth images corresponding to the selected video frames are then\ncut and pasted into the desired video footage being re-animated or dubbed, with appropriate\ngeometric transformations to account for head motion. During the analysis phase, features\ncorresponding to the lips, chin, and head are tracked using computer vision techniques. Dur-\ning synthesis, image morphing techniques are used to blend and stitch adjacent mouth shapes\ninto a more coherent whole. In more recent work, Ezzat, Geiger, and Poggio (2002) describe\nhow to use a multidimensional morphable model (Section 12.6.2) combined with regularized\ntrajectory synthesis to improve these results.\nA more sophisticated version of this system, called face transfer, uses a novel source\nvideo, instead of just an audio track, to drive the animation of a previously captured video, i.e.,\nto re-render a video of a talking head with the appropriate visual speech, expression, and head\npose elements (Vlasic, Brand, Pﬁster et al. 2005). This work is one of many performance-\ndriven animation systems (Section 4.1.5), which are often used to animate 3D facial models\n640\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Figures 12.18–12.19). While traditional performance-driven animation systems use marker-\nbased motion capture (Williams 1990; Litwinowicz and Williams 1994; Ma, Jones, Chiang\net al. 2008), video footage can now often be used directly to control the animation (Buck,\nFinkelstein, Jacobs et al. 2000; Pighin, Szeliski, and Salesin 2002; Zhang, Snavely, Curless\net al. 2004; Vlasic, Brand, Pﬁster et al. 2005; Roble and Zafar 2009).\nIn addition to its most common application to facial animation, video-based animation\ncan also be applied to whole body motion (Section 12.6.4), e.g., by matching the ﬂow ﬁelds\nbetween two different source videos and using one to drive the other (Efros, Berg, Mori et al.\n2003). Another approach to video-based rendering is to use ﬂow or 3D modeling to unwrap\nsurface textures into stabilized images, which can then be manipulated and re-rendered onto\nthe original video (Pighin, Szeliski, and Salesin 2002; Rav-Acha, Kohli, Fitzgibbon et al.\n2008).\n13.5.2 Video textures\nVideo-based animation is a powerful means of creating photo-realistic videos by re-purposing\nexisting video footage to match some other desired activity or script. What if instead of\nconstructing a special animation or narrative, we simply want the video to continue playing\nin a plausible manner? For example, many Web sites use images or videos to highlight their\ndestinations, e.g., to portray attractive beaches with surf and palm trees waving in the wind.\nInstead of using a static image or a video clip that has a discontinuity when it loops, can we\ntransform the video clip into an inﬁnite-length animation that plays forever?\nThis idea is the basis of video textures, in which a short video clip can be arbitrarily\nextended by re-arranging video frames while preserving visual continuity (Sch¨odl, Szeliski,\nSalesin et al. 2000). The basic problem in creating video textures is how to perform this\nre-arrangement without introducing visual artifacts. Can you think of how you might do this?\nThe simplest approach is to match frames by visual similarity (e.g., L2 distance) and to\njump between frames that appear similar. Unfortunately, if the motions in the two frames\nare different, a dramatic visual artifact will occur (the video will appear to “stutter”). For\nexample, if we fail to match the motions of the clock pendulum in Figure 13.13a, it can\nsuddenly change direction in mid-swing.\nHow can we extend our basic frame matching to also match motion? In principle, we\ncould compute optic ﬂow at each frame and match this. However, ﬂow estimates are often\nunreliable (especially in textureless regions) and it is not clear how to weight the visual and\nmotion similarities relative to each other. As an alternative, Sch¨odl, Szeliski, Salesin et al.\n(2000) suggest matching triplets or larger neighborhoods of adjacent video frames, much\nin the same way as Video Rewrite matches triphones. Once we have constructed an n ×\nn similarity matrix between all video frames (where n is the number of frames), a simple\n13.5 Video-based rendering\n641\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 13.13\nVideo textures (Sch¨odl, Szeliski, Salesin et al. 2000) c⃝2000 ACM: (a) a\nclock pendulum, with correctly matched direction of motion; (b) a candle ﬂame, showing\ntemporal transition arcs; (c) the ﬂag is generated using morphing at jumps; (d) a bonﬁre\nuses longer cross-dissolves; (e) a waterfall cross-dissolves several sequences at once; (f) a\nsmiling animated face; (g) two swinging children are animated separately; (h) the balloons\nare automatically segmented into separate moving regions; (i) a synthetic ﬁsh tank consisting\nof bubbles, plants, and ﬁsh. Videos corresponding to these images can be found at http:\n//www.cc.gatech.edu/gvu/perception/projects/videotexture/.",
  "image_path": "page_662.jpg",
  "pages": [
    661,
    662,
    663
  ]
}