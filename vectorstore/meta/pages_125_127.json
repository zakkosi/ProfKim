{
  "doc_id": "pages_125_127",
  "text": "3.1 Point operators\n103\n45\n60\n98\n127 132 133 137 133\n46\n65\n98\n123 126 128 131 133\n47\n65\n96\n115 119 123 135 137\n47\n63\n91\n107 113 122 138 134\n50\n59\n80\n97\n110 123 133 134\n49\n53\n68\n83\n97\n113 128 133\n50\n50\n58\n70\n84\n102 116 126\n50\n50\n52\n58\n69\n86\n101 120\n1\n3\n5\n7\n9\n11\n13\n15\nS1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\nS9\nS10\nS11\nS12\nS13\nS14\nS15\nS16\n0\n20\n40\n60\n80\n100\n120\n140\n160\nrange\ndomain\ndomain\n(a)\n(b)\n(c)\n(d)\nFigure 3.3 Visualizing image data: (a) original image; (b) cropped portion and scanline plot\nusing an image inspection tool; (c) grid of numbers; (d) surface plot. For ﬁgures (c)–(d), the\nimage was ﬁrst converted to grayscale.\nscaling and image addition. Next, we discuss how colors in images can be manipulated.\nWe then present image compositing and matting operations, which play an important role\nin computational photography (Chapter 10) and computer graphics applications. Finally, we\ndescribe the more global process of histogram equalization. We close with an example appli-\ncation that manipulates tonal values (exposure and contrast) to improve image appearance.\n3.1.1 Pixel transforms\nA general image processing operator is a function that takes one or more input images and\nproduces an output image. In the continuous domain, this can be denoted as\ng(x) = h(f(x)) or g(x) = h(f0(x), . . . , fn(x)),\n(3.1)\nwhere x is in the D-dimensional domain of the functions (usually D = 2 for images) and the\nfunctions f and g operate over some range, which can either be scalar or vector-valued, e.g.,\nfor color images or 2D motion. For discrete (sampled) images, the domain consists of a ﬁnite\nnumber of pixel locations, x = (i, j), and we can write\ng(i, j) = h(f(i, j)).\n(3.2)\nFigure 3.3 shows how an image can be represented either by its color (appearance), as a grid\nof numbers, or as a two-dimensional function (surface plot).\nTwo commonly used point processes are multiplication and addition with a constant,\ng(x) = af(x) + b.\n(3.3)\nThe parameters a > 0 and b are often called the gain and bias parameters; sometimes these\nparameters are said to control contrast and brightness, respectively (Figures 3.2b–c).1 The\n1 An image’s luminance characteristics can also be summarized by its key (average luminanance) and range\n(Kopf, Uyttendaele, Deussen et al. 2007).\n104\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nbias and gain parameters can also be spatially varying,\ng(x) = a(x)f(x) + b(x),\n(3.4)\ne.g., when simulating the graded density ﬁlter used by photographers to selectively darken\nthe sky or when modeling vignetting in an optical system.\nMultiplicative gain (both global and spatially varying) is a linear operation, since it obeys\nthe superposition principle,\nh(f0 + f1) = h(f0) + h(f1).\n(3.5)\n(We will have more to say about linear shift invariant operators in Section 3.2.) Operators\nsuch as image squaring (which is often used to get a local estimate of the energy in a band-\npass ﬁltered signal, see Section 3.5) are not linear.\nAnother commonly used dyadic (two-input) operator is the linear blend operator,\ng(x) = (1 −α)f0(x) + αf1(x).\n(3.6)\nBy varying α from 0 →1, this operator can be used to perform a temporal cross-dissolve\nbetween two images or videos, as seen in slide shows and ﬁlm production, or as a component\nof image morphing algorithms (Section 3.6.3).\nOne highly used non-linear transform that is often applied to images before further pro-\ncessing is gamma correction, which is used to remove the non-linear mapping between input\nradiance and quantized pixel values (Section 2.3.2).\nTo invert the gamma mapping applied\nby the sensor, we can use\ng(x) = [f(x)]1/γ ,\n(3.7)\nwhere a gamma value of γ ≈2.2 is a reasonable ﬁt for most digital cameras.\n3.1.2 Color transforms\nWhile color images can be treated as arbitrary vector-valued functions or collections of inde-\npendent bands, it usually makes sense to think about them as highly correlated signals with\nstrong connections to the image formation process (Section 2.2), sensor design (Section 2.3),\nand human perception (Section 2.3.2). Consider, for example, brightening a picture by adding\na constant value to all three channels, as shown in Figure 3.2b. Can you tell if this achieves the\ndesired effect of making the image look brighter? Can you see any undesirable side-effects\nor artifacts?\nIn fact, adding the same value to each color channel not only increases the apparent in-\ntensity of each pixel, it can also affect the pixel’s hue and saturation. How can we deﬁne and\nmanipulate such quantities in order to achieve the desired perceptual effects?\n3.1 Point operators\n105\n(a)\n(b)\n(c)\n(d)\nFigure 3.4 Image matting and compositing (Chuang, Curless, Salesin et al. 2001) c⃝2001\nIEEE: (a) source image; (b) extracted foreground object F; (c) alpha matte α shown in\ngrayscale; (d) new composite C.\nAs discussed in Section 2.3.2, chromaticity coordinates (2.104) or even simpler color ra-\ntios (2.116) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figure\n2.32g–i shows some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 × 3 color twist transform matrix.\nExercises 2.9 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn\n1996), while the process of inserting it into another image (without visible artifacts) is called\ncompositing (Porter and Duff 1984; Blinn 1994a).\nThe intermediate representation used for the foreground object between these two stages\nis called an alpha-matted color image (Figure 3.4b–c). In addition to the three color RGB\nchannels, an alpha-matted image contains a fourth alpha channel α (or A) that describes the\nrelative amount of opacity or fractional coverage at each pixel (Figures 3.4c and 3.5b). The\nopacity is the opposite of the transparency. Pixels within the object are fully opaque (α = 1),\nwhile pixels fully outside the object are transparent (α = 0). Pixels on the boundary of the\nobject vary smoothly between these two extremes, which hides the perceptual visible jaggies",
  "image_path": "page_126.jpg",
  "pages": [
    125,
    126,
    127
  ]
}