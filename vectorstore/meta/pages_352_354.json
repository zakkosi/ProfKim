{
  "doc_id": "pages_352_354",
  "text": "330\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx1\nx0\nx2\nx1\nx0\nx2\nc\n(a)\n(b)\nFigure 6.9 Calibration from vanishing points: (a) any pair of ﬁnite vanishing points (ˆxi, ˆxj)\ncan be used to estimate the focal length; (b) the orthocenter of the vanishing point triangle\ngives the optical center of the image c.\nand Zisserman 1998; Cipolla, Drummond, and Robertson 1999; Antone and Teller 2002;\nCriminisi, Reid, and Zisserman 2000; Hartley and Zisserman 2004; Pﬂugfelder 2008).\nLet us assume that we have detected two or more orthogonal vanishing points, all of which\nare ﬁnite, i.e., they are not obtained from lines that appear to be parallel in the image plane\n(Figure 6.9a). Let us also assume a simpliﬁed form for the calibration matrix K where only\nthe focal length is unknown (2.59). (It is often safe for rough 3D modeling to assume that\nthe optical center is at the center of the image, that the aspect ratio is 1, and that there is no\nskew.) In this case, the projection equation for the vanishing points can be written as\nˆxi =\n\n\nxi −cx\nyi −cy\nf\n\n∼Rpi = ri,\n(6.50)\nwhere pi corresponds to one of the cardinal directions (1, 0, 0), (0, 1, 0), or (0, 0, 1), and ri\nis the ith column of the rotation matrix R.\nFrom the orthogonality between columns of the rotation matrix, we have\nri · rj ∼(xi −cx)(xj −cy) + (yi −cy)(yj −cy) + f 2 = 0\n(6.51)\nfrom which we can obtain an estimate for f 2. Note that the accuracy of this estimate increases\nas the vanishing points move closer to the center of the image. In other words, it is best to tilt\nthe calibration pattern a decent amount around the 45◦axis, as in Figure 6.9a. Once the focal\nlength f has been determined, the individual columns of R can be estimated by normalizing\nthe left hand side of (6.50) and taking cross products. Alternatively, an SVD of the initial R\nestimate, which is a variant on orthogonal Procrustes (6.32), can be used.\nIf all three vanishing points are visible and ﬁnite in the same image, it is also possible to\nestimate the optical center as the orthocenter of the triangle formed by the three vanishing\npoints (Caprile and Torre 1990; Hartley and Zisserman 2004, Section 7.6) (Figure 6.9b).\n6.3 Geometric intrinsic calibration\n331\n(a)\n(b)\nFigure 6.10\nSingle view metrology (Criminisi, Reid, and Zisserman 2000)\nc⃝2000\nSpringer: (a) input image showing the three coordinate axes computed from the two hori-\nzontal vanishing points (which can be determined from the sidings on the shed); (b) a new\nview of the 3D reconstruction.\nIn practice, however, it is more accurate to re-estimate any unknown intrinsic calibration\nparameters using non-linear least squares (6.42).\n6.3.3 Application: Single view metrology\nA fun application of vanishing point estimation and camera calibration is the single view\nmetrology system developed by Criminisi, Reid, and Zisserman (2000). Their system allows\npeople to interactively measure heights and other dimensions as well as to build piecewise-\nplanar 3D models, as shown in Figure 6.10.\nThe ﬁrst step in their system is to identify two orthogonal vanishing points on the ground\nplane and the vanishing point for the vertical direction, which can be done by drawing some\nparallel sets of lines in the image. (Alternatively, automated techniques such as those dis-\ncussed in Section 4.3.3 or by Schaffalitzky and Zisserman (2000) could be used.) The user\nthen marks a few dimensions in the image, such as the height of a reference object, and\nthe system can automatically compute the height of another object. Walls and other planar\nimpostors (geometry) can also be sketched and reconstructed.\nIn the formulation originally developed by Criminisi, Reid, and Zisserman (2000), the\nsystem produces an afﬁne reconstruction, i.e., one that is only known up to a set of indepen-\ndent scaling factors along each axis. A potentially more useful system can be constructed by\nassuming that the camera is calibrated up to an unknown focal length, which can be recov-\nered from orthogonal (ﬁnite) vanishing directions, as we just described in Section 6.3.2. Once\nthis is done, the user can indicate an origin on the ground plane and another point a known\ndistance away. From this, points on the ground plane can be directly projected into 3D and\n332\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 6.11\nFour images taken with a hand-held camera registered using a 3D rotation\nmotion model, which can be used to estimate the focal length of the camera (Szeliski and\nShum 1997) c⃝2000 ACM.\npoints above the ground plane, when paired with their ground plane projections, can also be\nrecovered. A fully metric reconstruction of the scene then becomes possible.\nExercise 6.9 has you implement such a system and then use it to model some simple\n3D scenes. Section 12.6.1 describes other, potentially multi-view, approaches to architectural\nreconstruction, including an interactive piecewise-planar modeling system that uses vanishing\npoints to establish 3D line directions and plane normals (Sinha, Steedly, Szeliski et al. 2008).\n6.3.4 Rotational motion\nWhen no calibration targets or known structures are available but you can rotate the camera\naround its front nodal point (or, equivalently, work in a large open environment where all ob-\njects are distant), the camera can be calibrated from a set of overlapping images by assuming\nthat it is undergoing pure rotational motion, as shown in Figure 6.11 (Stein 1995; Hartley\n1997b; Hartley, Hayman, de Agapito et al. 2000; de Agapito, Hayman, and Reid 2001; Kang\nand Weiss 1999; Shum and Szeliski 2000; Frahm and Koch 2003). When a full 360◦mo-\ntion is used to perform this calibration, a very accurate estimate of the focal length f can be\nobtained, as the accuracy in this estimate is proportional to the total number of pixels in the\nresulting cylindrical panorama (Section 9.1.6) (Stein 1995; Shum and Szeliski 2000).\nTo use this technique, we ﬁrst compute the homographies ˜\nHij between all overlapping\npairs of images, as explained in Equations (6.19–6.23). Then, we use the observation, ﬁrst\nmade in Equation (2.72) and explored in more detail in Section 9.1.3 (9.5), that each homog-\nraphy is related to the inter-camera rotation Rij through the (unknown) calibration matrices",
  "image_path": "page_353.jpg",
  "pages": [
    352,
    353,
    354
  ]
}