{
  "doc_id": "pages_664_666",
  "text": "642\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nﬁnite impulse response (FIR) ﬁltering of each match sequence can be used to emphasize\nsubsequences that match well.\nThe results of this match computation gives us a jump table or, equivalently, a transition\nprobability between any two frames in the original video. This is shown schematically as\nred arcs in Figure 13.13b, where the red bar indicates which video frame is currently be-\ning displayed, and arcs light up as a forward or backward transition is taken. We can view\nthese transition probabilities as encoding the hidden Markov model (HMM) that underlies a\nstochastic video generation process.\nSometimes, it is not possible to ﬁnd exactly matching subsequences in the original video.\nIn this case, morphing, i.e., warping and blending frames during transitions (Section 3.6.3)\ncan be used to hide the visual differences (Figure 13.13c). If the motion is chaotic enough,\nas in a bonﬁre or a waterfall (Figures 13.13d–e), simple blending (extended cross-dissolves)\nmay be sufﬁcient. Improved transitions can also be obtained by performing 3D graph cuts on\nthe spatio-temporal volume around a transition (Kwatra, Sch¨odl, Essa et al. 2003).\nVideo textures need not be restricted to chaotic random phenomena such as ﬁre, wind,\nand water. Pleasing video textures can be created of people, e.g., a smiling face (as in Fig-\nure 13.13f) or someone running on a treadmill (Sch¨odl, Szeliski, Salesin et al. 2000). When\nmultiple people or objects are moving independently, as in Figures 13.13g–h, we must ﬁrst\nsegment the video into independently moving regions and animate each region separately.\nIt is also possible to create large panoramic video textures from a slowly panning camera\n(Agarwala, Zheng, Pal et al. 2005).\nInstead of just playing back the original frames in a stochastic (random) manner, video\ntextures can also be used to create scripted or interactive animations. If we extract individual\nelements, such as ﬁsh in a ﬁshtank (Figure 13.13i) into separate video sprites, we can animate\nthem along pre-speciﬁed paths (by matching the path direction with the original sprite motion)\nto make our video elements move in a desired fashion (Sch¨odl and Essa 2002). In fact, work\non video textures inspired research on systems that re-synthesize new motion sequences from\nmotion capture data, which some people refer to as “mocap soup” (Arikan and Forsyth 2002;\nKovar, Gleicher, and Pighin 2002; Lee, Chai, Reitsma et al. 2002; Li, Wang, and Shum 2002;\nPullen and Bregler 2002).\nWhile video textures primarily analyze the video as a sequence of frames (or regions) that\ncan be re-arranged in time, temporal textures (Szummer and Picard 1996; Bar-Joseph, El-\nYaniv, Lischinski et al. 2001) and dynamic textures (Doretto, Chiuso, Wu et al. 2003; Yuan,\nWen, Liu et al. 2004; Doretto and Soatto 2006) treat the video as a 3D spatio-temporal volume\nwith textural properties, which can be described using auto-regressive temporal models.\n13.5 Video-based rendering\n643\n    displacement map\n...\n(a)\n(b)\n(c)\n(d)\n(e)\n...\n...\n=\n=\n=\n=\n=\n \nL1\nL2\nLl-2\nLl-1\nLl\nL  (t)\n1\nL  (t)\n2\nL    (t)\nl-2\nL    (t)\nl-1\nL (t)\nl\n    displacement map\n    displacement map\n    displacement map\n    displacement map\nd    (t)\n l-1\nd (t)\n l\nd    (t)\n l-2\nd  (t)\n 2\nd  (t)\n 1\ntype=“boat”\ntype=“still”\ntype=“tree”\ntype=“cloud”\ntype=“water”\nFigure 13.14 Animating still pictures (Chuang, Goldman, Zheng et al. 2005) c⃝2005 ACM.\n(a) The input still image is manually segmented into (b) several layers. (c) Each layer is\nthen animated with a different stochastic motion texture (d) The animated layers are then\ncomposited to produce (e) the ﬁnal animation\n13.5.3 Application: Animating pictures\nWhile video textures can turn a short video clip into an inﬁnitely long video, can the same\nthing be done with a single still image? The answer is yes, if you are willing to ﬁrst segment\nthe image into different layers and then animate each layer separately.\nChuang, Goldman, Zheng et al. (2005) describe how an image can be decomposed into\nseparate layers using interactive matting techniques. Each layer is then animated using a\nclass-speciﬁc synthetic motion. As shown in Figure 13.14, boats rock back and forth, trees\nsway in the wind, clouds move horizontally, and water ripples, using a shaped noise displace-\nment map. All of these effects can be tied to some global control parameters, such as the\nvelocity and direction of a virtual wind. After being individually animated, the layers can be\ncomposited to create a ﬁnal dynamic rendering.\n13.5.4 3D Video\nIn recent years, the popularity of 3D movies has grown dramatically, with recent releases\nranging from Hannah Montana, through U2’s 3D concert movie, to James Cameron’s Avatar.\nCurrently, such releases are ﬁlmed using stereoscopic camera rigs and displayed in theaters\n(or at home) to viewers wearing polarized glasses.8 In the future, however, home audiences\nmay wish to view such movies with multi-zone auto-stereoscopic displays, where each person\ngets his or her own customized stereo stream and can move around a scene to see it from\n8 http://www.3d-summit.com/.\n644\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nRender\nbackground\nBi\nRender\nforeground\nFi\nOver\ncomposite\nCamera i\nRender\nbackground\nBi+1\nRender\nforeground\nFi+1\nOver\ncomposite\nBlend\nCamera i+1\n(a)\n(b)\ndi\nMi\nBi\nstrip\nwidth\nstrip\nwidth\ndepth\ndiscontinuity\nmatte\n(c)\n(d)\n(e)\n(f)\nFigure 13.15\nVideo view interpolation (Zitnick, Kang, Uyttendaele et al. 2004) c⃝2004\nACM: (a) the capture hardware consists of eight synchronized cameras; (b) the background\nand foreground images from each camera are rendered and composited before blending; (c)\nthe two-layer representation, before and after boundary matting; (d) background color esti-\nmates; (e) background depth estimates; (f) foreground color estimates.\ndifferent perspectives.9\nThe stereo matching techniques developed in the computer vision community along with\nimage-based rendering (view interpolation) techniques from graphics are both essential com-\nponents in such scenarios, which are sometimes called free-viewpoint video (Carranza, Theobalt,\nMagnor et al. 2003) or virtual viewpoint video (Zitnick, Kang, Uyttendaele et al. 2004). In\naddition to solving a series of per-frame reconstruction and view interpolation problems, the\ndepth maps or proxies produced by the analysis phase must be temporally consistent in order\nto avoid ﬂickering artifacts.\nShum, Chan, and Kang (2007) and Magnor (2005) present nice overviews of various\nvideo view interpolation techniques and systems. These include the Virtualized Reality sys-\ntem of Kanade, Rander, and Narayanan (1997) and Vedula, Baker, and Kanade (2005), Im-\nmersive Video (Moezzi, Katkere, Kuramura et al. 1996), Image-Based Visual Hulls (Matusik,\nBuehler, Raskar et al. 2000; Matusik, Buehler, and McMillan 2001), and Free-Viewpoint\nVideo (Carranza, Theobalt, Magnor et al. 2003), which all use global 3D geometric models\n(surface-based (Section 12.3) or volumetric (Section 12.5)) as their proxies for rendering.\nThe work of Vedula, Baker, and Kanade (2005) also computes scene ﬂow, i.e., the 3D motion\nbetween corresponding surface elements, which can then be used to perform spatio-temporal\ninterpolation of the multi-view video stream.\nThe Virtual Viewpoint Video system of Zitnick, Kang, Uyttendaele et al. (2004), on the\n9 http://www.siggraph.org/s2008/attendees/caf/3d/.",
  "image_path": "page_665.jpg",
  "pages": [
    664,
    665,
    666
  ]
}