{
  "doc_id": "pages_561_563",
  "text": "11.1 Epipolar geometry\n539\n(a)\n(b)\n(c)\n(d)\nFigure 11.4\nThe multi-stage stereo rectiﬁcation algorithm of Loop and Zhang (1999) c⃝\n1999 IEEE. (a) Original image pair overlaid with several epipolar lines; (b) images trans-\nformed so that epipolar lines are parallel; (c) images rectiﬁed so that epipolar lines are hori-\nzontal and in vertial correspondence; (d) ﬁnal rectiﬁcation that minimizes horizontal distor-\ntions.\nperpendicular to the camera center line. This ensures that corresponding epipolar lines are\nhorizontal and that the disparity for points at inﬁnity is 0. Finally, re-scale the images, if nec-\nessary, to account for different focal lengths, magnifying the smaller image to avoid aliasing.\n(The full details of this procedure can be found in Fusiello, Trucco, and Verri (2000) and Ex-\nercise 11.1.) Note that in general, it is not possible to rectify an arbitrary collection of images\nsimultaneously unless their optical centers are collinear, although rotating the cameras so that\nthey all point in the same direction reduces the inter-camera pixel movements to scalings and\ntranslations.\nThe resulting standard rectiﬁed geometry is employed in a lot of stereo camera setups and\nstereo algorithms, and leads to a very simple inverse relationship between 3D depths Z and\ndisparities d,\nd = f B\nZ ,\n(11.1)\nwhere f is the focal length (measured in pixels), B is the baseline, and\nx′ = x + d(x, y), y′ = y\n(11.2)\ndescribes the relationship between corresponding pixel coordinates in the left and right im-\nages (Bolles, Baker, and Marimont 1987; Okutomi and Kanade 1993; Scharstein and Szeliski\n540\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 11.5\nSlices through a typical disparity space image (DSI) (Scharstein and Szeliski\n2002) c⃝2002 Springer: (a) original color image; (b) ground truth disparities; (c–e) three\n(x, y) slices for d = 10, 16, 21; (f) an (x, d) slice for y = 151 (the dashed line in (b)).\nVarious dark (matching) regions are visible in (c–e), e.g., the bookshelves, table and cans,\nand head statue, and three disparity levels can be seen as horizontal lines in (f). The dark\nbands in the DSIs indicate regions that match at this disparity. (Smaller dark regions are often\nthe result of textureless regions.) Additional examples of DSIs are discussed by Bobick and\nIntille (1999).\n2002).3 The task of extracting depth from a set of images then becomes one of estimating the\ndisparity map d(x, y).\nAfter rectiﬁcation, we can easily compare the similarity of pixels at corresponding lo-\ncations (x, y) and (x′, y′) = (x + d, y) and store them in a disparity space image (DSI)\nC(x, y, d) for further processing (Figure 11.5). The concept of the disparity space (x, y, d)\ndates back to early work in stereo matching (Marr and Poggio 1976), while the concept of a\ndisparity space image (volume) is generally associated with Yang, Yuille, and Lu (1993) and\nIntille and Bobick (1994).\n11.1.2 Plane sweep\nAn alternative to pre-rectifying the images before matching is to sweep a set of planes through\nthe scene and to measure the photoconsistency of different images as they are re-projected\nonto these planes (Figure 11.6). This process is commonly known as the plane sweep algo-\nrithm (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).\nAs we saw in Section 2.1.5, where we introduced projective depth (also known as plane\nplus parallax (Kumar, Anandan, and Hanna 1994; Sawhney 1994; Szeliski and Coughlan\n3 The term disparity was ﬁrst introduced in the human vision literature to describe the difference in location\nof corresponding features seen by the left and right eyes (Marr 1982). Horizontal disparity is the most commonly\nstudied phenomenon, but vertical disparity is possible if the eyes are verged.\n11.1 Epipolar geometry\n541\nVirtual camera\nd\nx\ny\nInput  image k\nu\nv\nHomography:\n  u = H x\nx\ny\nk\nd\nk\n(a)\n(b)\nFigure 11.6 Sweeping a set of planes through a scene (Szeliski and Golland 1999) c⃝1999\nSpringer: (a) The set of planes seen from a virtual camera induces a set of homographies in\nany other source (input) camera image. (b) The warped images from all the other cameras can\nbe stacked into a generalized disparity space volume ˜I(x, y, d, k) indexed by pixel location\n(x, y), disparity d, and camera k.\n1997)), the last row of a full-rank 4 × 4 projection matrix ˜\nP can be set to an arbitrary plane\nequation p3 = s3[ˆn0|c0]. The resulting four-dimensional projective transform (collineation)\n(2.68) maps 3D world points p = (X, Y, Z, 1) into screen coordinates xs = (xs, ys, 1, d),\nwhere the projective depth (or parallax) d (2.66) is 0 on the reference plane (Figure 2.11).\nSweeping d through a series of disparity hypotheses, as shown in Figure 11.6a, corre-\nsponds to mapping each input image into the virtual camera ˜\nP deﬁning the disparity space\nthrough a series of homographies (2.68–2.71),\n˜xk ∼˜\nP k ˜\nP\n−1xs = ˜\nHk˜x + tkd = ( ˜\nHk + tk[0 0 d])˜x,\n(11.3)\nas shown in Figure 2.12b, where ˜xk and ˜x are the homogeneous pixel coordinates in the\nsource and virtual (reference) images (Szeliski and Golland 1999). The members of the fam-\nily of homographies ˜\nHk(d) = ˜\nHk + tk[0 0 d], which are parametererized by the addition of\na rank-1 matrix, are related to each other through a planar homology (Hartley and Zisserman\n2004, A5.2).\nThe choice of virtual camera and parameterization is application dependent and is what\ngives this framework a lot of its ﬂexibility. In many applications, one of the input cameras\n(the reference camera) is used, thus computing a depth map that is registered with one of the\ninput images and which can later be used for image-based rendering (Sections 13.1 and 13.2).\nIn other applications, such as view interpolation for gaze correction in video-conferencing",
  "image_path": "page_562.jpg",
  "pages": [
    561,
    562,
    563
  ]
}