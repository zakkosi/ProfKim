{
  "doc_id": "pages_710_712",
  "text": "688\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.28\nVisual words obtained from elliptical normalized afﬁne regions (Sivic and\nZisserman 2009) c⃝2009 IEEE. (a) Afﬁne covariant regions are extracted from each frame\nand clustered into visual words using k-means clustering on SIFT descriptors with a learned\nMahalanobis distance. (b) The central patch in each grid shows the query and the surrounding\npatches show the nearest neighbors.\nThe problem of quickly ﬁnding partial matches between documents is one of the cen-\ntral problems in information retrieval (IR) (Baeza-Yates and Ribeiro-Neto 1999; Manning,\nRaghavan, and Sch¨utze 2008). The basic approach in fast document retrieval algorithms is to\npre-compute an inverted index between individual words and the documents (or Web pages\nor news stories) where they occur. More precisely, the frequency of occurrence of particular\nwords in a document is used to quickly ﬁnd documents that match a particular query.\nSivic and Zisserman (2009) were the ﬁrst to adapt IR techniques to visual search. In their\nVideo Google system, afﬁne invariant features are ﬁrst detected in all the video frames they\nare indexing using both shape adapted regions around Harris feature points (Schaffalitzky\nand Zisserman 2002; Mikolajczyk and Schmid 2004) and maximally stable extremal regions\n(Matas, Chum, Urban et al. 2004), (Section 4.1.1), as shown in Figure 14.28a. Next, 128-\ndimensional SIFT descriptors are computed from each normalized region (i.e., the patches\nshown in Figure 14.28b). Then, an average covariance matrix for these descriptors is es-\ntimated by accumulating statistics for features tracked from frame to frame. The feature\ndescriptor covariance Σ is then used to deﬁne a Mahalanobis distance between feature de-\nscriptors,\nd(x0, x1) = ∥x0 −x1∥Σ\n−1 =\nq\n(x0 −x1)T Σ−1(x0 −x1).\n(14.32)\nIn practice, feature descriptors are whitened by pre-multiplying them by Σ−1/2 so that Eu-\nclidean distances can be used.17\nIn order to apply fast information retrieval techniques to images, the high-dimensional\nfeature descriptors that occur in each image must ﬁrst be mapped into discrete visual words.\n17 Note that the computation of feature covariances from matched feature points is much more sensible than simply\nperforming a PCA on the descriptor space (Winder and Brown 2007). This corresponds roughly to the within-class\nscatter matrix (14.17) we studied in Section 14.2.1.\n14.3 Instance recognition\n689\n(a)\n(b)\nFigure 14.29 Matching based on visual words (Sivic and Zisserman 2009) c⃝2009 IEEE.\n(a) Features in the query region on the left are matched to corresponding features in a highly\nranked video frame. (b) Results after removing the stop words and ﬁltering the results using\nspatial consistency.\nSivic and Zisserman (2003) perform this mapping using k-means clustering, while some of\nnewer methods discussed below (Nist´er and Stew´enius 2006; Philbin, Chum, Isard et al.\n2007) use alternative techniques, such as vocabulary trees or randomized forests. To keep the\nclustering time manageable, only a few hundred video frames are used to learn the cluster\ncenters, which still involves estimating several thousand clusters from about 300,000 descrip-\ntors. At visual query time, each feature in a new query region (e.g., Figure 14.28a, which is\na cropped region from a larger video frame) is mapped to its corresponding visual word. To\nkeep very common patterns from contaminating the results, a stop list of the most common\nvisual words is created and such words are dropped from further consideration.\nOnce a query image or region has been mapped into its constituent visual words, likely\nmatching images or video frames must then be retrieved from the database. Information\nretrieval systems do this by matching word distributions (term frequencies) nid/nd between\nthe query and target documents, where nid is how many times word i occurs in document d,\nand nd is the total number of words in document d. In order to downweight words that occur\nfrequently and to focus the search on rarer (and hence, more informative) terms, an inverse\ndocument frequency weighting log N/Ni is applied, where Ni is the number of documents\ncontaining word i, and N is the total number of documents in the database. The combination\nof these two factors results in the term frequency-inverse document frequency (tf-idf) measure,\nti = nid\nnd\nlog N\nNi\n.\n(14.33)\nAt match time, each document (or query region) is represented by its tf-idf vector,\nt = (t1, . . . , ti, . . . tm).\n(14.34)\nThe similarity between two documents is measured by the dot product between their corre-\nsponding normalized vectors ˆt = t/∥t∥, which means that their dissimilarity is proportional\nto their Euclidean distance. In their journal paper, Sivic and Zisserman (2009) compare this\n690\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Vocabulary construction (off-line)\n(a) Extract afﬁne covariant regions from each database image.\n(b) Compute descriptors and optionally whiten them to make Euclidean dis-\ntances meaningful (Sivic and Zisserman 2009).\n(c) Cluster the descriptors into visual words, either using k-means (Sivic and\nZisserman 2009), hierarchical clustering (Nist´er and Stew´enius 2006), or\nrandomized k-d trees (Philbin, Chum, Isard et al. 2007).\n(d) Decide which words are too common and put them in the stop list.\n2. Database construction (off-line)\n(a) Compute term frequencies for the visual word in each image, document fre-\nquencies for each word, and normalized tf-idf vectors for each document.\n(b) Compute inverted indices from visual words to images (with word counts).\n3. Image retrieval (on-line)\n(a) Extract regions, descriptors, and visual words, and compute a tf-idf vector\nfor the query image or region.\n(b) Retrieve the top image candidates, either by exhaustively comparing sparse\ntf-idf vectors (Sivic and Zisserman 2009) or by using inverted indices to ex-\namine only a subset of the images (Nist´er and Stew´enius 2006).\n(c) Optionally re-rank or verify all the candidate matches, using either spatial\nconsistency (Sivic and Zisserman 2009) or an afﬁne (or simpler) transforma-\ntion model (Philbin, Chum, Isard et al. 2007).\n(d) Optionally expand the answer set by re-submitting highly ranked matches as\nnew queries (Chum, Philbin, Sivic et al. 2007).\nAlgorithm 14.2 Image retrieval using visual words (Sivic and Zisserman 2009; Nist´er and\nStew´enius 2006; Philbin, Chum, Isard et al. 2007; Chum, Philbin, Sivic et al. 2007; Philbin,\nChum, Sivic et al. 2008).",
  "image_path": "page_711.jpg",
  "pages": [
    710,
    711,
    712
  ]
}