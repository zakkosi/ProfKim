{
  "doc_id": "pages_186_188",
  "text": "164\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTransformation\nMatrix\n# DoF\nPreserves\nIcon\ntranslation\nh\nI\nt\ni\n2×3\n2\norientation\nrigid (Euclidean)\nh\nR\nt\ni\n2×3\n3\nlengths\n\u001a\n\u001a\n\u001a\u001a\nS\nS S\nS\nsimilarity\nh\nsR\nt\ni\n2×3\n4\nangles\n\u001a\n\u001a\nS S\nafﬁne\nh\nA\ni\n2×3\n6\nparallelism\n\u0002\u0002\n\u0002\u0002\nprojective\nh\n˜\nH\ni\n3×3\n8\nstraight lines\n``\n  \nTable 3.5 Hierarchy of 2D coordinate transformations. Each transformation also preserves\nthe properties listed in the rows below it, i.e., similarity preserves not only angles but also\nparallelism and straight lines. The 2×3 matrices are extended with a third [0T 1] row to form\na full 3 × 3 matrix for homogeneous coordinate transformations.\namples of such transformations, which are based on the 2D geometric transformations shown\nin Figure 2.4. The formulas for these transformations were originally given in Table 2.1 and\nare reproduced here in Table 3.5 for ease of reference.\nIn general, given a transformation speciﬁed by a formula x′ = h(x) and a source image\nf(x), how do we compute the values of the pixels in the new image g(x), as given in (3.88)?\nThink about this for a minute before proceeding and see if you can ﬁgure it out.\nIf you are like most people, you will come up with an algorithm that looks something like\nAlgorithm 3.1. This process is called forward warping or forward mapping and is shown in\nFigure 3.46a. Can you think of any problems with this approach?\nprocedure forwardWarp(f, h, out g):\nFor every pixel x in f(x)\n1. Compute the destination location x′ = h(x).\n2. Copy the pixel f(x) to g(x′).\nAlgorithm 3.1\nForward warping algorithm for transforming an image f(x) into an image\ng(x′) through the parametric transform x′ = h(x).\n3.6 Geometric transformations\n165\nf(x)\ng(x’)\nx\nx’\nx’=h(x)\nf(x)\ng(x’)\nx\nx’\nx’=h(x)\n(a)\n(b)\nFigure 3.46\nForward warping algorithm: (a) a pixel f(x) is copied to its corresponding\nlocation x′ = h(x) in image g(x′); (b) detail of the source and destination pixel locations.\nIn fact, this approach suffers from several limitations. The process of copying a pixel\nf(x) to a location x′ in g is not well deﬁned when x′ has a non-integer value. What do we\ndo in such a case? What would you do?\nYou can round the value of x′ to the nearest integer coordinate and copy the pixel there,\nbut the resulting image has severe aliasing and pixels that jump around a lot when animating\nthe transformation. You can also “distribute” the value among its four nearest neighbors in\na weighted (bilinear) fashion, keeping track of the per-pixel weights and normalizing at the\nend. This technique is called splatting and is sometimes used for volume rendering in the\ngraphics community (Levoy and Whitted 1985; Levoy 1988; Westover 1989; Rusinkiewicz\nand Levoy 2000).\nUnfortunately, it suffers from both moderate amounts of aliasing and a\nfair amount of blur (loss of high-resolution detail).\nThe second major problem with forward warping is the appearance of cracks and holes,\nespecially when magnifying an image. Filling such holes with their nearby neighbors can\nlead to further aliasing and blurring.\nWhat can we do instead? A preferable solution is to use inverse warping (Algorithm 3.2),\nwhere each pixel in the destination image g(x′) is sampled from the original image f(x)\n(Figure 3.47).\nHow does this differ from the forward warping algorithm? For one thing, since ˆh(x′)\nis (presumably) deﬁned for all pixels in g(x′), we no longer have holes. More importantly,\nresampling an image at non-integer locations is a well-studied problem (general image inter-\npolation, see Section 3.5.2) and high-quality ﬁlters that control aliasing can be used.\nWhere does the function ˆh(x′) come from? Quite often, it can simply be computed as the\ninverse of h(x). In fact, all of the parametric transforms listed in Table 3.5 have closed form\nsolutions for the inverse transform: simply take the inverse of the 3 × 3 matrix specifying the\ntransform.\nIn other cases, it is preferable to formulate the problem of image warping as that of re-\nsampling a source image f(x) given a mapping x = ˆh(x′) from destination pixels x′ to\nsource pixels x. For example, in optical ﬂow (Section 8.4), we estimate the ﬂow ﬁeld as the\n166\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprocedure inverseWarp(f, h, out g):\nFor every pixel x′ in g(x′)\n1. Compute the source location x = ˆh(x′)\n2. Resample f(x) at location x and copy to g(x′)\nAlgorithm 3.2 Inverse warping algorithm for creating an image g(x′) from an image f(x)\nusing the parametric transform x′ = h(x).\nf(x)\ng(x’)\nx\nx’\nx=h(x’)\n^ \nf(x)\ng(x’)\nx\nx’\nx=h(x’)\n^ \n(a)\n(b)\nFigure 3.47 Inverse warping algorithm: (a) a pixel g(x′) is sampled from its corresponding\nlocation x = ˆh(x′) in image f(x); (b) detail of the source and destination pixel locations.\nlocation of the source pixel which produced the current pixel whose ﬂow is being estimated,\nas opposed to computing the destination pixel to which it is going. Similarly, when correcting\nfor radial distortion (Section 2.1.6), we calibrate the lens by computing for each pixel in the\nﬁnal (undistorted) image the corresponding pixel location in the original (distorted) image.\nWhat kinds of interpolation ﬁlter are suitable for the resampling process? Any of the ﬁl-\nters we studied in Section 3.5.2 can be used, including nearest neighbor, bilinear, bicubic, and\nwindowed sinc functions. While bilinear is often used for speed (e.g., inside the inner loop\nof a patch-tracking algorithm, see Section 8.1.3), bicubic, and windowed sinc are preferable\nwhere visual quality is important.\nTo compute the value of f(x) at a non-integer location x, we simply apply our usual FIR\nresampling ﬁlter,\ng(x, y) =\nX\nk,l\nf(k, l)h(x −k, y −l),\n(3.89)\nwhere (x, y) are the sub-pixel coordinate values and h(x, y) is some interpolating or smooth-\ning kernel. Recall from Section 3.5.2 that when decimation is being performed, the smoothing\nkernel is stretched and re-scaled according to the downsampling rate r.\nUnfortunately, for a general (non-zoom) image transformation, the resampling rate r is\nnot well deﬁned. Consider a transformation that stretches the x dimensions while squashing",
  "image_path": "page_187.jpg",
  "pages": [
    186,
    187,
    188
  ]
}