{
  "doc_id": "pages_077_079",
  "text": "2.1 Geometric primitives and transformations\n55\nC\n(xs,ys,d)\nZ\nimage plane\nd=1.0 d=0.67 d=0.5\nd = inverse depth\n(xw,yw,zw)\nd\nz\nC\n(xs,ys,d)\nZ\nimage plane\nd=0.5\nd=0\nd=-0.25\nd = projective depth\n(xw,yw,zw)\nz\nplane\nparallax\nFigure 2.11 Regular disparity (inverse depth) and projective depth (parallax from a reference\nplane).\nPlane plus parallax (projective depth)\nIn general, when using the 4 × 4 matrix ˜\nP , we have the freedom to remap the last row to\nwhatever suits our purpose (rather than just being the “standard” interpretation of disparity as\ninverse depth). Let us re-write the last row of ˜\nP as p3 = s3[ˆn0|c0], where ∥ˆn0∥= 1. We\nthen have the equation\nd = s3\nz (ˆn0 · pw + c0),\n(2.66)\nwhere z = p2 · ¯pw = rz · (pw −c) is the distance of pw from the camera center C (2.25)\nalong the optical axis Z (Figure 2.11). Thus, we can interpret d as the projective disparity\nor projective depth of a 3D scene point pw from the reference plane ˆn0 · pw + c0 = 0\n(Szeliski and Coughlan 1997; Szeliski and Golland 1999; Shade, Gortler, He et al. 1998;\nBaker, Szeliski, and Anandan 1998). (The projective depth is also sometimes called parallax\nin reconstruction algorithms that use the term plane plus parallax (Kumar, Anandan, and\nHanna 1994; Sawhney 1994).) Setting ˆn0 = 0 and c0 = 1, i.e., putting the reference plane\nat inﬁnity, results in the more standard d = 1/z version of disparity (Okutomi and Kanade\n1993).\nAnother way to see this is to invert the ˜\nP matrix so that we can map pixels plus disparity\ndirectly back to 3D points,\n˜pw = ˜\nP\n−1xs.\n(2.67)\nIn general, we can choose ˜\nP to have whatever form is convenient, i.e., to sample space us-\ning an arbitrary projection. This can come in particularly handy when setting up multi-view\nstereo reconstruction algorithms, since it allows us to sweep a series of planes (Section 11.1.2)\nthrough space with a variable (projective) sampling that best matches the sensed image mo-\ntions (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).\n56\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z,1)\nx1 = (x1,y1,1,d1)\nx0 = (x0,y0,1,d0)\n~\n~\nM10\nn0·p+c0= 0\nx1 = (x1,y1,1)\nx0 = (x0,y0,1)\n~\n~\nH10\n^\n.\n(a)\n(b)\nFigure 2.12 A point is projected into two images: (a) relationship between the 3D point co-\nordinate (X, Y, Z, 1) and the 2D projected point (x, y, 1, d); (b) planar homography induced\nby points all lying on a common plane ˆn0 · p + c0 = 0.\nMapping from one camera to another\nWhat happens when we take two images of a 3D scene from different camera positions or\norientations (Figure 2.12a)? Using the full rank 4 × 4 camera matrix ˜\nP = ˜\nKE from (2.64),\nwe can write the projection from world to screen coordinates as\n˜x0 ∼˜\nK0E0p = ˜\nP 0p.\n(2.68)\nAssuming that we know the z-buffer or disparity value d0 for a pixel in one image, we can\ncompute the 3D point location p using\np ∼E−1\n0\n˜\nK\n−1\n0 ˜x0\n(2.69)\nand then project it into another image yielding\n˜x1 ∼˜\nK1E1p = ˜\nK1E1E−1\n0\n˜\nK\n−1\n0 ˜x0 = ˜\nP 1 ˜\nP\n−1\n0 ˜x0 = M 10˜x0.\n(2.70)\nUnfortunately, we do not usually have access to the depth coordinates of pixels in a regular\nphotographic image. However, for a planar scene, as discussed above in (2.66), we can\nreplace the last row of P 0 in (2.64) with a general plane equation, ˆn0 · p + c0 that maps\npoints on the plane to d0 = 0 values (Figure 2.12b). Thus, if we set d0 = 0, we can ignore\nthe last column of M 10 in (2.70) and also its last row, since we do not care about the ﬁnal\nz-buffer depth. The mapping equation (2.70) thus reduces to\n˜x1 ∼˜\nH10˜x0,\n(2.71)\nwhere ˜\nH10 is a general 3 × 3 homography matrix and ˜x1 and ˜x0 are now 2D homogeneous\ncoordinates (i.e., 3-vectors) (Szeliski 1996).This justiﬁes the use of the 8-parameter homog-\nraphy as a general alignment model for mosaics of planar scenes (Mann and Picard 1994;\nSzeliski 1996).\n2.1 Geometric primitives and transformations\n57\nThe other special case where we do not need to know depth to perform inter-camera\nmapping is when the camera is undergoing pure rotation (Section 9.1.3), i.e., when t0 = t1.\nIn this case, we can write\n˜x1 ∼K1R1R−1\n0 K−1\n0 ˜x0 = K1R10K−1\n0 ˜x0,\n(2.72)\nwhich again can be represented with a 3 × 3 homography. If we assume that the calibration\nmatrices have known aspect ratios and centers of projection (2.59), this homography can be\nparameterized by the rotation amount and the two unknown focal lengths. This particular\nformulation is commonly used in image-stitching applications (Section 9.1.3).\nObject-centered projection\nWhen working with long focal length lenses, it often becomes difﬁcult to reliably estimate\nthe focal length from image measurements alone. This is because the focal length and the\ndistance to the object are highly correlated and it becomes difﬁcult to tease these two effects\napart. For example, the change in scale of an object viewed through a zoom telephoto lens\ncan either be due to a zoom change or a motion towards the user. (This effect was put to\ndramatic use in some of Alfred Hitchcock’s ﬁlm Vertigo, where the simultaneous change of\nzoom and camera motion produces a disquieting effect.)\nThis ambiguity becomes clearer if we write out the projection equation corresponding to\nthe simple calibration matrix K (2.59),\nxs\n=\nf rx · p + tx\nrz · p + tz\n+ cx\n(2.73)\nys\n=\nf ry · p + ty\nrz · p + tz\n+ cy,\n(2.74)\nwhere rx, ry, and rz are the three rows of R. If the distance to the object center tz ≫∥p∥\n(the size of the object), the denominator is approximately tz and the overall scale of the\nprojected object depends on the ratio of f to tz. It therefore becomes difﬁcult to disentangle\nthese two quantities.\nTo see this more clearly, let ηz = t−1\nz\nand s = ηzf. We can then re-write the above\nequations as\nxs\n=\ns rx · p + tx\n1 + ηzrz · p + cx\n(2.75)\nys\n=\ns ry · p + ty\n1 + ηzrz · p + cy\n(2.76)\n(Szeliski and Kang 1994; Pighin, Hecker, Lischinski et al. 1998). The scale of the projection\ns can be reliably estimated if we are looking at a known object (i.e., the 3D coordinates p",
  "image_path": "page_078.jpg",
  "pages": [
    77,
    78,
    79
  ]
}