{
  "doc_id": "pages_023_025",
  "text": "Chapter 1\nIntroduction\n1.1\nWhat is computer vision? . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nA brief history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n1.3\nBook overview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.4\nSample syllabus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n1.5\nA note on notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n1.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nFigure 1.1\nThe human visual system has no problem interpreting the subtle variations in\ntranslucency and shading in this photograph and correctly segmenting the object from its\nbackground.\n2\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 1.2\nSome examples of computer vision algorithms and applications. (a) Structure\nfrom motion algorithms can reconstruct a sparse 3D point model of a large complex scene\nfrom hundreds of partially overlapping photographs (Snavely, Seitz, and Szeliski 2006) c⃝\n2006 ACM. (b) Stereo matching algorithms can build a detailed 3D model of a building fac¸ade\nfrom hundreds of differently exposed photographs taken from the Internet (Goesele, Snavely,\nCurless et al. 2007) c⃝2007 IEEE. (c) Person tracking algorithms can track a person walking\nin front of a cluttered background (Sidenbladh, Black, and Fleet 2000) c⃝2000 Springer. (d)\nFace detection algorithms, coupled with color-based clothing and hair detection algorithms,\ncan locate and recognize the individuals in this image (Sivic, Zitnick, and Szeliski 2006) c⃝\n2006 Springer.\n1.1 What is computer vision?\n3\n1.1 What is computer vision?\nAs humans, we perceive the three-dimensional structure of the world around us with apparent\nease. Think of how vivid the three-dimensional percept is when you look at a vase of ﬂowers\nsitting on the table next to you. You can tell the shape and translucency of each petal through\nthe subtle patterns of light and shading that play across its surface and effortlessly segment\neach ﬂower from the background of the scene (Figure 1.1). Looking at a framed group por-\ntrait, you can easily count (and name) all of the people in the picture and even guess at their\nemotions from their facial appearance. Perceptual psychologists have spent decades trying to\nunderstand how the visual system works and, even though they can devise optical illusions1\nto tease apart some of its principles (Figure 1.3), a complete solution to this puzzle remains\nelusive (Marr 1982; Palmer 1999; Livingstone 2008).\nResearchers in computer vision have been developing, in parallel, mathematical tech-\nniques for recovering the three-dimensional shape and appearance of objects in imagery. We\nnow have reliable techniques for accurately computing a partial 3D model of an environment\nfrom thousands of partially overlapping photographs (Figure 1.2a). Given a large enough\nset of views of a particular object or fac¸ade, we can create accurate dense 3D surface mod-\nels using stereo matching (Figure 1.2b). We can track a person moving against a complex\nbackground (Figure 1.2c). We can even, with moderate success, attempt to ﬁnd and name\nall of the people in a photograph using a combination of face, clothing, and hair detection\nand recognition (Figure 1.2d). However, despite all of these advances, the dream of having a\ncomputer interpret an image at the same level as a two-year old (for example, counting all of\nthe animals in a picture) remains elusive.\nWhy is vision so difﬁcult? In part, it is because\nvision is an inverse problem, in which we seek to recover some unknowns given insufﬁcient\ninformation to fully specify the solution. We must therefore resort to physics-based and prob-\nabilistic models to disambiguate between potential solutions. However, modeling the visual\nworld in all of its rich complexity is far more difﬁcult than, say, modeling the vocal tract that\nproduces spoken sounds.\nThe forward models that we use in computer vision are usually developed in physics (ra-\ndiometry, optics, and sensor design) and in computer graphics. Both of these ﬁelds model\nhow objects move and animate, how light reﬂects off their surfaces, is scattered by the at-\nmosphere, refracted through camera lenses (or human eyes), and ﬁnally projected onto a ﬂat\n(or curved) image plane. While computer graphics are not yet perfect (no fully computer-\nanimated movie with human characters has yet succeeded at crossing the uncanny valley2\nthat separates real humans from android robots and computer-animated humans), in limited\n1 http://www.michaelbach.de/ot/sze muelue\n2 The term uncanny valley was originally coined by roboticist Masahiro Mori as applied to robotics (Mori 1970).\nIt is also commonly applied to computer-animated ﬁlms such as Final Fantasy and Polar Express (Geller 2008).",
  "image_path": "page_024.jpg",
  "pages": [
    23,
    24,
    25
  ]
}