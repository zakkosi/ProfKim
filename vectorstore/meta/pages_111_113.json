{
  "doc_id": "pages_111_113",
  "text": "2.3 The digital camera\n89\nsince it only roughly mimics true luminance) that would be comparable to the regular black-\nand-white TV signal, along with two lower frequency chroma channels.\nIn both systems, the Y signal (or more appropriately, the Y’ luma signal since it is gamma\ncompressed) is obtained from\nY ′\n601 = 0.299R′ + 0.587G′ + 0.114B′,\n(2.112)\nwhere R’G’B’ is the triplet of gamma-compressed color components. When using the newer\ncolor deﬁnitions for HDTV in BT.709, the formula is\nY ′\n709 = 0.2125R′ + 0.7154G′ + 0.0721B′.\n(2.113)\nThe UV components are derived from scaled versions of (B′−Y ′) and (R′−Y ′), namely,\nU = 0.492111(B′ −Y ′) and V = 0.877283(R′ −Y ′),\n(2.114)\nwhereas the IQ components are the UV components rotated through an angle of 33◦. In\ncomposite (NTSC and PAL) video, the chroma signals were then low-pass ﬁltered horizon-\ntally before being modulated and superimposed on top of the Y’ luma signal. Backward\ncompatibility was achieved by having older black-and-white TV sets effectively ignore the\nhigh-frequency chroma signal (because of slow electronics) or, at worst, superimposing it as\na high-frequency pattern on top of the main signal.\nWhile these conversions were important in the early days of computer vision, when frame\ngrabbers would directly digitize the composite TV signal, today all digital video and still\nimage compression standards are based on the newer YCbCr conversion. YCbCr is closely\nrelated to YUV (the Cb and Cr signals carry the blue and red color difference signals and have\nmore useful mnemonics than UV) but uses different scale factors to ﬁt within the eight-bit\nrange available with digital signals.\nFor video, the Y’ signal is re-scaled to ﬁt within the [16 . . . 235] range of values, while\nthe Cb and Cr signals are scaled to ﬁt within [16 . . . 240] (Gomes and Velho 1997; Fairchild\n2005). For still images, the JPEG standard uses the full eight-bit range with no reserved\nvalues,\n\n\nY ′\nCb\nCr\n\n=\n\n\n0.299\n0.587\n0.114\n−0.168736\n−0.331264\n0.5\n0.5\n−0.418688\n−0.081312\n\n\n\n\nR′\nG′\nB′\n\n+\n\n\n0\n128\n128\n\n,\n(2.115)\nwhere the R’G’B’ values are the eight-bit gamma-compressed color components (i.e., the\nactual RGB values we obtain when we open up or display a JPEG image). For most appli-\ncations, this formula is not that important, since your image reading software will directly\n90\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprovide you with the eight-bit gamma-compressed R’G’B’ values. However, if you are trying\nto do careful image deblocking (Exercise 3.30), this information may be useful.\nAnother color space you may come across is hue, saturation, value (HSV), which is a pro-\njection of the RGB color cube onto a non-linear chroma angle, a radial saturation percentage,\nand a luminance-inspired value. In more detail, value is deﬁned as either the mean or maxi-\nmum color value, saturation is deﬁned as scaled distance from the diagonal, and hue is deﬁned\nas the direction around a color wheel (the exact formulas are described by Hall (1989); Foley,\nvan Dam, Feiner et al. (1995)). Such a decomposition is quite natural in graphics applications\nsuch as color picking (it approximates the Munsell chart for color description). Figure 2.32l–\nn shows an HSV representation of a sample color image, where saturation is encoded using a\ngray scale (saturated = darker) and hue is depicted as a color.\nIf you want your computer vision algorithm to only affect the value (luminance) of an\nimage and not its saturation or hue, a simpler solution is to use either the Y xy (luminance +\nchromaticity) coordinates deﬁned in (2.104) or the even simpler color ratios,\nr =\nR\nR + G + B , g =\nG\nR + G + B , b =\nB\nR + G + B\n(2.116)\n(Figure 2.32e–h). After manipulating the luma (2.112), e.g., through the process of histogram\nequalization (Section 3.1.4), you can multiply each color ratio by the ratio of the new to old\nluma to obtain an adjusted RGB triplet.\nWhile all of these color systems may sound confusing, in the end, it often may not mat-\nter that much which one you use. Poynton, in his Color FAQ, http://www.poynton.com/\nColorFAQ.html, notes that the perceptually motivated L*a*b* system is qualitatively similar\nto the gamma-compressed R’G’B’ system we mostly deal with, since both have a fractional\npower scaling (which approximates a logarithmic response) between the actual intensity val-\nues and the numbers being manipulated. As in all cases, think carefully about what you are\ntrying to accomplish before deciding on a technique to use.24\n2.3.3 Compression\nThe last stage in a camera’s processing pipeline is usually some form of image compression\n(unless you are using a lossless compression scheme such as camera RAW or PNG).\nAll color video and image compression algorithms start by converting the signal into\nYCbCr (or some closely related variant), so that they can compress the luminance signal with\nhigher ﬁdelity than the chrominance signal. (Recall that the human visual system has poorer\n24 If you are at a loss for questions at a conference, you can always ask why the speaker did not use a perceptual\ncolor space, such as L*a*b*. Conversely, if they did use L*a*b*, you can ask if they have any concrete evidence that\nthis works better than regular colors.\n2.3 The digital camera\n91\n(a) RGB\n(b) R\n(c) G\n(d) B\n(e) rgb\n(f) r\n(g) g\n(h) b\n(i) L*\n(j) a*\n(k) b*\n(l) H\n(m) S\n(n) V\nFigure 2.32 Color space transformations: (a–d) RGB; (e–h) rgb. (i–k) L*a*b*; (l–n) HSV.\nNote that the rgb, L*a*b*, and HSV values are all re-scaled to ﬁt the dynamic range of the\nprinted page.\nfrequency response to color than to luminance changes.) In video, it is common to subsam-\nple Cb and Cr by a factor of two horizontally; with still images (JPEG), the subsampling\n(averaging) occurs both horizontally and vertically.\nOnce the luminance and chrominance images have been appropriately subsampled and\nseparated into individual images, they are then passed to a block transform stage. The most\ncommon technique used here is the discrete cosine transform (DCT), which is a real-valued\nvariant of the discrete Fourier transform (DFT) (see Section 3.4.3). The DCT is a reasonable\napproximation to the Karhunen–Lo`eve or eigenvalue decomposition of natural image patches,\ni.e., the decomposition that simultaneously packs the most energy into the ﬁrst coefﬁcients\nand diagonalizes the joint covariance matrix among the pixels (makes transform coefﬁcients",
  "image_path": "page_112.jpg",
  "pages": [
    111,
    112,
    113
  ]
}