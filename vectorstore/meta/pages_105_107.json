{
  "doc_id": "pages_105_107",
  "text": "2.3 The digital camera\n83\nFigure 2.29 CIE chromaticity diagram, showing colors and their corresponding (x, y) val-\nues. Pure spectral colors are arranged around the outside of the curve.\nfraction, so that the second row adds up to one, i.e., the RGB triplet (1, 1, 1) maps to a Y value\nof 1. Linearly blending the (¯r(λ), ¯g(λ),¯b(λ)) curves in Figure 2.28a according to (2.103), we\nobtain the resulting (¯x(λ), ¯y(λ), ¯z(λ)) curves shown in Figure 2.28b. Notice how all three\nspectra (color matching functions) now have only positive values and how the ¯y(λ) curve\nmatches that of the luminance perceived by humans.\nIf we divide the XYZ values by the sum of X+Y+Z, we obtain the chromaticity coordi-\nnates\nx =\nX\nX + Y + Z , y =\nY\nX + Y + Z , z =\nZ\nX + Y + Z ,\n(2.104)\nwhich sum up to 1. The chromaticity coordinates discard the absolute intensity of a given\ncolor sample and just represent its pure color. If we sweep the monochromatic color λ pa-\nrameter in Figure 2.28b from λ = 380nm to λ = 800nm, we obtain the familiar chromaticity\ndiagram shown in Figure 2.29. This ﬁgure shows the (x, y) value for every color value per-\nceivable by most humans. (Of course, the CMYK reproduction process in this book does not\nactually span the whole gamut of perceivable colors.) The outer curved rim represents where\nall of the pure monochromatic color values map in (x, y) space, while the lower straight line,\nwhich connects the two endpoints, is known as the purple line.\nA convenient representation for color values, when we want to tease apart luminance\nand chromaticity, is therefore Yxy (luminance plus the two most distinctive chrominance\ncomponents).\nL*a*b* color space\nWhile the XYZ color space has many convenient properties, including the ability to separate\nluminance from chrominance, it does not actually predict how well humans perceive differ-\nences in color or luminance.\n84\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBecause the response of the human visual system is roughly logarithmic (we can perceive\nrelative luminance differences of about 1%), the CIE deﬁned a non-linear re-mapping of the\nXYZ space called L*a*b* (also sometimes called CIELAB), where differences in luminance\nor chrominance are more perceptually uniform.19\nThe L* component of lightness is deﬁned as\nL∗= 116f\n\u0012 Y\nYn\n\u0013\n,\n(2.105)\nwhere Yn is the luminance value for nominal white (Fairchild 2005) and\nf(t) =\n(\nt1/3\nt > δ3\nt/(3δ2) + 2δ/3\nelse,\n(2.106)\nis a ﬁnite-slope approximation to the cube root with δ = 6/29. The resulting 0 . . . 100 scale\nroughly measures equal amounts of lightness perceptibility.\nIn a similar fashion, the a* and b* components are deﬁned as\na∗= 500\n\u0014\nf\n\u0012 X\nXn\n\u0013\n−f\n\u0012 Y\nYn\n\u0013\u0015\nand b∗= 200\n\u0014\nf\n\u0012 Y\nYn\n\u0013\n−f\n\u0012 Z\nZn\n\u0013\u0015\n,\n(2.107)\nwhere again, (Xn, Yn, Zn) is the measured white point. Figure 2.32i–k show the L*a*b*\nrepresentation for a sample color image.\nColor cameras\nWhile the preceding discussion tells us how we can uniquely describe the perceived tri-\nstimulus description of any color (spectral distribution), it does not tell us how RGB still\nand video cameras actually work. Do they just measure the amount of light at the nominal\nwavelengths of red (700.0nm), green (546.1nm), and blue (435.8nm)? Do color monitors just\nemit exactly these wavelengths and, if so, how can they emit negative red light to reproduce\ncolors in the cyan range?\nIn fact, the design of RGB video cameras has historically been based around the availabil-\nity of colored phosphors that go into television sets. When standard-deﬁnition color television\nwas invented (NTSC), a mapping was deﬁned between the RGB values that would drive the\nthree color guns in the cathode ray tube (CRT) and the XYZ values that unambiguously de-\nﬁne perceived color (this standard was called ITU-R BT.601). With the advent of HDTV and\nnewer monitors, a new standard called ITU-R BT.709 was created, which speciﬁes the XYZ\n19 Another perceptually motivated color space called L*u*v* was developed and standardized simultaneously\n(Fairchild 2005).\n2.3 The digital camera\n85\nvalues of each of the color primaries,\n\n\nX\nY\nZ\n\n=\n\n\n0.412453\n0.357580\n0.180423\n0.212671\n0.715160\n0.072169\n0.019334\n0.119193\n0.950227\n\n\n\n\nR709\nG709\nB709\n\n.\n(2.108)\nIn practice, each color camera integrates light according to the spectral response function\nof its red, green, and blue sensors,\nR\n=\nZ\nL(λ)SR(λ)dλ,\nG\n=\nZ\nL(λ)SG(λ)dλ,\n(2.109)\nB\n=\nZ\nL(λ)SB(λ)dλ,\nwhere L(λ) is the incoming spectrum of light at a given pixel and {SR(λ), SG(λ), SB(λ)}\nare the red, green, and blue spectral sensitivities of the corresponding sensors.\nCan we tell what spectral sensitivities the cameras actually have? Unless the camera\nmanufacturer provides us with this data or we observe the response of the camera to a whole\nspectrum of monochromatic lights, these sensitivities are not speciﬁed by a standard such as\nBT.709. Instead, all that matters is that the tri-stimulus values for a given color produce the\nspeciﬁed RGB values. The manufacturer is free to use sensors with sensitivities that do not\nmatch the standard XYZ deﬁnitions, so long as they can later be converted (through a linear\ntransform) to the standard colors.\nSimilarly, while TV and computer monitors are supposed to produce RGB values as spec-\niﬁed by Equation (2.108), there is no reason that they cannot use digital logic to transform the\nincoming RGB values into different signals to drive each of the color channels. Properly cal-\nibrated monitors make this information available to software applications that perform color\nmanagement, so that colors in real life, on the screen, and on the printer all match as closely\nas possible.\nColor ﬁlter arrays\nWhile early color TV cameras used three vidicons (tubes) to perform their sensing and later\ncameras used three separate RGB sensing chips, most of today’s digital still and video cam-\neras cameras use a color ﬁlter array (CFA), where alternating sensors are covered by different\ncolored ﬁlters.20\n20 A newer chip design by Foveon (http://www.foveon.com) stacks the red, green, and blue sensors beneath each\nother, but it has not yet gained widespread adoption.",
  "image_path": "page_106.jpg",
  "pages": [
    105,
    106,
    107
  ]
}