{
  "doc_id": "pages_765_767",
  "text": "A.2 Linear least squares\n743\nNote that the function being ﬁtted need not itself be linear to use linear least squares. All that\nis required is that the function be linear in the unknown parameters. For example, polynomial\nﬁtting can be written as\nEPLS =\nX\ni\n|yi −(\np\nX\nj=0\najxj\ni)|2,\n(A.26)\nwhile sinusoid ﬁtting with unknown amplitude A and phase φ (but known frequency f) can\nbe written as\nESLS =\nX\ni\n|yi −A sin(2πfxi +φ)|2 =\nX\ni\n|yi −(B sin 2πfxi +C cos 2πfxi)|2, (A.27)\nwhich is linear in (B, C).\nIn general, it is more common to denote the unknown parameters using x and to write the\ngeneral form of linear least squares as5\nELLS =\nX\ni\n|aix −bi|2 = ∥Ax −b∥2.\n(A.28)\nExpanding the above equation gives us\nELLS = xT (AT A)x −2xT (AT b) + ∥b∥2,\n(A.29)\nwhose minimum value for x can be found by solving the associated normal equations (Bj¨orck\n1996; Golub and Van Loan 1996)\n(AT A)x = AT b.\n(A.30)\nThe preferred way to solve the normal equations is to use Cholesky factorization. Let\nC = AT A = RT R,\n(A.31)\nwhere R is the upper-triangular Cholesky factor of the Hessian C, and\nd = AT b.\n(A.32)\nAfter factorization, the solution for x can be obtained as\nRT z = d,\nRx = z,\n(A.33)\nwhich involves the solution of two triangular systems, i.e., forward and backward substitution\n(Bj¨orck 1996).\n5 Be extra careful in interpreting the variable names here. In the 2D line-ﬁtting example, x is used to denote the\nhorizontal axis, but in the general least squares problem, x = (m, b) denotes the unknown parameter vector.\n744\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx\ny\nb\nm\ny=mx+b\n×\n×\n×\n×\nx\ny\nax+by+c=0\n×\n×\n×\n×\n(a)\n(b)\nFigure A.2 Least squares regression. (a) The line y = mx + b is ﬁt to the four noisy data\npoints, {(xi, yi)}, denoted by × by minimizing the squared vertical residuals between the\ndata points and the line, P\ni ∥yi −(mxi + b)∥2. (b) When the measurements {(xi, yi)} are\nassumed to have noise in all directions, the sum of orthogonal squared distances to the line\nP\ni ∥axi + byi + c∥2 is minimized using total least squares.\nIn cases where the least squares problem is numerically poorly conditioned (which should\ngenerally be avoided by adding sufﬁcient regularization or prior knowledge about the param-\neters, (Appendix A.3)), it is possible to use QR factorization or SVD directly on the matrix\nA (Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Nocedal and Wright\n2006; Bj¨orck and Dahlquist 2010), e.g.,\nAx = QRx = b\n−→\nRx = QT b.\n(A.34)\nNote that the upper triangular matrices R produced by the Cholesky factorization of C =\nAT A and the QR factorization of A are the same, but that solving (A.34) is generally more\nstable (less sensitive to roundoff error) but slower (by a constant factor).\nA.2.1 Total least squares\nIn some problems, e.g., when performing geometric line ﬁtting in 2D images or 3D plane\nﬁtting to point cloud data, instead of having measurement error along one particular axis, the\nmeasured points have uncertainty in all directions, which is known as the errors-in-variables\nmodel (Van Huffel and Lemmerling 2002; Matei and Meer 2006). In this case, it makes more\nsense to minimize a set of homogeneous squared errors of the form\nETLS =\nX\ni\n(aix)2 = ∥Ax∥2,\n(A.35)\nwhich is known as total least squares (TLS) (Van Huffel and Vandewalle 1991; Bj¨orck 1996;\nGolub and Van Loan 1996; Van Huffel and Lemmerling 2002).\nA.2 Linear least squares\n745\nThe above error metric has a trivial minimum solution at x = 0 and is, in fact, homoge-\nneous in x. For this reason, we augment this minimization problem with the requirement that\n∥x∥2 = 1. which results in the eigenvalue problem\nx = arg min\nx xT (AT A)x\nsuch that\n∥x∥2 = 1.\n(A.36)\nThe value of x that minimizes this constrained problem is the eigenvector associated with the\nsmallest eigenvalue of AT A. This is the same as the last right singular vector of A, since\nA\n=\nUΣV ,\n(A.37)\nAT A\n=\nV Σ2V ,\n(A.38)\nAT Avk\n=\nσ2\nk,\n(A.39)\nwhich is minimized by selecting the smallest σk value.\nFigure A.2b shows a line ﬁtting problem where, in this case, the measurement errors are\nassumed to be isotropic in (x, y). The solution for the best line equation ax + by + c = 0 is\nfound by minimizing\nETLS−2D =\nX\ni\n(axi + byi + c)2,\n(A.40)\ni.e., ﬁnding the eigenvector associated with the smallest eigenvalue of6\nC = AT A =\nX\ni\n\n\nxi\nyi\n1\n\n\nh\nxi\nyi\n1\ni\n.\n(A.41)\nNotice, however, that minimizing P\ni(aix)2 in (A.35) is only statistically optimal (Ap-\npendix B.1.1) if all of the measured terms in the ai, e.g., the (xi, yi, 1) measurements, have\nequal noise. This is deﬁnitely not the case in the line-ﬁtting example of Figure A.2b (A.40),\nsince the 1 values are noise-free. To mitigate this, we ﬁrst subtract the mean x and y values\nfrom all the measured points\nˆxi\n=\nxi −¯x\n(A.42)\nˆyi\n=\nyi −¯y\n(A.43)\nand then ﬁt the 2D line equation a(x −¯x) + b(y −¯y) = 0 by minimizing\nETLS−2Dm =\nX\ni\n(aˆxi + bˆyi)2.\n(A.44)\n6 Again, be careful with the variable names here. The measurement equation is ai = (xi, yi, 1) and the unknown\nparameters are x = (a, b, c).",
  "image_path": "page_766.jpg",
  "pages": [
    765,
    766,
    767
  ]
}