{
  "doc_id": "pages_460_462",
  "text": "438\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z)\nx = (sinθ,h,cosθ)\nθ\nh\nx\ny\np = (X,Y,Z)\nx = (sinθ cosφ, sinφ,\ncosθ cosφ)\nθ\nφ\nx\ny\n(a)\n(b)\nFigure 9.7 Projection from 3D to (a) cylindrical and (b) spherical coordinates.\ndepth. The same data set can also be used to explicitly reconstruct the depth using multi-\nbaseline stereo (Peleg, Ben-Ezra, and Pritch 2001; Li, Shum, Tang et al. 2004; Zheng, Kang,\nCohen et al. 2007).\n9.1.6 Cylindrical and spherical coordinates\nAn alternative to using homographies or 3D motions to align images is to ﬁrst warp the images\ninto cylindrical coordinates and then use a pure translational model to align them (Chen 1995;\nSzeliski 1996). Unfortunately, this only works if the images are all taken with a level camera\nor with a known tilt angle.\nAssume for now that the camera is in its canonical position, i.e., its rotation matrix is the\nidentity, R = I, so that the optical axis is aligned with the z axis and the y axis is aligned\nvertically. The 3D ray corresponding to an (x, y) pixel is therefore (x, y, f).\nWe wish to project this image onto a cylindrical surface of unit radius (Szeliski 1996).\nPoints on this surface are parameterized by an angle θ and a height h, with the 3D cylindrical\ncoordinates corresponding to (θ, h) given by\n(sin θ, h, cos θ) ∝(x, y, f),\n(9.12)\nas shown in Figure 9.7a. From this correspondence, we can compute the formula for the\nwarped or mapped coordinates (Szeliski and Shum 1997),\nx′\n=\nsθ = s tan−1 x\nf ,\n(9.13)\ny′\n=\nsh = s\ny\np\nx2 + f 2 ,\n(9.14)\nwhere s is an arbitrary scaling factor (sometimes called the radius of the cylinder) that can be\nset to s = f to minimize the distortion (scaling) near the center of the image.5 The inverse of\n5 The scale can also be set to a larger or smaller value for the ﬁnal compositing surface, depending on the desired\noutput panorama resolution—see Section 9.3.\n9.1 Motion models\n439\nthis mapping equation is given by\nx\n=\nf tan θ = f tan x′\ns ,\n(9.15)\ny\n=\nh\np\nx2 + f 2 = y′\ns f\nq\n1 + tan2 x′/s = f y′\ns sec x′\ns .\n(9.16)\nImages can also be projected onto a spherical surface (Szeliski and Shum 1997), which\nis useful if the ﬁnal panorama includes a full sphere or hemisphere of views, instead of just\na cylindrical strip. In this case, the sphere is parameterized by two angles (θ, φ), with 3D\nspherical coordinates given by\n(sin θ cos φ, sin φ, cos θ cos φ) ∝(x, y, f),\n(9.17)\nas shown in Figure 9.7b.6 The correspondence between coordinates is now given by (Szeliski\nand Shum 1997):\nx′\n=\nsθ = s tan−1 x\nf ,\n(9.18)\ny′\n=\nsφ = s tan−1\ny\np\nx2 + f 2 ,\n(9.19)\nwhile the inverse is given by\nx\n=\nf tan θ = f tan x′\ns ,\n(9.20)\ny\n=\np\nx2 + f 2 tan φ = tan y′\ns f\nq\n1 + tan2 x′/s = f tan y′\ns sec x′\ns .\n(9.21)\nNote that it may be simpler to generate a scaled (x, y, z) direction from Equation (9.17)\nfollowed by a perspective division by z and a scaling by f.\nCylindrical image stitching algorithms are most commonly used when the camera is\nknown to be level and only rotating around its vertical axis (Chen 1995). Under these condi-\ntions, images at different rotations are related by a pure horizontal translation.7 This makes\nit attractive as an initial class project in an introductory computer vision course, since the\nfull complexity of the perspective alignment algorithm (Sections 6.1, 8.2, and 9.1.3) can be\navoided. Figure 9.8 shows how two cylindrically warped images from a leveled rotational\npanorama are related by a pure translation (Szeliski and Shum 1997).\nProfessional panoramic photographers often use pan-tilt heads that make it easy to control\nthe tilt and to stop at speciﬁc detents in the rotation angle. Motorized rotation heads are also\n6 Note that these are not the usual spherical coordinates, ﬁrst presented in Equation (2.8). Here, the y axis points\nat the north pole instead of the z axis, since we are used to viewing images taken horizontally, i.e., with the y axis\npointing in the direction of the gravity vector.\n7Small vertical tilts can sometimes be compensated for with vertical translations.\n440\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 9.8 A cylindrical panorama (Szeliski and Shum 1997) c⃝1997 ACM: (a) two cylin-\ndrically warped images related by a horizontal translation; (b) part of a cylindrical panorama\ncomposited from a sequence of images.\nFigure 9.9\nA spherical panorama constructed from 54 photographs (Szeliski and Shum\n1997) c⃝1997 ACM.\nsometimes used for the acquisition of larger panoramas (Kopf, Uyttendaele, Deussen et al.\n2007).8 Not only do they ensure a uniform coverage of the visual ﬁeld with a desired amount\nof image overlap but they also make it possible to stitch the images using cylindrical or\nspherical coordinates and pure translations. In this case, pixel coordinates (x, y, f) must ﬁrst\nbe rotated using the known tilt and panning angles before being projected into cylindrical\nor spherical coordinates (Chen 1995). Having a roughly known panning angle also makes it\neasier to compute the alignment, since the rough relative positioning of all the input images is\nknown ahead of time, enabling a reduced search range for alignment. Figure 9.9 shows a full\n3D rotational panorama unwrapped onto the surface of a sphere (Szeliski and Shum 1997).\nOne ﬁnal coordinate mapping worth mentioning is the polar mapping, where the north\n8See also http://gigapan.org.",
  "image_path": "page_461.jpg",
  "pages": [
    460,
    461,
    462
  ]
}