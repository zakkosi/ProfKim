{
  "doc_id": "pages_658_660",
  "text": "636\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntransmittance (for colored glossy surfaces or glass), and B(x) is the background (environ-\nment) image, which is integrated over the area Ai(x). In follow-on work, Chuang, Zongker,\nHindorff et al. (2000) use a superposition of oriented Gaussians,\nIi =\nX\nj\nRij\nZ\nGij(x)B(x)dx,\n(13.3)\nwhere each 2D Gaussian\nGij(x) = G2D(x; cij, σij, θij)\n(13.4)\nis modeled by its center cij, unrotated widths σij = (σx\nij, σy\nij), and orientation θij.\nGiven a representation for an environment matte, how can we go about estimating it for a\nparticular object? The trick is to place the object in front of a monitor (or surrounded by a set\nof monitors), where we can change the illumination patterns B(x) and observe the value of\neach composite pixel Ci.7\nAs with traditional two-screen matting (Section 10.4.1), we can use a variety of solid\ncolored backgrounds to estimate each pixel’s foreground color αiFi and partial coverage\n(opacity) αi. To estimate the area of support Ai in (13.2), Zongker, Werner, Curless et al.\n(1999) use a series of periodic horizontal and vertical solid stripes at different frequencies and\nphases, which is reminiscent of the structured light patterns used in active rangeﬁnding (Sec-\ntion 12.2). For the more sophisticated mixture of Gaussian model (13.3), Chuang, Zongker,\nHindorff et al. (2000) sweep a series of narrow Gaussian stripes at four different orientations\n(horizontal, vertical, and two diagonals), which enables them to estimate multiple oriented\nGaussian responses at each pixel.\nOnce an environment matte has been “pulled”, it is then a simple matter to replace the\nbackground with a new image B(x) to obtain a novel composite of the object placed in a\ndifferent environment (Figure 13.10a–c). The use of multiple backgrounds during the matting\nprocess, however, precludes the use of this technique with dynamic scenes, e.g., water pouring\ninto a glass (Figure 13.10d). In this case, a single graded color background can be used to\nestimate a single 2D monochromatic displacement for each pixel (Chuang, Zongker, Hindorff\net al. 2000).\n13.4.1 Higher-dimensional light ﬁelds\nAs you can tell from the preceding discussion, an environment matte in principle maps every\npixel (x, y) into a 4D distribution over light rays and is, hence, a six-dimensional representa-\ntion. (In practice, each 2D pixel’s response is parameterized using a dozen or so parameters,\n7 If we relax the assumption that the environment is distant, the monitor can be placed at several depths to estimate\na depth-dependent mapping function (Zongker, Werner, Curless et al. 1999).\n13.4 Environment mattes\n637\nFigure 13.11\nThe geometry-image continuum in image-based rendering (Kang, Szeliski,\nand Anandan 2000) c⃝2000 IEEE. Representations at the left of the spectrum use more\ndetailed geometry and simpler image representations, while representations and algorithms\non the right use more images and less geometry.\ne.g., {F, α, B, R, A}, instead of a full mapping.) What if we want to model an object’s re-\nfractive properties from every potential point of view? In this case, we need a mapping from\nevery incoming 4D light ray to every potential exiting 4D light ray, which is an 8D represen-\ntation. If we use the same trick as with surface light ﬁelds, we can parameterize each surface\npoint by its 4D BRDF to reduce this mapping back down to 6D but this loses the ability to\nhandle multiple refractive paths.\nIf we want to handle dynamic light ﬁelds, we need to add another temporal dimension.\n(Wenger, Gardner, Tchou et al. (2005) gives a nice example of a dynamic appearance and\nillumination acquisition system.) Similarly, if we want a continuous distribution over wave-\nlengths, this becomes another dimension.\nThese examples illustrate how modeling the full complexity of a visual scene through\nsampling can be extremely expensive. Fortunately, constructing specialized models, which\nexploit knowledge about the physics of light transport along with the natural coherence of\nreal-world objects, can make these problems more tractable.\n13.4.2 The modeling to rendering continuum\nThe image-based rendering representations and algorithms we have studied in this chapter\nspan a continuum ranging from classic 3D texture-mapped models all the way to pure sampled\nray-based representations such as light ﬁelds (Figure 13.11). Representations such as view-\ndependent texture maps and Lumigraphs still use a single global geometric model, but select\nthe colors to map onto these surfaces from nearby images. View-dependent geometry, e.g.,\n638\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmultiple depth maps, sidestep the need for coherent 3D geometry, and can sometimes better\nmodel local non-rigid effects such as specular motion (Swaminathan, Kang, Szeliski et al.\n2002; Criminisi, Kang, Swaminathan et al. 2005). Sprites with depth and layered depth\nimages use image-based representations of both color and geometry and can be efﬁciently\nrendered using warping operations rather than 3D geometric rasterization.\nThe best choice of representation and rendering algorithm depends on both the quantity\nand quality of the input imagery as well as the intended application. When nearby views are\nbeing rendered, image-based representations capture more of the visual ﬁdelity of the real\nworld because they directly sample its appearance. On the other hand, if only a few input\nimages are available or the image-based models need to be manipulated, e.g., to change their\nshape or appearance, more abstract 3D representations such as geometric and local reﬂection\nmodels are a better ﬁt. As we continue to capture and manipulate increasingly larger quan-\ntities of visual data, research into these aspects of image-based modeling and rendering will\ncontinue to evolve.\n13.5 Video-based rendering\nSince multiple images can be used to render new images or interactive experiences, can some-\nthing similar be done with video? In fact, a fair amount of work has been done in the area\nof video-based rendering and video-based animation, two terms ﬁrst introduced by Sch¨odl,\nSzeliski, Salesin et al. (2000) to denote the process of generating new video sequences from\ncaptured video footage. An early example of such work is Video Rewrite (Bregler, Covell,\nand Slaney 1997), in which archival video footage is “re-animated” by having actors say new\nutterances (Figure 13.12). More recently, the term video-based rendering has been used by\nsome researchers to denote the creation of virtual camera moves from a set of synchronized\nvideo cameras placed in a studio (Magnor 2005). (The terms free-viewpoint video and 3D\nvideo are also sometimes used, see Section 13.5.4.)\nIn this section, we present a number of video-based rendering systems and applications.\nWe start with video-based animation (Section 13.5.1), in which video footage is re-arranged\nor modiﬁed, e.g., in the capture and re-rendering of facial expressions. A special case of this\nare video textures (Section 13.5.2), in which source video is automatically cut into segments\nand re-looped to create inﬁnitely long video animations. It is also possible to create such\nanimations from still pictures or paintings, by segmenting the image into separately moving\nregions and animating them using stochastic motion ﬁelds (Section 13.5.3).\nNext, we turn our attention to 3D video (Section 13.5.4), in which multiple synchronized\nvideo cameras are used to ﬁlm a scene from different directions. The source video frames can\nthen be re-combined using image-based rendering techniques, such as view interpolation, to",
  "image_path": "page_659.jpg",
  "pages": [
    658,
    659,
    660
  ]
}