{
  "doc_id": "pages_127_129",
  "text": "3.1 Point operators\n105\n(a)\n(b)\n(c)\n(d)\nFigure 3.4 Image matting and compositing (Chuang, Curless, Salesin et al. 2001) c⃝2001\nIEEE: (a) source image; (b) extracted foreground object F; (c) alpha matte α shown in\ngrayscale; (d) new composite C.\nAs discussed in Section 2.3.2, chromaticity coordinates (2.104) or even simpler color ra-\ntios (2.116) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figure\n2.32g–i shows some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 × 3 color twist transform matrix.\nExercises 2.9 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn\n1996), while the process of inserting it into another image (without visible artifacts) is called\ncompositing (Porter and Duff 1984; Blinn 1994a).\nThe intermediate representation used for the foreground object between these two stages\nis called an alpha-matted color image (Figure 3.4b–c). In addition to the three color RGB\nchannels, an alpha-matted image contains a fourth alpha channel α (or A) that describes the\nrelative amount of opacity or fractional coverage at each pixel (Figures 3.4c and 3.5b). The\nopacity is the opposite of the transparency. Pixels within the object are fully opaque (α = 1),\nwhile pixels fully outside the object are transparent (α = 0). Pixels on the boundary of the\nobject vary smoothly between these two extremes, which hides the perceptual visible jaggies\n106\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n×\n(1−\n)\n+\n=\nB\nα\nαF\nC\n(a)\n(b)\n(c)\n(d)\nFigure 3.5\nCompositing equation C = (1 −α)B + αF. The images are taken from a\nclose-up of the region of the hair in the upper right part of the lion in Figure 3.4.\nthat occur if only binary opacities are used.\nTo composite a new (or foreground) image on top of an old (background) image, the over\noperator, ﬁrst proposed by Porter and Duff (1984) and then studied extensively by Blinn\n(1994a; 1994b), is used,\nC = (1 −α)B + αF.\n(3.8)\nThis operator attenuates the inﬂuence of the background image B by a factor (1 −α) and\nthen adds in the color (and opacity) values corresponding to the foreground layer F, as shown\nin Figure 3.5.\nIn many situations, it is convenient to represent the foreground colors in pre-multiplied\nform, i.e., to store (and manipulate) the αF values directly. As Blinn (1994b) shows, the\npre-multiplied RGBA representation is preferred for several reasons, including the ability\nto blur or resample (e.g., rotate) alpha-matted images without any additional complications\n(just treating each RGBA band independently). However, when matting using local color\nconsistency (Ruzon and Tomasi 2000; Chuang, Curless, Salesin et al. 2001), the pure un-\nmultiplied foreground colors F are used, since these remain constant (or vary slowly) in the\nvicinity of the object edge.\nThe over operation is not the only kind of compositing operation that can be used. Porter\nand Duff (1984) describe a number of additional operations that can be useful in photo editing\nand visual effects applications. In this book, we concern ourselves with only one additional,\ncommonly occurring case (but see Exercise 3.2).\nWhen light reﬂects off clean transparent glass, the light passing through the glass and\nthe light reﬂecting off the glass are simply added together (Figure 3.6). This model is use-\nful in the analysis of transparent motion (Black and Anandan 1996; Szeliski, Avidan, and\nAnandan 2000), which occurs when such scenes are observed from a moving camera (see\nSection 8.5.2).\nThe actual process of matting, i.e., recovering the foreground, background, and alpha\nmatte values from one or more images, has a rich history, which we study in Section 10.4.\n3.1 Point operators\n107\nFigure 3.6 An example of light reﬂecting off the transparent glass of a picture frame (Black\nand Anandan 1996) c⃝1996 Elsevier. You can clearly see the woman’s portrait inside the\npicture frame superimposed with the reﬂection of a man’s face off the glass.\nSmith and Blinn (1996) have a nice survey of traditional blue-screen matting techniques,\nwhile Toyama, Krumm, Brumitt et al. (1999) review difference matting. More recently, there\nhas been a lot of activity in computational photography relating to natural image matting\n(Ruzon and Tomasi 2000; Chuang, Curless, Salesin et al. 2001; Wang and Cohen 2007a),\nwhich attempts to extract the mattes from a single natural image (Figure 3.4a) or from ex-\ntended video sequences (Chuang, Agarwala, Curless et al. 2002). All of these techniques are\ndescribed in more detail in Section 10.4.\n3.1.4 Histogram equalization\nWhile the brightness and gain controls described in Section 3.1.1 can improve the appearance\nof an image, how can we automatically determine their best values? One approach might\nbe to look at the darkest and brightest pixel values in an image and map them to pure black\nand pure white. Another approach might be to ﬁnd the average value in the image, push it\ntowards middle gray, and expand the range so that it more closely ﬁlls the displayable values\n(Kopf, Uyttendaele, Deussen et al. 2007).\nHow can we visualize the set of lightness values in an image in order to test some of\nthese heuristics? The answer is to plot the histogram of the individual color channels and\nluminance values, as shown in Figure 3.7b.2 From this distribution, we can compute relevant\nstatistics such as the minimum, maximum, and average intensity values. Notice that the image\nin Figure 3.7a has both an excess of dark values and light values, but that the mid-range values\nare largely under-populated. Would it not be better if we could simultaneously brighten some\n2 The histogram is simply the count of the number of pixels at each gray level value. For an eight-bit image, an\naccumulation table with 256 entries is needed. For higher bit depths, a table with the appropriate number of entries\n(probably fewer than the full number of gray levels) should be used.",
  "image_path": "page_128.jpg",
  "pages": [
    127,
    128,
    129
  ]
}