{
  "doc_id": "pages_405_407",
  "text": "8 Dense motion estimation\n383\nAlgorithms for aligning images and estimating motion in video sequences are among the most\nwidely used in computer vision. For example, frame-rate image alignment is widely used in\ncamcorders and digital cameras to implement their image stabilization (IS) feature.\nAn early example of a widely used image registration algorithm is the patch-based trans-\nlational alignment (optical ﬂow) technique developed by Lucas and Kanade (1981). Variants\nof this algorithm are used in almost all motion-compensated video compression schemes\nsuch as MPEG and H.263 (Le Gall 1991). Similar parametric motion estimation algorithms\nhave found a wide variety of applications, including video summarization (Teodosio and\nBender 1993; Irani and Anandan 1998), video stabilization (Hansen, Anandan, Dana et al.\n1994; Srinivasan, Chellappa, Veeraraghavan et al. 2005; Matsushita, Ofek, Ge et al. 2006),\nand video compression (Irani, Hsu, and Anandan 1995; Lee, ge Chen, lung Bruce Lin et\nal. 1997). More sophisticated image registration algorithms have also been developed for\nmedical imaging and remote sensing. Image registration techniques are surveyed by Brown\n(1992), Zitov’aa and Flusser (2003), Goshtasby (2005), and Szeliski (2006a).\nTo estimate the motion between two or more images, a suitable error metric must ﬁrst\nbe chosen to compare the images (Section 8.1). Once this has been established, a suitable\nsearch technique must be devised. The simplest technique is to exhaustively try all possible\nalignments, i.e., to do a full search. In practice, this may be too slow, so hierarchical coarse-\nto-ﬁne techniques (Section 8.1.1) based on image pyramids are normally used. Alternatively,\nFourier transforms (Section 8.1.2) can be used to speed up the computation.\nTo get sub-pixel precision in the alignment, incremental methods (Section 8.1.3) based\non a Taylor series expansion of the image function are often used. These can also be applied\nto parametric motion models (Section 8.2), which model global image transformations such\nas rotation or shearing. Motion estimation can be made more reliable by learning the typi-\ncal dynamics or motion statistics of the scenes or objects being tracked, e.g., the natural gait\nof walking people (Section 8.2.2). For more complex motions, piecewise parametric spline\nmotion models (Section 8.3) can be used. In the presence of multiple independent (and per-\nhaps non-rigid) motions, general-purpose optical ﬂow (or optic ﬂow) techniques need to be\nused (Section 8.4). For even more complex motions that include a lot of occlusions, layered\nmotion models (Section 8.5), which decompose the scene into coherently moving layers, can\nwork well.\nIn this chapter, we describe each of these techniques in more detail. Additional details\ncan be found in review and comparative evaluation papers on motion estimation (Barron,\nFleet, and Beauchemin 1994; Mitiche and Bouthemy 1996; Stiller and Konrad 1999; Szeliski\n2006a; Baker, Black, Lewis et al. 2007).\n384\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n8.1 Translational alignment\nThe simplest way to establish an alignment between two images or image patches is to shift\none image relative to the other. Given a template image I0(x) sampled at discrete pixel\nlocations {xi = (xi, yi)}, we wish to ﬁnd where it is located in image I1(x). A least squares\nsolution to this problem is to ﬁnd the minimum of the sum of squared differences (SSD)\nfunction\nESSD(u) =\nX\ni\n[I1(xi + u) −I0(xi)]2 =\nX\ni\ne2\ni ,\n(8.1)\nwhere u = (u, v) is the displacement and ei = I1(xi + u) −I0(xi) is called the residual\nerror (or the displaced frame difference in the video coding literature).1 (We ignore for the\nmoment the possibility that parts of I0 may lie outside the boundaries of I1 or be otherwise\nnot visible.) The assumption that corresponding pixel values remain the same in the two\nimages is often called the brightness constancy constraint.2\nIn general, the displacement u can be fractional, so a suitable interpolation function must\nbe applied to image I1(x). In practice, a bilinear interpolant is often used but bicubic inter-\npolation can yield slightly better results (Szeliski and Scharstein 2004). Color images can be\nprocessed by summing differences across all three color channels, although it is also possible\nto ﬁrst transform the images into a different color space or to only use the luminance (which\nis often done in video encoders).\nRobust error metrics.\nWe can make the above error metric more robust to outliers by re-\nplacing the squared error terms with a robust function ρ(ei) (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Black and Anandan 1996; Stewart 1999) to obtain\nESRD(u) =\nX\ni\nρ(I1(xi + u) −I0(xi)) =\nX\ni\nρ(ei).\n(8.2)\nThe robust norm ρ(e) is a function that grows less quickly than the quadratic penalty associ-\nated with least squares. One such function, sometimes used in motion estimation for video\ncoding because of its speed, is the sum of absolute differences (SAD) metric3 or L1 norm,\ni.e.,\nESAD(u) =\nX\ni\n|I1(xi + u) −I0(xi)| =\nX\ni\n|ei|.\n(8.3)\n1 The usual justiﬁcation for using least squares is that it is the optimal estimate with respect to Gaussian noise.\nSee the discussion below on robust error metrics as well as Appendix B.3.\n2 Brightness constancy (Horn 1974) is the tendency for objects to maintain their perceived brightness under\nvarying illumination conditions.\n3 In video compression, e.g., the H.264 standard (http://www.itu.int/rec/T-REC-H.264), the sum of absolute trans-\nformed differences (SATD), which measures the differences in a frequency transform space, e.g., using a Hadamard\ntransform, is often used since it more accurately predicts quality (Richardson 2003).\n8.1 Translational alignment\n385\nHowever, since this function is not differentiable at the origin, it is not well suited to gradient-\ndescent approaches such as the ones presented in Section 8.1.3.\nInstead, a smoothly varying function that is quadratic for small values but grows more\nslowly away from the origin is often used. Black and Rangarajan (1996) discuss a variety of\nsuch functions, including the Geman–McClure function,\nρGM(x) =\nx2\n1 + x2/a2 ,\n(8.4)\nwhere a is a constant that can be thought of as an outlier threshold. An appropriate value for\nthe threshold can itself be derived using robust statistics (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Rousseeuw and Leroy 1987), e.g., by computing the median absolute\ndeviation, MAD = medi|ei|, and multiplying it by 1.4 to obtain a robust estimate of the\nstandard deviation of the inlier noise process (Stewart 1999).\nSpatially varying weights.\nThe error metrics above ignore that fact that for a given align-\nment, some of the pixels being compared may lie outside the original image boundaries.\nFurthermore, we may want to partially or completely downweight the contributions of cer-\ntain pixels. For example, we may want to selectively “erase” some parts of an image from\nconsideration when stitching a mosaic where unwanted foreground objects have been cut out.\nFor applications such as background stabilization, we may want to downweight the middle\npart of the image, which often contains independently moving objects being tracked by the\ncamera.\nAll of these tasks can be accomplished by associating a spatially varying per-pixel weight\nvalue with each of the two images being matched.\nThe error metric then becomes the\nweighted (or windowed) SSD function,\nEWSSD(u) =\nX\ni\nw0(xi)w1(xi + u)[I1(xi + u) −I0(xi)]2,\n(8.5)\nwhere the weighting functions w0 and w1 are zero outside the image boundaries.\nIf a large range of potential motions is allowed, the above metric can have a bias towards\nsmaller overlap solutions. To counteract this bias, the windowed SSD score can be divided\nby the overlap area\nA =\nX\ni\nw0(xi)w1(xi + u)\n(8.6)\nto compute a per-pixel (or mean) squared pixel error EWSSD/A. The square root of this\nquantity is the root mean square intensity error\nRMS =\np\nEWSSD/A\n(8.7)\noften reported in comparative studies.",
  "image_path": "page_406.jpg",
  "pages": [
    405,
    406,
    407
  ]
}