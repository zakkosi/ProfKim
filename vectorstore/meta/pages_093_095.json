{
  "doc_id": "pages_093_095",
  "text": "2.2 Photometric image formation\n71\nzi’=103mm\nf’ = 101mm\nzo=5m\nP\nd\nc\nFigure 2.21\nIn a lens subject to chromatic aberration, light at different wavelengths (e.g.,\nthe red and blur arrows) is focused with a different focal length f ′ and hence a different depth\nz′\ni, resulting in both a geometric (in-plane) displacement and a loss of focus.\nChromatic aberration\nBecause the index of refraction for glass varies slightly as a function of wavelength, sim-\nple lenses suffer from chromatic aberration, which is the tendency for light of different\ncolors to focus at slightly different distances (and hence also with slightly different mag-\nniﬁcation factors), as shown in Figure 2.21. The wavelength-dependent magniﬁcation fac-\ntor, i.e., the transverse chromatic aberration, can be modeled as a per-color radial distortion\n(Section 2.1.6) and, hence, calibrated using the techniques described in Section 6.3.5. The\nwavelength-dependent blur caused by longitudinal chromatic aberration can be calibrated\nusing techniques described in Section 10.1.4. Unfortunately, the blur induced by longitudinal\naberration can be harder to undo, as higher frequencies can get strongly attenuated and hence\nhard to recover.\nIn order to reduce chromatic and other kinds of aberrations, most photographic lenses\ntoday are compound lenses made of different glass elements (with different coatings). Such\nlenses can no longer be modeled as having a single nodal point P through which all of the\nrays must pass (when approximating the lens with a pinhole model). Instead, these lenses\nhave both a front nodal point, through which the rays enter the lens, and a rear nodal point,\nthrough which they leave on their way to the sensor. In practice, only the location of the front\nnodal point is of interest when performing careful camera calibration, e.g., when determining\nthe point around which to rotate to capture a parallax-free panorama (see Section 9.1.3).\nNot all lenses, however, can be modeled as having a single nodal point. In particular, very\nwide-angle lenses such as ﬁsheye lenses (Section 2.1.6) and certain catadioptric imaging\nsystems consisting of lenses and curved mirrors (Baker and Nayar 1999) do not have a single\npoint through which all of the acquired light rays pass. In such cases, it is preferable to\nexplicitly construct a mapping function (look-up table) between pixel coordinates and 3D\nrays in space (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Sautot et al.\n72\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzi=102mm\nf = 100mm\nzo=5m\nδi\nd\nδo\nα\nα\nα\nP\nJ\nI\nO\nQ\nro\nFigure 2.22\nThe amount of light hitting a pixel of surface area δi depends on the square of\nthe ratio of the aperture diameter d to the focal length f, as well as the fourth power of the\noff-axis angle α cosine, cos4 α.\n1992; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm, Trudeau et\nal. 2009), as mentioned in Section 2.1.6.\nVignetting\nAnother property of real-world lenses is vignetting, which is the tendency for the brightness\nof the image to fall off towards the edge of the image.\nTwo kinds of phenomena usually contribute to this effect (Ray 2002). The ﬁrst is called\nnatural vignetting and is due to the foreshortening in the object surface, projected pixel, and\nlens aperture, as shown in Figure 2.22. Consider the light leaving the object surface patch\nof size δo located at an off-axis angle α. Because this patch is foreshortened with respect\nto the camera lens, the amount of light reaching the lens is reduced by a factor cos α. The\namount of light reaching the lens is also subject to the usual 1/r2 fall-off; in this case, the\ndistance ro = zo/ cos α. The actual area of the aperture through which the light passes\nis foreshortened by an additional factor cos α, i.e., the aperture as seen from point O is an\nellipse of dimensions d×d cos α. Putting all of these factors together, we see that the amount\nof light leaving O and passing through the aperture on its way to the image pixel located at I\nis proportional to\nδo cos α\nr2o\nπ\n\u0012d\n2\n\u00132\ncos α = δoπ\n4\nd2\nz2o\ncos4 α.\n(2.98)\nSince triangles ∆OPQ and ∆IPJ are similar, the projected areas of of the object surface δo\nand image pixel δi are in the same (squared) ratio as zo : zi,\nδo\nδi = z2\no\nz2\ni\n.\n(2.99)\nPutting these together, we obtain the ﬁnal relationship between the amount of light reaching\n2.3 The digital camera\n73\npixel i and the aperture diameter d, the focusing distance zi ≈f, and the off-axis angle α,\nδoπ\n4\nd2\nz2o\ncos4 α = δiπ\n4\nd2\nz2\ni\ncos4 α ≈δiπ\n4\n\u0012 d\nf\n\u00132\ncos4 α,\n(2.100)\nwhich is called the fundamental radiometric relation between the scene radiance L and the\nlight (irradiance) E reaching the pixel sensor,\nE = Lπ\n4\n\u0012 d\nf\n\u00132\ncos4 α,\n(2.101)\n(Horn 1986; Nalwa 1993; Hecht 2001; Ray 2002). Notice in this equation how the amount of\nlight depends on the pixel surface area (which is why the smaller sensors in point-and-shoot\ncameras are so much noisier than digital single lens reﬂex (SLR) cameras), the inverse square\nof the f-stop N = f/d (2.97), and the fourth power of the cos4 α off-axis fall-off, which is\nthe natural vignetting term.\nThe other major kind of vignetting, called mechanical vignetting, is caused by the internal\nocclusion of rays near the periphery of lens elements in a compound lens, and cannot easily\nbe described mathematically without performing a full ray-tracing of the actual lens design.9\nHowever, unlike natural vignetting, mechanical vignetting can be decreased by reducing the\ncamera aperture (increasing the f-number). It can also be calibrated (along with natural vi-\ngnetting) using special devices such as integrating spheres, uniformly illuminated targets, or\ncamera rotation, as discussed in Section 10.1.3.\n2.3 The digital camera\nAfter starting from one or more light sources, reﬂecting off one or more surfaces in the world,\nand passing through the camera’s optics (lenses), light ﬁnally reaches the imaging sensor.\nHow are the photons arriving at this sensor converted into the digital (R, G, B) values that\nwe observe when we look at a digital image? In this section, we develop a simple model\nthat accounts for the most important effects such as exposure (gain and shutter speed), non-\nlinear mappings, sampling and aliasing, and noise. Figure 2.23, which is based on camera\nmodels developed by Healey and Kondepudy (1994); Tsin, Ramesh, and Kanade (2001); Liu,\nSzeliski, Kang et al. (2008), shows a simple version of the processing stages that occur in\nmodern digital cameras. Chakrabarti, Scharstein, and Zickler (2009) developed a sophisti-\ncated 24-parameter model that is an even better match to the processing performed in today’s\ncameras.\n9 There are some empirical models that work well in practice (Kang and Weiss 2000; Zheng, Lin, and Kang\n2006).",
  "image_path": "page_094.jpg",
  "pages": [
    93,
    94,
    95
  ]
}