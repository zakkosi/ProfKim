{
  "doc_id": "pages_644_646",
  "text": "622\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 13.2\nView interpolation (Chen and Williams 1993) c⃝1993 ACM: (a) holes from\none source image (shown in blue); (b) holes after combining two widely spaced images; (c)\nholes after combining two closely spaced images; (d) after interpolation (hole ﬁlling).\nmimic what a virtual camera would see in between the two reference views.\nView interpolation combines two ideas that were previously used in computer vision and\ncomputer graphics. The ﬁrst is the idea of pairing a recovered depth map with the refer-\nence image used in its computation and then using the resulting texture-mapped 3D model\nto generate novel views (Figure 11.1). The second is the idea of morphing (Section 3.6.3)\n(Figure 3.53), where correspondences between pairs of images are used to warp each refer-\nence image to an in-between location while simultaneously cross-dissolving between the two\nwarped images.\nFigure 13.2 illustrates this process in more detail. First, both source images are warped\nto the novel view, using both the knowledge of the reference and virtual 3D camera pose\nalong with each image’s depth map (2.68–2.70). In the paper by Chen and Williams (1993),\na forward warping algorithm (Algorithm 3.1 and Figure 3.46) is used. The depth maps are\nrepresented as quadtrees for both space and rendering time efﬁciency (Samet 1989).\nDuring the forward warping process, multiple pixels (which occlude one another) may\nland on the same destination pixel. To resolve this conﬂict, either a z-buffer depth value can\nbe associated with each destination pixel or the images can be warped in back-to-front order,\nwhich can be computed based on the knowledge of epipolar geometry (Chen and Williams\n1993; Laveau and Faugeras 1994; McMillan and Bishop 1995).\nOnce the two reference images have been warped to the novel view (Figure 13.2a–b), they\ncan be merged to create a coherent composite (Figure 13.2c). Whenever one of the images\nhas a hole (illustrated as a cyan pixel), the other image is used as the ﬁnal value. When both\nimages have pixels to contribute, these can be blended as in usual morphing, i.e., according\nto the relative distances between the virtual and source cameras. Note that if the two images\nhave very different exposures, which can happen when performing view interpolation on real\nimages, the hole-ﬁlled regions and the blended regions will have different exposures, leading\n13.1 View interpolation\n623\nto subtle artifacts.\nThe ﬁnal step in view interpolation (Figure 13.2d) is to ﬁll any remaining holes or cracks\ndue to the forward warping process or lack of source data (scene visibility). This can be done\nby copying pixels from the further pixels adjacent to the hole. (Otherwise, foreground objects\nare subject to a “fattening effect”.)\nThe above process works well for rigid scenes, although its visual quality (lack of alias-\ning) can be improved using a two-pass, forward–backward algorithm (Section 13.2.1) (Shade,\nGortler, He et al. 1998) or full 3D rendering (Zitnick, Kang, Uyttendaele et al. 2004). In the\ncase where the two reference images are views of a non-rigid scene, e.g., a person smiling\nin one image and frowning in the other, view morphing, which combines ideas from view\ninterpolation with regular morphing, can be used (Seitz and Dyer 1996).\nWhile the original view interpolation paper describes how to generate novel views based\non similar pre-computed (linear perspective) images, the plenoptic modeling paper of McMil-\nlan and Bishop (1995) argues that cylindrical images should be used to store the pre-computed\nrendering or real-world images. (Chen 1995) also propose using environment maps (cylin-\ndrical, cubic, or spherical) as source images for view interpolation.\n13.1.1 View-dependent texture maps\nView-dependent texture maps (Debevec, Taylor, and Malik 1996) are closely related to view\ninterpolation. Instead of associating a separate depth map with each input image, a single 3D\nmodel is created for the scene, but different images are used as texture map sources depending\non the virtual camera’s current position (Figure 13.3a).3\nIn more detail, given a new virtual camera position, the similarity of this camera’s view of\neach polygon (or pixel) is compared to that of potential source images. The images are then\nblended using a weighting that is inversely proportional to the angles αi between the virtual\nview and the source views (Figure 13.3a). Even though the geometric model can be fairly\ncoarse (Figure 13.3b), blending between different views gives a strong sense of more detailed\ngeometry because of the parallax (visual motion) between corresponding pixels. While the\noriginal paper performs the weighted blend computation separately at each pixel or coarsened\npolygon face, follow-on work by Debevec, Yu, and Borshukov (1998) presents a more efﬁ-\ncient implementation based on precomputing contributions for various portions of viewing\nspace and then using projective texture mapping (OpenGL-ARB 1997).\nThe idea of view-dependent texture mapping has been used in a large number of sub-\nsequent image-based rendering systems, including facial modeling and animation (Pighin,\n3 The term image-based modeling, which is now commonly used to describe the creation of texture-mapped 3D\nmodels from multiple images, appears to have ﬁrst been used by Debevec, Taylor, and Malik (1996), who also used\nthe term photogrammetric modeling to describe the same process.\n624\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 13.3 View-dependent texture mapping (Debevec, Taylor, and Malik 1996) c⃝1996\nACM. (a) The weighting given to each input view depends on the relative angles between the\nnovel (virtual) view and the original views; (b) simpliﬁed 3D model geometry; (c) with view-\ndependent texture mapping, the geometry appears to have more detail (recessed windows).\nHecker, Lischinski et al. 1998) and 3D scanning and visualization (Pulli, Abi-Rached, Duchamp\net al. 1998). Closely related to view-dependent texture mapping is the idea of blending be-\ntween light rays in 4D space, which forms the basis of the Lumigraph and unstructured Lu-\nmigraph systems (Section 13.3) (Gortler, Grzeszczuk, Szeliski et al. 1996; Buehler, Bosse,\nMcMillan et al. 2001).\nIn order to provide even more realism in their Fac¸ade system, Debevec, Taylor, and Malik\n(1996) also include a model-based stereo component, which optionally computes an offset\n(parallax) map for each coarse planar facet of their 3D model. They call the resulting analysis\nand rendering system a hybrid geometry- and image-based approach, since it uses traditional\n3D geometric modeling to create the global 3D model, but then uses local depth offsets, along\nwith view interpolation, to add visual realism.\n13.1.2 Application: Photo Tourism\nWhile view interpolation was originally developed to accelerate the rendering of 3D scenes\non low-powered processors and systems without graphics acceleration, it turns out that it\ncan be applied directly to large collections of casually acquired photographs. The Photo\nTourism system developed by Snavely, Seitz, and Szeliski (2006) uses structure from motion\nto compute the 3D locations and poses of all the cameras taking the images, along with a\nsparse 3D point-cloud model of the scene (Section 7.4.4, Figure 7.11).\nTo perform an image-based exploration of the resulting sea of images (Aliaga, Funkhouser,\nYanovsky et al. 2003), Photo Tourism ﬁrst associates a 3D proxy with each image. While a\ntriangulated mesh obtained from the point cloud can sometimes form a suitable proxy, e.g.,\nfor outdoor terrain models, a simple dominant plane ﬁt to the 3D points visible in each image",
  "image_path": "page_645.jpg",
  "pages": [
    644,
    645,
    646
  ]
}