{
  "doc_id": "pages_728_730",
  "text": "706\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.43\nInterleaved recognition and segmentation (Leibe, Leonardis, and Schiele\n2008) c⃝2008 Springer. The process starts by re-recognizing visual words (codebook en-\ntries) in a new image (scene) and having each part vote for likely locations and size in a\n3D (x, y, s) voting space (top row). Once a maximum has been found, the parts (features)\ncorresponding to this instance are determined by backprojecting the contributing votes. The\nforeground–background segmentation for each object can be found by backprojecting proba-\nbilistic masks associated with each codebook entry. The whole recognition and segmentation\nprocess can then be repeated.\nthe scene (Lowe 2004), as shown in Figure 14.1d, or matching portions of the new scene to\npre-learned (segmented) object models (Ferrari, Tuytelaars, and Van Gool 2006b; Kannala,\nRahtu, Brandt et al. 2008).\nFor more complex (ﬂexible) object models, such as those for humans Figure 14.1f, a\ndifferent approach is to pre-segment the image into larger or smaller pieces (Chapter 5) and\nthen match such pieces to portions of the model (Mori, Ren, Efros et al. 2004; Mori 2005;\nHe, Zemel, and Ray 2006; Gu, Lim, Arbelaez et al. 2009).\nAn alternative approach by Leibe, Leonardis, and Schiele (2008), which we introduced\nin the previous section, votes for potential object locations and scales based on the detec-\ntion of features corresponding to pre-clustered visual codebook entries (Figure 14.43). To\nsupport segmentation, each codebook entry has an associated foreground–background mask,\nwhich is learned as part of the codebook clustering process from pre-labeled object segmen-\ntation masks. During recognition, once a maximum in the voting space is found, the masks\nassociated with the entries that voted for this instance are combined to obtain an object seg-\nmentation, as shown on the left side of Figure 14.43.\nA more holistic approach to recognition and segmentation is to formulate the problem as\none of labeling every pixel in an image with its class membership, and to solve this prob-\n14.4 Category recognition\n707\n(a)\n(b)\nFigure 14.44\nSimultaneous recognition and segmentation using TextonBoost (Shotton,\nWinn, Rother et al. 2009) c⃝2009 Springer: (a) successful recognition results; (b) less suc-\ncessful results.\n708\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.45 Layout consistent random ﬁeld (Winn and Shotton 2006) c⃝2006 IEEE. The\nnumbers indicate the kind of neighborhood relations that can exist between pixels assigned\nto the same or different classes. Each pairwise relationship carries its own likelihood (energy\npenalty).\nlem using energy minimization or Bayesian inference techniques, i.e., conditional random\nﬁelds (Section 3.7.2, (3.118)) (Kumar and Hebert 2006; He, Zemel, and Carreira-Perpi˜n´an\n2004). The TextonBoost system of Shotton, Winn, Rother et al. (2009) uses unary (pixel-\nwise) potentials based on image-speciﬁc color distributions (Section 5.5) (Boykov and Jolly\n2001; Rother, Kolmogorov, and Blake 2004), location information (e.g., foreground objects\nare more likely to be in the middle of the image, sky is likely to be higher, and road is likely\nto be lower), and novel texture-layout classiﬁers trained using shared boosting. It also uses\ntraditional pairwise potentials that look at image color gradients (Veksler 2001; Boykov and\nJolly 2001; Rother, Kolmogorov, and Blake 2004). The texton-layout features ﬁrst ﬁlter the\nimage with a series of 17 oriented ﬁlter banks and then cluster the responses to classify each\npixel into 30 different texton classes (Malik, Belongie, Leung et al. 2001). The responses\nare then ﬁltered using offset rectangular regions trained with joint boosting (Viola and Jones\n2004) to produce the texton-layout features used as unary potentials.\nFigure 14.44a shows some examples of images successfully labeled and segmented using\nTextonBoost, while Figure 14.44b shows examples where it does not do as well. As you can\nsee, this kind of semantic labeling can be extremely challenging.\nThe TextonBoost conditional random ﬁeld framework has been extended to LayoutCRFs\nby Winn and Shotton (2006), who incorporate additional constraints to recognize multiple\nobject instances and deal with occlusions (Figure 14.45), and even more recently by Hoiem,\nRother, and Winn (2007) to incorporate full 3D models.\nConditional random ﬁelds continue to be widely used and extended for simultaneous\nrecognition and segmentation applications (Kumar and Hebert 2006; He, Zemel, and Ray\n2006; Levin and Weiss 2006; Verbeek and Triggs 2007; Yang, Meer, and Foran 2007; Rabi-\nnovich, Vedaldi, Galleguillos et al. 2007; Batra, Sukthankar, and Chen 2008; Larlus and Jurie",
  "image_path": "page_729.jpg",
  "pages": [
    728,
    729,
    730
  ]
}