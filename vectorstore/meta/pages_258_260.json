{
  "doc_id": "pages_258_260",
  "text": "236\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.28 Feature tracking using an afﬁne motion model (Shi and Tomasi 1994) c⃝1994\nIEEE, Top row: image patch around the tracked feature location. Bottom row: image patch\nafter warping back toward the ﬁrst frame using an afﬁne deformation. Even though the speed\nsign gets larger from frame to frame, the afﬁne transformation maintains a good resemblance\nbetween the original and subsequent tracked frames.\nSince their original work on feature tracking, Shi and Tomasi’s approach has generated a\nstring of interesting follow-on papers and applications. Beardsley, Torr, and Zisserman (1996)\nuse extended feature tracking combined with structure from motion (Chapter 7) to incremen-\ntally build up sparse 3D models from video sequences. Kang, Szeliski, and Shum (1997)\ntie together the corners of adjacent (regularly gridded) patches to provide some additional\nstability to the tracking, at the cost of poorer handling of occlusions. Tommasini, Fusiello,\nTrucco et al. (1998) provide a better spurious match rejection criterion for the basic Shi and\nTomasi algorithm, Collins and Liu (2003) provide improved mechanisms for feature selec-\ntion and dealing with larger appearance changes over time, and Shaﬁque and Shah (2005)\ndevelop algorithms for feature matching (data association) for videos with large numbers of\nmoving objects or points. Yilmaz, Javed, and Shah (2006) and Lepetit and Fua (2005) survey\nthe larger ﬁeld of object tracking, which includes not only feature-based techniques but also\nalternative techniques based on contour and region (Section 5.1).\nOne of the newest developments in feature tracking is the use of learning algorithms to\nbuild special-purpose recognizers to rapidly search for matching features anywhere in an\nimage (Lepetit, Pilet, and Fua 2006; Hinterstoisser, Benhimane, Navab et al. 2008; Rogez,\nRihan, Ramalingam et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010).2 By taking the time\nto train classiﬁers on sample patches and their afﬁne deformations, extremely fast and reliable\nfeature detectors can be constructed, which enables much faster motions to be supported\n(Figure 4.29). Coupling such features to deformable models (Pilet, Lepetit, and Fua 2008) or\nstructure-from-motion algorithms (Klein and Murray 2008) can result in even higher stability.\n2 See also my previous comment on earlier work in learning-based tracking (Avidan 2001; Jurie and Dhome\n2002; Williams, Blake, and Cipolla 2003).\n4.1 Points and patches\n237\nFigure 4.29 Real-time head tracking using the fast trained classiﬁers of Lepetit, Pilet, and\nFua (2004) c⃝2004 IEEE.\n4.1.5 Application: Performance-driven animation\nOne of the most compelling applications of fast feature tracking is performance-driven an-\nimation, i.e., the interactive deformation of a 3D graphics model based on tracking a user’s\nmotions (Williams 1990; Litwinowicz and Williams 1994; Lepetit, Pilet, and Fua 2004).\nBuck, Finkelstein, Jacobs et al. (2000) present a system that tracks a user’s facial expres-\nsions and head motions and then uses them to morph among a series of hand-drawn sketches.\nAn animator ﬁrst extracts the eye and mouth regions of each sketch and draws control lines\nover each image (Figure 4.30a). At run time, a face-tracking system (Toyama 1998) deter-\nmines the current location of these features (Figure 4.30b). The animation system decides\nwhich input images to morph based on nearest neighbor feature appearance matching and\ntriangular barycentric interpolation. It also computes the global location and orientation of\nthe head from the tracked features. The resulting morphed eye and mouth regions are then\ncomposited back into the overall head model to yield a frame of hand-drawn animation (Fig-\nure 4.30d).\nIn more recent work, Barnes, Jacobs, Sanders et al. (2008) watch users animate paper\ncutouts on a desk and then turn the resulting motions and drawings into seamless 2D anima-\ntions.\n238\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 4.30\nPerformance-driven, hand-drawn animation (Buck, Finkelstein, Jacobs et al.\n2000) c⃝2000 ACM: (a) eye and mouth portions of hand-drawn sketch with their overlaid\ncontrol lines; (b) an input video frame with the tracked features overlaid; (c) a different input\nvideo frame along with its (d) corresponding hand-drawn animation.\n4.2 Edges\nWhile interest points are useful for ﬁnding image locations that can be accurately matched\nin 2D, edge points are far more plentiful and often carry important semantic associations.\nFor example, the boundaries of objects, which also correspond to occlusion events in 3D, are\nusually delineated by visible contours. Other kinds of edges correspond to shadow boundaries\nor crease edges, where surface orientation changes rapidly. Isolated edge points can also be\ngrouped into longer curves or contours, as well as straight line segments (Section 4.3). It\nis interesting that even young children have no difﬁculty in recognizing familiar objects or\nanimals from such simple line drawings.\n4.2.1 Edge detection\nGiven an image, how can we ﬁnd the salient edges? Consider the color images in Figure 4.31.\nIf someone asked you to point out the most “salient” or “strongest” edges or the object bound-\naries (Martin, Fowlkes, and Malik 2004; Arbel´aez, Maire, Fowlkes et al. 2010), which ones\nwould you trace? How closely do your perceptions match the edge images shown in Fig-\nure 4.31?\nQualitatively, edges occur at boundaries between regions of different color, intensity, or\ntexture. Unfortunately, segmenting an image into coherent regions is a difﬁcult task, which\nwe address in Chapter 5. Often, it is preferable to detect edges using only purely local infor-\nmation.\nUnder such conditions, a reasonable approach is to deﬁne an edge as a location of rapid",
  "image_path": "page_259.jpg",
  "pages": [
    258,
    259,
    260
  ]
}