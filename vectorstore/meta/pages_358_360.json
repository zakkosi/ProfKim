{
  "doc_id": "pages_358_360",
  "text": "336\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTechniques for robust estimation are discussed in more detail in Appendix B.3 and in\nmonographs and review articles on this topic (Huber 1981; Hampel, Ronchetti, Rousseeuw et\nal. 1986; Rousseeuw and Leroy 1987; Black and Rangarajan 1996; Stewart 1999). The most\ncommonly used robust initialization technique in computer vision is RANdom SAmple Con-\nsensus (RANSAC) (Fischler and Bolles 1981), which has spawned a series of more efﬁcient\nvariants (Nist´er 2003; Chum and Matas 2005).\nThe topic of registering 3D point data sets is called absolute orientation (Horn 1987) and\n3D pose estimation (Lorusso, Eggert, and Fisher 1995). A variety of techniques has been\ndeveloped for simultaneously computing 3D point correspondences and their corresponding\nrigid transformations (Besl and McKay 1992; Zhang 1994; Szeliski and Lavall´ee 1996; Gold,\nRangarajan, Lu et al. 1998; David, DeMenthon, Duraiswami et al. 2004; Li and Hartley 2007;\nEnqvist, Josephson, and Kahl 2009).\nCamera calibration was ﬁrst studied in photogrammetry (Brown 1971; Slama 1980; Atkin-\nson 1996; Kraus 1997) but it has also been widely studied in computer vision (Tsai 1987;\nGremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Szeliski et al. 1992; Zhang\n2000; Grossberg and Nayar 2001). Vanishing points observed either from rectahedral cali-\nbration objects or man-made architecture are often used to perform rudimentary calibration\n(Caprile and Torre 1990; Becker and Bove 1995; Liebowitz and Zisserman 1998; Cipolla,\nDrummond, and Robertson 1999; Antone and Teller 2002; Criminisi, Reid, and Zisserman\n2000; Hartley and Zisserman 2004; Pﬂugfelder 2008). Performing camera calibration without\nusing known targets is known as self-calibration and is discussed in textbooks and surveys on\nstructure from motion (Faugeras, Luong, and Maybank 1992; Hartley and Zisserman 2004;\nMoons, Van Gool, and Vergauwen 2010). One popular subset of such techniques uses pure\nrotational motion (Stein 1995; Hartley 1997b; Hartley, Hayman, de Agapito et al. 2000; de\nAgapito, Hayman, and Reid 2001; Kang and Weiss 1999; Shum and Szeliski 2000; Frahm\nand Koch 2003).\n6.5 Exercises\nEx 6.1: Feature-based image alignment for ﬂip-book animations\nTake a set of photos of\nan action scene or portrait (preferably in motor-drive—continuous shooting—mode) and\nalign them to make a composite or ﬂip-book animation.\n1. Extract features and feature descriptors using some of the techniques described in Sec-\ntions 4.1.1–4.1.2.\n2. Match your features using nearest neighbor matching with a nearest neighbor distance\nratio test (4.18).\n6.5 Exercises\n337\n3. Compute an optimal 2D translation and rotation between the ﬁrst image and all subse-\nquent images, using least squares (Section 6.1.1) with optional RANSAC for robustness\n(Section 6.1.4).\n4. Resample all of the images onto the ﬁrst image’s coordinate frame (Section 3.6.1) using\neither bilinear or bicubic resampling and optionally crop them to their common area.\n5. Convert the resulting images into an animated GIF (using software available from the\nWeb) or optionally implement cross-dissolves to turn them into a “slo-mo” video.\n6. (Optional) Combine this technique with feature-based (Exercise 3.25) morphing.\nEx 6.2: Panography\nCreate the kind of panograph discussed in Section 6.1.2 and com-\nmonly found on the Web.\n1. Take a series of interesting overlapping photos.\n2. Use the feature detector, descriptor, and matcher developed in Exercises 4.1–4.4 (or\nexisting software) to match features among the images.\n3. Turn each connected component of matching features into a track, i.e., assign a unique\nindex i to each track, discarding any tracks that are inconsistent (contain two different\nfeatures in the same image).\n4. Compute a global translation for each image using Equation (6.12).\n5. Since your matches probably contain errors, turn the above least square metric into a\nrobust metric (6.25) and re-solve your system using iteratively reweighted least squares.\n6. Compute the size of the resulting composite canvas and resample each image into its\nﬁnal position on the canvas. (Keeping track of bounding boxes will make this more\nefﬁcient.)\n7. Average all of the images, or choose some kind of ordering and implement translucent\nover compositing (3.8).\n8. (Optional) Extend your parametric motion model to include rotations and scale, i.e.,\nthe similarity transform given in Table 6.1. Discuss how you could handle the case of\ntranslations and rotations only (no scale).\n9. (Optional) Write a simple tool to let the user adjust the ordering and opacity, and add\nor remove images.\n338\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n10. (Optional) Write down a different least squares problem that involves pairwise match-\ning of images. Discuss why this might be better or worse than the global matching\nformula given in (6.12).\nEx 6.3: 2D rigid/Euclidean matching\nSeveral alternative approaches are given in Section 6.1.3\nfor estimating a 2D rigid (Euclidean) alignment.\n1. Implement the various alternatives and compare their accuracy on synthetic data, i.e.,\nrandom 2D point clouds with noisy feature positions.\n2. One approach is to estimate the translations from the centroids and then estimate ro-\ntation in polar coordinates. Do you need to weight the angles obtained from a polar\ndecomposition in some way to get the statistically correct estimate?\n3. How can you modify your techniques to take into account either scalar (6.10) or full\ntwo-dimensional point covariance weightings (6.11)? Do all of the previously devel-\noped “shortcuts” still work or does full weighting require iterative optimization?\nEx 6.4: 2D match move/augmented reality\nReplace a picture in a magazine or a book\nwith a different image or video.\n1. With a webcam, take a picture of a magazine or book page.\n2. Outline a ﬁgure or picture on the page with a rectangle, i.e., draw over the four sides as\nthey appear in the image.\n3. Match features in this area with each new image frame.\n4. Replace the original image with an “advertising” insert, warping the new image with\nthe appropriate homography.\n5. Try your approach on a clip from a sporting event (e.g., indoor or outdoor soccer) to\nimplement a billboard replacement.\nEx 6.5: 3D joystick\nTrack a Rubik’s cube to implement a 3D joystick/mouse control.\n1. Get out an old Rubik’s cube (or get one from your parents).\n2. Write a program to detect the center of each colored square.\n3. Group these centers into lines and then ﬁnd the vanishing points for each face.\n4. Estimate the rotation angle and focal length from the vanishing points.",
  "image_path": "page_359.jpg",
  "pages": [
    358,
    359,
    360
  ]
}