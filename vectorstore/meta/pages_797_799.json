{
  "doc_id": "pages_797_799",
  "text": "B.6 Uncertainty estimation (error analysis)\n775\nalso given by Rother, Kumar, Kolmogorov et al. (2005).\nB.6 Uncertainty estimation (error analysis)\nIn addition to computing the most likely estimate, many applications require an estimate for\nthe uncertainty in this estimate.9 The most general way to do this is to compute a complete\nprobability distribution over all of the unknowns but this is generally intractable. The one spe-\ncial case where it is easy to obtain a simple description for this distribution is linear estimation\nproblems with Gaussian noise, where the joint energy function (negative log likelihood of the\nposterior estimate) is a quadratic. In this case, the posterior distribution is a multi-variate\nGaussian and the covariance can be computed directly from the inverse of the problem Hes-\nsian. (Another name for the inverse covariance matrix, which is equal to the Hessian in such\nsimple cases, is the information matrix.)\nEven here, however, the full covariance matrix may be too large to compute and store. For\nexample, in large structure from motion problems, a large sparse Hessian normally results in a\nfull dense covariance matrix. In such cases, it is often considered acceptable to report only the\nvariance in the estimated quantities or simple covariance estimates on individual parameters,\nsuch as 3D point positions or camera pose estimates (Szeliski 1990a). More insight into the\nproblem, e.g., the dominant modes of uncertainty, can be obtained using eigenvalue analysis\n(Szeliski and Kang 1997).\nFor problems where the posterior energy is non-quadratic, e.g., in non-linear or robustiﬁed\nleast squares, it is still often possible to obtain an estimate of the Hessian in the vicinity of the\noptimal solution. In this case, the Cramer–Rao lower bound on the uncertainty (covariance)\ncan be computed as the inverse of the Hessian. Another way of saying this is that while the\nlocal Hessian can underestimate how “wide” the energy function can be, the covariance can\nnever be smaller than the estimate based on this local quadratic approximation. It is also\npossible to estimate a different kind of uncertainty (min-marginal energies) in general MRFs\nwhere the MAP inference is performed using graph cuts (Kohli and Torr 2008).\nWhile many computer vision applications ignore uncertainty modeling, it is often useful\nto compute these estimates just to get an intuitive feeling for the reliability of the estimates.\nCertain applications, such as Kalman ﬁltering, require the computation of this uncertainty\n(either explicitly as posterior covariances or implicitly as inverse covariances) in order to\noptimally integrate new measurements with previously computed estimates.\n9 This is particularly true of classic photogrammetry applications, where the reporting of precision is almost\nalways considered mandatory (F¨orstner 2005).\n776\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAppendix C\nSupplementary material\nC.1\nData sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778\nC.2\nSoftware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780\nC.3\nSlides and lectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 789\nC.4\nBibliography\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790",
  "image_path": "page_798.jpg",
  "pages": [
    797,
    798,
    799
  ]
}