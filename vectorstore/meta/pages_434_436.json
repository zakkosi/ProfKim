{
  "doc_id": "pages_434_436",
  "text": "412\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.12\nEvaluation of the results of 24 optical ﬂow algorithms, October 2009, http:\n//vision.middlebury.edu/ﬂow/, (Baker, Scharstein, Lewis et al. 2009). By moving the mouse\npointer over an underlined performance score, the user can interactively view the correspond-\ning ﬂow and error maps. Clicking on a score toggles between the computed and ground truth\nﬂows. Next to each score, the corresponding rank in the current column is indicated by a\nsmaller blue number. The minimum (best) score in each column is shown in boldface. The\ntable is sorted by the average rank (computed over all 24 columns, three region masks for each\nof the eight sequences). The average rank serves as an approximate measure of performance\nunder the selected metric/statistic.\n8.4 Optical ﬂow\n413\nginning to appear and tend to be among the better-performing methods on the recently re-\nleased optical ﬂow database (Baker, Black, Lewis et al. 2007).13\nExamples of such techniques include the one developed by Glocker, Paragios, Komodakis\net al. (2008), who use a coarse-to-ﬁne strategy with per-pixel 2D uncertainty estimates, which\nare then used to guide the reﬁnement and search at the next ﬁner level. Instead of using gra-\ndient descent to reﬁne the ﬂow estimates, a combinatorial search over discrete displacement\nlabels (which is able to ﬁnd better energy minima) is performed using their Fast-PD algorithm\n(Komodakis, Tziritas, and Paragios 2008).\nLempitsky, Roth, and Rother. (2008) use fusion moves (Lempitsky, Rother, and Blake\n2007) over proposals generated from basic ﬂow algorithms (Horn and Schunck 1981; Lucas\nand Kanade 1981) to ﬁnd good solutions. The basic idea behind fusion moves is to replace\nportions of the current best estimate with hypotheses generated by more basic techniques\n(or their shifted versions) and to alternate them with local gradient descent for better energy\nminimization.\nThe ﬁeld of accurate motion estimation continues to evolve at a rapid pace, with signif-\nicant advances in performance occurring every year. The optical ﬂow evaluation Web site\n(http://vision.middlebury.edu/ﬂow/) is a good source of pointers to high-performing recently\ndeveloped algorithms (Figure 8.12).\n8.4.1 Multi-frame motion estimation\nSo far, we have looked at motion estimation as a two-frame problem, where the goal is to\ncompute a motion ﬁeld that aligns pixels from one image with those in another. In practice,\nmotion estimation is usually applied to video, where a whole sequence of frames is available\nto perform this task.\nOne classic approach to multi-frame motion is to ﬁlter the spatio-temporal volume using\noriented or steerable ﬁlters (Heeger 1988), in a manner analogous to oriented edge detec-\ntion (Section 3.2.3). Figure 8.13 shows two frames from the commonly used ﬂower garden\nsequence, as well as a horizontal slice through the spatio-temporal volume, i.e., the 3D vol-\nume created by stacking all of the video frames together. Because the pixel motion is mostly\nhorizontal, the slopes of individual (textured) pixel tracks, which correspond to their horizon-\ntal velocities, can clearly be seen. Spatio-temporal ﬁltering uses a 3D volume around each\npixel to determine the best orientation in space–time, which corresponds directly to a pixel’s\nvelocity.\nUnfortunately, in order to obtain reasonably accurate velocity estimates everywhere in\nan image, spatio-temporal ﬁlters have moderately large extents, which severely degrades the\nquality of their estimates near motion discontinuities. (This same problem is endemic in\n13 http://vision.middlebury.edu/ﬂow/.\n414\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 8.13\nSlice through a spatio-temporal volume (Szeliski 1999) c⃝1999 IEEE: (a–b)\ntwo frames from the ﬂower garden sequence; (c) a horizontal slice through the complete\nspatio-temporal volume, with the arrows indicating locations of potential key frames where\nﬂow is estimated. Note that the colors for the ﬂower garden sequence are incorrect; the correct\ncolors (yellow ﬂowers) are shown in Figure 8.15.\n2D window-based motion estimators.) An alternative to full spatio-temporal ﬁltering is to\nestimate more local spatio-temporal derivatives and use them inside a global optimization\nframework to ﬁll in textureless regions (Bruhn, Weickert, and Schn¨orr 2005; Govindu 2006).\nAnother alternative is to simultaneously estimate multiple motion estimates, while also\noptionally reasoning about occlusion relationships (Szeliski 1999). Figure 8.13c shows schemat-\nically one potential approach to this problem. The horizontal arrows show the locations of\nkeyframes s where motion is estimated, while other slices indicate video frames t whose\ncolors are matched with those predicted by interpolating between the keyframes. Motion es-\ntimation can be cast as a global energy minimization problem that simultaneously minimizes\nbrightness compatibility and ﬂow compatibility terms between keyframes and other frames,\nin addition to using robust smoothness terms.\nThe multi-view framework is potentially even more appropriate for rigid scene motion\n(multi-view stereo) (Section 11.6), where the unknowns at each pixel are disparities and\nocclusion relationships can be determined directly from pixel depths (Szeliski 1999; Kol-\nmogorov and Zabih 2002). However, it may also be applicable to general motion, with the\naddition of models for object accelerations and occlusion relationships.\n8.4.2 Application: Video denoising\nVideo denoising is the process of removing noise and other artifacts such as scratches from\nﬁlm and video (Kokaram 2004). Unlike single image denoising, where the only information\navailable is in the current picture, video denoisers can average or borrow information from\nadjacent frames. However, in order to do this without introducing blur or jitter (irregular\nmotion), they need accurate per-pixel motion estimates.\nExercise 8.7 lists some of the steps required, which include the ability to determine if the",
  "image_path": "page_435.jpg",
  "pages": [
    434,
    435,
    436
  ]
}