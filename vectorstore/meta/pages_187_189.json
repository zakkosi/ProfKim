{
  "doc_id": "pages_187_189",
  "text": "3.6 Geometric transformations\n165\nf(x)\ng(x’)\nx\nx’\nx’=h(x)\nf(x)\ng(x’)\nx\nx’\nx’=h(x)\n(a)\n(b)\nFigure 3.46\nForward warping algorithm: (a) a pixel f(x) is copied to its corresponding\nlocation x′ = h(x) in image g(x′); (b) detail of the source and destination pixel locations.\nIn fact, this approach suffers from several limitations. The process of copying a pixel\nf(x) to a location x′ in g is not well deﬁned when x′ has a non-integer value. What do we\ndo in such a case? What would you do?\nYou can round the value of x′ to the nearest integer coordinate and copy the pixel there,\nbut the resulting image has severe aliasing and pixels that jump around a lot when animating\nthe transformation. You can also “distribute” the value among its four nearest neighbors in\na weighted (bilinear) fashion, keeping track of the per-pixel weights and normalizing at the\nend. This technique is called splatting and is sometimes used for volume rendering in the\ngraphics community (Levoy and Whitted 1985; Levoy 1988; Westover 1989; Rusinkiewicz\nand Levoy 2000).\nUnfortunately, it suffers from both moderate amounts of aliasing and a\nfair amount of blur (loss of high-resolution detail).\nThe second major problem with forward warping is the appearance of cracks and holes,\nespecially when magnifying an image. Filling such holes with their nearby neighbors can\nlead to further aliasing and blurring.\nWhat can we do instead? A preferable solution is to use inverse warping (Algorithm 3.2),\nwhere each pixel in the destination image g(x′) is sampled from the original image f(x)\n(Figure 3.47).\nHow does this differ from the forward warping algorithm? For one thing, since ˆh(x′)\nis (presumably) deﬁned for all pixels in g(x′), we no longer have holes. More importantly,\nresampling an image at non-integer locations is a well-studied problem (general image inter-\npolation, see Section 3.5.2) and high-quality ﬁlters that control aliasing can be used.\nWhere does the function ˆh(x′) come from? Quite often, it can simply be computed as the\ninverse of h(x). In fact, all of the parametric transforms listed in Table 3.5 have closed form\nsolutions for the inverse transform: simply take the inverse of the 3 × 3 matrix specifying the\ntransform.\nIn other cases, it is preferable to formulate the problem of image warping as that of re-\nsampling a source image f(x) given a mapping x = ˆh(x′) from destination pixels x′ to\nsource pixels x. For example, in optical ﬂow (Section 8.4), we estimate the ﬂow ﬁeld as the\n166\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprocedure inverseWarp(f, h, out g):\nFor every pixel x′ in g(x′)\n1. Compute the source location x = ˆh(x′)\n2. Resample f(x) at location x and copy to g(x′)\nAlgorithm 3.2 Inverse warping algorithm for creating an image g(x′) from an image f(x)\nusing the parametric transform x′ = h(x).\nf(x)\ng(x’)\nx\nx’\nx=h(x’)\n^ \nf(x)\ng(x’)\nx\nx’\nx=h(x’)\n^ \n(a)\n(b)\nFigure 3.47 Inverse warping algorithm: (a) a pixel g(x′) is sampled from its corresponding\nlocation x = ˆh(x′) in image f(x); (b) detail of the source and destination pixel locations.\nlocation of the source pixel which produced the current pixel whose ﬂow is being estimated,\nas opposed to computing the destination pixel to which it is going. Similarly, when correcting\nfor radial distortion (Section 2.1.6), we calibrate the lens by computing for each pixel in the\nﬁnal (undistorted) image the corresponding pixel location in the original (distorted) image.\nWhat kinds of interpolation ﬁlter are suitable for the resampling process? Any of the ﬁl-\nters we studied in Section 3.5.2 can be used, including nearest neighbor, bilinear, bicubic, and\nwindowed sinc functions. While bilinear is often used for speed (e.g., inside the inner loop\nof a patch-tracking algorithm, see Section 8.1.3), bicubic, and windowed sinc are preferable\nwhere visual quality is important.\nTo compute the value of f(x) at a non-integer location x, we simply apply our usual FIR\nresampling ﬁlter,\ng(x, y) =\nX\nk,l\nf(k, l)h(x −k, y −l),\n(3.89)\nwhere (x, y) are the sub-pixel coordinate values and h(x, y) is some interpolating or smooth-\ning kernel. Recall from Section 3.5.2 that when decimation is being performed, the smoothing\nkernel is stretched and re-scaled according to the downsampling rate r.\nUnfortunately, for a general (non-zoom) image transformation, the resampling rate r is\nnot well deﬁned. Consider a transformation that stretches the x dimensions while squashing\n3.6 Geometric transformations\n167\nx\ny\nx’\ny’\nx\ny\nx’\ny’\nx\ny\nx’\ny’\nay’y\nay’x\nax’x\nax’y\n(a)\n(b)\n(c)\nmajor axis\nminor axis\nFigure 3.48\nAnisotropic texture ﬁltering: (a) Jacobian of transform A and the induced\nhorizontal and vertical resampling rates {ax′x, ax′y, ay′x, ay′y}; (b) elliptical footprint of an\nEWA smoothing kernel; (c) anisotropic ﬁltering using multiple samples along the major axis.\nImage pixels lie at line intersections.\nthe y dimensions. The resampling kernel should be performing regular interpolation along\nthe x dimension and smoothing (to anti-alias the blurred image) in the y direction. This gets\neven more complicated for the case of general afﬁne or perspective transforms.\nWhat can we do? Fortunately, Fourier analysis can help. The two-dimensional general-\nization of the one-dimensional domain scaling law given in Table 3.1 is\ng(Ax) ⇔|A|−1G(A−T f).\n(3.90)\nFor all of the transforms in Table 3.5 except perspective, the matrix A is already deﬁned.\nFor perspective transformations, the matrix A is the linearized derivative of the perspective\ntransformation (Figure 3.48a), i.e., the local afﬁne approximation to the stretching induced\nby the projection (Heckbert 1986; Wolberg 1990; Gomes, Darsa, Costa et al. 1999; Akenine-\nM¨oller and Haines 2002).\nTo prevent aliasing, we need to pre-ﬁlter the image f(x) with a ﬁlter whose frequency\nresponse is the projection of the ﬁnal desired spectrum through the A−T transform (Szeliski,\nWinder, and Uyttendaele 2010). In general (for non-zoom transforms), this ﬁlter is non-\nseparable and hence is very slow to compute. Therefore, a number of approximations to this\nﬁlter are used in practice, include MIP-mapping, elliptically weighted Gaussian averaging,\nand anisotropic ﬁltering (Akenine-M¨oller and Haines 2002).\nMIP-mapping\nMIP-mapping was ﬁrst proposed by Williams (1983) as a means to rapidly pre-ﬁlter images\nbeing used for texture mapping in computer graphics. A MIP-map18 is a standard image\n18 The term ‘MIP’ stands for multi in parvo, meaning ‘many in one’.",
  "image_path": "page_188.jpg",
  "pages": [
    187,
    188,
    189
  ]
}