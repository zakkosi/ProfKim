{
  "doc_id": "pages_501_503",
  "text": "10.2 High dynamic range imaging\n479\nMin\nMax\nR\nR\nValid  Region\n(a)\n(b)\n(c)\n(d)\nFigure 10.9\nEstimating the PSF without using a calibration pattern (Joshi, Szeliski, and\nKriegman 2008) c⃝2008 IEEE: (a) Input image with blue cross-section (proﬁle) location, (b)\nProﬁle of sensed and predicted step edges, (c–d) Locations and values of the predicted colors\nnear the edge locations.\nin Figure 10.9d. For every pixel that is surrounded by a complete set of valid estimated\nneighbors (green pixels in Figure 10.9c), apply the least squares formula (10.1) to estimate\nthe kernel K. The resulting locally estimated PSFs can be used to correct for chromatic\naberration (since the relative displacements between per-channel PSFs can be computed), as\nshown by Joshi, Szeliski, and Kriegman (2008).\nExercise 10.4 provides some more detailed instructions for implementing and testing\nedge-based PSF estimation algorithms. An alternative approach, which does not require the\nexplicit detection of edges but uses image statistics (gradient distributions) instead, is pre-\nsented by Fergus, Singh, Hertzmann et al. (2006).\n10.2 High dynamic range imaging\nAs we mentioned earlier in this chapter, registered images taken at different exposures can be\nused to calibrate the radiometric response function of a camera. More importantly, they can\nhelp you create well-exposed photographs under challenging conditions, such as brightly lit\n480\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.10\nSample indoor image where the areas outside the window are overexposed\nand inside the room are too dark.\n1\n1,500\n25,000\n400,000\n2,000,000\nFigure 10.11 Relative brightness of different scenes, ranging from 1 inside a dark room lit\nby a monitor to 2,000,000 looking at the sun. Photos courtesy of Paul Debevec.\nscenes where any single exposure contains saturated (overexposed) and dark (underexposed)\nregions (Figure 10.10). This problem is quite common, because the natural world contains a\nrange of radiance values that is far greater than can be captured with any photographic sensor\nor ﬁlm (Figure 10.11). Taking a set of bracketed exposures (exposures taken by a camera\nin automatic exposure bracketing (AEB) mode to deliberately under- and over-expose the\nimage) gives you the material from which to create a properly exposed photograph, as shown\nin Figure 10.12 (Reinhard, Ward, Pattanaik et al. 2005; Freeman 2008; Gulbins and Gulbins\n2009; Hasinoff, Durand, and Freeman 2010).\nWhile it is possible to combine pixels from different exposures directly into a ﬁnal com-\n+\n+\n⇒\nFigure 10.12\nA bracketed set of shots (using the camera’s automatic exposure bracketing\n(AEB) mode) and the resulting high dynamic range (HDR) composite.\n10.2 High dynamic range imaging\n481\nposite (Burt and Kolczynski 1993; Mertens, Kautz, and Reeth 2007), this approach runs the\nrisk of creating contrast reversals and halos. Instead, the more common approach is to pro-\nceed in three stages:\n1. Estimate the radiometric response function from the aligned images.\n2. Estimate a radiance map by selecting or blending pixels from different exposures.\n3. Tone map the resulting high dynamic range (HDR) image back into a displayable\ngamut.\nThe idea behind estimating the radiometric response function is relatively straightforward\n(Mann and Picard 1995; Debevec and Malik 1997; Mitsunaga and Nayar 1999; Reinhard,\nWard, Pattanaik et al. 2005). Suppose you take three sets of images at different exposures\n(shutter speeds), say at ±2 exposure values.11 If we were able to determine the irradiance\n(exposure) Ei at each pixel (2.101), we could plot it against the measured pixel value zij for\neach exposure time tj, as shown in Figure 10.13.\nUnfortunately, we do not know the irradiance values Ei, so these have to be estimated\nat the same time as the radiometric response function f, which can be written (Debevec and\nMalik 1997) as\nzij = f(Ei tj),\n(10.3)\nwhere tj is the exposure time for the jth image. The inverse response curve f −1 is given by\nf −1(zij) = Ei tj.\n(10.4)\nTaking logarithms of both sides (base 2 is convenient, as we can now measure quantities in\nEVs), we obtain\ng(zij) = log f −1(zij) = log Ei + log tj,\n(10.5)\nwhere g = log f −1 (which maps pixel values zij into log irradiance) is the curve we are\nestimating (Figure 10.13 turned on its side).\nDebevec and Malik (1997) assume that the exposure times tj are known. (Recall that\nthese can be obtained from a camera’s EXIF tags, but that they actually follow a power of 2\nprogression . . . , 1/128, 1/64, 1/32, 1/16, 1/8, . . . instead of the marked . . . , 1/125, 1/60, 1/30,\n1/15, 1/8, . . . values—see Exercise 2.5.) The unknowns are therefore the per-pixel exposures\nEi and the response values gk = g(k), where g can be discretized according to the 256\npixel values commonly observed in eight-bit images. (The response curves are calibrated\nseparately for each color channel.)\n11 Changing the shutter speed is preferable to changing the aperture, as the latter can modify the vignetting and\nfocus. Using ±2 “f-stops” (technically, exposure values, or EVs, since f-stops refer to apertures) is usually the right\ncompromise between capturing a good dynamic range and having properly exposed pixels everywhere.",
  "image_path": "page_502.jpg",
  "pages": [
    501,
    502,
    503
  ]
}