{
  "doc_id": "pages_081_083",
  "text": "2.1 Geometric primitives and transformations\n59\n(a)\n(b)\n(c)\nFigure 2.13 Radial lens distortions: (a) barrel, (b) pincushion, and (c) ﬁsheye. The ﬁsheye\nimage spans almost 180◦from side-to-side.\nwhere r2\nc = x2\nc + y2\nc and κ1 and κ2 are called the radial distortion parameters.4 After the\nradial distortion step, the ﬁnal pixel coordinates can be computed using\nxs\n=\nfx′\nc + cx\nys\n=\nfy′\nc + cy.\n(2.79)\nA variety of techniques can be used to estimate the radial distortion parameters for a given\nlens, as discussed in Section 6.3.5.\nSometimes the above simpliﬁed model does not model the true distortions produced by\ncomplex lenses accurately enough (especially at very wide angles). A more complete ana-\nlytic model also includes tangential distortions and decentering distortions (Slama 1980), but\nthese distortions are not covered in this book.\nFisheye lenses (Figure 2.13c) require a model that differs from traditional polynomial\nmodels of radial distortion. Fisheye lenses behave, to a ﬁrst approximation, as equi-distance\nprojectors of angles away from the optical axis (Xiong and Turkowski 1997), which is the\nsame as the polar projection described by Equations (9.22–9.24). Xiong and Turkowski\n(1997) describe how this model can be extended with the addition of an extra quadratic cor-\nrection in φ and how the unknown parameters (center of projection, scaling factor s, etc.)\ncan be estimated from a set of overlapping ﬁsheye images using a direct (intensity-based)\nnon-linear minimization algorithm.\nFor even larger, less regular distortions, a parametric distortion model using splines may\nbe necessary (Goshtasby 1989). If the lens does not have a single center of projection, it\n4 Sometimes the relationship between xc and ˆxc is expressed the other way around, i.e., xc = ˆxc(1 + κ1ˆr2\nc +\nκ2ˆr4\nc). This is convenient if we map image pixels into (warped) rays by dividing through by f. We can then undistort\nthe rays and have true 3D rays in space.\n60\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmay become necessary to model the 3D line (as opposed to direction) corresponding to each\npixel separately (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Sautot et al.\n1992; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm, Trudeau et\nal. 2009). Some of these techniques are described in more detail in Section 6.3.5, which\ndiscusses how to calibrate lens distortions.\nThere is one subtle issue associated with the simple radial distortion model that is often\nglossed over. We have introduced a non-linearity between the perspective projection and ﬁnal\nsensor array projection steps. Therefore, we cannot, in general, post-multiply an arbitrary 3×\n3 matrix K with a rotation to put it into upper-triangular form and absorb this into the global\nrotation. However, this situation is not as bad as it may at ﬁrst appear. For many applications,\nkeeping the simpliﬁed diagonal form of (2.59) is still an adequate model. Furthermore, if we\ncorrect radial and other distortions to an accuracy where straight lines are preserved, we have\nessentially converted the sensor back into a linear imager and the previous decomposition still\napplies.\n2.2 Photometric image formation\nIn modeling the image formation process, we have described how 3D geometric features in\nthe world are projected into 2D features in an image. However, images are not composed of\n2D features. Instead, they are made up of discrete color or intensity values. Where do these\nvalues come from? How do they relate to the lighting in the environment, surface properties\nand geometry, camera optics, and sensor properties (Figure 2.14)? In this section, we develop\na set of models to describe these interactions and formulate a generative process of image\nformation. A more detailed treatment of these topics can be found in other textbooks on\ncomputer graphics and image synthesis (Glassner 1995; Weyrich, Lawrence, Lensch et al.\n2008; Foley, van Dam, Feiner et al. 1995; Watt 1995; Cohen and Wallace 1993; Sillion and\nPuech 1994).\n2.2.1 Lighting\nImages cannot exist without light. To produce an image, the scene must be illuminated with\none or more light sources. (Certain modalities such as ﬂuorescent microscopy and X-ray\ntomography do not ﬁt this model, but we do not deal with them in this book.) Light sources\ncan generally be divided into point and area light sources.\nA point light source originates at a single location in space (e.g., a small light bulb),\npotentially at inﬁnity (e.g., the sun). (Note that for some applications such as modeling soft\nshadows (penumbras), the sun may have to be treated as an area light source.) In addition to\nits location, a point light source has an intensity and a color spectrum, i.e., a distribution over\n2.2 Photometric image formation\n61\nn^\nsurface\nlight \nsource\nimage plane\nsensor \nplane\noptics\nFigure 2.14\nA simpliﬁed model of photometric image formation. Light is emitted by one\nor more light sources and is then reﬂected from an object’s surface. A portion of this light is\ndirected towards the camera. This simpliﬁed model ignores multiple reﬂections, which often\noccur in real-world scenes.\nwavelengths L(λ). The intensity of a light source falls off with the square of the distance\nbetween the source and the object being lit, because the same light is being spread over a\nlarger (spherical) area. A light source may also have a directional falloff (dependence), but\nwe ignore this in our simpliﬁed model.\nArea light sources are more complicated. A simple area light source such as a ﬂuorescent\nceiling light ﬁxture with a diffuser can be modeled as a ﬁnite rectangular area emitting light\nequally in all directions (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995).\nWhen the distribution is strongly directional, a four-dimensional lightﬁeld can be used instead\n(Ashdown 1993).\nA more complex light distribution that approximates, say, the incident illumination on an\nobject sitting in an outdoor courtyard, can often be represented using an environment map\n(Greene 1986) (originally called a reﬂection map (Blinn and Newell 1976)). This representa-\ntion maps incident light directions ˆv to color values (or wavelengths, λ),\nL(ˆv; λ),\n(2.80)\nand is equivalent to assuming that all light sources are at inﬁnity. Environment maps can be\nrepresented as a collection of cubical faces (Greene 1986), as a single longitude–latitude map\n(Blinn and Newell 1976), or as the image of a reﬂecting sphere (Watt 1995). A convenient\nway to get a rough model of a real-world environment map is to take an image of a reﬂective\nmirrored sphere and to unwrap this image onto the desired environment map (Debevec 1998).\nWatt (1995) gives a nice discussion of environment mapping, including the formulas needed\nto map directions to pixels for the three most commonly used representations.",
  "image_path": "page_082.jpg",
  "pages": [
    81,
    82,
    83
  ]
}