{
  "doc_id": "pages_796_798",
  "text": "774\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis relaxation has been extensively studied in the literature, starting with the work of\nSchlesinger (1976). An important question is how to solve this LP efﬁciently. Unfortunately,\ngeneral-purpose LP solvers cannot handle large problems in vision (Yanover, Meltzer, and\nWeiss 2006). A large number of customized iterative techniques have been proposed. Most\nof these solve the dual problem, i.e., they formulate a lower bound on (B.36) and then try to\nmaximize this bound. The bound is often formulated using a convex combination of trees, as\nproposed in (Wainwright, Jaakkola, and Willsky 2005).\nThe LP lower bound can be maximized via a number of techniques, such as max-sum dif-\nfusion (Werner 2007), tree-reweighted message passing (TRW) (Wainwright, Jaakkola, and\nWillsky 2005; Kolmogorov 2006), subgradient methods (Schlesinger and Giginyak 2007a,b;\nKomodakis, Paragios, and Tziritas 2007), and Bregman projections (Ravikumar, Agarwal,\nand Wainwright 2008). Note that the max-sum diffusion and TRW algorithms are not guar-\nanteed to converge to a global maximum of LP—they may get stuck at a suboptimal point\n(Kolmogorov 2006; Werner 2007). However, in practice, this does not appear to be a problem\n(Kolmogorov 2006).\nFor some vision applications, algorithms based on relaxation (B.36) produce excellent\nresults. However, this is not guaranteed in all cases—after all, the problem is NP-hard.\nRecently, researchers have investigated alternative linear programming relaxations (Sontag\nand Jaakkola 2007; Sontag, Meltzer, Globerson et al. 2008; Komodakis and Paragios 2008;\nSchraudolph 2010). These algorithms are capable of producing tighter bounds compared to\n(B.36) at the expense of additional computational cost.\nLP relaxation and alpha expansion.\nSolving a linear program produces primal and dual\nsolutions that satisfy complementary slackness conditions. In general, the primal solution\nof (B.36) does not have to be integer-valued so, in practice, we may have to round it to\nobtain a valid labeling x. An alternative proposed by Komodakis and Tziritas (2007a); Ko-\nmodakis, Tziritas, and Paragios (2007) is to search for primal and dual solutions such that\nthey satisfy approximate complementary slackness conditions and the primal solution is al-\nready integer-valued. Several max-ﬂow-based algorithms are proposed by (Komodakis and\nTziritas 2007a; Komodakis, Tziritas, and Paragios 2007) for this purpose and the Fast-PD\nmethod (Komodakis, Tziritas, and Paragios 2007) is shown to perform best. In the case of\nmetric interactions, the default version of Fast-PD produces the same primal solution as the\nalpha-expansion algorithm (Boykov, Veksler, and Zabih 2001). This provides an interesting\ninterpretation of the alpha expansion algorithm as trying to approximately solve relaxation\n(B.36).\nUnlike the standard alpha expansion algorithm, Fast-PD also maintains a dual solution\nand thus runs faster in practice. Fast-PD can be extended to the case of semi-metric interac-\ntions (Komodakis, Tziritas, and Paragios 2007). The primal version of such extension was\nB.6 Uncertainty estimation (error analysis)\n775\nalso given by Rother, Kumar, Kolmogorov et al. (2005).\nB.6 Uncertainty estimation (error analysis)\nIn addition to computing the most likely estimate, many applications require an estimate for\nthe uncertainty in this estimate.9 The most general way to do this is to compute a complete\nprobability distribution over all of the unknowns but this is generally intractable. The one spe-\ncial case where it is easy to obtain a simple description for this distribution is linear estimation\nproblems with Gaussian noise, where the joint energy function (negative log likelihood of the\nposterior estimate) is a quadratic. In this case, the posterior distribution is a multi-variate\nGaussian and the covariance can be computed directly from the inverse of the problem Hes-\nsian. (Another name for the inverse covariance matrix, which is equal to the Hessian in such\nsimple cases, is the information matrix.)\nEven here, however, the full covariance matrix may be too large to compute and store. For\nexample, in large structure from motion problems, a large sparse Hessian normally results in a\nfull dense covariance matrix. In such cases, it is often considered acceptable to report only the\nvariance in the estimated quantities or simple covariance estimates on individual parameters,\nsuch as 3D point positions or camera pose estimates (Szeliski 1990a). More insight into the\nproblem, e.g., the dominant modes of uncertainty, can be obtained using eigenvalue analysis\n(Szeliski and Kang 1997).\nFor problems where the posterior energy is non-quadratic, e.g., in non-linear or robustiﬁed\nleast squares, it is still often possible to obtain an estimate of the Hessian in the vicinity of the\noptimal solution. In this case, the Cramer–Rao lower bound on the uncertainty (covariance)\ncan be computed as the inverse of the Hessian. Another way of saying this is that while the\nlocal Hessian can underestimate how “wide” the energy function can be, the covariance can\nnever be smaller than the estimate based on this local quadratic approximation. It is also\npossible to estimate a different kind of uncertainty (min-marginal energies) in general MRFs\nwhere the MAP inference is performed using graph cuts (Kohli and Torr 2008).\nWhile many computer vision applications ignore uncertainty modeling, it is often useful\nto compute these estimates just to get an intuitive feeling for the reliability of the estimates.\nCertain applications, such as Kalman ﬁltering, require the computation of this uncertainty\n(either explicitly as posterior covariances or implicitly as inverse covariances) in order to\noptimally integrate new measurements with previously computed estimates.\n9 This is particularly true of classic photogrammetry applications, where the reporting of precision is almost\nalways considered mandatory (F¨orstner 2005).\n776\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "image_path": "page_797.jpg",
  "pages": [
    796,
    797,
    798
  ]
}