{
  "doc_id": "pages_243_245",
  "text": "4.1 Points and patches\n221\nFigure 4.15\nMaximally stable extremal regions (MSERs) extracted and matched from a\nnumber of images (Matas, Chum, Urban et al. 2004) c⃝2004 Elsevier.\nFigure 4.16\nFeature matching: how can we extract local descriptors that are invariant\nto inter-image variations and yet still discriminative enough to establish correct correspon-\ndences?\nare therefore invariant to both afﬁne geometric and photometric (linear bias-gain or smooth\nmonotonic) transformations (Figure 4.15). If desired, an afﬁne coordinate frame can be ﬁt to\neach detected region using its moment matrix.\nThe area of feature point detectors continues to be very active, with papers appearing ev-\nery year at major computer vision conferences (Xiao and Shah 2003; Koethe 2003; Carneiro\nand Jepson 2005; Kenney, Zuliani, and Manjunath 2005; Bay, Tuytelaars, and Van Gool 2006;\nPlatel, Balmachnova, Florack et al. 2006; Rosten and Drummond 2006). Mikolajczyk, Tuyte-\nlaars, Schmid et al. (2005) survey a number of popular afﬁne region detectors and provide\nexperimental comparisons of their invariance to common image transformations such as scal-\ning, rotations, noise, and blur. These experimental results, code, and pointers to the surveyed\npapers can be found on their Web site at http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/.\nOf course, keypoints are not the only features that can be used for registering images.\nZoghlami, Faugeras, and Deriche (1997) use line segments as well as point-like features to\nestimate homographies between pairs of images, whereas Bartoli, Coquerelle, and Sturm\n(2004) use line segments with local correspondences along the edges to extract 3D structure\nand motion. Tuytelaars and Van Gool (2004) use afﬁne invariant regions to detect corre-\nspondences for wide baseline stereo matching, whereas Kadir, Zisserman, and Brady (2004)\ndetect salient regions where patch entropy and its rate of change with scale are locally max-\nimal. Corso and Hager (2005) use a related technique to ﬁt 2D oriented Gaussian kernels\nto homogeneous regions. More details on techniques for ﬁnding and matching curves, lines,\nand regions can be found later in this chapter.\n222\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.17\nMOPS descriptors are formed using an 8 × 8 sampling of bias and gain nor-\nmalized intensity values, with a sample spacing of ﬁve pixels relative to the detection scale\n(Brown, Szeliski, and Winder 2005) c⃝2005 IEEE. This low frequency sampling gives the\nfeatures some robustness to interest point location error and is achieved by sampling at a\nhigher pyramid level than the detection scale.\n4.1.2 Feature descriptors\nAfter detecting features (keypoints), we must match them, i.e., we must determine which\nfeatures come from corresponding locations in different images. In some situations, e.g., for\nvideo sequences (Shi and Tomasi 1994) or for stereo pairs that have been rectiﬁed (Zhang,\nDeriche, Faugeras et al. 1995; Loop and Zhang 1999; Scharstein and Szeliski 2002), the lo-\ncal motion around each feature point may be mostly translational. In this case, simple error\nmetrics, such as the sum of squared differences or normalized cross-correlation, described\nin Section 8.1 can be used to directly compare the intensities in small patches around each\nfeature point. (The comparative study by Mikolajczyk and Schmid (2005), discussed below,\nuses cross-correlation.) Because feature points may not be exactly located, a more accurate\nmatching score can be computed by performing incremental motion reﬁnement as described\nin Section 8.1.3 but this can be time consuming and can sometimes even decrease perfor-\nmance (Brown, Szeliski, and Winder 2005).\nIn most cases, however, the local appearance of features will change in orientation and\nscale, and sometimes even undergo afﬁne deformations. Extracting a local scale, orientation,\nor afﬁne frame estimate and then using this to resample the patch before forming the feature\ndescriptor is thus usually preferable (Figure 4.17).\nEven after compensating for these changes, the local appearance of image patches will\nusually still vary from image to image. How can we make image descriptors more invariant to\nsuch changes, while still preserving discriminability between different (non-corresponding)\npatches (Figure 4.16)? Mikolajczyk and Schmid (2005) review some recently developed\nview-invariant local image descriptors and experimentally compare their performance. Be-\nlow, we describe a few of these descriptors in more detail.\n4.1 Points and patches\n223\nBias and gain normalization (MOPS).\nFor tasks that do not exhibit large amounts of fore-\nshortening, such as image stitching, simple normalized intensity patches perform reasonably\nwell and are simple to implement (Brown, Szeliski, and Winder 2005) (Figure 4.17). In or-\nder to compensate for slight inaccuracies in the feature point detector (location, orientation,\nand scale), these multi-scale oriented patches (MOPS) are sampled at a spacing of ﬁve pixels\nrelative to the detection scale, using a coarser level of the image pyramid to avoid aliasing.\nTo compensate for afﬁne photometric variations (linear exposure changes or bias and gain,\n(3.3)), patch intensities are re-scaled so that their mean is zero and their variance is one.\nScale invariant feature transform (SIFT).\nSIFT features are formed by computing the\ngradient at each pixel in a 16×16 window around the detected keypoint, using the appropriate\nlevel of the Gaussian pyramid at which the keypoint was detected. The gradient magnitudes\nare downweighted by a Gaussian fall-off function (shown as a blue circle in (Figure 4.18a) in\norder to reduce the inﬂuence of gradients far from the center, as these are more affected by\nsmall misregistrations.\nIn each 4 × 4 quadrant, a gradient orientation histogram is formed by (conceptually)\nadding the weighted gradient value to one of eight orientation histogram bins. To reduce the\neffects of location and dominant orientation misestimation, each of the original 256 weighted\ngradient magnitudes is softly added to 2 × 2 × 2 histogram bins using trilinear interpolation.\nSoftly distributing values to adjacent histogram bins is generally a good idea in any appli-\ncation where histograms are being computed, e.g., for Hough transforms (Section 4.3.2) or\nlocal histogram equalization (Section 3.1.4).\nThe resulting 128 non-negative values form a raw version of the SIFT descriptor vector.\nTo reduce the effects of contrast or gain (additive variations are already removed by the gra-\ndient), the 128-D vector is normalized to unit length. To further make the descriptor robust to\nother photometric variations, values are clipped to 0.2 and the resulting vector is once again\nrenormalized to unit length.\nPCA-SIFT.\nKe and Sukthankar (2004) propose a simpler way to compute descriptors in-\nspired by SIFT; it computes the x and y (gradient) derivatives over a 39 × 39 patch and\nthen reduces the resulting 3042-dimensional vector to 36 using principal component analysis\n(PCA) (Section 14.2.1 and Appendix A.1.2). Another popular variant of SIFT is SURF (Bay,\nTuytelaars, and Van Gool 2006), which uses box ﬁlters to approximate the derivatives and\nintegrals used in SIFT.\nGradient location-orientation histogram (GLOH).\nThis descriptor, developed by Miko-\nlajczyk and Schmid (2005), is a variant on SIFT that uses a log-polar binning structure instead\nof the four quadrants used by Lowe (2004) (Figure 4.19). The spatial bins are of radius 6,",
  "image_path": "page_244.jpg",
  "pages": [
    243,
    244,
    245
  ]
}