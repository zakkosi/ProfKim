{
  "doc_id": "pages_203_205",
  "text": "3.7 Global optimization\n181\nthat the data were observed given the unknown state x. The second term Ep(x) is the prior\nenergy; it plays a role analogous to the smoothness energy in regularization. Note that the\nMAP estimate may not always be desirable, since it selects the “peak” in the posterior dis-\ntribution rather than some more stable statistic—see the discussion in Appendix B.2 and by\nLevin, Weiss, Durand et al. (2009).\nFor image processing applications, the unknowns x are the set of output pixels\nx = [f(0, 0) . . . f(m −1, n −1)],\nand the data are (in the simplest case) the input pixels\ny = [d(0, 0) . . . d(m −1, n −1)]\nas shown in Figure 3.56.\nFor a Markov random ﬁeld, the probability p(x) is a Gibbs or Boltzmann distribution,\nwhose negative log likelihood (according to the Hammersley–Clifford theorem) can be writ-\nten as a sum of pairwise interaction potentials,\nEp(x) =\nX\n{(i,j),(k,l)}∈N\nVi,j,k,l(f(i, j), f(k, l)),\n(3.109)\nwhere N(i, j) denotes the neighbors of pixel (i, j). In fact, the general version of the theorem\nsays that the energy may have to be evaluated over a larger set of cliques, which depend on\nthe order of the Markov random ﬁeld (Kindermann and Snell 1980; Geman and Geman 1984;\nBishop 2006; Kohli, Ladick´y, and Torr 2009; Kohli, Kumar, and Torr 2009).\nThe most commonly used neighborhood in Markov random ﬁeld modeling is the N4\nneighborhood, where each pixel in the ﬁeld f(i, j) interacts only with its immediate neigh-\nbors. The model in Figure 3.56, which we previously used in Figure 3.55 to illustrate the\ndiscrete version of ﬁrst-order regularization, shows an N4 MRF. The sx(i, j) and sy(i, j)\nblack boxes denote arbitrary interaction potentials between adjacent nodes in the random\nﬁeld and the w(i, j) denote the data penalty functions. These square nodes can also be inter-\npreted as factors in a factor graph version of the (undirected) graphical model (Bishop 2006),\nwhich is another name for interaction potentials. (Strictly speaking, the factors are (improper)\nprobability functions whose product is the (un-normalized) posterior distribution.)\nAs we will see in (3.112–3.113), there is a close relationship between these interaction\npotentials and the discretized versions of regularized image restoration problems. Thus, to\na ﬁrst approximation, we can view energy minimization being performed when solving a\nregularized problem and the maximum a posteriori inference being performed in an MRF as\nequivalent.\nWhile N4 neighborhoods are most commonly used, in some applications N8 (or even\nhigher order) neighborhoods perform better at tasks such as image segmentation because\n182\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure 3.56\nGraphical model for an N4 neighborhood Markov random ﬁeld. (The blue\nedges are added for an N8 neighborhood.) The white circles are the unknowns f(i, j), while\nthe dark circles are the input data d(i, j). The sx(i, j) and sy(i, j) black boxes denote arbi-\ntrary interaction potentials between adjacent nodes in the random ﬁeld, and the w(i, j) denote\nthe data penalty functions. The same graphical model can be used to depict a discrete version\nof a ﬁrst-order regularization problem (Figure 3.55).\nthey can better model discontinuities at different orientations (Boykov and Kolmogorov 2003;\nRother, Kohli, Feng et al. 2009; Kohli, Ladick´y, and Torr 2009; Kohli, Kumar, and Torr 2009).\nBinary MRFs\nThe simplest possible example of a Markov random ﬁeld is a binary ﬁeld. Examples of such\nﬁelds include 1-bit (black and white) scanned document images as well as images segmented\ninto foreground and background regions.\nTo denoise a scanned image, we set the data penalty to reﬂect the agreement between the\nscanned and ﬁnal images,\nEd(i, j) = wδ(f(i, j), d(i, j))\n(3.110)\nand the smoothness penalty to reﬂect the agreement between neighboring pixels\nEp(i, j) = Ex(i, j) + Ey(i, j) = sδ(f(i, j), f(i + 1, j)) + sδ(f(i, j), f(i, j + 1)). (3.111)\nOnce we have formulated the energy, how do we minimize it? The simplest approach is\nto perform gradient descent, ﬂipping one state at a time if it produces a lower energy. This ap-\nproach is known as contextual classiﬁcation (Kittler and F¨oglein 1984), iterated conditional\nmodes (ICM) (Besag 1986), or highest conﬁdence ﬁrst (HCF) (Chou and Brown 1990) if the\npixel with the largest energy decrease is selected ﬁrst.\nUnfortunately, these downhill methods tend to get easily stuck in local minima. An al-\nternative approach is to add some randomness to the process, which is known as stochastic\n3.7 Global optimization\n183\ngradient descent (Metropolis, Rosenbluth, Rosenbluth et al. 1953; Geman and Geman 1984).\nWhen the amount of noise is decreased over time, this technique is known as simulated an-\nnealing (Kirkpatrick, Gelatt, and Vecchi 1983; Carnevali, Coletti, and Patarnello 1985; Wol-\nberg and Pavlidis 1985; Swendsen and Wang 1987) and was ﬁrst popularized in computer\nvision by Geman and Geman (1984) and later applied to stereo matching by Barnard (1989),\namong others.\nEven this technique, however, does not perform that well (Boykov, Veksler, and Zabih\n2001). For binary images, a much better technique, introduced to the computer vision com-\nmunity by Boykov, Veksler, and Zabih (2001) is to re-formulate the energy minimization as\na max-ﬂow/min-cut graph optimization problem (Greig, Porteous, and Seheult 1989). This\ntechnique has informally come to be known as graph cuts in the computer vision community\n(Boykov and Kolmogorov 2010). For simple energy functions, e.g., those where the penalty\nfor non-identical neighboring pixels is a constant, this algorithm is guaranteed to produce the\nglobal minimum. Kolmogorov and Zabih (2004) formally characterize the class of binary\nenergy potentials (regularity conditions) for which these results hold, while newer work by\nKomodakis, Tziritas, and Paragios (2008) and Rother, Kolmogorov, Lempitsky et al. (2007)\nprovide good algorithms for the cases when they do not.\nIn addition to the above mentioned techniques, a number of other optimization approaches\nhave been developed for MRF energy minimization, such as (loopy) belief propagation and\ndynamic programming (for one-dimensional problems). These are discussed in more detail\nin Appendix B.5 as well as the comparative survey paper by Szeliski, Zabih, Scharstein et al.\n(2008).\nOrdinal-valued MRFs\nIn addition to binary images, Markov random ﬁelds can be applied to ordinal-valued labels\nsuch as grayscale images or depth maps. The term ”ordinal” indicates that the labels have an\nimplied ordering, e.g., that higher values are lighter pixels. In the next section, we look at\nunordered labels, such as source image labels for image compositing.\nIn many cases, it is common to extend the binary data and smoothness prior terms as\nEd(i, j) = w(i, j)ρd(f(i, j) −d(i, j))\n(3.112)\nand\nEp(i, j) = sx(i, j)ρp(f(i, j) −f(i + 1, j)) + sy(i, j)ρp(f(i, j) −f(i, j + 1)),\n(3.113)\nwhich are robust generalizations of the quadratic penalty terms (3.101) and (3.100), ﬁrst\nintroduced in (3.105). As before, the w(i, j), sx(i, j) and sy(i, j) weights can be used to\nlocally control the data weighting and the horizontal and vertical smoothness. Instead of",
  "image_path": "page_204.jpg",
  "pages": [
    203,
    204,
    205
  ]
}