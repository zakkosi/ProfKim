{
  "doc_id": "pages_472_474",
  "text": "450\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nheuristics, such as priors on typical camera motions as well as machine learning techniques\napplied to the problem of match validation.\n9.2.4 Direct vs. feature-based alignment\nGiven that there exist these two approaches to aligning images, which is preferable?\nEarly feature-based methods would get confused in regions that were either too textured\nor not textured enough. The features would often be distributed unevenly over the images,\nthereby failing to match image pairs that should have been aligned. Furthermore, establishing\ncorrespondences relied on simple cross-correlation between patches surrounding the feature\npoints, which did not work well when the images were rotated or had foreshortening due to\nhomographies.\nToday, feature detection and matching schemes are remarkably robust and can even be\nused for known object recognition from widely separated views (Lowe 2004). Features not\nonly respond to regions of high “cornerness” (F¨orstner 1986; Harris and Stephens 1988) but\nalso to “blob-like” regions (Lowe 2004), and uniform areas (Matas, Chum, Urban et al. 2004;\nTuytelaars and Van Gool 2004). Furthermore, because they operate in scale-space and use a\ndominant orientation (or orientation invariant descriptors), they can match images that differ\nin scale, orientation, and even foreshortening. Our own experience in working with feature-\nbased approaches is that if the features are well distributed over the image and the descriptors\nreasonably designed for repeatability, enough correspondences to permit image stitching can\nusually be found (Brown, Szeliski, and Winder 2005).\nThe biggest disadvantage of direct pixel-based alignment techniques is that they have a\nlimited range of convergence. Even though they can be used in a hierarchical (coarse-to-\nﬁne) estimation framework, in practice it is hard to use more than two or three levels of a\npyramid before important details start to be blurred away.11 For matching sequential frames\nin a video, direct approaches can usually be made to work. However, for matching partially\noverlapping images in photo-based panoramas or for image collections where the contrast or\ncontent varies too much, they fail too often to be useful and feature-based approaches are\ntherefore preferred.\n9.3 Compositing\nOnce we have registered all of the input images with respect to each other, we need to decide\nhow to produce the ﬁnal stitched mosaic image. This involves selecting a ﬁnal compositing\nsurface (ﬂat, cylindrical, spherical, etc.) and view (reference image). It also involves selecting\n11 Fourier-based correlation (Szeliski 1996; Szeliski and Shum 1997) can extend this range but requires cylindrical\nimages or motion prediction to be useful.\n9.3 Compositing\n451\nwhich pixels contribute to the ﬁnal composite and how to optimally blend these pixels to\nminimize visible seams, blur, and ghosting.\nIn this section, we review techniques that address these problems, namely compositing\nsurface parameterization, pixel and seam selection, blending, and exposure compensation.\nMy emphasis is on fully automated approaches to the problem. Since the creation of high-\nquality panoramas and composites is as much an artistic endeavor as a computational one,\nvarious interactive tools have been developed to assist this process (Agarwala, Dontcheva,\nAgrawala et al. 2004; Li, Sun, Tang et al. 2004; Rother, Kolmogorov, and Blake 2004).\nSome of these are covered in more detail in Section 10.4.\n9.3.1 Choosing a compositing surface\nThe ﬁrst choice to be made is how to represent the ﬁnal image. If only a few images are\nstitched together, a natural approach is to select one of the images as the reference and to\nthen warp all of the other images into its reference coordinate system. The resulting com-\nposite is sometimes called a ﬂat panorama, since the projection onto the ﬁnal surface is still\na perspective projection, and hence straight lines remain straight (which is often a desirable\nattribute).12\nFor larger ﬁelds of view, however, we cannot maintain a ﬂat representation without ex-\ncessively stretching pixels near the border of the image. (In practice, ﬂat panoramas start\nto look severely distorted once the ﬁeld of view exceeds 90◦or so.) The usual choice for\ncompositing larger panoramas is to use a cylindrical (Chen 1995; Szeliski 1996) or spherical\n(Szeliski and Shum 1997) projection, as described in Section 9.1.6. In fact, any surface used\nfor environment mapping in computer graphics can be used, including a cube map, which\nrepresents the full viewing sphere with the six square faces of a cube (Greene 1986; Szeliski\nand Shum 1997). Cartographers have also developed a number of alternative methods for\nrepresenting the globe (Bugayevskiy and Snyder 1995).\nThe choice of parameterization is somewhat application dependent and involves a trade-\noff between keeping the local appearance undistorted (e.g., keeping straight lines straight)\nand providing a reasonably uniform sampling of the environment. Automatically making\nthis selection and smoothly transitioning between representations based on the extent of the\npanorama is an active area of current research (Kopf, Uyttendaele, Deussen et al. 2007).\nAn interesting recent development in panoramic photography has been the use of stereo-\ngraphic projections looking down at the ground (in an outdoor scene) to create “little planet”\nrenderings.13\n12 Recently, some techniques have been developed to straighten curved lines in cylindrical and spherical panora-\nmas (Carroll, Agrawala, and Agarwala 2009; Kopf, Lischinski, Deussen et al. 2009).\n13 These are inspired by The Little Prince by Antoine De Saint-Exupery. Go to http://www.ﬂickr.com and search\n452\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nView selection.\nOnce we have chosen the output parameterization, we still need to deter-\nmine which part of the scene will be centered in the ﬁnal view. As mentioned above, for a ﬂat\ncomposite, we can choose one of the images as a reference. Often, a reasonable choice is the\none that is geometrically most central. For example, for rotational panoramas represented as\na collection of 3D rotation matrices, we can choose the image whose z-axis is closest to the\naverage z-axis (assuming a reasonable ﬁeld of view). Alternatively, we can use the average\nz-axis (or quaternion, but this is trickier) to deﬁne the reference rotation matrix.\nFor larger, e.g., cylindrical or spherical, panoramas, we can use the same heuristic if a\nsubset of the viewing sphere has been imaged. In the case of full 360◦panoramas, a better\nchoice might be to choose the middle image from the sequence of inputs, or sometimes the\nﬁrst image, assuming this contains the object of greatest interest. In all of these cases, having\nthe user control the ﬁnal view is often highly desirable. If the “up vector” computation de-\nscribed in Section 9.2.1 is working correctly, this can be as simple as panning over the image\nor setting a vertical “center line” for the ﬁnal panorama.\nCoordinate transformations.\nAfter selecting the parameterization and reference view, we\nstill need to compute the mappings between the input and output pixels coordinates.\nIf the ﬁnal compositing surface is ﬂat (e.g., a single plane or the face of a cube map)\nand the input images have no radial distortion, the coordinate transformation is the simple\nhomography described by (9.5). This kind of warping can be performed in graphics hardware\nby appropriately setting texture mapping coordinates and rendering a single quadrilateral.\nIf the ﬁnal composite surface has some other analytic form (e.g., cylindrical or spherical),\nwe need to convert every pixel in the ﬁnal panorama into a viewing ray (3D point) and then\nmap it back into each image according to the projection (and optionally radial distortion)\nequations. This process can be made more efﬁcient by precomputing some lookup tables,\ne.g., the partial trigonometric functions needed to map cylindrical or spherical coordinates to\n3D coordinates or the radial distortion ﬁeld at each pixel. It is also possible to accelerate this\nprocess by computing exact pixel mappings on a coarser grid and then interpolating these\nvalues.\nWhen the ﬁnal compositing surface is a texture-mapped polyhedron, a slightly more so-\nphisticated algorithm must be used. Not only do the 3D and texture map coordinates have to\nbe properly handled, but a small amount of overdraw outside the triangle footprints in the tex-\nture map is necessary, to ensure that the texture pixels being interpolated during 3D rendering\nhave valid values (Szeliski and Shum 1997).\nSampling issues.\nWhile the above computations can yield the correct (fractional) pixel\naddresses in each input image, we still need to pay attention to sampling issues. For example,\nfor “little planet projection”.",
  "image_path": "page_473.jpg",
  "pages": [
    472,
    473,
    474
  ]
}