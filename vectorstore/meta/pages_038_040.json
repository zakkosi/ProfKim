{
  "doc_id": "pages_038_040",
  "text": "16\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.9 Examples of computer vision algorithms from the 1990s: (a) factorization-based\nstructure from motion (Tomasi and Kanade 1992) c⃝1992 Springer, (b) dense stereo match-\ning (Boykov, Veksler, and Zabih 2001), (c) multi-view reconstruction (Seitz and Dyer 1999)\nc⃝1999 Springer, (d) face tracking (Matthews, Xiao, and Baker 2007), (e) image segmenta-\ntion (Belongie, Fowlkes, Chung et al. 2002) c⃝2002 Springer, (f) face recognition (Turk and\nPentland 1991a).\nwith accurate physical models of radiance transport and color image formation created its own\nsubﬁeld known as physics-based vision. A good survey of the ﬁeld can be found in the three-\nvolume collection on this topic (Wolff, Shafer, and Healey 1992a; Healey and Shafer 1992;\nShafer, Healey, and Wolff 1992).\nOptical ﬂow methods (see Chapter 8) continued to be improved (Nagel and Enkelmann\n1986; Bolles, Baker, and Marimont 1987; Horn and Weldon Jr. 1988; Anandan 1989; Bergen,\nAnandan, Hanna et al. 1992; Black and Anandan 1996; Bruhn, Weickert, and Schn¨orr 2005;\nPapenberg, Bruhn, Brox et al. 2006), with (Nagel 1986; Barron, Fleet, and Beauchemin 1994;\nBaker, Black, Lewis et al. 2007) being good surveys. Similarly, a lot of progress was made\non dense stereo correspondence algorithms (see Chapter 11, Okutomi and Kanade (1993,\n1994); Boykov, Veksler, and Zabih (1998); Birchﬁeld and Tomasi (1999); Boykov, Veksler,\nand Zabih (2001), and the survey and comparison in Scharstein and Szeliski (2002)), with\nthe biggest breakthrough being perhaps global optimization using graph cut techniques (Fig-\nure 1.9b) (Boykov, Veksler, and Zabih 2001).\n1.2 A brief history\n17\nMulti-view stereo algorithms (Figure 1.9c) that produce complete 3D surfaces (see Sec-\ntion 11.6) were also an active topic of research (Seitz and Dyer 1999; Kutulakos and Seitz\n2000) that continues to be active today (Seitz, Curless, Diebel et al. 2006). Techniques for\nproducing 3D volumetric descriptions from binary silhouettes (see Section 11.6.2) continued\nto be developed (Potmesil 1987; Srivasan, Liang, and Hackwood 1990; Szeliski 1993; Lau-\nrentini 1994), along with techniques based on tracking and reconstructing smooth occluding\ncontours (see Section 11.2.1 and Cipolla and Blake 1992; Vaillant and Faugeras 1992; Zheng\n1994; Boyer and Berger 1997; Szeliski and Weiss 1998; Cipolla and Giblin 2000).\nTracking algorithms also improved a lot, including contour tracking using active contours\n(see Section 5.1), such as snakes (Kass, Witkin, and Terzopoulos 1988), particle ﬁlters (Blake\nand Isard 1998), and level sets (Malladi, Sethian, and Vemuri 1995), as well as intensity-based\n(direct) techniques (Lucas and Kanade 1981; Shi and Tomasi 1994; Rehg and Kanade 1994),\noften applied to tracking faces (Figure 1.9d) (Lanitis, Taylor, and Cootes 1997; Matthews and\nBaker 2004; Matthews, Xiao, and Baker 2007) and whole bodies (Sidenbladh, Black, and\nFleet 2000; Hilton, Fua, and Ronfard 2006; Moeslund, Hilton, and Kr¨uger 2006).\nImage segmentation (see Chapter 5) (Figure 1.9e), a topic which has been active since\nthe earliest days of computer vision (Brice and Fennema 1970; Horowitz and Pavlidis 1976;\nRiseman and Arbib 1977; Rosenfeld and Davis 1979; Haralick and Shapiro 1985; Pavlidis\nand Liow 1990), was also an active topic of research, producing techniques based on min-\nimum energy (Mumford and Shah 1989) and minimum description length (Leclerc 1989),\nnormalized cuts (Shi and Malik 2000), and mean shift (Comaniciu and Meer 2002).\nStatistical learning techniques started appearing, ﬁrst in the application of principal com-\nponent eigenface analysis to face recognition (Figure 1.9f) (see Section 14.2.1 and Turk and\nPentland 1991a) and linear dynamical systems for curve tracking (see Section 5.1.1 and Blake\nand Isard 1998).\nPerhaps the most notable development in computer vision during this decade was the\nincreased interaction with computer graphics (Seitz and Szeliski 1999), especially in the\ncross-disciplinary area of image-based modeling and rendering (see Chapter 13). The idea of\nmanipulating real-world imagery directly to create new animations ﬁrst came to prominence\nwith image morphing techniques (Figure1.5c) (see Section 3.6.3 and Beier and Neely 1992)\nand was later applied to view interpolation (Chen and Williams 1993; Seitz and Dyer 1996),\npanoramic image stitching (Figure1.5a) (see Chapter 9 and Mann and Picard 1994; Chen\n1995; Szeliski 1996; Szeliski and Shum 1997; Szeliski 2006a), and full light-ﬁeld rendering\n(Figure 1.10a) (see Section 13.3 and Gortler, Grzeszczuk, Szeliski et al. 1996; Levoy and\nHanrahan 1996; Shade, Gortler, He et al. 1998). At the same time, image-based modeling\ntechniques (Figure 1.10b) for automatically creating realistic 3D models from collections of\nimages were also being introduced (Beardsley, Torr, and Zisserman 1996; Debevec, Taylor,\nand Malik 1996; Taylor, Debevec, and Malik 1996).\n18\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.10\nRecent examples of computer vision algorithms: (a) image-based rendering\n(Gortler, Grzeszczuk, Szeliski et al. 1996), (b) image-based modeling (Debevec, Taylor, and\nMalik 1996) c⃝1996 ACM, (c) interactive tone mapping (Lischinski, Farbman, Uyttendaele\net al. 2006a) (d) texture synthesis (Efros and Freeman 2001), (e) feature-based recognition\n(Fergus, Perona, and Zisserman 2007), (f) region-based recognition (Mori, Ren, Efros et al.\n2004) c⃝2004 IEEE.\n2000s.\nThis past decade has continued to see a deepening interplay between the vision and\ngraphics ﬁelds. In particular, many of the topics introduced under the rubric of image-based\nrendering, such as image stitching (see Chapter 9), light-ﬁeld capture and rendering (see\nSection 13.3), and high dynamic range (HDR) image capture through exposure bracketing\n(Figure1.5b) (see Section 10.2 and Mann and Picard 1995; Debevec and Malik 1997), were\nre-christened as computational photography (see Chapter 10) to acknowledge the increased\nuse of such techniques in everyday digital photography. For example, the rapid adoption of\nexposure bracketing to create high dynamic range images necessitated the development of\ntone mapping algorithms (Figure 1.10c) (see Section 10.2.1) to convert such images back\nto displayable results (Fattal, Lischinski, and Werman 2002; Durand and Dorsey 2002; Rein-\nhard, Stark, Shirley et al. 2002; Lischinski, Farbman, Uyttendaele et al. 2006a). In addition to\nmerging multiple exposures, techniques were developed to merge ﬂash images with non-ﬂash\ncounterparts (Eisemann and Durand 2004; Petschnigg, Agrawala, Hoppe et al. 2004) and to\ninteractively or automatically select different regions from overlapping images (Agarwala,",
  "image_path": "page_039.jpg",
  "pages": [
    38,
    39,
    40
  ]
}