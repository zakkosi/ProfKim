{
  "doc_id": "pages_357_359",
  "text": "6.4 Additional reading\n335\nof straight lines, especially lines aligned with and near the edges of the image. The radial\ndistortion parameters can then be adjusted until all of the lines in the image are straight,\nwhich is commonly called the plumb-line method (Brown 1971; Kang 2001; El-Melegy and\nFarag 2003). Exercise 6.10 gives some more details on how to implement such a technique.\nAnother approach is to use several overlapping images and to combine the estimation\nof the radial distortion parameters with the image alignment process, i.e., by extending the\npipeline used for stitching in Section 9.2.1. Sawhney and Kumar (1999) use a hierarchy\nof motion models (translation, afﬁne, projective) in a coarse-to-ﬁne strategy coupled with\na quadratic radial distortion correction term. They use direct (intensity-based) minimiza-\ntion to compute the alignment. Stein (1997) uses a feature-based approach combined with\na general 3D motion model (and quadratic radial distortion), which requires more matches\nthan a parallax-free rotational panorama but is potentially more general. More recent ap-\nproaches sometimes simultaneously compute both the unknown intrinsic parameters and the\nradial distortion coefﬁcients, which may include higher-order terms or more complex rational\nor non-parametric forms (Claus and Fitzgibbon 2005; Sturm 2005; Thirthala and Pollefeys\n2005; Barreto and Daniilidis 2005; Hartley and Kang 2005; Steele and Jaynes 2006; Tardif,\nSturm, Trudeau et al. 2009).\nWhen a known calibration target is being used (Figure 6.8), the radial distortion estima-\ntion can be folded into the estimation of the other intrinsic and extrinsic parameters (Zhang\n2000; Hartley and Kang 2007; Tardif, Sturm, Trudeau et al. 2009). This can be viewed as\nadding another stage to the general non-linear minimization pipeline shown in Figure 6.5\nbetween the intrinsic parameter multiplication box f C and the perspective division box f P.\n(See Exercise 6.11 on more details for the case of a planar calibration target.)\nOf course, as discussed in Section 2.1.6, more general models of lens distortion, such as\nﬁsheye and non-central projection, may sometimes be required. While the parameterization\nof such lenses may be more complicated (Section 2.1.6), the general approach of either us-\ning calibration rigs with known 3D positions or self-calibration through the use of multiple\noverlapping images of a scene can both be used (Hartley and Kang 2007; Tardif, Sturm, and\nRoy 2007). The same techniques used to calibrate for radial distortion can also be used to\nreduce the amount of chromatic aberration by separately calibrating each color channel and\nthen warping the channels to put them back into alignment (Exercise 6.12).\n6.4 Additional reading\nHartley and Zisserman (2004) provide a wonderful introduction to the topics of feature-based\nalignment and optimal motion estimation, as well as an in-depth discussion of camera cali-\nbration and pose estimation techniques.\n336\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTechniques for robust estimation are discussed in more detail in Appendix B.3 and in\nmonographs and review articles on this topic (Huber 1981; Hampel, Ronchetti, Rousseeuw et\nal. 1986; Rousseeuw and Leroy 1987; Black and Rangarajan 1996; Stewart 1999). The most\ncommonly used robust initialization technique in computer vision is RANdom SAmple Con-\nsensus (RANSAC) (Fischler and Bolles 1981), which has spawned a series of more efﬁcient\nvariants (Nist´er 2003; Chum and Matas 2005).\nThe topic of registering 3D point data sets is called absolute orientation (Horn 1987) and\n3D pose estimation (Lorusso, Eggert, and Fisher 1995). A variety of techniques has been\ndeveloped for simultaneously computing 3D point correspondences and their corresponding\nrigid transformations (Besl and McKay 1992; Zhang 1994; Szeliski and Lavall´ee 1996; Gold,\nRangarajan, Lu et al. 1998; David, DeMenthon, Duraiswami et al. 2004; Li and Hartley 2007;\nEnqvist, Josephson, and Kahl 2009).\nCamera calibration was ﬁrst studied in photogrammetry (Brown 1971; Slama 1980; Atkin-\nson 1996; Kraus 1997) but it has also been widely studied in computer vision (Tsai 1987;\nGremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Szeliski et al. 1992; Zhang\n2000; Grossberg and Nayar 2001). Vanishing points observed either from rectahedral cali-\nbration objects or man-made architecture are often used to perform rudimentary calibration\n(Caprile and Torre 1990; Becker and Bove 1995; Liebowitz and Zisserman 1998; Cipolla,\nDrummond, and Robertson 1999; Antone and Teller 2002; Criminisi, Reid, and Zisserman\n2000; Hartley and Zisserman 2004; Pﬂugfelder 2008). Performing camera calibration without\nusing known targets is known as self-calibration and is discussed in textbooks and surveys on\nstructure from motion (Faugeras, Luong, and Maybank 1992; Hartley and Zisserman 2004;\nMoons, Van Gool, and Vergauwen 2010). One popular subset of such techniques uses pure\nrotational motion (Stein 1995; Hartley 1997b; Hartley, Hayman, de Agapito et al. 2000; de\nAgapito, Hayman, and Reid 2001; Kang and Weiss 1999; Shum and Szeliski 2000; Frahm\nand Koch 2003).\n6.5 Exercises\nEx 6.1: Feature-based image alignment for ﬂip-book animations\nTake a set of photos of\nan action scene or portrait (preferably in motor-drive—continuous shooting—mode) and\nalign them to make a composite or ﬂip-book animation.\n1. Extract features and feature descriptors using some of the techniques described in Sec-\ntions 4.1.1–4.1.2.\n2. Match your features using nearest neighbor matching with a nearest neighbor distance\nratio test (4.18).\n6.5 Exercises\n337\n3. Compute an optimal 2D translation and rotation between the ﬁrst image and all subse-\nquent images, using least squares (Section 6.1.1) with optional RANSAC for robustness\n(Section 6.1.4).\n4. Resample all of the images onto the ﬁrst image’s coordinate frame (Section 3.6.1) using\neither bilinear or bicubic resampling and optionally crop them to their common area.\n5. Convert the resulting images into an animated GIF (using software available from the\nWeb) or optionally implement cross-dissolves to turn them into a “slo-mo” video.\n6. (Optional) Combine this technique with feature-based (Exercise 3.25) morphing.\nEx 6.2: Panography\nCreate the kind of panograph discussed in Section 6.1.2 and com-\nmonly found on the Web.\n1. Take a series of interesting overlapping photos.\n2. Use the feature detector, descriptor, and matcher developed in Exercises 4.1–4.4 (or\nexisting software) to match features among the images.\n3. Turn each connected component of matching features into a track, i.e., assign a unique\nindex i to each track, discarding any tracks that are inconsistent (contain two different\nfeatures in the same image).\n4. Compute a global translation for each image using Equation (6.12).\n5. Since your matches probably contain errors, turn the above least square metric into a\nrobust metric (6.25) and re-solve your system using iteratively reweighted least squares.\n6. Compute the size of the resulting composite canvas and resample each image into its\nﬁnal position on the canvas. (Keeping track of bounding boxes will make this more\nefﬁcient.)\n7. Average all of the images, or choose some kind of ordering and implement translucent\nover compositing (3.8).\n8. (Optional) Extend your parametric motion model to include rotations and scale, i.e.,\nthe similarity transform given in Table 6.1. Discuss how you could handle the case of\ntranslations and rotations only (no scale).\n9. (Optional) Write a simple tool to let the user adjust the ordering and opacity, and add\nor remove images.",
  "image_path": "page_358.jpg",
  "pages": [
    357,
    358,
    359
  ]
}