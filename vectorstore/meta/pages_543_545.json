{
  "doc_id": "pages_543_545",
  "text": "10.5 Texture analysis and synthesis\n521\n(a)\n(b)\n(c)\n(d)\nFigure 10.52\nImage inpainting (hole ﬁlling): (a–b) propagation along isophote directions\n(Bertalmio, Sapiro, Caselles et al. 2000) c⃝2000 ACM; (c–d) exemplar-based inpainting\nwith conﬁdence-based ﬁlling order (Criminisi, P´erez, and Toyama 2004).\n10.5.1 Application: Hole ﬁlling and inpainting\nFilling holes left behind when objects or defects are excised from photographs, which is\nknown as inpainting, is one of the most common applications of texture synthesis. Such\ntechniques are used not only to remove unwanted people or interlopers from photographs\n(King 1997) but also to ﬁx small defects in old photos and movies (scratch removal) or to\nremove wires holding props or actors in mid-air during ﬁlming (wire removal). Bertalmio,\nSapiro, Caselles et al. (2000) solve the problem by propagating pixel values along isophote\n(constant-value) directions interleaved with some anisotropic diffusion steps (Figure 10.52a–\nb). Telea (2004) develops a faster technique that uses the fast marching method from level\nsets (Section 5.1.4). However, these techniques will not hallucinate texture in the missing\nregions. Bertalmio, Vese, Sapiro et al. (2003) augment their earlier technique by adding\nsynthetic texture to the inﬁlled regions.\nThe example-based (non-parametric) texture generation techniques discussed in the pre-\nvious section can also be used by ﬁlling the holes from the outside in (the “onion-peel” or-\ndering). However, this approach may fail to propagate strong oriented structures. Criminisi,\nP´erez, and Toyama (2004) use exemplar-based texture synthesis where the order of synthesis\nis determined by the strength of the gradient along the region boundary (Figures 10.1d and\n10.52c–d). Sun, Yuan, Jia et al. (2004) present a related approach where the user draws in-\nteractive lines to indicate where structures should be preferentially propagated. Additional\ntechniques related to these approaches include those developed by Drori, Cohen-Or, and\nYeshurun (2003), Kwatra, Sch¨odl, Essa et al. (2003), Kwatra, Essa, Bobick et al. (2005),\nWilczkowiak, Brostow, Tordoff et al. (2005), Komodakis and Tziritas (2007b), and Wexler,\nShechtman, and Irani (2007).\nMost hole ﬁlling algorithms borrow small pieces of the original image to ﬁll in the holes.\nWhen a large database of source images is available, e.g., when images are taken from a\n522\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 10.53 Texture transfer (Efros and Freeman 2001) c⃝2001 ACM: (a) reference (tar-\nget) image; (b) source texture; (c) image (partially) rendered using the texture.\nphoto sharing site or the Internet, it is sometimes possible to copy a single contiguous image\nregion to ﬁll the hole. Hays and Efros (2007) present such a technique, which uses image\ncontext and boundary compatibility to select the source image, which is then blended with\nthe original (holey) image using graph cuts and Poisson blending. This technique is discussed\nin more detail in Section 14.4.4 and Figure 14.46.\n10.5.2 Application: Non-photorealistic rendering\nTwo more applications of the exemplar-based texture synthesis ideas are texture transfer\n(Efros and Freeman 2001) and image analogies (Hertzmann, Jacobs, Oliver et al. 2001),\nwhich are both examples of non-photorealistic rendering (Gooch and Gooch 2001).\nIn addition to using a source texture image, texture transfer also takes a reference (or\ntarget) image, and tries to match certain characteristics of the target image with the newly\nsynthesized image. For example, the new image being rendered in Figure 10.53c not only\ntries to satisfy the usual similarity constraints with the source texture in Figure 10.53b, but it\nalso tries to match the luminance characteristics of the reference image. Efros and Freeman\n(2001) mention that blurred image intensities or local image orientation angles are alternative\nquantities that could be matched.\nHertzmann, Jacobs, Oliver et al. (2001) formulate the following problem:\nGiven a pair of images A and A′ (the unﬁltered and ﬁltered source images, re-\nspectively), along with some additional unﬁltered target image B, synthesize a\nnew ﬁltered target image B′ such that\nA : A′ :: B : B′.\n10.5 Texture analysis and synthesis\n523\nA\nA′\nB\nB′\nFigure 10.54 Image analogies (Hertzmann, Jacobs, Oliver et al. 2001) c⃝2001 ACM. Given\nan example pair of a source image A and its rendered (ﬁltered) version A′, generate the\nrendered version B′ from another unﬁltered source image B.\nInstead of having the user program a certain non-photorealistic rendering effect, it is sufﬁcient\nto supply the system with examples of before and after images, and let the system synthesize\nthe novel image using exemplar-based synthesis, as shown in Figure 10.54.\nThe algorithm used to solve image analogies proceeds in a manner analogous to the tex-\nture synthesis algorithms of (Efros and Leung 1999; Wei and Levoy 2000). Once Gaus-\nsian pyramids have been computed for all of the source and reference images, the algorithm\nlooks for neighborhoods in the source ﬁltered pyramids generated from A′ that are simi-\nlar to the partially constructed neighborhood in B′, while at the same time having similar\nmulti-resolution appearances at corresponding locations in A and B. As with texture trans-\nfer, appearance characteristics can include not only (blurred) color or luminance values but\nalso orientations.\nThis general framework allows image analogies to be applied to a variety of rendering\ntasks. In addition to exemplar-based non-photorealistic rendering, image analogies can be\nused for traditional texture synthesis, super-resolution, and texture transfer (using the same\ntextured image for both A and A′). If only the ﬁltered (rendered) image A′ is available, as\nis the case with paintings, the missing reference image A can be hallucinated using a smart\n(edge preserving) blur operator. Finally, it is possible to train a system to perform texture-by-\nnumbers by manually painting over a natural image with pseudocolors corresponding to pix-\nels’ semantic meanings, e.g., water, trees, and grass (Figure 10.55a–b). The resulting system\ncan then convert a novel sketch into a fully rendered synthetic photograph (Figure 10.55c–d).\nIn more recent work, Cheng, Vishwanathan, and Zhang (2008) add ideas from image quilting\n(Efros and Freeman 2001) and MRF inference (Komodakis, Tziritas, and Paragios 2008) to\nthe basic image analogies algorithm, while Ramanarayanan and Bala (2007) recast this pro-\ncess as energy minimization, which means it can also be viewed as a conditional random ﬁeld\n(Section 3.7.2), and devise an efﬁcient algorithm to ﬁnd a good minimum.\nMore traditional ﬁltering and feature detection techniques can also be used for non-",
  "image_path": "page_544.jpg",
  "pages": [
    543,
    544,
    545
  ]
}