{
  "doc_id": "pages_433_435",
  "text": "8.4 Optical ﬂow\n411\nmoving in a static scene (rigid motion), we can re-formulate the problem as the estimation of\na per-pixel depth along with the parameters of the global camera motion (Adiv 1989; Hanna\n1991; Bergen, Anandan, Hanna et al. 1992; Szeliski and Coughlan 1997; Nir, Bruckstein,\nand Kimmel 2008; Wedel, Cremers, Pock et al. 2009). Such techniques are closely related to\nstereo matching (Chapter 11). Alternatively, we can estimate either per-image or per-segment\nafﬁne motion models combined with per-pixel residual corrections (Black and Jepson 1996;\nJu, Black, and Jepson 1996; Chang, Tekalp, and Sezan 1997; M´emin and P´erez 2002). We\nrevisit this topic in Section 8.5.\nOf course, image brightness may not always be an appropriate metric for measuring ap-\npearance consistency, e.g., when the lighting in an image is varying. As discussed in Sec-\ntion 8.1, matching gradients, ﬁltered images, or other metrics such as image Hessians (sec-\nond derivative measures) may be more appropriate. It is also possible to locally compute the\nphase of steerable ﬁlters in the image, which is insensitive to both bias and gain transforma-\ntions (Fleet and Jepson 1990). Papenberg, Bruhn, Brox et al. (2006) review and explore such\nconstraints and also provide a detailed analysis and justiﬁcation for iteratively re-warping\nimages during incremental ﬂow computation.\nBecause the brightness constancy constraint is evaluated at each pixel independently,\nrather than being summed over patches where the constant ﬂow assumption may be violated,\nglobal optimization approaches tend to perform better near motion discontinuities. This is\nespecially true if robust metrics are used in the smoothness constraint (Black and Anandan\n1996; Bab-Hadiashar and Suter 1998a).12 One popular choice for robust metrics in the L1\nnorm, also known as total variation (TV), which results in a convex energy whose global\nminimum can be found (Bruhn, Weickert, and Schn¨orr 2005; Papenberg, Bruhn, Brox et\nal. 2006). Anisotropic smoothness priors, which apply a different smoothness in the direc-\ntions parallel and perpendicular to the image gradient, are another popular choice (Nagel and\nEnkelmann 1986; Sun, Roth, Lewis et al. 2008; Werlberger, Trobin, Pock et al. 2009). It\nis also possible to learn a set of better smoothness constraints (derivative ﬁlters and robust\nfunctions) from a set of paired ﬂow and intensity images (Sun, Roth, Lewis et al. 2008). Ad-\nditional details on some of these techniques are given by Baker, Black, Lewis et al. (2007)\nand Baker, Scharstein, Lewis et al. (2009).\nBecause of the large, two-dimensional search space in estimating ﬂow, most algorithms\nuse variations of gradient descent and coarse-to-ﬁne continuation methods to minimize the\nglobal energy function. This contrasts starkly with stereo matching (which is an “easier”\none-dimensional disparity estimation problem), where combinatorial optimization techniques\nhave been the method of choice for the last decade.\nFortunately, combinatorial optimization methods based on Markov random ﬁelds are be-\n12 Robust brightness metrics (Section 8.1, (8.2)) can also help improve the performance of window-based ap-\nproaches (Black and Anandan 1996).\n412\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.12\nEvaluation of the results of 24 optical ﬂow algorithms, October 2009, http:\n//vision.middlebury.edu/ﬂow/, (Baker, Scharstein, Lewis et al. 2009). By moving the mouse\npointer over an underlined performance score, the user can interactively view the correspond-\ning ﬂow and error maps. Clicking on a score toggles between the computed and ground truth\nﬂows. Next to each score, the corresponding rank in the current column is indicated by a\nsmaller blue number. The minimum (best) score in each column is shown in boldface. The\ntable is sorted by the average rank (computed over all 24 columns, three region masks for each\nof the eight sequences). The average rank serves as an approximate measure of performance\nunder the selected metric/statistic.\n8.4 Optical ﬂow\n413\nginning to appear and tend to be among the better-performing methods on the recently re-\nleased optical ﬂow database (Baker, Black, Lewis et al. 2007).13\nExamples of such techniques include the one developed by Glocker, Paragios, Komodakis\net al. (2008), who use a coarse-to-ﬁne strategy with per-pixel 2D uncertainty estimates, which\nare then used to guide the reﬁnement and search at the next ﬁner level. Instead of using gra-\ndient descent to reﬁne the ﬂow estimates, a combinatorial search over discrete displacement\nlabels (which is able to ﬁnd better energy minima) is performed using their Fast-PD algorithm\n(Komodakis, Tziritas, and Paragios 2008).\nLempitsky, Roth, and Rother. (2008) use fusion moves (Lempitsky, Rother, and Blake\n2007) over proposals generated from basic ﬂow algorithms (Horn and Schunck 1981; Lucas\nand Kanade 1981) to ﬁnd good solutions. The basic idea behind fusion moves is to replace\nportions of the current best estimate with hypotheses generated by more basic techniques\n(or their shifted versions) and to alternate them with local gradient descent for better energy\nminimization.\nThe ﬁeld of accurate motion estimation continues to evolve at a rapid pace, with signif-\nicant advances in performance occurring every year. The optical ﬂow evaluation Web site\n(http://vision.middlebury.edu/ﬂow/) is a good source of pointers to high-performing recently\ndeveloped algorithms (Figure 8.12).\n8.4.1 Multi-frame motion estimation\nSo far, we have looked at motion estimation as a two-frame problem, where the goal is to\ncompute a motion ﬁeld that aligns pixels from one image with those in another. In practice,\nmotion estimation is usually applied to video, where a whole sequence of frames is available\nto perform this task.\nOne classic approach to multi-frame motion is to ﬁlter the spatio-temporal volume using\noriented or steerable ﬁlters (Heeger 1988), in a manner analogous to oriented edge detec-\ntion (Section 3.2.3). Figure 8.13 shows two frames from the commonly used ﬂower garden\nsequence, as well as a horizontal slice through the spatio-temporal volume, i.e., the 3D vol-\nume created by stacking all of the video frames together. Because the pixel motion is mostly\nhorizontal, the slopes of individual (textured) pixel tracks, which correspond to their horizon-\ntal velocities, can clearly be seen. Spatio-temporal ﬁltering uses a 3D volume around each\npixel to determine the best orientation in space–time, which corresponds directly to a pixel’s\nvelocity.\nUnfortunately, in order to obtain reasonably accurate velocity estimates everywhere in\nan image, spatio-temporal ﬁlters have moderately large extents, which severely degrades the\nquality of their estimates near motion discontinuities. (This same problem is endemic in\n13 http://vision.middlebury.edu/ﬂow/.",
  "image_path": "page_434.jpg",
  "pages": [
    433,
    434,
    435
  ]
}