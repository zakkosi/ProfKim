{
  "doc_id": "pages_348_350",
  "text": "326\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 6.6 The VideoMouse can sense six degrees of freedom relative to a specially printed\nmouse pad using its embedded camera (Hinckley, Sinclair, Hanson et al. 1999) c⃝1999\nACM: (a) top view of the mouse; (b) view of the mouse showing the curved base for rocking;\n(c) moving the mouse pad with the other hand extends the interaction capabilities; (d) the\nresulting movement seen on the screen.\npartial derivative of the transform with respect to these parameters, which results in a simple\ncross product of the backward chaining partial derivative and the outgoing 3D vector (2.36).\n6.2.3 Application: Augmented reality\nA widely used application of pose estimation is augmented reality, where virtual 3D images\nor annotations are superimposed on top of a live video feed, either through the use of see-\nthrough glasses (a head-mounted display) or on a regular computer or mobile device screen\n(Azuma, Baillot, Behringer et al. 2001; Haller, Billinghurst, and Thomas 2007). In some\napplications, a special pattern printed on cards or in a book is tracked to perform the aug-\nmentation (Kato, Billinghurst, Poupyrev et al. 2000; Billinghurst, Kato, and Poupyrev 2001).\nFor a desktop application, a grid of dots printed on a mouse pad can be tracked by a camera\nembedded in an augmented mouse to give the user control of a full six degrees of freedom\nover their position and orientation in a 3D space (Hinckley, Sinclair, Hanson et al. 1999), as\nshown in Figure 6.6.\nSometimes, the scene itself provides a convenient object to track, such as the rectangle\ndeﬁning a desktop used in through-the-lens camera control (Gleicher and Witkin 1992). In\noutdoor locations, such as ﬁlm sets, it is more common to place special markers such as\nbrightly colored balls in the scene to make it easier to ﬁnd and track them (Bogart 1991). In\nolder applications, surveying techniques were used to determine the locations of these balls\nbefore ﬁlming. Today, it is more common to apply structure-from-motion directly to the ﬁlm\nfootage itself (Section 7.4.2).\nRapid pose estimation is also central to tracking the position and orientation of the hand-\nheld remote controls used in Nintendo’s Wii game systems. A high-speed camera embedded\nin the remote control is used to track the locations of the infrared (IR) LEDs in the bar that\n6.3 Geometric intrinsic calibration\n327\nis mounted on the TV monitor. Pose estimation is then used to infer the remote control’s\nlocation and orientation at very high frame rates. The Wii system can be extended to a variety\nof other user interaction applications by mounting the bar on a hand-held device, as described\nby Johnny Lee.11\nExercises 6.4 and 6.5 have you implement two different tracking and pose estimation sys-\ntems for augmented-reality applications. The ﬁrst system tracks the outline of a rectangular\nobject, such as a book cover or magazine page, and the second has you track the pose of a\nhand-held Rubik’s cube.\n6.3 Geometric intrinsic calibration\nAs described above in Equations (6.42–6.43), the computation of the internal (intrinsic) cam-\nera calibration parameters can occur simultaneously with the estimation of the (extrinsic)\npose of the camera with respect to a known calibration target. This, indeed, is the “classic”\napproach to camera calibration used in both the photogrammetry (Slama 1980) and the com-\nputer vision (Tsai 1987) communities. In this section, we look at alternative formulations\n(which may not involve the full solution of a non-linear regression problem), the use of alter-\nnative calibration targets, and the estimation of the non-linear part of camera optics such as\nradial distortion.12\n6.3.1 Calibration patterns\nThe use of a calibration pattern or set of markers is one of the more reliable ways to estimate\na camera’s intrinsic parameters. In photogrammetry, it is common to set up a camera in a\nlarge ﬁeld looking at distant calibration targets whose exact location has been precomputed\nusing surveying equipment (Slama 1980; Atkinson 1996; Kraus 1997). In this case, the trans-\nlational component of the pose becomes irrelevant and only the camera rotation and intrinsic\nparameters need to be recovered.\nIf a smaller calibration rig needs to be used, e.g., for indoor robotics applications or for\nmobile robots that carry their own calibration target, it is best if the calibration object can span\nas much of the workspace as possible (Figure 6.8a), as planar targets often fail to accurately\npredict the components of the pose that lie far away from the plane. A good way to determine\nif the calibration has been successfully performed is to estimate the covariance in the param-\neters (Section 6.1.4) and then project 3D points from various points in the workspace into the\nimage in order to estimate their 2D positional uncertainty.\n11 http://johnnylee.net/projects/wii/.\n12 In some applications, you can use the EXIF tags associated with a JPEG image to obtain a rough estimate of a\ncamera’s focal length but this technique should be used with caution as the results are often inaccurate.\n328\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 6.7\nCalibrating a lens by drawing straight lines on cardboard (Debevec, Wenger,\nTchou et al. 2002) c⃝2002 ACM: (a) an image taken by the video camera showing a hand\nholding a metal ruler whose right edge appears vertical in the image; (b) the set of lines drawn\non the cardboard converging on the front nodal point (center of projection) of the lens and\nindicating the horizontal ﬁeld of view.\nAn alternative method for estimating the focal length and center of projection of a lens\nis to place the camera on a large ﬂat piece of cardboard and use a long metal ruler to draw\nlines on the cardboard that appear vertical in the image, as shown in Figure 6.7a (Debevec,\nWenger, Tchou et al. 2002). Such lines lie on planes that are parallel to the vertical axis of\nthe camera sensor and also pass through the lens’ front nodal point. The location of the nodal\npoint (projected vertically onto the cardboard plane) and the horizontal ﬁeld of view (deter-\nmined from lines that graze the left and right edges of the visible image) can be recovered by\nintersecting these lines and measuring their angular extent (Figure 6.7b).\nIf no calibration pattern is available, it is also possible to perform calibration simulta-\nneously with structure and pose recovery (Sections 6.3.4 and 7.4), which is known as self-\ncalibration (Faugeras, Luong, and Maybank 1992; Hartley and Zisserman 2004; Moons, Van\nGool, and Vergauwen 2010). However, such an approach requires a large amount of imagery\nto be accurate.\nPlanar calibration patterns\nWhen a ﬁnite workspace is being used and accurate machining and motion control platforms\nare available, a good way to perform calibration is to move a planar calibration target in a\ncontrolled fashion through the workspace volume. This approach is sometimes called the N-\nplanes calibration approach (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee,\nSzeliski et al. 1992; Grossberg and Nayar 2001) and has the advantage that each camera pixel\ncan be mapped to a unique 3D ray in space, which takes care of both linear effects modeled",
  "image_path": "page_349.jpg",
  "pages": [
    348,
    349,
    350
  ]
}