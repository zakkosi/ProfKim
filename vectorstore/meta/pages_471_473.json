{
  "doc_id": "pages_471_473",
  "text": "9.2 Global alignment\n449\nFigure 9.12\nMatching errors (Brown, Szeliski, and Winder 2004): accidental matching of\nseveral features can lead to matches between pairs of images that do not actually overlap.\nFigure 9.13\nValidation of image matches by direct pixel error comparison can fail when the\nscene contains moving objects (Uyttendaele, Eden, and Szeliski 2001) c⃝2001 IEEE.\n450\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nheuristics, such as priors on typical camera motions as well as machine learning techniques\napplied to the problem of match validation.\n9.2.4 Direct vs. feature-based alignment\nGiven that there exist these two approaches to aligning images, which is preferable?\nEarly feature-based methods would get confused in regions that were either too textured\nor not textured enough. The features would often be distributed unevenly over the images,\nthereby failing to match image pairs that should have been aligned. Furthermore, establishing\ncorrespondences relied on simple cross-correlation between patches surrounding the feature\npoints, which did not work well when the images were rotated or had foreshortening due to\nhomographies.\nToday, feature detection and matching schemes are remarkably robust and can even be\nused for known object recognition from widely separated views (Lowe 2004). Features not\nonly respond to regions of high “cornerness” (F¨orstner 1986; Harris and Stephens 1988) but\nalso to “blob-like” regions (Lowe 2004), and uniform areas (Matas, Chum, Urban et al. 2004;\nTuytelaars and Van Gool 2004). Furthermore, because they operate in scale-space and use a\ndominant orientation (or orientation invariant descriptors), they can match images that differ\nin scale, orientation, and even foreshortening. Our own experience in working with feature-\nbased approaches is that if the features are well distributed over the image and the descriptors\nreasonably designed for repeatability, enough correspondences to permit image stitching can\nusually be found (Brown, Szeliski, and Winder 2005).\nThe biggest disadvantage of direct pixel-based alignment techniques is that they have a\nlimited range of convergence. Even though they can be used in a hierarchical (coarse-to-\nﬁne) estimation framework, in practice it is hard to use more than two or three levels of a\npyramid before important details start to be blurred away.11 For matching sequential frames\nin a video, direct approaches can usually be made to work. However, for matching partially\noverlapping images in photo-based panoramas or for image collections where the contrast or\ncontent varies too much, they fail too often to be useful and feature-based approaches are\ntherefore preferred.\n9.3 Compositing\nOnce we have registered all of the input images with respect to each other, we need to decide\nhow to produce the ﬁnal stitched mosaic image. This involves selecting a ﬁnal compositing\nsurface (ﬂat, cylindrical, spherical, etc.) and view (reference image). It also involves selecting\n11 Fourier-based correlation (Szeliski 1996; Szeliski and Shum 1997) can extend this range but requires cylindrical\nimages or motion prediction to be useful.\n9.3 Compositing\n451\nwhich pixels contribute to the ﬁnal composite and how to optimally blend these pixels to\nminimize visible seams, blur, and ghosting.\nIn this section, we review techniques that address these problems, namely compositing\nsurface parameterization, pixel and seam selection, blending, and exposure compensation.\nMy emphasis is on fully automated approaches to the problem. Since the creation of high-\nquality panoramas and composites is as much an artistic endeavor as a computational one,\nvarious interactive tools have been developed to assist this process (Agarwala, Dontcheva,\nAgrawala et al. 2004; Li, Sun, Tang et al. 2004; Rother, Kolmogorov, and Blake 2004).\nSome of these are covered in more detail in Section 10.4.\n9.3.1 Choosing a compositing surface\nThe ﬁrst choice to be made is how to represent the ﬁnal image. If only a few images are\nstitched together, a natural approach is to select one of the images as the reference and to\nthen warp all of the other images into its reference coordinate system. The resulting com-\nposite is sometimes called a ﬂat panorama, since the projection onto the ﬁnal surface is still\na perspective projection, and hence straight lines remain straight (which is often a desirable\nattribute).12\nFor larger ﬁelds of view, however, we cannot maintain a ﬂat representation without ex-\ncessively stretching pixels near the border of the image. (In practice, ﬂat panoramas start\nto look severely distorted once the ﬁeld of view exceeds 90◦or so.) The usual choice for\ncompositing larger panoramas is to use a cylindrical (Chen 1995; Szeliski 1996) or spherical\n(Szeliski and Shum 1997) projection, as described in Section 9.1.6. In fact, any surface used\nfor environment mapping in computer graphics can be used, including a cube map, which\nrepresents the full viewing sphere with the six square faces of a cube (Greene 1986; Szeliski\nand Shum 1997). Cartographers have also developed a number of alternative methods for\nrepresenting the globe (Bugayevskiy and Snyder 1995).\nThe choice of parameterization is somewhat application dependent and involves a trade-\noff between keeping the local appearance undistorted (e.g., keeping straight lines straight)\nand providing a reasonably uniform sampling of the environment. Automatically making\nthis selection and smoothly transitioning between representations based on the extent of the\npanorama is an active area of current research (Kopf, Uyttendaele, Deussen et al. 2007).\nAn interesting recent development in panoramic photography has been the use of stereo-\ngraphic projections looking down at the ground (in an outdoor scene) to create “little planet”\nrenderings.13\n12 Recently, some techniques have been developed to straighten curved lines in cylindrical and spherical panora-\nmas (Carroll, Agrawala, and Agarwala 2009; Kopf, Lischinski, Deussen et al. 2009).\n13 These are inspired by The Little Prince by Antoine De Saint-Exupery. Go to http://www.ﬂickr.com and search",
  "image_path": "page_472.jpg",
  "pages": [
    471,
    472,
    473
  ]
}