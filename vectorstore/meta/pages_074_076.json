{
  "doc_id": "pages_074_076",
  "text": "52\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\n0\nyc\nxs\nys\nW-1\nH-1\n(cx,cy)\n0\nf\nFigure 2.9\nSimpliﬁed camera intrinsics showing the focal length f and the optical center\n(cx, cy). The image width and height are W and H.\nfactorization (Golub and Van Loan 1996). (Note the unfortunate clash of terminologies: In\nmatrix algebra textbooks, R represents an upper-triangular (right of the diagonal) matrix; in\ncomputer vision, R is an orthogonal rotation.)\nThere are several ways to write the upper-triangular form of K. One possibility is\nK =\n\n\nfx\ns\ncx\n0\nfy\ncy\n0\n0\n1\n\n,\n(2.57)\nwhich uses independent focal lengths fx and fy for the sensor x and y dimensions. The entry\ns encodes any possible skew between the sensor axes due to the sensor not being mounted\nperpendicular to the optical axis and (cx, cy) denotes the optical center expressed in pixel\ncoordinates. Another possibility is\nK =\n\n\nf\ns\ncx\n0\naf\ncy\n0\n0\n1\n\n,\n(2.58)\nwhere the aspect ratio a has been made explicit and a common focal length f is used.\nIn practice, for many applications an even simpler form can be obtained by setting a = 1\nand s = 0,\nK =\n\n\nf\n0\ncx\n0\nf\ncy\n0\n0\n1\n\n.\n(2.59)\nOften, setting the origin at roughly the center of the image, e.g., (cx, cy) = (W/2, H/2),\nwhere W and H are the image height and width, can result in a perfectly usable camera\nmodel with a single unknown, i.e., the focal length f.\n2.1 Geometric primitives and transformations\n53\nW/2\nf\nθ/2\n(x,y,1)\n(X,Y,Z)\nZ\nFigure 2.10\nCentral projection, showing the relationship between the 3D and 2D coordi-\nnates, p and x, as well as the relationship between the focal length f, image width W, and\nthe ﬁeld of view θ.\nFigure 2.9 shows how these quantities can be visualized as part of a simpliﬁed imaging\nmodel. Note that now we have placed the image plane in front of the nodal point (projection\ncenter of the lens). The sense of the y axis has also been ﬂipped to get a coordinate system\ncompatible with the way that most imaging libraries treat the vertical (row) coordinate. Cer-\ntain graphics libraries, such as Direct3D, use a left-handed coordinate system, which can lead\nto some confusion.\nA note on focal lengths\nThe issue of how to express focal lengths is one that often causes confusion in implementing\ncomputer vision algorithms and discussing their results. This is because the focal length\ndepends on the units used to measure pixels.\nIf we number pixel coordinates using integer values, say [0, W)×[0, H), the focal length\nf and camera center (cx, cy) in (2.59) can be expressed as pixel values. How do these quan-\ntities relate to the more familiar focal lengths used by photographers?\nFigure 2.10 illustrates the relationship between the focal length f, the sensor width W,\nand the ﬁeld of view θ, which obey the formula\ntan θ\n2 = W\n2f\nor\nf = W\n2\n\u0014\ntan θ\n2\n\u0015−1\n.\n(2.60)\nFor conventional ﬁlm cameras, W = 35mm, and hence f is also expressed in millimeters.\nSince we work with digital images, it is more convenient to express W in pixels so that the\nfocal length f can be used directly in the calibration matrix K as in (2.59).\nAnother possibility is to scale the pixel coordinates so that they go from [−1, 1) along\nthe longer image dimension and [−a−1, a−1) along the shorter axis, where a ≥1 is the\nimage aspect ratio (as opposed to the sensor cell aspect ratio introduced earlier). This can be\n54\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\naccomplished using modiﬁed normalized device coordinates,\nx′\ns = (2xs −W)/S and y′\ns = (2ys −H)/S,\nwhere\nS = max(W, H).\n(2.61)\nThis has the advantage that the focal length f and optical center (cx, cy) become independent\nof the image resolution, which can be useful when using multi-resolution, image-processing\nalgorithms, such as image pyramids (Section 3.5).2 The use of S instead of W also makes the\nfocal length the same for landscape (horizontal) and portrait (vertical) pictures, as is the case\nin 35mm photography. (In some computer graphics textbooks and systems, normalized device\ncoordinates go from [−1, 1] × [−1, 1], which requires the use of two different focal lengths\nto describe the camera intrinsics (Watt 1995; OpenGL-ARB 1997).) Setting S = W = 2 in\n(2.60), we obtain the simpler (unitless) relationship\nf −1 = tan θ\n2.\n(2.62)\nThe conversion between the various focal length representations is straightforward, e.g.,\nto go from a unitless f to one expressed in pixels, multiply by W/2, while to convert from an\nf expressed in pixels to the equivalent 35mm focal length, multiply by 35/W.\nCamera matrix\nNow that we have shown how to parameterize the calibration matrix K, we can put the\ncamera intrinsics and extrinsics together to obtain a single 3 × 4 camera matrix\nP = K\nh\nR\nt\ni\n.\n(2.63)\nIt is sometimes preferable to use an invertible 4 × 4 matrix, which can be obtained by not\ndropping the last row in the P matrix,\n˜\nP =\n\"\nK\n0\n0T\n1\n# \"\nR\nt\n0T\n1\n#\n= ˜\nKE,\n(2.64)\nwhere E is a 3D rigid-body (Euclidean) transformation and ˜\nK is the full-rank calibration\nmatrix. The 4 × 4 camera matrix ˜\nP can be used to map directly from 3D world coordinates\n¯pw = (xw, yw, zw, 1) to screen coordinates (plus disparity), xs = (xs, ys, 1, d),\nxs ∼˜\nP ¯pw,\n(2.65)\nwhere ∼indicates equality up to scale. Note that after multiplication by ˜\nP , the vector is\ndivided by the third element of the vector to obtain the normalized form xs = (xs, ys, 1, d).\n2 To make the conversion truly accurate after a downsampling step in a pyramid, ﬂoating point values of W and\nH would have to be maintained since they can become non-integral if they are ever odd at a larger resolution in the\npyramid.",
  "image_path": "page_075.jpg",
  "pages": [
    74,
    75,
    76
  ]
}