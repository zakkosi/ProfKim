{
  "doc_id": "pages_708_710",
  "text": "686\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nter extracting informative sparse 2D features from both the new image and the images in the\ndatabase, image features are matched against the object database, using one of the sparse fea-\nture matching strategies described in Section 4.1.3. Whenever a sufﬁcient number of matches\nhave been found, they are veriﬁed by ﬁnding a geometric transformation that aligns the two\nsets of features (Figure 14.26).\nBelow, we describe some of the techniques that have been proposed for representing the\ngeometric relationships between such features (Section 14.3.1). We also discuss how to make\nthe feature matching process more efﬁcient using ideas from text and information retrieval\n(Section 14.3.2).\n14.3.1 Geometric alignment\nTo recognize one or more instances of some known objects, such as those shown in the left\ncolumn of Figure 14.26, the recognition system ﬁrst extracts a set of interest points in each\ndatabase image and stores the associated descriptors (and original positions) in an indexing\nstructure such as a search tree (Section 4.1.3). At recognition time, features are extracted\nfrom the new image and compared against the stored object features. Whenever a sufﬁcient\nnumber of matching features (say, three or more) are found for a given object, the system then\ninvokes a match veriﬁcation stage, whose job is to determine whether the spatial arrangement\nof matching features is consistent with those in the database image.\nBecause images can be highly cluttered and similar features may belong to several objects,\nthe original set of feature matches can have a large number of outliers. For this reason, Lowe\n(2004) suggests using a Hough transform (Section 4.3.2) to accumulate votes for likely geo-\nmetric transformations. In his system, he uses an afﬁne transformation between the database\nobject and the collection of scene features, which works well for objects that are mostly pla-\nnar, or where at least several corresponding features share a quasi-planar geometry.16\nSince SIFT features carry with them their own location, scale, and orientation, Lowe uses\na four-dimensional similarity transformation as the original Hough binning structure, i.e.,\neach bin denotes a particular location for the object center, scale, and in-plane rotation. Each\nmatching feature votes for the nearest 24 bins and peaks in the transform are then selected for\na more careful afﬁne motion ﬁt. Figure 14.26 (right image) shows three instances of the two\nobjects on the left that were recognized by the system. Obdrˇz´alek and Matas (2006) general-\nize Lowe’s approach to use feature descriptors with full local afﬁne frames and evaluate their\napproach on a number of object recognition databases.\nAnother system that uses local afﬁne frames is the one developed by Rothganger, Lazeb-\n16 When a larger number of features is available, a full fundamental matrix can be used (Brown and Lowe 2002;\nGordon and Lowe 2006). When image stitching is being performed (Brown and Lowe 2007), the motion models\ndiscussed in Section 9.1 can be used instead.\n14.3 Instance recognition\n687\n(a)\n(b)\n(c)\n(d)\nFigure 14.27\n3D object recognition with afﬁne regions (Rothganger, Lazebnik, Schmid et\nal. 2006) c⃝2006 Springer: (a) sample input image; (b) ﬁve of the recognized (reprojected)\nobjects along with their bounding boxes; (c) a few of the local afﬁne regions; (d) local afﬁne\nregion (patch) reprojected into a canonical (square) frame, along with its geometric afﬁne\ntransformations.\nnik, Schmid et al. (2006). In their system, the afﬁne region detector of Mikolajczyk and\nSchmid (2004) is used to rectify local image patches (Figure 14.27d), from which both a\nSIFT descriptor and a 10 × 10 UV color histogram are computed and used for matching\nand recognition. Corresponding patches in different views of the same object, along with\ntheir local afﬁne deformations, are used to compute a 3D afﬁne model for the object using\nan extension of the factorization algorithm of Section 7.3, which can then be upgraded to a\nEuclidean reconstruction (Tomasi and Kanade 1992).\nAt recognition time, local Euclidean neighborhood constraints are used to ﬁlter potential\nmatches, in a manner analogous to the afﬁne geometric constraints used by Lowe (2004) and\nObdrˇz´alek and Matas (2006). Figure 14.27 shows the results of recognizing ﬁve objects in a\ncluttered scene using this approach.\nWhile feature-based approaches are normally used to detect and localize known objects in\nscenes, it is also possible to get pixel-level segmentations of the scene based on such matches.\nFerrari, Tuytelaars, and Van Gool (2006b) describe such a system for simultaneously recog-\nnizing objects and segmenting scenes, while Kannala, Rahtu, Brandt et al. (2008) extend this\napproach to non-rigid deformations. Section 14.4.3 re-visits this topic of joint recognition\nand segmentation in the context of generic class (category) recognition.\n14.3.2 Large databases\nAs the number of objects in the database starts to grow large (say, millions of objects or video\nframes being searched), the time it takes to match a new image against each database image\ncan become prohibitive. Instead of comparing the images one at a time, techniques are needed\nto quickly narrow down the search to a few likely images, which can then be compared using\na more detailed and conservative veriﬁcation stage.\n688\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.28\nVisual words obtained from elliptical normalized afﬁne regions (Sivic and\nZisserman 2009) c⃝2009 IEEE. (a) Afﬁne covariant regions are extracted from each frame\nand clustered into visual words using k-means clustering on SIFT descriptors with a learned\nMahalanobis distance. (b) The central patch in each grid shows the query and the surrounding\npatches show the nearest neighbors.\nThe problem of quickly ﬁnding partial matches between documents is one of the cen-\ntral problems in information retrieval (IR) (Baeza-Yates and Ribeiro-Neto 1999; Manning,\nRaghavan, and Sch¨utze 2008). The basic approach in fast document retrieval algorithms is to\npre-compute an inverted index between individual words and the documents (or Web pages\nor news stories) where they occur. More precisely, the frequency of occurrence of particular\nwords in a document is used to quickly ﬁnd documents that match a particular query.\nSivic and Zisserman (2009) were the ﬁrst to adapt IR techniques to visual search. In their\nVideo Google system, afﬁne invariant features are ﬁrst detected in all the video frames they\nare indexing using both shape adapted regions around Harris feature points (Schaffalitzky\nand Zisserman 2002; Mikolajczyk and Schmid 2004) and maximally stable extremal regions\n(Matas, Chum, Urban et al. 2004), (Section 4.1.1), as shown in Figure 14.28a. Next, 128-\ndimensional SIFT descriptors are computed from each normalized region (i.e., the patches\nshown in Figure 14.28b). Then, an average covariance matrix for these descriptors is es-\ntimated by accumulating statistics for features tracked from frame to frame. The feature\ndescriptor covariance Σ is then used to deﬁne a Mahalanobis distance between feature de-\nscriptors,\nd(x0, x1) = ∥x0 −x1∥Σ\n−1 =\nq\n(x0 −x1)T Σ−1(x0 −x1).\n(14.32)\nIn practice, feature descriptors are whitened by pre-multiplying them by Σ−1/2 so that Eu-\nclidean distances can be used.17\nIn order to apply fast information retrieval techniques to images, the high-dimensional\nfeature descriptors that occur in each image must ﬁrst be mapped into discrete visual words.\n17 Note that the computation of feature covariances from matched feature points is much more sensible than simply\nperforming a PCA on the descriptor space (Winder and Brown 2007). This corresponds roughly to the within-class\nscatter matrix (14.17) we studied in Section 14.2.1.",
  "image_path": "page_709.jpg",
  "pages": [
    708,
    709,
    710
  ]
}