{
  "doc_id": "pages_620_622",
  "text": "598\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSigned distance functions also play an integral role in level-set evolution equations ((Sec-\ntions 5.1.4 and 11.6.1), where the values of distance transforms on the mesh are updated as\nthe surface evolves to ﬁt multi-view stereo photoconsistency measures (Faugeras and Keriven\n1998).\n12.6 Model-based reconstruction\nWhen we know something ahead of time about the objects we are trying to model, we can\nconstruct more detailed and reliable 3D models using specialized techniques and representa-\ntions. For example, architecture is usually made up of large planar regions and other para-\nmetric forms (such as surfaces of revolution), usually oriented perpendicular to gravity and\nto each other (Section 12.6.1). Heads and faces can be represented using low-dimensional,\nnon-rigid shape models, since the variability in shape and appearance of human faces, while\nextremely large, is still bounded (Section 12.6.2). Human bodies or parts, such as hands, form\nhighly articulated structures, which can be represented using kinematic chains of piecewise\nrigid skeletal elements linked by joints (Section 12.6.4).\nIn this section, we highlight some of the main ideas, representations, and modeling algo-\nrithms used for these three cases. Additional details and references can be found in special-\nized conferences and workshops devoted to these topics, e.g., the International Symposium on\n3D Data Processing, Visualization, and Transmission (3DPVT), the International Conference\non 3D Digital Imaging and Modeling (3DIM), the International Conference on Automatic\nFace and Gesture Recognition (FG), the IEEE Workshop on Analysis and Modeling of Faces\nand Gestures, and the International Workshop on Tracking Humans for the Evaluation of their\nMotion in Image Sequences (THEMIS).\n12.6.1 Architecture\nArchitectural modeling, especially from aerial photography, has been one of the longest stud-\nied problems in both photogrammetry and computer vision (Walker and Herman 1988). Re-\ncently, the development of reliable image-based modeling techniques, as well as the preva-\nlence of digital cameras and 3D computer games, has spurred renewed interest in this area.\nThe work by Debevec, Taylor, and Malik (1996) was one of the earliest hybrid geometry-\nand image-based modeling and rendering systems. Their Fac¸ade system combines an inter-\nactive image-guided geometric modeling tool with model-based (local plane plus parallax)\nstereo matching and view-dependent texture mapping. During the interactive photogrammet-\nric modeling phase, the user selects block elements and aligns their edges with visible edges\nin the input images (Figure 12.14a). The system then automatically computes the dimensions\nand locations of the blocks along with the camera positions using constrained optimization\n12.6 Model-based reconstruction\n599\nFigure 12.14 Interactive architectural modeling using the Fac¸ade system (Debevec, Taylor,\nand Malik 1996) c⃝1996 ACM: (a) input image with user-drawn edges shown in green;\n(b) shaded 3D solid model; (c) geometric primitives overlaid onto the input image; (d) ﬁnal\nview-dependent, texture-mapped 3D model.\n(Figure 12.14b–c). This approach is intrinsically more reliable than general feature-based\nstructure from motion, because it exploits the strong geometry available in the block primi-\ntives. Related work by Becker and Bove (1995), Horry, Anjyo, and Arai (1997), and Crimin-\nisi, Reid, and Zisserman (2000) exploits similar information available from vanishing points.\nIn the interactive, image-based modeling system of Sinha, Steedly, Szeliski et al. (2008),\nvanishing point directions are used to guide the user drawing of polygons, which are then\nautomatically ﬁtted to sparse 3D points recovered using structure from motion.\nOnce the rough geometry has been estimated, more detailed offset maps can be com-\nputed for each planar face using a local plane sweep, which Debevec, Taylor, and Malik\n(1996) call model-based stereo. Finally, during rendering, images from different viewpoints\nare warped and blended together as the camera moves around the scene, using a process (re-\nlated to light ﬁeld and Lumigraph rendering, see Section 13.3) called view-dependent texture\nmapping (Figure 12.14d).\nFor interior modeling, instead of working with single pictures, it is more useful to work\nwith panoramas, since you can see larger extents of walls and other structures. The 3D mod-\neling system developed by Shum, Han, and Szeliski (1998) ﬁrst constructs calibrated panora-\nmas from multiple images (Section 7.4) and then has the user draw vertical and horizontal\nlines in the image to demarcate the boundaries of planar regions. The lines are initially used\nto establish an absolute rotation for each panorama and are later used (along with the inferred\nvertices and planes) to optimize the 3D structure, which can be recovered up to scale from\none or more images (Figure 12.15). 360◦high dynamic range panoramas can also be used for\noutdoor modeling, since they provide highly reliable estimates of relative camera orientations\nas well as vanishing point directions (Antone and Teller 2002; Teller, Antone, Bodnar et al.\n600\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 12.15\nInteractive 3D modeling from panoramas (Shum, Han, and Szeliski 1998)\nc⃝1998 IEEE: (a) wide-angle view of a panorama with user-drawn vertical and horizontal\n(axis-aligned) lines; (b) single-view reconstruction of the corridors.\n2003).\nWhile earlier image-based modeling systems required some user authoring, Werner and\nZisserman (2002) present a fully automated line-based reconstruction system. As described\nin Section 7.5.1, they ﬁrst detect lines and vanishing points and use them to calibrate the\ncamera; then they establish line correspondences using both appearance matching and tri-\nfocal tensors, which enables them to reconstruct families of 3D line segments, as shown in\nFigure 12.16a. They then generate plane hypotheses, using both co-planar 3D lines and a\nplane sweep (Section 11.1.2) based on cross-correlation scores evaluated at interest points.\nIntersections of planes are used to determine the extent of each plane, i.e., an initial coarse ge-\nometry, which is then reﬁned with the addition of rectangular or wedge-shaped indentations\nand extrusions (Figure 12.16c). Note that when top-down maps of the buildings being mod-\neled are available, these can be used to further constrain the 3D modeling process (Robertson\nand Cipolla 2002, 2009). The idea of using matched 3D lines for estimating vanishing point\ndirections and dominant planes continues to be used in a number of recent fully automated\nimage-based architectural modeling systems (Zebedin, Bauer, Karner et al. 2008; Miˇcuˇs´ık\nand Koˇseck´a 2009; Furukawa, Curless, Seitz et al. 2009b; Sinha, Steedly, and Szeliski 2009).\nAnother common characteristic of architecture is the repeated use of primitives such as\nwindows, doors, and colonnades. Architectural modeling systems can be designed to search\nfor such repeated elements and to use them as part of the structure inference process (Dick,\nTorr, and Cipolla 2004; Mueller, Zeng, Wonka et al. 2007; Schindler, Krishnamurthy, Lublin-\nerman et al. 2008; Sinha, Steedly, Szeliski et al. 2008).\nThe combination of all these techniques now makes it possible to reconstruct the structure\nof large 3D scenes (Zhu and Kanade 2008). For example, the Urbanscan system of Polle-\nfeys, Nist´er, Frahm et al. (2008) reconstructs texture-mapped 3D models of city streets from\nvideos acquired with a GPS-equipped vehicle. To obtain real-time performance, they use\nboth optimized on-line structure-from-motion algorithms, as well as GPU implementations",
  "image_path": "page_621.jpg",
  "pages": [
    620,
    621,
    622
  ]
}