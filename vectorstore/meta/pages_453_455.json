{
  "doc_id": "pages_453_455",
  "text": "9.1 Motion models\n431\n(a) translation [2 dof]\n(b) afﬁne [6 dof]\n(c) perspective [8 dof]\n(d) 3D rotation [3+ dof]\nFigure 9.2 Two-dimensional motion models and how they can be used for image stitching.\n9.1.1 Planar perspective motion\nThe simplest possible motion model to use when aligning images is to simply translate and\nrotate them in 2D (Figure 9.2a). This is exactly the same kind of motion that you would\nuse if you had overlapping photographic prints. It is also the kind of technique favored by\nDavid Hockney to create the collages that he calls joiners (Zelnik-Manor and Perona 2007;\nNomura, Zhang, and Nayar 2007). Creating such collages, which show visible seams and\ninconsistencies that add to the artistic effect, is popular on Web sites such as Flickr, where they\nmore commonly go under the name panography (Section 6.1.2). Translation and rotation are\nalso usually adequate motion models to compensate for small camera motions in applications\nsuch as photo and video stabilization and merging (Exercise 6.1 and Section 8.2.1).\nIn Section 6.1.3, we saw how the mapping between two cameras viewing a common plane\ncan be described using a 3×3 homography (2.71). Consider the matrix M 10 that arises when\nmapping a pixel in one image to a 3D point and then back onto a second image,\n˜x1 ∼˜\nP 1 ˜\nP\n−1\n0 ˜x0 = M 10˜x0.\n(9.1)\nWhen the last row of the P 0 matrix is replaced with a plane equation ˆn0·p+c0 and points are\nassumed to lie on this plane, i.e., their disparity is d0 = 0, we can ignore the last column of\nM 10 and also its last row, since we do not care about the ﬁnal z-buffer depth. The resulting\nhomography matrix ˜\nH10 (the upper left 3 × 3 sub-matrix of M 10) describes the mapping\nbetween pixels in the two images,\n˜x1 ∼˜\nH10˜x0.\n(9.2)\nThis observation formed the basis of some of the earliest automated image stitching al-\ngorithms (Mann and Picard 1994; Szeliski 1994, 1996). Because reliable feature matching\ntechniques had not yet been developed, these algorithms used direct pixel value matching, i.e.,\ndirect parametric motion estimation, as described in Section 8.2 and Equations (6.19–6.20).\n432\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMore recent stitching algorithms ﬁrst extract features and then match them up, often using\nrobust techniques such as RANSAC (Section 6.1.4) to compute a good set of inliers. The ﬁnal\ncomputation of the homography (9.2), i.e., the solution of the least squares ﬁtting problem\ngiven pairs of corresponding features,\nx1 = (1 + h00)x0 + h01y0 + h02\nh20x0 + h21y0 + 1\nand y1 = h10x0 + (1 + h11)y0 + h12\nh20x0 + h21y0 + 1\n,\n(9.3)\nuses iterative least squares, as described in Section 6.1.3 and Equations (6.21–6.23).\n9.1.2 Application: Whiteboard and document scanning\nThe simplest image-stitching application is to stitch together a number of image scans taken\non a ﬂatbed scanner. Say you have a large map, or a piece of child’s artwork, that is too large\nto ﬁt on your scanner. Simply take multiple scans of the document, making sure to overlap\nthe scans by a large enough amount to ensure that there are enough common features. Next,\ntake successive pairs of images that you know overlap, extract features, match them up, and\nestimate the 2D rigid transform (2.16),\nxk+1 = Rkxk + tk,\n(9.4)\nthat best matches the features, using two-point RANSAC, if necessary, to ﬁnd a good set\nof inliers. Then, on a ﬁnal compositing surface (aligned with the ﬁrst scan, for example),\nresample your images (Section 3.6.1) and average them together. Can you see any potential\nproblems with this scheme?\nOne complication is that a 2D rigid transformation is non-linear in the rotation angle θ,\nso you will have to either use non-linear least squares or constrain R to be orthonormal, as\ndescribed in Section 6.1.3.\nA bigger problem lies in the pairwise alignment process. As you align more and more\npairs, the solution may drift so that it is no longer globally consistent. In this case, a global op-\ntimization procedure, as described in Section 9.2, may be required. Such global optimization\noften requires a large system of non-linear equations to be solved, although in some cases,\nsuch as linearized homographies (Section 9.1.3) or similarity transforms (Section 6.1.2), reg-\nular least squares may be an option.\nA slightly more complex scenario is when you take multiple overlapping handheld pic-\ntures of a whiteboard or other large planar object (He and Zhang 2005; Zhang and He 2007).\nHere, the natural motion model to use is a homography, although a more complex model that\nestimates the 3D rigid motion relative to the plane (plus the focal length, if unknown), could\nin principle be used.\n9.1 Motion models\n433\nΠ∞:\n(0,0,0,1)·p= 0\nR10\nx1 = (x1,y1,f1)\n~\nx0 = (x0,y0,f0)\n~\nFigure 9.3 Pure 3D camera rotation. The form of the homography (mapping) is particularly\nsimple and depends only on the 3D rotation matrix and focal lengths.\n9.1.3 Rotational panoramas\nThe most typical case for panoramic image stitching is when the camera undergoes a pure ro-\ntation. Think of standing at the rim of the Grand Canyon. Relative to the distant geometry in\nthe scene, as you snap away, the camera is undergoing a pure rotation, which is equivalent to\nassuming that all points are very far from the camera, i.e., on the plane at inﬁnity (Figure 9.3).\nSetting t0 = t1 = 0, we get the simpliﬁed 3 × 3 homography\n˜\nH10 = K1R1R−1\n0 K−1\n0\n= K1R10K−1\n0 ,\n(9.5)\nwhere Kk = diag(fk, fk, 1) is the simpliﬁed camera intrinsic matrix (2.59), assuming that\ncx = cy = 0, i.e., we are indexing the pixels starting from the optical center (Szeliski 1996).\nThis can also be re-written as\n\n\nx1\ny1\n1\n\n∼\n\n\nf1\nf1\n1\n\nR10\n\n\nf −1\n0\nf −1\n0\n1\n\n\n\n\nx0\ny0\n1\n\n\n(9.6)\nor\n\n\nx1\ny1\nf1\n\n∼R10\n\n\nx0\ny0\nf0\n\n,\n(9.7)\nwhich reveals the simplicity of the mapping equations and makes all of the motion parameters\nexplicit. Thus, instead of the general eight-parameter homography relating a pair of images,\nwe get the three-, four-, or ﬁve-parameter 3D rotation motion models corresponding to the\ncases where the focal length f is known, ﬁxed, or variable (Szeliski and Shum 1997).3 Es-\ntimating the 3D rotation matrix (and, optionally, focal length) associated with each image is\n3 An initial estimate of the focal lengths can be obtained using the intrinsic calibration techniques described in\nSection 6.3.4 or from EXIF tags.",
  "image_path": "page_454.jpg",
  "pages": [
    453,
    454,
    455
  ]
}