{
  "1": "Computer Vision:\nAlgorithms and Applications\nRichard Szeliski\nSeptember 3, 2010 draft\nc⃝2010 Springer\nThis electronic draft is for non-commercial personal use only,\nand may not be posted or re-distributed in any form.\nPlease refer interested readers to the book’s Web site at\nhttp://szeliski.org/Book/.",
  "2": "",
  "3": "This book is dedicated to my parents,\nZdzisław and Jadwiga,\nand my family,\nLyn, Anne, and Stephen.",
  "4": "1 Introduction\n1\nWhat is computer vision?\n• A brief history •\nBook overview • Sample syllabus • Notation\nn^\n2 Image formation\n29\nGeometric primitives and transformations •\nPhotometric image formation •\nThe digital camera\n3 Image processing\n99\nPoint operators • Linear ﬁltering •\nMore neighborhood operators • Fourier transforms •\nPyramids and wavelets • Geometric transformations •\nGlobal optimization\n4 Feature detection and matching\n205\nPoints and patches •\nEdges • Lines\n5 Segmentation\n267\nActive contours • Split and merge •\nMean shift and mode ﬁnding • Normalized cuts •\nGraph cuts and energy-based methods\n6 Feature-based alignment\n309\n2D and 3D feature-based alignment •\nPose estimation •\nGeometric intrinsic calibration\n7 Structure from motion\n343\nTriangulation • Two-frame structure from motion •\nFactorization • Bundle adjustment •\nConstrained structure and motion",
  "5": "8 Dense motion estimation\n381\nTranslational alignment • Parametric motion •\nSpline-based motion • Optical ﬂow •\nLayered motion\n9 Image stitching\n427\nMotion models • Global alignment •\nCompositing\n10 Computational photography\n467\nPhotometric calibration • High dynamic range imaging •\nSuper-resolution and blur removal •\nImage matting and compositing •\nTexture analysis and synthesis\n11 Stereo correspondence\n533\nEpipolar geometry • Sparse correspondence •\nDense correspondence • Local methods •\nGlobal optimization • Multi-view stereo\n12 3D reconstruction\n577\nShape from X • Active rangeﬁnding •\nSurface representations • Point-based representations •\nVolumetric representations • Model-based reconstruction •\nRecovering texture maps and albedos\n13 Image-based rendering\n619\nView interpolation • Layered depth images •\nLight ﬁelds and Lumigraphs • Environment mattes •\nVideo-based rendering\n14 Recognition\n655\nObject detection • Face recognition •\nInstance recognition • Category recognition •\nContext and scene understanding •\nRecognition databases and test sets",
  "6": "",
  "7": "Preface\nThe seeds for this book were ﬁrst planted in 2001 when Steve Seitz at the University of Wash-\nington invited me to co-teach a course called “Computer Vision for Computer Graphics”. At\nthat time, computer vision techniques were increasingly being used in computer graphics to\ncreate image-based models of real-world objects, to create visual effects, and to merge real-\nworld imagery using computational photography techniques. Our decision to focus on the\napplications of computer vision to fun problems such as image stitching and photo-based 3D\nmodeling from personal photos seemed to resonate well with our students.\nSince that time, a similar syllabus and project-oriented course structure has been used to\nteach general computer vision courses both at the University of Washington and at Stanford.\n(The latter was a course I co-taught with David Fleet in 2003.) Similar curricula have been\nadopted at a number of other universities and also incorporated into more specialized courses\non computational photography. (For ideas on how to use this book in your own course, please\nsee Table 1.1 in Section 1.4.)\nThis book also reﬂects my 20 years’ experience doing computer vision research in corpo-\nrate research labs, mostly at Digital Equipment Corporation’s Cambridge Research Lab and\nat Microsoft Research. In pursuing my work, I have mostly focused on problems and solu-\ntion techniques (algorithms) that have practical real-world applications and that work well in\npractice. Thus, this book has more emphasis on basic techniques that work under real-world\nconditions and less on more esoteric mathematics that has intrinsic elegance but less practical\napplicability.\nThis book is suitable for teaching a senior-level undergraduate course in computer vision\nto students in both computer science and electrical engineering. I prefer students to have\neither an image processing or a computer graphics course as a prerequisite so that they can\nspend less time learning general background mathematics and more time studying computer\nvision techniques. The book is also suitable for teaching graduate-level courses in computer\nvision (by delving into the more demanding application and algorithmic areas) and as a gen-\neral reference to fundamental techniques and the recent research literature. To this end, I have\nattempted wherever possible to at least cite the newest research in each sub-ﬁeld, even if the",
  "8": "viii\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntechnical details are too complex to cover in the book itself.\nIn teaching our courses, we have found it useful for the students to attempt a number of\nsmall implementation projects, which often build on one another, in order to get them used to\nworking with real-world images and the challenges that these present. The students are then\nasked to choose an individual topic for each of their small-group, ﬁnal projects. (Sometimes\nthese projects even turn into conference papers!) The exercises at the end of each chapter\ncontain numerous suggestions for smaller mid-term projects, as well as more open-ended\nproblems whose solutions are still active research topics. Wherever possible, I encourage\nstudents to try their algorithms on their own personal photographs, since this better motivates\nthem, often leads to creative variants on the problems, and better acquaints them with the\nvariety and complexity of real-world imagery.\nIn formulating and solving computer vision problems, I have often found it useful to draw\ninspiration from three high-level approaches:\n• Scientiﬁc: build detailed models of the image formation process and develop mathe-\nmatical techniques to invert these in order to recover the quantities of interest (where\nnecessary, making simplifying assumption to make the mathematics more tractable).\n• Statistical: use probabilistic models to quantify the prior likelihood of your unknowns\nand the noisy measurement processes that produce the input images, then infer the best\npossible estimates of your desired quantities and analyze their resulting uncertainties.\nThe inference algorithms used are often closely related to the optimization techniques\nused to invert the (scientiﬁc) image formation processes.\n• Engineering: develop techniques that are simple to describe and implement but that\nare also known to work well in practice. Test these techniques to understand their\nlimitation and failure modes, as well as their expected computational costs (run-time\nperformance).\nThese three approaches build on each other and are used throughout the book.\nMy personal research and development philosophy (and hence the exercises in the book)\nhave a strong emphasis on testing algorithms. It’s too easy in computer vision to develop an\nalgorithm that does something plausible on a few images rather than something correct. The\nbest way to validate your algorithms is to use a three-part strategy.\nFirst, test your algorithm on clean synthetic data, for which the exact results are known.\nSecond, add noise to the data and evaluate how the performance degrades as a function of\nnoise level. Finally, test the algorithm on real-world data, preferably drawn from a wide\nvariety of sources, such as photos found on the Web. Only then can you truly know if your\nalgorithm can deal with real-world complexity, i.e., images that do not ﬁt some simpliﬁed\nmodel or assumptions.",
  "9": "Preface\nix\nIn order to help students in this process, this books comes with a large amount of supple-\nmentary material, which can be found on the book’s Web site http://szeliski.org/Book. This\nmaterial, which is described in Appendix C, includes:\n• pointers to commonly used data sets for the problems, which can be found on the Web\n• pointers to software libraries, which can help students get started with basic tasks such\nas reading/writing images or creating and manipulating images\n• slide sets corresponding to the material covered in this book\n• a BibTeX bibliography of the papers cited in this book.\nThe latter two resources may be of more interest to instructors and researchers publishing\nnew papers in this ﬁeld, but they will probably come in handy even with regular students.\nSome of the software libraries contain implementations of a wide variety of computer vision\nalgorithms, which can enable you to tackle more ambitious projects (with your instructor’s\nconsent).\nAcknowledgements\nI would like to gratefully acknowledge all of the people whose passion for research and\ninquiry as well as encouragement have helped me write this book.\nSteve Zucker at McGill University ﬁrst introduced me to computer vision, taught all of\nhis students to question and debate research results and techniques, and encouraged me to\npursue a graduate career in this area.\nTakeo Kanade and Geoff Hinton, my Ph. D. thesis advisors at Carnegie Mellon University,\ntaught me the fundamentals of good research, writing, and presentation. They ﬁred up my\ninterest in visual processing, 3D modeling, and statistical methods, while Larry Matthies\nintroduced me to Kalman ﬁltering and stereo matching.\nDemetri Terzopoulos was my mentor at my ﬁrst industrial research job and taught me the\nropes of successful publishing. Yvan Leclerc and Pascal Fua, colleagues from my brief in-\nterlude at SRI International, gave me new perspectives on alternative approaches to computer\nvision.\nDuring my six years of research at Digital Equipment Corporation’s Cambridge Research\nLab, I was fortunate to work with a great set of colleagues, including Ingrid Carlbom, Gudrun\nKlinker, Keith Waters, Richard Weiss, St´ephane Lavall´ee, and Sing Bing Kang, as well as to\nsupervise the ﬁrst of a long string of outstanding summer interns, including David Tonnesen,\nSing Bing Kang, James Coughlan, and Harry Shum. This is also where I began my long-term\ncollaboration with Daniel Scharstein, now at Middlebury College.",
  "10": "x\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAt Microsoft Research, I’ve had the outstanding fortune to work with some of the world’s\nbest researchers in computer vision and computer graphics, including Michael Cohen, Hugues\nHoppe, Stephen Gortler, Steve Shafer, Matthew Turk, Harry Shum, Anandan, Phil Torr, An-\ntonio Criminisi, Georg Petschnigg, Kentaro Toyama, Ramin Zabih, Shai Avidan, Sing Bing\nKang, Matt Uyttendaele, Patrice Simard, Larry Zitnick, Richard Hartley, Simon Winder,\nDrew Steedly, Chris Pal, Nebojsa Jojic, Patrick Baudisch, Dani Lischinski, Matthew Brown,\nSimon Baker, Michael Goesele, Eric Stollnitz, David Nist´er, Blaise Aguera y Arcas, Sudipta\nSinha, Johannes Kopf, Neel Joshi, and Krishnan Ramnath. I was also lucky to have as in-\nterns such great students as Polina Golland, Simon Baker, Mei Han, Arno Sch¨odl, Ron Dror,\nAshley Eden, Jinxiang Chai, Rahul Swaminathan, Yanghai Tsin, Sam Hasinoff, Anat Levin,\nMatthew Brown, Eric Bennett, Vaibhav Vaish, Jan-Michael Frahm, James Diebel, Ce Liu,\nJosef Sivic, Grant Schindler, Colin Zheng, Neel Joshi, Sudipta Sinha, Zeev Farbman, Rahul\nGarg, Tim Cho, Yekeun Jeong, Richard Roberts, Varsha Hedau, and Dilip Krishnan.\nWhile working at Microsoft, I’ve also had the opportunity to collaborate with wonderful\ncolleagues at the University of Washington, where I hold an Afﬁliate Professor appointment.\nI’m indebted to Tony DeRose and David Salesin, who ﬁrst encouraged me to get involved\nwith the research going on at UW, my long-time collaborators Brian Curless, Steve Seitz,\nManeesh Agrawala, Sameer Agarwal, and Yasu Furukawa, as well as the students I have\nhad the privilege to supervise and interact with, including Fr´ederic Pighin, Yung-Yu Chuang,\nDoug Zongker, Colin Zheng, Aseem Agarwala, Dan Goldman, Noah Snavely, Rahul Garg,\nand Ryan Kaminsky. As I mentioned at the beginning of this preface, this book owes its\ninception to the vision course that Steve Seitz invited me to co-teach, as well as to Steve’s\nencouragement, course notes, and editorial input.\nI’m also grateful to the many other computer vision researchers who have given me so\nmany constructive suggestions about the book, including Sing Bing Kang, who was my infor-\nmal book editor, Vladimir Kolmogorov, who contributed Appendix B.5.5 on linear program-\nming techniques for MRF inference, Daniel Scharstein, Richard Hartley, Simon Baker, Noah\nSnavely, Bill Freeman, Svetlana Lazebnik, Matthew Turk, Jitendra Malik, Alyosha Efros,\nMichael Black, Brian Curless, Sameer Agarwal, Li Zhang, Deva Ramanan, Olga Veksler,\nYuri Boykov, Carsten Rother, Phil Torr, Bill Triggs, Bruce Maxwell, Jana Koˇseck´a, Eero Si-\nmoncelli, Aaron Hertzmann, Antonio Torralba, Tomaso Poggio, Theo Pavlidis, Baba Vemuri,\nNando de Freitas, Chuck Dyer, Song Yi, Falk Schubert, Roman Pﬂugfelder, Marshall Tap-\npen, James Coughlan, Sammy Rogmans, Klaus Strobel, Shanmuganathan, Andreas Siebert,\nYongjun Wu, Fred Pighin, Juan Cockburn, Ronald Mallet, Tim Soper, Georgios Evangelidis,\nDwight Fowler, Itzik Bayaz, Daniel O’Connor, and Srikrishna Bhat. Shena Deuchers did a\nfantastic job copy-editing the book and suggesting many useful improvements and Wayne\nWheeler and Simon Rees at Springer were most helpful throughout the whole book pub-\nlishing process. Keith Price’s Annotated Computer Vision Bibliography was invaluable in",
  "11": "Preface\nxi\ntracking down references and ﬁnding related work.\nIf you have any suggestions for improving the book, please send me an e-mail, as I would\nlike to keep the book as accurate, informative, and timely as possible.\nLastly, this book would not have been possible or worthwhile without the incredible sup-\nport and encouragement of my family. I dedicate this book to my parents, Zdzisław and\nJadwiga, whose love, generosity, and accomplishments have always inspired me; to my sis-\nter Basia for her lifelong friendship; and especially to Lyn, Anne, and Stephen, whose daily\nencouragement in all matters (including this book project) makes it all worthwhile.\nLake Wenatchee\nAugust, 2010",
  "12": "xii\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "13": "Contents\nPreface\nvii\nContents\nxiii\n1\nIntroduction\n1\n1.1\nWhat is computer vision? . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nA brief history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n1.3\nBook overview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.4\nSample syllabus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n1.5\nA note on notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n1.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n2\nImage formation\n29\n2.1\nGeometric primitives and transformations . . . . . . . . . . . . . . . . . . .\n31\n2.1.1\nGeometric primitives . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n2.1.2\n2D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n2.1.3\n3D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n2.1.4\n3D rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n2.1.5\n3D to 2D projections . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n2.1.6\nLens distortions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n2.2\nPhotometric image formation . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n2.2.1\nLighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n2.2.2\nReﬂectance and shading . . . . . . . . . . . . . . . . . . . . . . . .\n62\n2.2.3\nOptics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n2.3\nThe digital camera\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n2.3.1\nSampling and aliasing\n. . . . . . . . . . . . . . . . . . . . . . . . .\n77\n2.3.2\nColor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n2.3.3\nCompression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90",
  "14": "xiv\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n2.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n2.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n3\nImage processing\n99\n3.1\nPoint operators\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n3.1.1\nPixel transforms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n3.1.2\nColor transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n3.1.3\nCompositing and matting . . . . . . . . . . . . . . . . . . . . . . . . 105\n3.1.4\nHistogram equalization . . . . . . . . . . . . . . . . . . . . . . . . . 107\n3.1.5\nApplication: Tonal adjustment . . . . . . . . . . . . . . . . . . . . . 111\n3.2\nLinear ﬁltering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n3.2.1\nSeparable ﬁltering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n3.2.2\nExamples of linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . 117\n3.2.3\nBand-pass and steerable ﬁlters . . . . . . . . . . . . . . . . . . . . . 118\n3.3\nMore neighborhood operators . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n3.3.1\nNon-linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n3.3.2\nMorphology\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n3.3.3\nDistance transforms\n. . . . . . . . . . . . . . . . . . . . . . . . . . 129\n3.3.4\nConnected components . . . . . . . . . . . . . . . . . . . . . . . . . 131\n3.4\nFourier transforms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n3.4.1\nFourier transform pairs . . . . . . . . . . . . . . . . . . . . . . . . . 136\n3.4.2\nTwo-dimensional Fourier transforms . . . . . . . . . . . . . . . . . . 140\n3.4.3\nWiener ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n3.4.4\nApplication: Sharpening, blur, and noise removal . . . . . . . . . . . 144\n3.5\nPyramids and wavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n3.5.1\nInterpolation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n3.5.2\nDecimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n3.5.3\nMulti-resolution representations . . . . . . . . . . . . . . . . . . . . 150\n3.5.4\nWavelets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n3.5.5\nApplication: Image blending . . . . . . . . . . . . . . . . . . . . . . 160\n3.6\nGeometric transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n3.6.1\nParametric transformations . . . . . . . . . . . . . . . . . . . . . . . 163\n3.6.2\nMesh-based warping . . . . . . . . . . . . . . . . . . . . . . . . . . 170\n3.6.3\nApplication: Feature-based morphing . . . . . . . . . . . . . . . . . 173\n3.7\nGlobal optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n3.7.1\nRegularization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n3.7.2\nMarkov random ﬁelds\n. . . . . . . . . . . . . . . . . . . . . . . . . 180\n3.7.3\nApplication: Image restoration . . . . . . . . . . . . . . . . . . . . . 192",
  "15": "Contents\nxv\n3.8\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n3.9\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n4\nFeature detection and matching\n205\n4.1\nPoints and patches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n4.1.1\nFeature detectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n4.1.2\nFeature descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n4.1.3\nFeature matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n4.1.4\nFeature tracking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n4.1.5\nApplication: Performance-driven animation . . . . . . . . . . . . . . 237\n4.2\nEdges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n4.2.1\nEdge detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n4.2.2\nEdge linking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n4.2.3\nApplication: Edge editing and enhancement . . . . . . . . . . . . . . 249\n4.3\nLines\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n4.3.1\nSuccessive approximation\n. . . . . . . . . . . . . . . . . . . . . . . 250\n4.3.2\nHough transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\n4.3.3\nVanishing points\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n4.3.4\nApplication: Rectangle detection . . . . . . . . . . . . . . . . . . . . 257\n4.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n4.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\n5\nSegmentation\n267\n5.1\nActive contours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n5.1.1\nSnakes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n5.1.2\nDynamic snakes and CONDENSATION . . . . . . . . . . . . . . . . 276\n5.1.3\nScissors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\n5.1.4\nLevel Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n5.1.5\nApplication: Contour tracking and rotoscoping . . . . . . . . . . . . 282\n5.2\nSplit and merge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n5.2.1\nWatershed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n5.2.2\nRegion splitting (divisive clustering) . . . . . . . . . . . . . . . . . . 286\n5.2.3\nRegion merging (agglomerative clustering) . . . . . . . . . . . . . . 286\n5.2.4\nGraph-based segmentation . . . . . . . . . . . . . . . . . . . . . . . 286\n5.2.5\nProbabilistic aggregation . . . . . . . . . . . . . . . . . . . . . . . . 288\n5.3\nMean shift and mode ﬁnding . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n5.3.1\nK-means and mixtures of Gaussians . . . . . . . . . . . . . . . . . . 289\n5.3.2\nMean shift\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292",
  "16": "xvi\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n5.4\nNormalized cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\n5.5\nGraph cuts and energy-based methods . . . . . . . . . . . . . . . . . . . . . 300\n5.5.1\nApplication: Medical image segmentation . . . . . . . . . . . . . . . 304\n5.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\n5.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306\n6\nFeature-based alignment\n309\n6.1\n2D and 3D feature-based alignment\n. . . . . . . . . . . . . . . . . . . . . . 311\n6.1.1\n2D alignment using least squares . . . . . . . . . . . . . . . . . . . . 312\n6.1.2\nApplication: Panography . . . . . . . . . . . . . . . . . . . . . . . . 314\n6.1.3\nIterative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\n6.1.4\nRobust least squares and RANSAC\n. . . . . . . . . . . . . . . . . . 318\n6.1.5\n3D alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320\n6.2\nPose estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\n6.2.1\nLinear algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\n6.2.2\nIterative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n6.2.3\nApplication: Augmented reality . . . . . . . . . . . . . . . . . . . . 326\n6.3\nGeometric intrinsic calibration . . . . . . . . . . . . . . . . . . . . . . . . . 327\n6.3.1\nCalibration patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n6.3.2\nVanishing points\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\n6.3.3\nApplication: Single view metrology . . . . . . . . . . . . . . . . . . 331\n6.3.4\nRotational motion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 332\n6.3.5\nRadial distortion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n6.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n6.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n7\nStructure from motion\n343\n7.1\nTriangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\n7.2\nTwo-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . . 347\n7.2.1\nProjective (uncalibrated) reconstruction . . . . . . . . . . . . . . . . 353\n7.2.2\nSelf-calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n7.2.3\nApplication: View morphing . . . . . . . . . . . . . . . . . . . . . . 357\n7.3\nFactorization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\n7.3.1\nPerspective and projective factorization . . . . . . . . . . . . . . . . 360\n7.3.2\nApplication: Sparse 3D model extraction\n. . . . . . . . . . . . . . . 362\n7.4\nBundle adjustment\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\n7.4.1\nExploiting sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n7.4.2\nApplication: Match move and augmented reality\n. . . . . . . . . . . 368",
  "17": "Contents\nxvii\n7.4.3\nUncertainty and ambiguities . . . . . . . . . . . . . . . . . . . . . . 370\n7.4.4\nApplication: Reconstruction from Internet photos . . . . . . . . . . . 371\n7.5\nConstrained structure and motion . . . . . . . . . . . . . . . . . . . . . . . . 374\n7.5.1\nLine-based techniques . . . . . . . . . . . . . . . . . . . . . . . . . 374\n7.5.2\nPlane-based techniques . . . . . . . . . . . . . . . . . . . . . . . . . 376\n7.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n7.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n8\nDense motion estimation\n381\n8.1\nTranslational alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n8.1.1\nHierarchical motion estimation . . . . . . . . . . . . . . . . . . . . . 387\n8.1.2\nFourier-based alignment . . . . . . . . . . . . . . . . . . . . . . . . 388\n8.1.3\nIncremental reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . 392\n8.2\nParametric motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398\n8.2.1\nApplication: Video stabilization . . . . . . . . . . . . . . . . . . . . 401\n8.2.2\nLearned motion models . . . . . . . . . . . . . . . . . . . . . . . . . 403\n8.3\nSpline-based motion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\n8.3.1\nApplication: Medical image registration . . . . . . . . . . . . . . . . 408\n8.4\nOptical ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\n8.4.1\nMulti-frame motion estimation . . . . . . . . . . . . . . . . . . . . . 413\n8.4.2\nApplication: Video denoising\n. . . . . . . . . . . . . . . . . . . . . 414\n8.4.3\nApplication: De-interlacing\n. . . . . . . . . . . . . . . . . . . . . . 415\n8.5\nLayered motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n8.5.1\nApplication: Frame interpolation . . . . . . . . . . . . . . . . . . . . 418\n8.5.2\nTransparent layers and reﬂections . . . . . . . . . . . . . . . . . . . 419\n8.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\n8.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422\n9\nImage stitching\n427\n9.1\nMotion models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430\n9.1.1\nPlanar perspective motion\n. . . . . . . . . . . . . . . . . . . . . . . 431\n9.1.2\nApplication: Whiteboard and document scanning . . . . . . . . . . . 432\n9.1.3\nRotational panoramas . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n9.1.4\nGap closing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435\n9.1.5\nApplication: Video summarization and compression\n. . . . . . . . . 436\n9.1.6\nCylindrical and spherical coordinates\n. . . . . . . . . . . . . . . . . 438\n9.2\nGlobal alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n9.2.1\nBundle adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . 441",
  "18": "xviii\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n9.2.2\nParallax removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n9.2.3\nRecognizing panoramas\n. . . . . . . . . . . . . . . . . . . . . . . . 446\n9.2.4\nDirect vs. feature-based alignment . . . . . . . . . . . . . . . . . . . 450\n9.3\nCompositing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450\n9.3.1\nChoosing a compositing surface . . . . . . . . . . . . . . . . . . . . 451\n9.3.2\nPixel selection and weighting (de-ghosting) . . . . . . . . . . . . . . 453\n9.3.3\nApplication: Photomontage\n. . . . . . . . . . . . . . . . . . . . . . 459\n9.3.4\nBlending\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459\n9.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462\n9.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463\n10 Computational photography\n467\n10.1 Photometric calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\n10.1.1 Radiometric response function . . . . . . . . . . . . . . . . . . . . . 470\n10.1.2 Noise level estimation\n. . . . . . . . . . . . . . . . . . . . . . . . . 473\n10.1.3 Vignetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474\n10.1.4 Optical blur (spatial response) estimation . . . . . . . . . . . . . . . 476\n10.2 High dynamic range imaging . . . . . . . . . . . . . . . . . . . . . . . . . . 479\n10.2.1 Tone mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487\n10.2.2 Application: Flash photography . . . . . . . . . . . . . . . . . . . . 494\n10.3 Super-resolution and blur removal . . . . . . . . . . . . . . . . . . . . . . . 497\n10.3.1 Color image demosaicing\n. . . . . . . . . . . . . . . . . . . . . . . 502\n10.3.2 Application: Colorization\n. . . . . . . . . . . . . . . . . . . . . . . 504\n10.4 Image matting and compositing . . . . . . . . . . . . . . . . . . . . . . . . . 505\n10.4.1 Blue screen matting . . . . . . . . . . . . . . . . . . . . . . . . . . . 507\n10.4.2 Natural image matting . . . . . . . . . . . . . . . . . . . . . . . . . 509\n10.4.3 Optimization-based matting\n. . . . . . . . . . . . . . . . . . . . . . 513\n10.4.4 Smoke, shadow, and ﬂash matting . . . . . . . . . . . . . . . . . . . 516\n10.4.5 Video matting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518\n10.5 Texture analysis and synthesis\n. . . . . . . . . . . . . . . . . . . . . . . . . 518\n10.5.1 Application: Hole ﬁlling and inpainting . . . . . . . . . . . . . . . . 521\n10.5.2 Application: Non-photorealistic rendering . . . . . . . . . . . . . . . 522\n10.6 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n10.7 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526\n11 Stereo correspondence\n533\n11.1 Epipolar geometry\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\n11.1.1 Rectiﬁcation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538",
  "19": "Contents\nxix\n11.1.2 Plane sweep . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540\n11.2 Sparse correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543\n11.2.1 3D curves and proﬁles . . . . . . . . . . . . . . . . . . . . . . . . . 543\n11.3 Dense correspondence\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545\n11.3.1 Similarity measures . . . . . . . . . . . . . . . . . . . . . . . . . . . 546\n11.4 Local methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548\n11.4.1 Sub-pixel estimation and uncertainty . . . . . . . . . . . . . . . . . . 550\n11.4.2 Application: Stereo-based head tracking . . . . . . . . . . . . . . . . 551\n11.5 Global optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552\n11.5.1 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 554\n11.5.2 Segmentation-based techniques\n. . . . . . . . . . . . . . . . . . . . 556\n11.5.3 Application: Z-keying and background replacement . . . . . . . . . . 558\n11.6 Multi-view stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558\n11.6.1 Volumetric and 3D surface reconstruction . . . . . . . . . . . . . . . 562\n11.6.2 Shape from silhouettes . . . . . . . . . . . . . . . . . . . . . . . . . 567\n11.7 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570\n11.8 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571\n12 3D reconstruction\n577\n12.1 Shape from X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580\n12.1.1 Shape from shading and photometric stereo . . . . . . . . . . . . . . 580\n12.1.2 Shape from texture . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\n12.1.3 Shape from focus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584\n12.2 Active rangeﬁnding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585\n12.2.1 Range data merging\n. . . . . . . . . . . . . . . . . . . . . . . . . . 588\n12.2.2 Application: Digital heritage . . . . . . . . . . . . . . . . . . . . . . 590\n12.3 Surface representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591\n12.3.1 Surface interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 592\n12.3.2 Surface simpliﬁcation\n. . . . . . . . . . . . . . . . . . . . . . . . . 594\n12.3.3 Geometry images . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594\n12.4 Point-based representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 595\n12.5 Volumetric representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 596\n12.5.1 Implicit surfaces and level sets . . . . . . . . . . . . . . . . . . . . . 596\n12.6 Model-based reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . 598\n12.6.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598\n12.6.2 Heads and faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601\n12.6.3 Application: Facial animation\n. . . . . . . . . . . . . . . . . . . . . 603\n12.6.4 Whole body modeling and tracking\n. . . . . . . . . . . . . . . . . . 605",
  "20": "xx\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n12.7 Recovering texture maps and albedos\n. . . . . . . . . . . . . . . . . . . . . 610\n12.7.1 Estimating BRDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n12.7.2 Application: 3D photography\n. . . . . . . . . . . . . . . . . . . . . 613\n12.8 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614\n12.9 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616\n13 Image-based rendering\n619\n13.1 View interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621\n13.1.1 View-dependent texture maps\n. . . . . . . . . . . . . . . . . . . . . 623\n13.1.2 Application: Photo Tourism\n. . . . . . . . . . . . . . . . . . . . . . 624\n13.2 Layered depth images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626\n13.2.1 Impostors, sprites, and layers . . . . . . . . . . . . . . . . . . . . . . 626\n13.3 Light ﬁelds and Lumigraphs\n. . . . . . . . . . . . . . . . . . . . . . . . . . 628\n13.3.1 Unstructured Lumigraph . . . . . . . . . . . . . . . . . . . . . . . . 632\n13.3.2 Surface light ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . 632\n13.3.3 Application: Concentric mosaics . . . . . . . . . . . . . . . . . . . . 634\n13.4 Environment mattes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634\n13.4.1 Higher-dimensional light ﬁelds . . . . . . . . . . . . . . . . . . . . . 636\n13.4.2 The modeling to rendering continuum . . . . . . . . . . . . . . . . . 637\n13.5 Video-based rendering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638\n13.5.1 Video-based animation . . . . . . . . . . . . . . . . . . . . . . . . . 639\n13.5.2 Video textures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640\n13.5.3 Application: Animating pictures . . . . . . . . . . . . . . . . . . . . 643\n13.5.4 3D Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\n13.5.5 Application: Video-based walkthroughs . . . . . . . . . . . . . . . . 645\n13.6 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648\n13.7 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650\n14 Recognition\n655\n14.1 Object detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658\n14.1.1 Face detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658\n14.1.2 Pedestrian detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . 666\n14.2 Face recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668\n14.2.1 Eigenfaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671\n14.2.2 Active appearance and 3D shape models . . . . . . . . . . . . . . . . 679\n14.2.3 Application: Personal photo collections . . . . . . . . . . . . . . . . 684\n14.3 Instance recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685\n14.3.1 Geometric alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 686",
  "21": "Contents\nxxi\n14.3.2 Large databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687\n14.3.3 Application: Location recognition . . . . . . . . . . . . . . . . . . . 693\n14.4 Category recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 696\n14.4.1 Bag of words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 697\n14.4.2 Part-based models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 701\n14.4.3 Recognition with segmentation . . . . . . . . . . . . . . . . . . . . . 704\n14.4.4 Application: Intelligent photo editing\n. . . . . . . . . . . . . . . . . 709\n14.5 Context and scene understanding . . . . . . . . . . . . . . . . . . . . . . . . 712\n14.5.1 Learning and large image collections\n. . . . . . . . . . . . . . . . . 714\n14.5.2 Application: Image search . . . . . . . . . . . . . . . . . . . . . . . 717\n14.6 Recognition databases and test sets . . . . . . . . . . . . . . . . . . . . . . . 718\n14.7 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722\n14.8 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 725\n15 Conclusion\n731\nA Linear algebra and numerical techniques\n735\nA.1\nMatrix decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 736\nA.1.1\nSingular value decomposition\n. . . . . . . . . . . . . . . . . . . . . 736\nA.1.2\nEigenvalue decomposition . . . . . . . . . . . . . . . . . . . . . . . 737\nA.1.3\nQR factorization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 740\nA.1.4\nCholesky factorization . . . . . . . . . . . . . . . . . . . . . . . . . 741\nA.2\nLinear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742\nA.2.1\nTotal least squares\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 744\nA.3\nNon-linear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 746\nA.4\nDirect sparse matrix techniques . . . . . . . . . . . . . . . . . . . . . . . . . 747\nA.4.1\nVariable reordering . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\nA.5\nIterative techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\nA.5.1\nConjugate gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . 749\nA.5.2\nPreconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 751\nA.5.3\nMultigrid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753\nB\nBayesian modeling and inference\n755\nB.1\nEstimation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757\nB.1.1\nLikelihood for multivariate Gaussian noise\n. . . . . . . . . . . . . . 757\nB.2\nMaximum likelihood estimation and least squares . . . . . . . . . . . . . . . 759\nB.3\nRobust statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 760\nB.4\nPrior models and Bayesian inference . . . . . . . . . . . . . . . . . . . . . . 762\nB.5\nMarkov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763",
  "22": "xxii\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nB.5.1\nGradient descent and simulated annealing . . . . . . . . . . . . . . . 765\nB.5.2\nDynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 766\nB.5.3\nBelief propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\nB.5.4\nGraph cuts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770\nB.5.5\nLinear programming . . . . . . . . . . . . . . . . . . . . . . . . . . 773\nB.6\nUncertainty estimation (error analysis) . . . . . . . . . . . . . . . . . . . . . 775\nC Supplementary material\n777\nC.1\nData sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778\nC.2\nSoftware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780\nC.3\nSlides and lectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 789\nC.4\nBibliography\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790\nReferences\n791\nIndex\n933",
  "23": "Chapter 1\nIntroduction\n1.1\nWhat is computer vision? . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nA brief history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n1.3\nBook overview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.4\nSample syllabus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n1.5\nA note on notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n1.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nFigure 1.1\nThe human visual system has no problem interpreting the subtle variations in\ntranslucency and shading in this photograph and correctly segmenting the object from its\nbackground.",
  "24": "2\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 1.2\nSome examples of computer vision algorithms and applications. (a) Structure\nfrom motion algorithms can reconstruct a sparse 3D point model of a large complex scene\nfrom hundreds of partially overlapping photographs (Snavely, Seitz, and Szeliski 2006) c⃝\n2006 ACM. (b) Stereo matching algorithms can build a detailed 3D model of a building fac¸ade\nfrom hundreds of differently exposed photographs taken from the Internet (Goesele, Snavely,\nCurless et al. 2007) c⃝2007 IEEE. (c) Person tracking algorithms can track a person walking\nin front of a cluttered background (Sidenbladh, Black, and Fleet 2000) c⃝2000 Springer. (d)\nFace detection algorithms, coupled with color-based clothing and hair detection algorithms,\ncan locate and recognize the individuals in this image (Sivic, Zitnick, and Szeliski 2006) c⃝\n2006 Springer.",
  "25": "1.1 What is computer vision?\n3\n1.1 What is computer vision?\nAs humans, we perceive the three-dimensional structure of the world around us with apparent\nease. Think of how vivid the three-dimensional percept is when you look at a vase of ﬂowers\nsitting on the table next to you. You can tell the shape and translucency of each petal through\nthe subtle patterns of light and shading that play across its surface and effortlessly segment\neach ﬂower from the background of the scene (Figure 1.1). Looking at a framed group por-\ntrait, you can easily count (and name) all of the people in the picture and even guess at their\nemotions from their facial appearance. Perceptual psychologists have spent decades trying to\nunderstand how the visual system works and, even though they can devise optical illusions1\nto tease apart some of its principles (Figure 1.3), a complete solution to this puzzle remains\nelusive (Marr 1982; Palmer 1999; Livingstone 2008).\nResearchers in computer vision have been developing, in parallel, mathematical tech-\nniques for recovering the three-dimensional shape and appearance of objects in imagery. We\nnow have reliable techniques for accurately computing a partial 3D model of an environment\nfrom thousands of partially overlapping photographs (Figure 1.2a). Given a large enough\nset of views of a particular object or fac¸ade, we can create accurate dense 3D surface mod-\nels using stereo matching (Figure 1.2b). We can track a person moving against a complex\nbackground (Figure 1.2c). We can even, with moderate success, attempt to ﬁnd and name\nall of the people in a photograph using a combination of face, clothing, and hair detection\nand recognition (Figure 1.2d). However, despite all of these advances, the dream of having a\ncomputer interpret an image at the same level as a two-year old (for example, counting all of\nthe animals in a picture) remains elusive.\nWhy is vision so difﬁcult? In part, it is because\nvision is an inverse problem, in which we seek to recover some unknowns given insufﬁcient\ninformation to fully specify the solution. We must therefore resort to physics-based and prob-\nabilistic models to disambiguate between potential solutions. However, modeling the visual\nworld in all of its rich complexity is far more difﬁcult than, say, modeling the vocal tract that\nproduces spoken sounds.\nThe forward models that we use in computer vision are usually developed in physics (ra-\ndiometry, optics, and sensor design) and in computer graphics. Both of these ﬁelds model\nhow objects move and animate, how light reﬂects off their surfaces, is scattered by the at-\nmosphere, refracted through camera lenses (or human eyes), and ﬁnally projected onto a ﬂat\n(or curved) image plane. While computer graphics are not yet perfect (no fully computer-\nanimated movie with human characters has yet succeeded at crossing the uncanny valley2\nthat separates real humans from android robots and computer-animated humans), in limited\n1 http://www.michaelbach.de/ot/sze muelue\n2 The term uncanny valley was originally coined by roboticist Masahiro Mori as applied to robotics (Mori 1970).\nIt is also commonly applied to computer-animated ﬁlms such as Final Fantasy and Polar Express (Geller 2008).",
  "26": "4\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nX\nX\nX\nX\nX\nX\nX\nO\nX\nO\nX\nO\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nX\nX\nO\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nX\nO\nX\nX\nO\nX\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nO\nO\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nX\nO\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nX\nX\nO\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nX\nO\nX\nX\nO\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nX\nX\nO\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nO\nO\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nO\nX\nX\nX\nO\nX\n(c)\n(d)\nFigure 1.3 Some common optical illusions and what they might tell us about the visual sys-\ntem: (a) The classic M¨uller-Lyer illusion, where the length of the two horizontal lines appear\ndifferent, probably due to the imagined perspective effects. (b) The “white” square B in the\nshadow and the “black” square A in the light actually have the same absolute intensity value.\nThe percept is due to brightness constancy, the visual system’s attempt to discount illumi-\nnation when interpreting colors. Image courtesy of Ted Adelson, http://web.mit.edu/persci/\npeople/adelson/checkershadow illusion.html. (c) A variation of the Hermann grid illusion,\ncourtesy of Hany Farid, http://www.cs.dartmouth.edu/∼farid/illusions/hermann.html. As you\nmove your eyes over the ﬁgure, gray spots appear at the intersections. (d) Count the red Xs\nin the left half of the ﬁgure. Now count them in the right half. Is it signiﬁcantly harder?\nThe explanation has to do with a pop-out effect (Treisman 1985), which tells us about the\noperations of parallel perception and integration pathways in the brain.",
  "27": "1.1 What is computer vision?\n5\ndomains, such as rendering a still scene composed of everyday objects or animating extinct\ncreatures such as dinosaurs, the illusion of reality is perfect.\nIn computer vision, we are trying to do the inverse, i.e., to describe the world that we see\nin one or more images and to reconstruct its properties, such as shape, illumination, and color\ndistributions. It is amazing that humans and animals do this so effortlessly, while computer\nvision algorithms are so error prone. People who have not worked in the ﬁeld often under-\nestimate the difﬁculty of the problem. (Colleagues at work often ask me for software to ﬁnd\nand name all the people in photos, so they can get on with the more “interesting” work.) This\nmisperception that vision should be easy dates back to the early days of artiﬁcial intelligence\n(see Section 1.2), when it was initially believed that the cognitive (logic proving and plan-\nning) parts of intelligence were intrinsically more difﬁcult than the perceptual components\n(Boden 2006).\nThe good news is that computer vision is being used today in a wide variety of real-world\napplications, which include:\n• Optical character recognition (OCR): reading handwritten postal codes on letters\n(Figure 1.4a) and automatic number plate recognition (ANPR);\n• Machine inspection: rapid parts inspection for quality assurance using stereo vision\nwith specialized illumination to measure tolerances on aircraft wings or auto body parts\n(Figure 1.4b) or looking for defects in steel castings using X-ray vision;\n• Retail: object recognition for automated checkout lanes (Figure 1.4c);\n• 3D model building (photogrammetry): fully automated construction of 3D models\nfrom aerial photographs used in systems such as Bing Maps;\n• Medical imaging: registering pre-operative and intra-operative imagery (Figure 1.4d)\nor performing long-term studies of people’s brain morphology as they age;\n• Automotive safety: detecting unexpected obstacles such as pedestrians on the street,\nunder conditions where active vision techniques such as radar or lidar do not work\nwell (Figure 1.4e; see also Miller, Campbell, Huttenlocher et al. (2008); Montemerlo,\nBecker, Bhat et al. (2008); Urmson, Anhalt, Bagnell et al. (2008) for examples of fully\nautomated driving);\n• Match move: merging computer-generated imagery (CGI) with live action footage by\ntracking feature points in the source video to estimate the 3D camera motion and shape\nof the environment. Such techniques are widely used in Hollywood (e.g., in movies\nsuch as Jurassic Park) (Roble 1999; Roble and Zafar 2009); they also require the use of\nprecise matting to insert new elements between foreground and background elements\n(Chuang, Agarwala, Curless et al. 2002).",
  "28": "6\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.4 Some industrial applications of computer vision: (a) optical character recognition\n(OCR) http://yann.lecun.com/exdb/lenet/; (b) mechanical inspection http://www.cognitens.\ncom/; (c) retail http://www.evoretail.com/; (d) medical imaging http://www.clarontech.com/;\n(e) automotive safety http://www.mobileye.com/; (f) surveillance and trafﬁc monitoring http:\n//www.honeywellvideo.com/, courtesy of Honeywell International Inc.",
  "29": "1.1 What is computer vision?\n7\n• Motion capture (mocap): using retro-reﬂective markers viewed from multiple cam-\neras or other vision-based techniques to capture actors for computer animation;\n• Surveillance: monitoring for intruders, analyzing highway trafﬁc (Figure 1.4f), and\nmonitoring pools for drowning victims;\n• Fingerprint recognition and biometrics: for automatic access authentication as well\nas forensic applications.\nDavid Lowe’s Web site of industrial vision applications (http://www.cs.ubc.ca/spider/lowe/\nvision.html) lists many other interesting industrial applications of computer vision. While the\nabove applications are all extremely important, they mostly pertain to fairly specialized kinds\nof imagery and narrow domains.\nIn this book, we focus more on broader consumer-level applications, such as fun things\nyou can do with your own personal photographs and video. These include:\n• Stitching: turning overlapping photos into a single seamlessly stitched panorama (Fig-\nure 1.5a), as described in Chapter 9;\n• Exposure bracketing: merging multiple exposures taken under challenging lighting\nconditions (strong sunlight and shadows) into a single perfectly exposed image (Fig-\nure 1.5b), as described in Section 10.2;\n• Morphing: turning a picture of one of your friends into another, using a seamless\nmorph transition (Figure 1.5c);\n• 3D modeling: converting one or more snapshots into a 3D model of the object or\nperson you are photographing (Figure 1.5d), as described in Section 12.6\n• Video match move and stabilization: inserting 2D pictures or 3D models into your\nvideos by automatically tracking nearby reference points (see Section 7.4.2)3 or using\nmotion estimates to remove shake from your videos (see Section 8.2.1);\n• Photo-based walkthroughs: navigating a large collection of photographs, such as the\ninterior of your house, by ﬂying between different photos in 3D (see Sections 13.1.2\nand 13.5.5)\n• Face detection: for improved camera focusing as well as more relevant image search-\ning (see Section 14.1.1);\n• Visual authentication: automatically logging family members onto your home com-\nputer as they sit down in front of the webcam (see Section 14.2).",
  "30": "8\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 1.5\nSome consumer applications of computer vision: (a) image stitching: merging\ndifferent views (Szeliski and Shum 1997) c⃝1997 ACM; (b) exposure bracketing: merging\ndifferent exposures; (c) morphing: blending between two photographs (Gomes, Darsa, Costa\net al. 1999) c⃝1999 Morgan Kaufmann; (d) turning a collection of photographs into a 3D\nmodel (Sinha, Steedly, Szeliski et al. 2008) c⃝2008 ACM.",
  "31": "1.1 What is computer vision?\n9\nThe great thing about these applications is that they are already familiar to most students;\nthey are, at least, technologies that students can immediately appreciate and use with their\nown personal media. Since computer vision is a challenging topic, given the wide range\nof mathematics being covered4 and the intrinsically difﬁcult nature of the problems being\nsolved, having fun and relevant problems to work on can be highly motivating and inspiring.\nThe other major reason why this book has a strong focus on applications is that they can\nbe used to formulate and constrain the potentially open-ended problems endemic in vision.\nFor example, if someone comes to me and asks for a good edge detector, my ﬁrst question is\nusually to ask why? What kind of problem are they trying to solve and why do they believe\nthat edge detection is an important component? If they are trying to locate faces, I usually\npoint out that most successful face detectors use a combination of skin color detection (Exer-\ncise 2.8) and simple blob features Section 14.1.1; they do not rely on edge detection. If they\nare trying to match door and window edges in a building for the purpose of 3D reconstruction,\nI tell them that edges are a ﬁne idea but it is better to tune the edge detector for long edges\n(see Sections 3.2.3 and 4.2) and link them together into straight lines with common vanishing\npoints before matching (see Section 4.3).\nThus, it is better to think back from the problem at hand to suitable techniques, rather\nthan to grab the ﬁrst technique that you may have heard of. This kind of working back from\nproblems to solutions is typical of an engineering approach to the study of vision and reﬂects\nmy own background in the ﬁeld. First, I come up with a detailed problem deﬁnition and\ndecide on the constraints and speciﬁcations for the problem. Then, I try to ﬁnd out which\ntechniques are known to work, implement a few of these, evaluate their performance, and\nﬁnally make a selection. In order for this process to work, it is important to have realistic test\ndata, both synthetic, which can be used to verify correctness and analyze noise sensitivity,\nand real-world data typical of the way the system will ﬁnally be used.\nHowever, this book is not just an engineering text (a source of recipes). It also takes a\nscientiﬁc approach to basic vision problems. Here, I try to come up with the best possible\nmodels of the physics of the system at hand: how the scene is created, how light interacts\nwith the scene and atmospheric effects, and how the sensors work, including sources of noise\nand uncertainty. The task is then to try to invert the acquisition process to come up with the\nbest possible description of the scene.\nThe book often uses a statistical approach to formulating and solving computer vision\nproblems. Where appropriate, probability distributions are used to model the scene and the\nnoisy image acquisition process. The association of prior distributions with unknowns is often\n3 For a fun student project on this topic, see the “PhotoBook” project at http://www.cc.gatech.edu/dvfx/videos/\ndvfx2005.html.\n4 These techniques include physics, Euclidean and projective geometry, statistics, and optimization. They make\ncomputer vision a fascinating ﬁeld to study and a great way to learn techniques widely applicable in other ﬁelds.",
  "32": "10\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncalled Bayesian modeling (Appendix B). It is possible to associate a risk or loss function with\nmis-estimating the answer (Section B.2) and to set up your inference algorithm to minimize\nthe expected risk. (Consider a robot trying to estimate the distance to an obstacle: it is\nusually safer to underestimate than to overestimate.) With statistical techniques, it often helps\nto gather lots of training data from which to learn probabilistic models. Finally, statistical\napproaches enable you to use proven inference techniques to estimate the best answer (or\ndistribution of answers) and to quantify the uncertainty in the resulting estimates.\nBecause so much of computer vision involves the solution of inverse problems or the esti-\nmation of unknown quantities, my book also has a heavy emphasis on algorithms, especially\nthose that are known to work well in practice. For many vision problems, it is all too easy to\ncome up with a mathematical description of the problem that either does not match realistic\nreal-world conditions or does not lend itself to the stable estimation of the unknowns. What\nwe need are algorithms that are both robust to noise and deviation from our models and rea-\nsonably efﬁcient in terms of run-time resources and space. In this book, I go into these issues\nin detail, using Bayesian techniques, where applicable, to ensure robustness, and efﬁcient\nsearch, minimization, and linear system solving algorithms to ensure efﬁciency. Most of the\nalgorithms described in this book are at a high level, being mostly a list of steps that have to\nbe ﬁlled in by students or by reading more detailed descriptions elsewhere. In fact, many of\nthe algorithms are sketched out in the exercises.\nNow that I’ve described the goals of this book and the frameworks that I use, I devote the\nrest of this chapter to two additional topics. Section 1.2 is a brief synopsis of the history of\ncomputer vision. It can easily be skipped by those who want to get to “the meat” of the new\nmaterial in this book and do not care as much about who invented what when.\nThe second is an overview of the book’s contents, Section 1.3, which is useful reading for\neveryone who intends to make a study of this topic (or to jump in partway, since it describes\nchapter inter-dependencies). This outline is also useful for instructors looking to structure\none or more courses around this topic, as it provides sample curricula based on the book’s\ncontents.\n1.2 A brief history\nIn this section, I provide a brief personal synopsis of the main developments in computer\nvision over the last 30 years (Figure 1.6); at least, those that I ﬁnd personally interesting\nand which appear to have stood the test of time. Readers not interested in the provenance\nof various ideas and the evolution of this ﬁeld should skip ahead to the book overview in\nSection 1.3.",
  "33": "1.2 A brief history\n11\nDigital image processing\nBlocks world, line labeling\nGeneralized cylinders\n197\nGeneralized cylinders\nPictorial structures\nStereo correspondence\nIntrinsic images\nOptical flow\nStructure from motion\n70\nImage pyramids\nScale-space processing\nShape from shading, \ntexture, and focus\nPhysically-based  modeling\n1980\nRegularization\nMarkov Random Fields\nKalman filters\n3D range data processing\nProjective invariants\nFactorization\n1\nFactorization\nPhysics-based vision\nGraph cuts\nParticle filtering\nEnergy-based segmentation\nFace recognition and detection\n1990\nFace recognition and detection\nSubspace methods\nImage-based modeling \nand rendering\nTexture synthesis and inpainting\nComputational photography\n2000\nFeature-based  recognition\nMRF inference algorithms\nCategory recognition\nLearning\nFigure 1.6\nA rough timeline of some of the most active topics of research in computer\nvision.\n1970s.\nWhen computer vision ﬁrst started out in the early 1970s, it was viewed as the\nvisual perception component of an ambitious agenda to mimic human intelligence and to\nendow robots with intelligent behavior. At the time, it was believed by some of the early\npioneers of artiﬁcial intelligence and robotics (at places such as MIT, Stanford, and CMU)\nthat solving the “visual input” problem would be an easy step along the path to solving more\ndifﬁcult problems such as higher-level reasoning and planning. According to one well-known\nstory, in 1966, Marvin Minsky at MIT asked his undergraduate student Gerald Jay Sussman\nto “spend the summer linking a camera to a computer and getting the computer to describe\nwhat it saw” (Boden 2006, p. 781).5 We now know that the problem is slightly more difﬁcult\nthan that.6\nWhat distinguished computer vision from the already existing ﬁeld of digital image pro-\ncessing (Rosenfeld and Pfaltz 1966; Rosenfeld and Kak 1976) was a desire to recover the\nthree-dimensional structure of the world from images and to use this as a stepping stone to-\nwards full scene understanding. Winston (1975) and Hanson and Riseman (1978) provide\ntwo nice collections of classic papers from this early period.\nEarly attempts at scene understanding involved extracting edges and then inferring the\n3D structure of an object or a “blocks world” from the topological structure of the 2D lines\n(Roberts 1965). Several line labeling algorithms (Figure 1.7a) were developed at that time\n(Huffman 1971; Clowes 1971; Waltz 1975; Rosenfeld, Hummel, and Zucker 1976; Kanade\n1980). Nalwa (1993) gives a nice review of this area. The topic of edge detection was also\n5 Boden (2006) cites (Crevier 1993) as the original source. The actual Vision Memo was authored by Seymour\nPapert (1966) and involved a whole cohort of students.\n6 To see how far robotic vision has come in the last four decades, have a look at the towel-folding robot at\nhttp://rll.eecs.berkeley.edu/pr/icra10/ (Maitin-Shepard, Cusumano-Towner, Lei et al. 2010).",
  "34": "12\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.7\nSome early (1970s) examples of computer vision algorithms: (a) line label-\ning (Nalwa 1993) c⃝1993 Addison-Wesley, (b) pictorial structures (Fischler and Elschlager\n1973) c⃝1973 IEEE, (c) articulated body model (Marr 1982) c⃝1982 David Marr, (d) intrin-\nsic images (Barrow and Tenenbaum 1981) c⃝1973 IEEE, (e) stereo correspondence (Marr\n1982) c⃝1982 David Marr, (f) optical ﬂow (Nagel and Enkelmann 1986) c⃝1986 IEEE.\nan active area of research; a nice survey of contemporaneous work can be found in (Davis\n1975).\nThree-dimensional modeling of non-polyhedral objects was also being studied (Baum-\ngart 1974; Baker 1977). One popular approach used generalized cylinders, i.e., solids of\nrevolution and swept closed curves (Agin and Binford 1976; Nevatia and Binford 1977), of-\nten arranged into parts relationships7 (Hinton 1977; Marr 1982) (Figure 1.7c). Fischler and\nElschlager (1973) called such elastic arrangements of parts pictorial structures (Figure 1.7b).\nThis is currently one of the favored approaches being used in object recognition (see Sec-\ntion 14.4 and Felzenszwalb and Huttenlocher 2005).\nA qualitative approach to understanding intensities and shading variations and explaining\nthem by the effects of image formation phenomena, such as surface orientation and shadows,\nwas championed by Barrow and Tenenbaum (1981) in their paper on intrinsic images (Fig-\nure 1.7d), along with the related 2 1/2 -D sketch ideas of Marr (1982). This approach is again\nseeing a bit of a revival in the work of Tappen, Freeman, and Adelson (2005).\nMore quantitative approaches to computer vision were also developed at the time, in-\ncluding the ﬁrst of many feature-based stereo correspondence algorithms (Figure 1.7e) (Dev\n7 In robotics and computer animation, these linked-part graphs are often called kinematic chains.",
  "35": "1.2 A brief history\n13\n1974; Marr and Poggio 1976; Moravec 1977; Marr and Poggio 1979; Mayhew and Frisby\n1981; Baker 1982; Barnard and Fischler 1982; Ohta and Kanade 1985; Grimson 1985; Pol-\nlard, Mayhew, and Frisby 1985; Prazdny 1985) and intensity-based optical ﬂow algorithms\n(Figure 1.7f) (Horn and Schunck 1981; Huang 1981; Lucas and Kanade 1981; Nagel 1986).\nThe early work in simultaneously recovering 3D structure and camera motion (see Chapter 7)\nalso began around this time (Ullman 1979; Longuet-Higgins 1981).\nA lot of the philosophy of how vision was believed to work at the time is summarized\nin David Marr’s (1982) book.8 In particular, Marr introduced his notion of the three levels\nof description of a (visual) information processing system. These three levels, very loosely\nparaphrased according to my own interpretation, are:\n• Computational theory: What is the goal of the computation (task) and what are the\nconstraints that are known or can be brought to bear on the problem?\n• Representations and algorithms: How are the input, output, and intermediate infor-\nmation represented and which algorithms are used to calculate the desired result?\n• Hardware implementation: How are the representations and algorithms mapped onto\nactual hardware, e.g., a biological vision system or a specialized piece of silicon? Con-\nversely, how can hardware constraints be used to guide the choice of representation\nand algorithm? With the increasing use of graphics chips (GPUs) and many-core ar-\nchitectures for computer vision (see Section C.2), this question is again becoming quite\nrelevant.\nAs I mentioned earlier in this introduction, it is my conviction that a careful analysis of the\nproblem speciﬁcation and known constraints from image formation and priors (the scientiﬁc\nand statistical approaches) must be married with efﬁcient and robust algorithms (the engineer-\ning approach) to design successful vision algorithms. Thus, it seems that Marr’s philosophy\nis as good a guide to framing and solving problems in our ﬁeld today as it was 25 years ago.\n1980s.\nIn the 1980s, a lot of attention was focused on more sophisticated mathematical\ntechniques for performing quantitative image and scene analysis.\nImage pyramids (see Section 3.5) started being widely used to perform tasks such as im-\nage blending (Figure 1.8a) and coarse-to-ﬁne correspondence search (Rosenfeld 1980; Burt\nand Adelson 1983a,b; Rosenfeld 1984; Quam 1984; Anandan 1989). Continuous versions\nof pyramids using the concept of scale-space processing were also developed (Witkin 1983;\nWitkin, Terzopoulos, and Kass 1986; Lindeberg 1990). In the late 1980s, wavelets (see Sec-\ntion 3.5.4) started displacing or augmenting regular image pyramids in some applications\n8 More recent developments in visual perception theory are covered in (Palmer 1999; Livingstone 2008).",
  "36": "14\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.8 Examples of computer vision algorithms from the 1980s: (a) pyramid blending\n(Burt and Adelson 1983b) c⃝1983 ACM, (b) shape from shading (Freeman and Adelson\n1991) c⃝1991 IEEE, (c) edge detection (Freeman and Adelson 1991) c⃝1991 IEEE, (d)\nphysically based models (Terzopoulos and Witkin 1988) c⃝1988 IEEE, (e) regularization-\nbased surface reconstruction (Terzopoulos 1988) c⃝1988 IEEE, (f) range data acquisition\nand merging (Banno, Masuda, Oishi et al. 2008) c⃝2008 Springer.\n(Adelson, Simoncelli, and Hingorani 1987; Mallat 1989; Simoncelli and Adelson 1990a,b;\nSimoncelli, Freeman, Adelson et al. 1992).\nThe use of stereo as a quantitative shape cue was extended by a wide variety of shape-\nfrom-X techniques, including shape from shading (Figure 1.8b) (see Section 12.1.1 and Horn\n1975; Pentland 1984; Blake, Zimmerman, and Knowles 1985; Horn and Brooks 1986, 1989),\nphotometric stereo (see Section 12.1.1 and Woodham 1981), shape from texture (see Sec-\ntion 12.1.2 and Witkin 1981; Pentland 1984; Malik and Rosenholtz 1997), and shape from\nfocus (see Section 12.1.3 and Nayar, Watanabe, and Noguchi 1995). Horn (1986) has a nice\ndiscussion of most of these techniques.\nResearch into better edge and contour detection (Figure 1.8c) (see Section 4.2) was also\nactive during this period (Canny 1986; Nalwa and Binford 1986), including the introduc-\ntion of dynamically evolving contour trackers (Section 5.1.1) such as snakes (Kass, Witkin,\nand Terzopoulos 1988), as well as three-dimensional physically based models (Figure 1.8d)\n(Terzopoulos, Witkin, and Kass 1987; Kass, Witkin, and Terzopoulos 1988; Terzopoulos and\nFleischer 1988; Terzopoulos, Witkin, and Kass 1988).\nResearchers noticed that a lot of the stereo, ﬂow, shape-from-X, and edge detection al-",
  "37": "1.2 A brief history\n15\ngorithms could be uniﬁed, or at least described, using the same mathematical framework if\nthey were posed as variational optimization problems (see Section 3.7) and made more ro-\nbust (well-posed) using regularization (Figure 1.8e) (see Section 3.7.1 and Terzopoulos 1983;\nPoggio, Torre, and Koch 1985; Terzopoulos 1986b; Blake and Zisserman 1987; Bertero, Pog-\ngio, and Torre 1988; Terzopoulos 1988). Around the same time, Geman and Geman (1984)\npointed out that such problems could equally well be formulated using discrete Markov Ran-\ndom Field (MRF) models (see Section 3.7.2), which enabled the use of better (global) search\nand optimization algorithms, such as simulated annealing.\nOnline variants of MRF algorithms that modeled and updated uncertainties using the\nKalman ﬁlter were introduced a little later (Dickmanns and Graefe 1988; Matthies, Kanade,\nand Szeliski 1989; Szeliski 1989). Attempts were also made to map both regularized and\nMRF algorithms onto parallel hardware (Poggio and Koch 1985; Poggio, Little, Gamble\net al. 1988; Fischler, Firschein, Barnard et al. 1989). The book by Fischler and Firschein\n(1987) contains a nice collection of articles focusing on all of these topics (stereo, ﬂow,\nregularization, MRFs, and even higher-level vision).\nThree-dimensional range data processing (acquisition, merging, modeling, and recogni-\ntion; see Figure 1.8f) continued being actively explored during this decade (Agin and Binford\n1976; Besl and Jain 1985; Faugeras and Hebert 1987; Curless and Levoy 1996). The compi-\nlation by Kanade (1987) contains a lot of the interesting papers in this area.\n1990s.\nWhile a lot of the previously mentioned topics continued to be explored, a few of\nthem became signiﬁcantly more active.\nA burst of activity in using projective invariants for recognition (Mundy and Zisserman\n1992) evolved into a concerted effort to solve the structure from motion problem (see Chap-\nter 7). A lot of the initial activity was directed at projective reconstructions, which did not\nrequire knowledge of camera calibration (Faugeras 1992; Hartley, Gupta, and Chang 1992;\nHartley 1994a; Faugeras and Luong 2001; Hartley and Zisserman 2004). Simultaneously, fac-\ntorization techniques (Section 7.3) were developed to solve efﬁciently problems for which or-\nthographic camera approximations were applicable (Figure 1.9a) (Tomasi and Kanade 1992;\nPoelman and Kanade 1997; Anandan and Irani 2002) and then later extended to the perspec-\ntive case (Christy and Horaud 1996; Triggs 1996). Eventually, the ﬁeld started using full\nglobal optimization (see Section 7.4 and Taylor, Kriegman, and Anandan 1991; Szeliski and\nKang 1994; Azarbayejani and Pentland 1995), which was later recognized as being the same\nas the bundle adjustment techniques traditionally used in photogrammetry (Triggs, McLauch-\nlan, Hartley et al. 1999). Fully automated (sparse) 3D modeling systems were built using such\ntechniques (Beardsley, Torr, and Zisserman 1996; Schaffalitzky and Zisserman 2002; Brown\nand Lowe 2003; Snavely, Seitz, and Szeliski 2006).\nWork begun in the 1980s on using detailed measurements of color and intensity combined",
  "38": "16\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.9 Examples of computer vision algorithms from the 1990s: (a) factorization-based\nstructure from motion (Tomasi and Kanade 1992) c⃝1992 Springer, (b) dense stereo match-\ning (Boykov, Veksler, and Zabih 2001), (c) multi-view reconstruction (Seitz and Dyer 1999)\nc⃝1999 Springer, (d) face tracking (Matthews, Xiao, and Baker 2007), (e) image segmenta-\ntion (Belongie, Fowlkes, Chung et al. 2002) c⃝2002 Springer, (f) face recognition (Turk and\nPentland 1991a).\nwith accurate physical models of radiance transport and color image formation created its own\nsubﬁeld known as physics-based vision. A good survey of the ﬁeld can be found in the three-\nvolume collection on this topic (Wolff, Shafer, and Healey 1992a; Healey and Shafer 1992;\nShafer, Healey, and Wolff 1992).\nOptical ﬂow methods (see Chapter 8) continued to be improved (Nagel and Enkelmann\n1986; Bolles, Baker, and Marimont 1987; Horn and Weldon Jr. 1988; Anandan 1989; Bergen,\nAnandan, Hanna et al. 1992; Black and Anandan 1996; Bruhn, Weickert, and Schn¨orr 2005;\nPapenberg, Bruhn, Brox et al. 2006), with (Nagel 1986; Barron, Fleet, and Beauchemin 1994;\nBaker, Black, Lewis et al. 2007) being good surveys. Similarly, a lot of progress was made\non dense stereo correspondence algorithms (see Chapter 11, Okutomi and Kanade (1993,\n1994); Boykov, Veksler, and Zabih (1998); Birchﬁeld and Tomasi (1999); Boykov, Veksler,\nand Zabih (2001), and the survey and comparison in Scharstein and Szeliski (2002)), with\nthe biggest breakthrough being perhaps global optimization using graph cut techniques (Fig-\nure 1.9b) (Boykov, Veksler, and Zabih 2001).",
  "39": "1.2 A brief history\n17\nMulti-view stereo algorithms (Figure 1.9c) that produce complete 3D surfaces (see Sec-\ntion 11.6) were also an active topic of research (Seitz and Dyer 1999; Kutulakos and Seitz\n2000) that continues to be active today (Seitz, Curless, Diebel et al. 2006). Techniques for\nproducing 3D volumetric descriptions from binary silhouettes (see Section 11.6.2) continued\nto be developed (Potmesil 1987; Srivasan, Liang, and Hackwood 1990; Szeliski 1993; Lau-\nrentini 1994), along with techniques based on tracking and reconstructing smooth occluding\ncontours (see Section 11.2.1 and Cipolla and Blake 1992; Vaillant and Faugeras 1992; Zheng\n1994; Boyer and Berger 1997; Szeliski and Weiss 1998; Cipolla and Giblin 2000).\nTracking algorithms also improved a lot, including contour tracking using active contours\n(see Section 5.1), such as snakes (Kass, Witkin, and Terzopoulos 1988), particle ﬁlters (Blake\nand Isard 1998), and level sets (Malladi, Sethian, and Vemuri 1995), as well as intensity-based\n(direct) techniques (Lucas and Kanade 1981; Shi and Tomasi 1994; Rehg and Kanade 1994),\noften applied to tracking faces (Figure 1.9d) (Lanitis, Taylor, and Cootes 1997; Matthews and\nBaker 2004; Matthews, Xiao, and Baker 2007) and whole bodies (Sidenbladh, Black, and\nFleet 2000; Hilton, Fua, and Ronfard 2006; Moeslund, Hilton, and Kr¨uger 2006).\nImage segmentation (see Chapter 5) (Figure 1.9e), a topic which has been active since\nthe earliest days of computer vision (Brice and Fennema 1970; Horowitz and Pavlidis 1976;\nRiseman and Arbib 1977; Rosenfeld and Davis 1979; Haralick and Shapiro 1985; Pavlidis\nand Liow 1990), was also an active topic of research, producing techniques based on min-\nimum energy (Mumford and Shah 1989) and minimum description length (Leclerc 1989),\nnormalized cuts (Shi and Malik 2000), and mean shift (Comaniciu and Meer 2002).\nStatistical learning techniques started appearing, ﬁrst in the application of principal com-\nponent eigenface analysis to face recognition (Figure 1.9f) (see Section 14.2.1 and Turk and\nPentland 1991a) and linear dynamical systems for curve tracking (see Section 5.1.1 and Blake\nand Isard 1998).\nPerhaps the most notable development in computer vision during this decade was the\nincreased interaction with computer graphics (Seitz and Szeliski 1999), especially in the\ncross-disciplinary area of image-based modeling and rendering (see Chapter 13). The idea of\nmanipulating real-world imagery directly to create new animations ﬁrst came to prominence\nwith image morphing techniques (Figure1.5c) (see Section 3.6.3 and Beier and Neely 1992)\nand was later applied to view interpolation (Chen and Williams 1993; Seitz and Dyer 1996),\npanoramic image stitching (Figure1.5a) (see Chapter 9 and Mann and Picard 1994; Chen\n1995; Szeliski 1996; Szeliski and Shum 1997; Szeliski 2006a), and full light-ﬁeld rendering\n(Figure 1.10a) (see Section 13.3 and Gortler, Grzeszczuk, Szeliski et al. 1996; Levoy and\nHanrahan 1996; Shade, Gortler, He et al. 1998). At the same time, image-based modeling\ntechniques (Figure 1.10b) for automatically creating realistic 3D models from collections of\nimages were also being introduced (Beardsley, Torr, and Zisserman 1996; Debevec, Taylor,\nand Malik 1996; Taylor, Debevec, and Malik 1996).",
  "40": "18\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1.10\nRecent examples of computer vision algorithms: (a) image-based rendering\n(Gortler, Grzeszczuk, Szeliski et al. 1996), (b) image-based modeling (Debevec, Taylor, and\nMalik 1996) c⃝1996 ACM, (c) interactive tone mapping (Lischinski, Farbman, Uyttendaele\net al. 2006a) (d) texture synthesis (Efros and Freeman 2001), (e) feature-based recognition\n(Fergus, Perona, and Zisserman 2007), (f) region-based recognition (Mori, Ren, Efros et al.\n2004) c⃝2004 IEEE.\n2000s.\nThis past decade has continued to see a deepening interplay between the vision and\ngraphics ﬁelds. In particular, many of the topics introduced under the rubric of image-based\nrendering, such as image stitching (see Chapter 9), light-ﬁeld capture and rendering (see\nSection 13.3), and high dynamic range (HDR) image capture through exposure bracketing\n(Figure1.5b) (see Section 10.2 and Mann and Picard 1995; Debevec and Malik 1997), were\nre-christened as computational photography (see Chapter 10) to acknowledge the increased\nuse of such techniques in everyday digital photography. For example, the rapid adoption of\nexposure bracketing to create high dynamic range images necessitated the development of\ntone mapping algorithms (Figure 1.10c) (see Section 10.2.1) to convert such images back\nto displayable results (Fattal, Lischinski, and Werman 2002; Durand and Dorsey 2002; Rein-\nhard, Stark, Shirley et al. 2002; Lischinski, Farbman, Uyttendaele et al. 2006a). In addition to\nmerging multiple exposures, techniques were developed to merge ﬂash images with non-ﬂash\ncounterparts (Eisemann and Durand 2004; Petschnigg, Agrawala, Hoppe et al. 2004) and to\ninteractively or automatically select different regions from overlapping images (Agarwala,",
  "41": "1.3 Book overview\n19\nDontcheva, Agrawala et al. 2004).\nTexture synthesis (Figure 1.10d) (see Section 10.5), quilting (Efros and Leung 1999; Efros\nand Freeman 2001; Kwatra, Sch¨odl, Essa et al. 2003) and inpainting (Bertalmio, Sapiro,\nCaselles et al. 2000; Bertalmio, Vese, Sapiro et al. 2003; Criminisi, P´erez, and Toyama 2004)\nare additional topics that can be classiﬁed as computational photography techniques, since\nthey re-combine input image samples to produce new photographs.\nA second notable trend during this past decade has been the emergence of feature-based\ntechniques (combined with learning) for object recognition (see Section 14.3 and Ponce,\nHebert, Schmid et al. 2006). Some of the notable papers in this area include the constellation\nmodel of Fergus, Perona, and Zisserman (2007) (Figure 1.10e) and the pictorial structures\nof Felzenszwalb and Huttenlocher (2005). Feature-based techniques also dominate other\nrecognition tasks, such as scene recognition (Zhang, Marszalek, Lazebnik et al. 2007) and\npanorama and location recognition (Brown and Lowe 2007; Schindler, Brown, and Szeliski\n2007). And while interest point (patch-based) features tend to dominate current research,\nsome groups are pursuing recognition based on contours (Belongie, Malik, and Puzicha 2002)\nand region segmentation (Figure 1.10f) (Mori, Ren, Efros et al. 2004).\nAnother signiﬁcant trend from this past decade has been the development of more efﬁcient\nalgorithms for complex global optimization problems (see Sections 3.7 and B.5 and Szeliski,\nZabih, Scharstein et al. 2008; Blake, Kohli, and Rother 2010). While this trend began with\nwork on graph cuts (Boykov, Veksler, and Zabih 2001; Kohli and Torr 2007), a lot of progress\nhas also been made in message passing algorithms, such as loopy belief propagation (LBP)\n(Yedidia, Freeman, and Weiss 2001; Kumar and Torr 2006).\nThe ﬁnal trend, which now dominates a lot of the visual recognition research in our com-\nmunity, is the application of sophisticated machine learning techniques to computer vision\nproblems (see Section 14.5.1 and Freeman, Perona, and Sch¨olkopf 2008). This trend coin-\ncides with the increased availability of immense quantities of partially labelled data on the\nInternet, which makes it more feasible to learn object categories without the use of careful\nhuman supervision.\n1.3 Book overview\nIn the ﬁnal part of this introduction, I give a brief tour of the material in this book, as well\nas a few notes on notation and some additional general references. Since computer vision is\nsuch a broad ﬁeld, it is possible to study certain aspects of it, e.g., geometric image formation\nand 3D structure recovery, without engaging other parts, e.g., the modeling of reﬂectance and\nshading. Some of the chapters in this book are only loosely coupled with others, and it is not\nstrictly necessary to read all of the material in sequence.",
  "42": "20\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nImages (2D)\nGeometry (3D) \nshape\nPhotometry \nappearance\n+\nvision\ngraphics\nimage processing\n2.1 Geometric \nimage formation\n2.2 Photometric \nimage formation\n2.3 Sampling\nand aliasing\n3 Image \nprocessing\n4 Feature \ndetection\n6 Feature-based \nalignment\n7 Structure \nfrom motion\n8 Motion\nestimation\n10 Computational \nphotography\n11 Stereo \ncorrespondence\n12 3D shape \nrecovery\n12 Texture \nrecovery\n13 Image-based \nrendering\n14 Recognition\n5 Segmentation\n9 Stitching\nFigure 1.11 Relationship between images, geometry, and photometry, as well as a taxonomy\nof the topics covered in this book. Topics are roughly positioned along the left–right axis\ndepending on whether they are more closely related to image-based (left), geometry-based\n(middle) or appearance-based (right) representations, and on the vertical axis by increasing\nlevel of abstraction. The whole ﬁgure should be taken with a large grain of salt, as there are\nmany additional subtle connections between topics not illustrated here.",
  "43": "1.3 Book overview\n21\nFigure 1.11 shows a rough layout of the contents of this book. Since computer vision\ninvolves going from images to a structural description of the scene (and computer graphics\nthe converse), I have positioned the chapters horizontally in terms of which major component\nthey address, in addition to vertically according to their dependence.\nGoing from left to right, we see the major column headings as Images (which are 2D\nin nature), Geometry (which encompasses 3D descriptions), and Photometry (which encom-\npasses object appearance). (An alternative labeling for these latter two could also be shape\nand appearance—see, e.g., Chapter 13 and Kang, Szeliski, and Anandan (2000).) Going\nfrom top to bottom, we see increasing levels of modeling and abstraction, as well as tech-\nniques that build on previously developed algorithms. Of course, this taxonomy should be\ntaken with a large grain of salt, as the processing and dependencies in this diagram are not\nstrictly sequential and subtle additional dependencies and relationships also exist (e.g., some\nrecognition techniques make use of 3D information). The placement of topics along the hor-\nizontal axis should also be taken lightly, as most vision algorithms involve mapping between\nat least two different representations.9\nInterspersed throughout the book are sample applications, which relate the algorithms\nand mathematical material being presented in various chapters to useful, real-world applica-\ntions. Many of these applications are also presented in the exercises sections, so that students\ncan write their own.\nAt the end of each section, I provide a set of exercises that the students can use to imple-\nment, test, and reﬁne the algorithms and techniques presented in each section. Some of the\nexercises are suitable as written homework assignments, others as shorter one-week projects,\nand still others as open-ended research problems that make for challenging ﬁnal projects.\nMotivated students who implement a reasonable subset of these exercises will, by the end of\nthe book, have a computer vision software library that can be used for a variety of interesting\ntasks and projects.\nAs a reference book, I try wherever possible to discuss which techniques and algorithms\nwork well in practice, as well as providing up-to-date pointers to the latest research results in\nthe areas that I cover. The exercises can be used to build up your own personal library of self-\ntested and validated vision algorithms, which is more worthwhile in the long term (assuming\nyou have the time) than simply pulling algorithms out of a library whose performance you do\nnot really understand.\nThe book begins in Chapter 2 with a review of the image formation processes that create\nthe images that we see and capture. Understanding this process is fundamental if you want\nto take a scientiﬁc (model-based) approach to computer vision. Students who are eager to\njust start implementing algorithms (or courses that have limited time) can skip ahead to the\n9 For an interesting comparison with what is known about the human visual system, e.g., the largely parallel what\nand where pathways, see some textbooks on human perception (Palmer 1999; Livingstone 2008).",
  "44": "22\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nn^\n2. Image Formation\n3. Image Processing\n4. Features\n5. Segmentation\n6-7. Structure from Motion\n8. Motion\n9. Stitching\n10. Computational Photography\n11. Stereo\n12. 3D Shape\n13. Image-based Rendering\n14. Recognition\nFigure 1.12\nA pictorial summary of the chapter contents. Sources: Brown, Szeliski, and\nWinder (2005); Comaniciu and Meer (2002); Snavely, Seitz, and Szeliski (2006); Nagel\nand Enkelmann (1986); Szeliski and Shum (1997); Debevec and Malik (1997); Gortler,\nGrzeszczuk, Szeliski et al. (1996); Viola and Jones (2004)—see the ﬁgures in the respec-\ntive chapters for copyright information.",
  "45": "1.3 Book overview\n23\nnext chapter and dip into this material later. In Chapter 2, we break down image formation\ninto three major components. Geometric image formation (Section 2.1) deals with points,\nlines, and planes, and how these are mapped onto images using projective geometry and other\nmodels (including radial lens distortion). Photometric image formation (Section 2.2) covers\nradiometry, which describes how light interacts with surfaces in the world, and optics, which\nprojects light onto the sensor plane. Finally, Section 2.3 covers how sensors work, including\ntopics such as sampling and aliasing, color sensing, and in-camera compression.\nChapter 3 covers image processing, which is needed in almost all computer vision appli-\ncations. This includes topics such as linear and non-linear ﬁltering (Section 3.3), the Fourier\ntransform (Section 3.4), image pyramids and wavelets (Section 3.5), geometric transforma-\ntions such as image warping (Section 3.6), and global optimization techniques such as regu-\nlarization and Markov Random Fields (MRFs) (Section 3.7). While most of this material is\ncovered in courses and textbooks on image processing, the use of optimization techniques is\nmore typically associated with computer vision (although MRFs are now being widely used\nin image processing as well). The section on MRFs is also the ﬁrst introduction to the use\nof Bayesian inference techniques, which are covered at a more abstract level in Appendix B.\nChapter 3 also presents applications such as seamless image blending and image restoration.\nIn Chapter 4, we cover feature detection and matching. A lot of current 3D reconstruction\nand recognition techniques are built on extracting and matching feature points (Section 4.1),\nso this is a fundamental technique required by many subsequent chapters (Chapters 6, 7, 9\nand 14). We also cover edge and straight line detection in Sections 4.2 and 4.3.\nChapter 5 covers region segmentation techniques, including active contour detection and\ntracking (Section 5.1). Segmentation techniques include top-down (split) and bottom-up\n(merge) techniques, mean shift techniques that ﬁnd modes of clusters, and various graph-\nbased segmentation approaches. All of these techniques are essential building blocks that are\nwidely used in a variety of applications, including performance-driven animation, interactive\nimage editing, and recognition.\nIn Chapter 6, we cover geometric alignment and camera calibration. We introduce the\nbasic techniques of feature-based alignment in Section 6.1 and show how this problem can\nbe solved using either linear or non-linear least squares, depending on the motion involved.\nWe also introduce additional concepts, such as uncertainty weighting and robust regression,\nwhich are essential to making real-world systems work. Feature-based alignment is then used\nas a building block for 3D pose estimation (extrinsic calibration) in Section 6.2 and camera\n(intrinsic) calibration in Section 6.3. Chapter 6 also describes applications of these techniques\nto photo alignment for ﬂip-book animations, 3D pose estimation from a hand-held camera,\nand single-view reconstruction of building models.\nChapter 7 covers the topic of structure from motion, which involves the simultaneous\nrecovery of 3D camera motion and 3D scene structure from a collection of tracked 2D fea-",
  "46": "24\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntures. This chapter begins with the easier problem of 3D point triangulation (Section 7.1),\nwhich is the 3D reconstruction of points from matched features when the camera positions\nare known. It then describes two-frame structure from motion (Section 7.2), for which al-\ngebraic techniques exist, as well as robust sampling techniques such as RANSAC that can\ndiscount erroneous feature matches. The second half of Chapter 7 describes techniques for\nmulti-frame structure from motion, including factorization (Section 7.3), bundle adjustment\n(Section 7.4), and constrained motion and structure models (Section 7.5). It also presents\napplications in view morphing, sparse 3D model construction, and match move.\nIn Chapter 8, we go back to a topic that deals directly with image intensities (as op-\nposed to feature tracks), namely dense intensity-based motion estimation (optical ﬂow). We\nstart with the simplest possible motion models, translational motion (Section 8.1), and cover\ntopics such as hierarchical (coarse-to-ﬁne) motion estimation, Fourier-based techniques, and\niterative reﬁnement. We then present parametric motion models, which can be used to com-\npensate for camera rotation and zooming, as well as afﬁne or planar perspective motion (Sec-\ntion 8.2). This is then generalized to spline-based motion models (Section 8.3) and ﬁnally\nto general per-pixel optical ﬂow (Section 8.4), including layered and learned motion models\n(Section 8.5). Applications of these techniques include automated morphing, frame interpo-\nlation (slow motion), and motion-based user interfaces.\nChapter 9 is devoted to image stitching, i.e., the construction of large panoramas and com-\nposites. While stitching is just one example of computation photography (see Chapter 10),\nthere is enough depth here to warrant a separate chapter. We start by discussing various pos-\nsible motion models (Section 9.1), including planar motion and pure camera rotation. We\nthen discuss global alignment (Section 9.2), which is a special (simpliﬁed) case of general\nbundle adjustment, and then present panorama recognition, i.e., techniques for automatically\ndiscovering which images actually form overlapping panoramas. Finally, we cover the topics\nof image compositing and blending (Section 9.3), which involve both selecting which pixels\nfrom which images to use and blending them together so as to disguise exposure differences.\nImage stitching is a wonderful application that ties together most of the material covered\nin earlier parts of this book. It also makes for a good mid-term course project that can build\non previously developed techniques such as image warping and feature detection and match-\ning. Chapter 9 also presents more specialized variants of stitching such as whiteboard and\ndocument scanning, video summarization, panography, full 360◦spherical panoramas, and\ninteractive photomontage for blending repeated action shots together.\nChapter 10 presents additional examples of computational photography, which is the pro-\ncess of creating new images from one or more input photographs, often based on the careful\nmodeling and calibration of the image formation process (Section 10.1). Computational pho-\ntography techniques include merging multiple exposures to create high dynamic range images\n(Section 10.2), increasing image resolution through blur removal and super-resolution (Sec-",
  "47": "1.3 Book overview\n25\ntion 10.3), and image editing and compositing operations (Section 10.4). We also cover the\ntopics of texture analysis, synthesis and inpainting (hole ﬁlling) in Section 10.5, as well as\nnon-photorealistic rendering (Section 10.5.2).\nIn Chapter 11, we turn to the issue of stereo correspondence, which can be thought of\nas a special case of motion estimation where the camera positions are already known (Sec-\ntion 11.1). This additional knowledge enables stereo algorithms to search over a much smaller\nspace of correspondences and, in many cases, to produce dense depth estimates that can\nbe converted into visible surface models (Section 11.3). We also cover multi-view stereo\nalgorithms that build a true 3D surface representation instead of just a single depth map\n(Section 11.6). Applications of stereo matching include head and gaze tracking, as well as\ndepth-based background replacement (Z-keying).\nChapter 12 covers additional 3D shape and appearance modeling techniques. These in-\nclude classic shape-from-X techniques such as shape from shading, shape from texture, and\nshape from focus (Section 12.1), as well as shape from smooth occluding contours (Sec-\ntion 11.2.1) and silhouettes (Section 12.5). An alternative to all of these passive computer\nvision techniques is to use active rangeﬁnding (Section 12.2), i.e., to project patterned light\nonto scenes and recover the 3D geometry through triangulation. Processing all of these 3D\nrepresentations often involves interpolating or simplifying the geometry (Section 12.3), or\nusing alternative representations such as surface point sets (Section 12.4).\nThe collection of techniques for going from one or more images to partial or full 3D\nmodels is often called image-based modeling or 3D photography. Section 12.6 examines\nthree more specialized application areas (architecture, faces, and human bodies), which can\nuse model-based reconstruction to ﬁt parameterized models to the sensed data. Section 12.7\nexamines the topic of appearance modeling, i.e., techniques for estimating the texture maps,\nalbedos, or even sometimes complete bi-directional reﬂectance distribution functions (BRDFs)\nthat describe the appearance of 3D surfaces.\nIn Chapter 13, we discuss the large number of image-based rendering techniques that\nhave been developed in the last two decades, including simpler techniques such as view in-\nterpolation (Section 13.1), layered depth images (Section 13.2), and sprites and layers (Sec-\ntion 13.2.1), as well as the more general framework of light ﬁelds and Lumigraphs (Sec-\ntion 13.3) and higher-order ﬁelds such as environment mattes (Section 13.4). Applications of\nthese techniques include navigating 3D collections of photographs using photo tourism and\nviewing 3D models as object movies.\nIn Chapter 13, we also discuss video-based rendering, which is the temporal extension of\nimage-based rendering. The topics we cover include video-based animation (Section 13.5.1),\nperiodic video turned into video textures (Section 13.5.2), and 3D video constructed from\nmultiple video streams (Section 13.5.4). Applications of these techniques include video de-\nnoising, morphing, and tours based on 360◦video.",
  "48": "26\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWeek\nMaterial\nProject\n(1.)\nChapter 2 Image formation\n2.\nChapter 3 Image processing\n3.\nChapter 4 Feature detection and matching\nP1\n4.\nChapter 6 Feature-based alignment\n5.\nChapter 9 Image stitching\nP2\n6.\nChapter 8 Dense motion estimation\n7.\nChapter 7 Structure from motion\nPP\n8.\nChapter 14 Recognition\n(9.)\nChapter 10 Computational photography\n10.\nChapter 11 Stereo correspondence\n(11.)\nChapter 12 3D reconstruction\n12.\nChapter 13 Image-based rendering\n13.\nFinal project presentations\nFP\nTable 1.1 Sample syllabi for 10-week and 13-week courses. The weeks in parentheses are\nnot used in the shorter version. P1 and P2 are two early-term mini-projects, PP is when the\n(student-selected) ﬁnal project proposals are due, and FP is the ﬁnal project presentations.\nChapter 14 describes different approaches to recognition. It begins with techniques for\ndetecting and recognizing faces (Sections 14.1 and 14.2), then looks at techniques for ﬁnding\nand recognizing particular objects (instance recognition) in Section 14.3. Next, we cover the\nmost difﬁcult variant of recognition, namely the recognition of broad categories, such as cars,\nmotorcycles, horses and other animals (Section 14.4), and the role that scene context plays in\nrecognition (Section 14.5).\nTo support the book’s use as a textbook, the appendices and associated Web site contain\nmore detailed mathematical topics and additional material. Appendix A covers linear algebra\nand numerical techniques, including matrix algebra, least squares, and iterative techniques.\nAppendix B covers Bayesian estimation theory, including maximum likelihood estimation,\nrobust statistics, Markov random ﬁelds, and uncertainty modeling. Appendix C describes the\nsupplementary material available to complement this book, including images and data sets,\npointers to software, course slides, and an on-line bibliography.\n1.4 Sample syllabus\nTeaching all of the material covered in this book in a single quarter or semester course is a\nHerculean task and likely one not worth attempting. It is better to simply pick and choose",
  "49": "1.5 A note on notation\n27\ntopics related to the lecturer’s preferred emphasis and tailored to the set of mini-projects\nenvisioned for the students.\nSteve Seitz and I have successfully used a 10-week syllabus similar to the one shown in\nTable 1.1 (omitting the parenthesized weeks) as both an undergraduate and a graduate-level\ncourse in computer vision. The undergraduate course10 tends to go lighter on the mathematics\nand takes more time reviewing basics, while the graduate-level course11 dives more deeply\ninto techniques and assumes the students already have a decent grounding in either vision\nor related mathematical techniques. (See also the Introduction to Computer Vision course at\nStanford,12 which uses a similar curriculum.) Related courses have also been taught on the\ntopics of 3D photography13 and computational photography.14\nWhen Steve and I teach the course, we prefer to give the students several small program-\nming projects early in the course rather than focusing on written homework or quizzes. With\na suitable choice of topics, it is possible for these projects to build on each other. For exam-\nple, introducing feature matching early on can be used in a second assignment to do image\nalignment and stitching. Alternatively, direct (optical ﬂow) techniques can be used to do the\nalignment and more focus can be put on either graph cut seam selection or multi-resolution\nblending techniques.\nWe also ask the students to propose a ﬁnal project (we provide a set of suggested topics\nfor those who need ideas) by the middle of the course and reserve the last week of the class\nfor student presentations. With any luck, some of these ﬁnal projects can actually turn into\nconference submissions!\nNo matter how you decide to structure the course or how you choose to use this book, I\nencourage you to try at least a few small programming tasks to get a good feel for how vision\ntechniques work, and when they do not. Better yet, pick topics that are fun and can be used on\nyour own photographs, and try to push your creative boundaries to come up with surprising\nresults.\n1.5 A note on notation\nFor better or worse, the notation found in computer vision and multi-view geometry textbooks\ntends to vary all over the map (Faugeras 1993; Hartley and Zisserman 2004; Girod, Greiner,\nand Niemann 2000; Faugeras and Luong 2001; Forsyth and Ponce 2003). In this book, I\nuse the convention I ﬁrst learned in my high school physics class (and later multi-variate\n10 http://www.cs.washington.edu/education/courses/455/\n11 http://www.cs.washington.edu/education/courses/576/\n12http://vision.stanford.edu/teaching/cs223b/\n13 http://www.cs.washington.edu/education/courses/558/06sp/\n14 http://graphics.cs.cmu.edu/courses/15-463/",
  "50": "28\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncalculus and computer graphics courses), which is that vectors v are lower case bold, matrices\nM are upper case bold, and scalars (T, s) are mixed case italic. Unless otherwise noted,\nvectors operate as column vectors, i.e., they post-multiply matrices, Mv, although they are\nsometimes written as comma-separated parenthesized lists x = (x, y) instead of bracketed\ncolumn vectors x = [x y]T . Some commonly used matrices are R for rotations, K for\ncalibration matrices, and I for the identity matrix. Homogeneous coordinates (Section 2.1)\nare denoted with a tilde over the vector, e.g., ˜x = (˜x, ˜y, ˜w) = ˜w(x, y, 1) = ˜w¯x in P2. The\ncross product operator in matrix form is denoted by [ ]×.\n1.6 Additional reading\nThis book attempts to be self-contained, so that students can implement the basic assignments\nand algorithms described here without the need for outside references. However, it does pre-\nsuppose a general familiarity with basic concepts in linear algebra and numerical techniques,\nwhich are reviewed in Appendix A, and image processing, which is reviewed in Chapter 3.\nStudents who want to delve more deeply into these topics can look in (Golub and Van\nLoan 1996) for matrix algebra and (Strang 1988) for linear algebra. In image processing,\nthere are a number of popular textbooks, including (Crane 1997; Gomes and Velho 1997;\nJ¨ahne 1997; Pratt 2007; Russ 2007; Burger and Burge 2008; Gonzales and Woods 2008). For\ncomputer graphics, popular texts include (Foley, van Dam, Feiner et al. 1995; Watt 1995),\nwith (Glassner 1995) providing a more in-depth look at image formation and rendering. For\nstatistics and machine learning, Chris Bishop’s (2006) book is a wonderful and comprehen-\nsive introduction with a wealth of exercises. Students may also want to look in other textbooks\non computer vision for material that we do not cover here, as well as for additional project\nideas (Ballard and Brown 1982; Faugeras 1993; Nalwa 1993; Trucco and Verri 1998; Forsyth\nand Ponce 2003).\nThere is, however, no substitute for reading the latest research literature, both for the lat-\nest ideas and techniques and for the most up-to-date references to related literature.15 In this\nbook, I have attempted to cite the most recent work in each ﬁeld so that students can read them\ndirectly and use them as inspiration for their own work. Browsing the last few years’ con-\nference proceedings from the major vision and graphics conferences, such as CVPR, ECCV,\nICCV, and SIGGRAPH, will provide a wealth of new ideas. The tutorials offered at these\nconferences, for which slides or notes are often available on-line, are also an invaluable re-\nsource.\n15 For a comprehensive bibliography and taxonomy of computer vision research, Keith Price’s Annotated Com-\nputer Vision Bibliography http://www.visionbib.com/bibliography/contents.html is an invaluable resource.",
  "51": "Chapter 2\nImage formation\n2.1\nGeometric primitives and transformations . . . . . . . . . . . . . . . . . . .\n31\n2.1.1\nGeometric primitives . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n2.1.2\n2D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n2.1.3\n3D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n2.1.4\n3D rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n2.1.5\n3D to 2D projections . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n2.1.6\nLens distortions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n2.2\nPhotometric image formation . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n2.2.1\nLighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n2.2.2\nReﬂectance and shading . . . . . . . . . . . . . . . . . . . . . . . .\n62\n2.2.3\nOptics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n2.3\nThe digital camera\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n2.3.1\nSampling and aliasing\n. . . . . . . . . . . . . . . . . . . . . . . . .\n77\n2.3.2\nColor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n2.3.3\nCompression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n2.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n2.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93",
  "52": "30\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nn^\n(a)\n(b)\nzi=102mm\nf = 100mm\nzo=5m\nd\nG\nR\nG\nR\nB\nG\nB\nG\nG\nR\nG\nR\nB\nG\nB\nG\n(c)\n(d)\nFigure 2.1\nA few components of the image formation process: (a) perspective projection;\n(b) light scattering when hitting a surface; (c) lens optics; (d) Bayer color ﬁlter array.",
  "53": "2.1 Geometric primitives and transformations\n31\nBefore we can intelligently analyze and manipulate images, we need to establish a vocabulary\nfor describing the geometry of a scene. We also need to understand the image formation\nprocess that produced a particular image given a set of lighting conditions, scene geometry,\nsurface properties, and camera optics. In this chapter, we present a simpliﬁed model of such\nan image formation process.\nSection 2.1 introduces the basic geometric primitives used throughout the book (points,\nlines, and planes) and the geometric transformations that project these 3D quantities into 2D\nimage features (Figure 2.1a). Section 2.2 describes how lighting, surface properties (Fig-\nure 2.1b), and camera optics (Figure 2.1c) interact in order to produce the color values that\nfall onto the image sensor. Section 2.3 describes how continuous color images are turned into\ndiscrete digital samples inside the image sensor (Figure 2.1d) and how to avoid (or at least\ncharacterize) sampling deﬁciencies, such as aliasing.\nThe material covered in this chapter is but a brief summary of a very rich and deep set of\ntopics, traditionally covered in a number of separate ﬁelds. A more thorough introduction to\nthe geometry of points, lines, planes, and projections can be found in textbooks on multi-view\ngeometry (Hartley and Zisserman 2004; Faugeras and Luong 2001) and computer graphics\n(Foley, van Dam, Feiner et al. 1995). The image formation (synthesis) process is traditionally\ntaught as part of a computer graphics curriculum (Foley, van Dam, Feiner et al. 1995; Glass-\nner 1995; Watt 1995; Shirley 2005) but it is also studied in physics-based computer vision\n(Wolff, Shafer, and Healey 1992a). The behavior of camera lens systems is studied in optics\n(M¨oller 1988; Hecht 2001; Ray 2002). Two good books on color theory are (Wyszecki and\nStiles 2000; Healey and Shafer 1992), with (Livingstone 2008) providing a more fun and in-\nformal introduction to the topic of color perception. Topics relating to sampling and aliasing\nare covered in textbooks on signal and image processing (Crane 1997; J¨ahne 1997; Oppen-\nheim and Schafer 1996; Oppenheim, Schafer, and Buck 1999; Pratt 2007; Russ 2007; Burger\nand Burge 2008; Gonzales and Woods 2008).\nA note to students: If you have already studied computer graphics, you may want to\nskim the material in Section 2.1, although the sections on projective depth and object-centered\nprojection near the end of Section 2.1.5 may be new to you. Similarly, physics students (as\nwell as computer graphics students) will mostly be familiar with Section 2.2. Finally, students\nwith a good background in image processing will already be familiar with sampling issues\n(Section 2.3) as well as some of the material in Chapter 3.\n2.1 Geometric primitives and transformations\nIn this section, we introduce the basic 2D and 3D primitives used in this textbook, namely\npoints, lines, and planes. We also describe how 3D features are projected into 2D features.",
  "54": "32\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMore detailed descriptions of these topics (along with a gentler and more intuitive introduc-\ntion) can be found in textbooks on multiple-view geometry (Hartley and Zisserman 2004;\nFaugeras and Luong 2001).\n2.1.1 Geometric primitives\nGeometric primitives form the basic building blocks used to describe three-dimensional shapes.\nIn this section, we introduce points, lines, and planes. Later sections of the book discuss\ncurves (Sections 5.1 and 11.2), surfaces (Section 12.3), and volumes (Section 12.5).\n2D points.\n2D points (pixel coordinates in an image) can be denoted using a pair of values,\nx = (x, y) ∈R2, or alternatively,\nx =\n\"\nx\ny\n#\n.\n(2.1)\n(As stated in the introduction, we use the (x1, x2, . . .) notation to denote column vectors.)\n2D points can also be represented using homogeneous coordinates, ˜x = (˜x, ˜y, ˜w) ∈P2,\nwhere vectors that differ only by scale are considered to be equivalent. P2 = R3 −(0, 0, 0)\nis called the 2D projective space.\nA homogeneous vector ˜x can be converted back into an inhomogeneous vector x by\ndividing through by the last element ˜w, i.e.,\n˜x = (˜x, ˜y, ˜w) = ˜w(x, y, 1) = ˜w¯x,\n(2.2)\nwhere ¯x = (x, y, 1) is the augmented vector. Homogeneous points whose last element is ˜w =\n0 are called ideal points or points at inﬁnity and do not have an equivalent inhomogeneous\nrepresentation.\n2D lines.\n2D lines can also be represented using homogeneous coordinates ˜l = (a, b, c).\nThe corresponding line equation is\n¯x · ˜l = ax + by + c = 0.\n(2.3)\nWe can normalize the line equation vector so that l = (ˆnx, ˆny, d) = (ˆn, d) with ∥ˆn∥= 1. In\nthis case, ˆn is the normal vector perpendicular to the line and d is its distance to the origin\n(Figure 2.2). (The one exception to this normalization is the line at inﬁnity ˜l = (0, 0, 1),\nwhich includes all (ideal) points at inﬁnity.)\nWe can also express ˆn as a function of rotation angle θ, ˆn = (ˆnx, ˆny) = (cos θ, sin θ)\n(Figure 2.2a). This representation is commonly used in the Hough transform line-ﬁnding",
  "55": "2.1 Geometric primitives and transformations\n33\ny\nx\nd\nθ\nn\nl\n^\nz\nx\nd\nn\nm\ny\n^\n(a)\n(b)\nFigure 2.2 (a) 2D line equation and (b) 3D plane equation, expressed in terms of the normal\nˆn and distance to the origin d.\nalgorithm, which is discussed in Section 4.3.2. The combination (θ, d) is also known as\npolar coordinates.\nWhen using homogeneous coordinates, we can compute the intersection of two lines as\n˜x = ˜l1 × ˜l2,\n(2.4)\nwhere × is the cross product operator. Similarly, the line joining two points can be written as\n˜l = ˜x1 × ˜x2.\n(2.5)\nWhen trying to ﬁt an intersection point to multiple lines or, conversely, a line to multiple\npoints, least squares techniques (Section 6.1.1 and Appendix A.2) can be used, as discussed\nin Exercise 2.1.\n2D conics.\nThere are other algebraic curves that can be expressed with simple polynomial\nhomogeneous equations. For example, the conic sections (so called because they arise as the\nintersection of a plane and a 3D cone) can be written using a quadric equation\n˜xT Q˜x = 0.\n(2.6)\nQuadric equations play useful roles in the study of multi-view geometry and camera calibra-\ntion (Hartley and Zisserman 2004; Faugeras and Luong 2001) but are not used extensively in\nthis book.\n3D points.\nPoint coordinates in three dimensions can be written using inhomogeneous co-\nordinates x = (x, y, z) ∈R3 or homogeneous coordinates ˜x = (˜x, ˜y, ˜z, ˜w) ∈P3. As before,\nit is sometimes useful to denote a 3D point using the augmented vector ¯x = (x, y, z, 1) with\n˜x = ˜w¯x.",
  "56": "34\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nz\nx\nλ\np\nq\ny\nr=(1-λ)p+λq\nFigure 2.3 3D line equation, r = (1 −λ)p + λq.\n3D planes.\n3D planes can also be represented as homogeneous coordinates ˜\nm = (a, b, c, d)\nwith a corresponding plane equation\n¯x · ˜\nm = ax + by + cz + d = 0.\n(2.7)\nWe can also normalize the plane equation as m = (ˆnx, ˆny, ˆnz, d) = (ˆn, d) with ∥ˆn∥= 1.\nIn this case, ˆn is the normal vector perpendicular to the plane and d is its distance to the\norigin (Figure 2.2b). As with the case of 2D lines, the plane at inﬁnity ˜\nm = (0, 0, 0, 1),\nwhich contains all the points at inﬁnity, cannot be normalized (i.e., it does not have a unique\nnormal or a ﬁnite distance).\nWe can express ˆn as a function of two angles (θ, φ),\nˆn = (cos θ cos φ, sin θ cos φ, sin φ),\n(2.8)\ni.e., using spherical coordinates, but these are less commonly used than polar coordinates\nsince they do not uniformly sample the space of possible normal vectors.\n3D lines.\nLines in 3D are less elegant than either lines in 2D or planes in 3D. One possible\nrepresentation is to use two points on the line, (p, q). Any other point on the line can be\nexpressed as a linear combination of these two points\nr = (1 −λ)p + λq,\n(2.9)\nas shown in Figure 2.3. If we restrict 0 ≤λ ≤1, we get the line segment joining p and q.\nIf we use homogeneous coordinates, we can write the line as\n˜r = µ˜p + λ˜q.\n(2.10)\nA special case of this is when the second point is at inﬁnity, i.e., ˜q = ( ˆdx, ˆdy, ˆdz, 0) = ( ˆd, 0).\nHere, we see that ˆd is the direction of the line. We can then re-write the inhomogeneous 3D\nline equation as\nr = p + λ ˆd.\n(2.11)",
  "57": "2.1 Geometric primitives and transformations\n35\nA disadvantage of the endpoint representation for 3D lines is that it has too many degrees\nof freedom, i.e., six (three for each endpoint) instead of the four degrees that a 3D line truly\nhas. However, if we ﬁx the two points on the line to lie in speciﬁc planes, we obtain a rep-\nresentation with four degrees of freedom. For example, if we are representing nearly vertical\nlines, then z = 0 and z = 1 form two suitable planes, i.e., the (x, y) coordinates in both\nplanes provide the four coordinates describing the line. This kind of two-plane parameteri-\nzation is used in the light ﬁeld and Lumigraph image-based rendering systems described in\nChapter 13 to represent the collection of rays seen by a camera as it moves in front of an\nobject. The two-endpoint representation is also useful for representing line segments, even\nwhen their exact endpoints cannot be seen (only guessed at).\nIf we wish to represent all possible lines without bias towards any particular orientation,\nwe can use Pl¨ucker coordinates (Hartley and Zisserman 2004, Chapter 2; Faugeras and Luong\n2001, Chapter 3). These coordinates are the six independent non-zero entries in the 4×4 skew\nsymmetric matrix\nL = ˜p˜qT −˜q˜pT ,\n(2.12)\nwhere ˜p and ˜q are any two (non-identical) points on the line. This representation has only\nfour degrees of freedom, since L is homogeneous and also satisﬁes det(L) = 0, which results\nin a quadratic constraint on the Pl¨ucker coordinates.\nIn practice, the minimal representation is not essential for most applications. An ade-\nquate model of 3D lines can be obtained by estimating their direction (which may be known\nahead of time, e.g., for architecture) and some point within the visible portion of the line\n(see Section 7.5.1) or by using the two endpoints, since lines are most often visible as ﬁnite\nline segments. However, if you are interested in more details about the topic of minimal\nline parameterizations, F¨orstner (2005) discusses various ways to infer and model 3D lines in\nprojective geometry, as well as how to estimate the uncertainty in such ﬁtted models.\n3D quadrics.\nThe 3D analog of a conic section is a quadric surface\n¯xT Q¯x = 0\n(2.13)\n(Hartley and Zisserman 2004, Chapter 2). Again, while quadric surfaces are useful in the\nstudy of multi-view geometry and can also serve as useful modeling primitives (spheres,\nellipsoids, cylinders), we do not study them in great detail in this book.\n2.1.2 2D transformations\nHaving deﬁned our basic primitives, we can now turn our attention to how they can be trans-\nformed. The simplest transformations occur in the 2D plane and are illustrated in Figure 2.4.",
  "58": "36\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ny\nx\nsimilarity\nEuclidean\naffine\nprojective\ntranslation\nFigure 2.4 Basic set of 2D planar transformations.\nTranslation.\n2D translations can be written as x′ = x + t or\nx′ =\nh\nI\nt\ni\n¯x\n(2.14)\nwhere I is the (2 × 2) identity matrix or\n¯x′ =\n\"\nI\nt\n0T\n1\n#\n¯x\n(2.15)\nwhere 0 is the zero vector. Using a 2 × 3 matrix results in a more compact notation, whereas\nusing a full-rank 3 × 3 matrix (which can be obtained from the 2 × 3 matrix by appending a\n[0T 1] row) makes it possible to chain transformations using matrix multiplication. Note that\nin any equation where an augmented vector such as ¯x appears on both sides, it can always be\nreplaced with a full homogeneous vector ˜x.\nRotation + translation.\nThis transformation is also known as 2D rigid body motion or the\n2D Euclidean transformation (since Euclidean distances are preserved). It can be written as\nx′ = Rx + t or\nx′ =\nh\nR\nt\ni\n¯x\n(2.16)\nwhere\nR =\n\"\ncos θ\n−sin θ\nsin θ\ncos θ\n#\n(2.17)\nis an orthonormal rotation matrix with RRT = I and |R| = 1.\nScaled rotation.\nAlso known as the similarity transform, this transformation can be ex-\npressed as x′ = sRx + t where s is an arbitrary scale factor. It can also be written as\nx′ =\nh\nsR\nt\ni\n¯x =\n\"\na\n−b\ntx\nb\na\nty\n#\n¯x,\n(2.18)\nwhere we no longer require that a2 + b2 = 1. The similarity transform preserves angles\nbetween lines.",
  "59": "2.1 Geometric primitives and transformations\n37\nAfﬁne.\nThe afﬁne transformation is written as x′ = A¯x, where A is an arbitrary 2 × 3\nmatrix, i.e.,\nx′ =\n\"\na00\na01\na02\na10\na11\na12\n#\n¯x.\n(2.19)\nParallel lines remain parallel under afﬁne transformations.\nProjective.\nThis transformation, also known as a perspective transform or homography,\noperates on homogeneous coordinates,\n˜x′ = ˜\nH ˜x,\n(2.20)\nwhere ˜\nH is an arbitrary 3 × 3 matrix. Note that ˜\nH is homogeneous, i.e., it is only deﬁned\nup to a scale, and that two ˜\nH matrices that differ only by scale are equivalent. The resulting\nhomogeneous coordinate ˜x′ must be normalized in order to obtain an inhomogeneous result\nx, i.e.,\nx′ = h00x + h01y + h02\nh20x + h21y + h22\nand y′ = h10x + h11y + h12\nh20x + h21y + h22\n.\n(2.21)\nPerspective transformations preserve straight lines (i.e., they remain straight after the trans-\nformation).\nHierarchy of 2D transformations.\nThe preceding set of transformations are illustrated\nin Figure 2.4 and summarized in Table 2.1. The easiest way to think of them is as a set\nof (potentially restricted) 3 × 3 matrices operating on 2D homogeneous coordinate vectors.\nHartley and Zisserman (2004) contains a more detailed description of the hierarchy of 2D\nplanar transformations.\nThe above transformations form a nested set of groups, i.e., they are closed under com-\nposition and have an inverse that is a member of the same group. (This will be important\nlater when applying these transformations to images in Section 3.6.) Each (simpler) group is\na subset of the more complex group below it.\nCo-vectors.\nWhile the above transformations can be used to transform points in a 2D plane,\ncan they also be used directly to transform a line equation? Consider the homogeneous equa-\ntion ˜l · ˜x = 0. If we transform x′ = ˜\nHx, we obtain\n˜l\n′ · ˜x′ = ˜l\n′T ˜\nH ˜x = ( ˜\nH\nT˜l\n′)T ˜x = ˜l · ˜x = 0,\n(2.22)\ni.e., ˜l\n′ = ˜\nH\n−T˜l. Thus, the action of a projective transformation on a co-vector such as a 2D\nline or 3D normal can be represented by the transposed inverse of the matrix, which is equiv-\nalent to the adjoint of ˜\nH, since projective transformation matrices are homogeneous. Jim",
  "60": "38\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTransformation\nMatrix\n# DoF\nPreserves\nIcon\ntranslation\nh\nI\nt\ni\n2×3\n2\norientation\nrigid (Euclidean)\nh\nR\nt\ni\n2×3\n3\nlengths\n\u001a\n\u001a\n\u001a\u001a\nS\nS S\nS\nsimilarity\nh\nsR\nt\ni\n2×3\n4\nangles\n\u001a\n\u001a\nS S\nafﬁne\nh\nA\ni\n2×3\n6\nparallelism\n\u0002\u0002\n\u0002\u0002\nprojective\nh\n˜\nH\ni\n3×3\n8\nstraight lines\n``\n  \nTable 2.1 Hierarchy of 2D coordinate transformations. Each transformation also preserves\nthe properties listed in the rows below it, i.e., similarity preserves not only angles but also\nparallelism and straight lines. The 2×3 matrices are extended with a third [0T 1] row to form\na full 3 × 3 matrix for homogeneous coordinate transformations.\nBlinn (1998) describes (in Chapters 9 and 10) the ins and outs of notating and manipulating\nco-vectors.\nWhile the above transformations are the ones we use most extensively, a number of addi-\ntional transformations are sometimes used.\nStretch/squash.\nThis transformation changes the aspect ratio of an image,\nx′\n=\nsxx + tx\ny′\n=\nsyy + ty,\nand is a restricted form of an afﬁne transformation. Unfortunately, it does not nest cleanly\nwith the groups listed in Table 2.1.\nPlanar surface ﬂow.\nThis eight-parameter transformation (Horn 1986; Bergen, Anandan,\nHanna et al. 1992; Girod, Greiner, and Niemann 2000),\nx′\n=\na0 + a1x + a2y + a6x2 + a7xy\ny′\n=\na3 + a4x + a5y + a7x2 + a6xy,\narises when a planar surface undergoes a small 3D motion. It can thus be thought of as a\nsmall motion approximation to a full homography. Its main attraction is that it is linear in the\nmotion parameters, ak, which are often the quantities being estimated.",
  "61": "2.1 Geometric primitives and transformations\n39\nTransformation\nMatrix\n# DoF\nPreserves\nIcon\ntranslation\nh\nI\nt\ni\n3×4\n3\norientation\nrigid (Euclidean)\nh\nR\nt\ni\n3×4\n6\nlengths\n\u001a\n\u001a\n\u001a\u001a\nS\nS S\nS\nsimilarity\nh\nsR\nt\ni\n3×4\n7\nangles\n\u001a\n\u001a\nS S\nafﬁne\nh\nA\ni\n3×4\n12\nparallelism\n\u0002\u0002\n\u0002\u0002\nprojective\nh\n˜\nH\ni\n4×4\n15\nstraight lines\n``\n  \nTable 2.2 Hierarchy of 3D coordinate transformations. Each transformation also preserves\nthe properties listed in the rows below it, i.e., similarity preserves not only angles but also\nparallelism and straight lines. The 3 × 4 matrices are extended with a fourth [0T 1] row to\nform a full 4 × 4 matrix for homogeneous coordinate transformations. The mnemonic icons\nare drawn in 2D but are meant to suggest transformations occurring in a full 3D cube.\nBilinear interpolant.\nThis eight-parameter transform (Wolberg 1990),\nx′\n=\na0 + a1x + a2y + a6xy\ny′\n=\na3 + a4x + a5y + a7xy,\ncan be used to interpolate the deformation due to the motion of the four corner points of\na square. (In fact, it can interpolate the motion of any four non-collinear points.) While\nthe deformation is linear in the motion parameters, it does not generally preserve straight\nlines (only lines parallel to the square axes). However, it is often quite useful, e.g., in the\ninterpolation of sparse grids using splines (Section 8.3).\n2.1.3 3D transformations\nThe set of three-dimensional coordinate transformations is very similar to that available for\n2D transformations and is summarized in Table 2.2. As in 2D, these transformations form a\nnested set of groups. Hartley and Zisserman (2004, Section 2.4) give a more detailed descrip-\ntion of this hierarchy.\nTranslation.\n3D translations can be written as x′ = x + t or\nx′ =\nh\nI\nt\ni\n¯x\n(2.23)",
  "62": "40\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhere I is the (3 × 3) identity matrix and 0 is the zero vector.\nRotation + translation.\nAlso known as 3D rigid body motion or the 3D Euclidean trans-\nformation, it can be written as x′ = Rx + t or\nx′ =\nh\nR\nt\ni\n¯x\n(2.24)\nwhere R is a 3 × 3 orthonormal rotation matrix with RRT = I and |R| = 1. Note that\nsometimes it is more convenient to describe a rigid motion using\nx′ = R(x −c) = Rx −Rc,\n(2.25)\nwhere c is the center of rotation (often the camera center).\nCompactly parameterizing a 3D rotation is a non-trivial task, which we describe in more\ndetail below.\nScaled rotation.\nThe 3D similarity transform can be expressed as x′ = sRx + t where s\nis an arbitrary scale factor. It can also be written as\nx′ =\nh\nsR\nt\ni\n¯x.\n(2.26)\nThis transformation preserves angles between lines and planes.\nAfﬁne.\nThe afﬁne transform is written as x′ = A¯x, where A is an arbitrary 3 × 4 matrix,\ni.e.,\nx′ =\n\n\na00\na01\na02\na03\na10\na11\na12\na13\na20\na21\na22\na23\n\n¯x.\n(2.27)\nParallel lines and planes remain parallel under afﬁne transformations.\nProjective.\nThis transformation, variously known as a 3D perspective transform, homogra-\nphy, or collineation, operates on homogeneous coordinates,\n˜x′ = ˜\nH ˜x,\n(2.28)\nwhere ˜\nH is an arbitrary 4 × 4 homogeneous matrix. As in 2D, the resulting homogeneous\ncoordinate ˜x′ must be normalized in order to obtain an inhomogeneous result x. Perspective\ntransformations preserve straight lines (i.e., they remain straight after the transformation).",
  "63": "2.1 Geometric primitives and transformations\n41\nv\nv┴\nn^\nv×\nv║\nv××\nu┴\nu\nθ\nFigure 2.5 Rotation around an axis ˆn by an angle θ.\n2.1.4 3D rotations\nThe biggest difference between 2D and 3D coordinate transformations is that the parameter-\nization of the 3D rotation matrix R is not as straightforward but several possibilities exist.\nEuler angles\nA rotation matrix can be formed as the product of three rotations around three cardinal axes,\ne.g., x, y, and z, or x, y, and x. This is generally a bad idea, as the result depends on the\norder in which the transforms are applied. What is worse, it is not always possible to move\nsmoothly in the parameter space, i.e., sometimes one or more of the Euler angles change\ndramatically in response to a small change in rotation.1 For these reasons, we do not even\ngive the formula for Euler angles in this book—interested readers can look in other textbooks\nor technical reports (Faugeras 1993; Diebel 2006). Note that, in some applications, if the\nrotations are known to be a set of uni-axial transforms, they can always be represented using\nan explicit set of rigid transformations.\nAxis/angle (exponential twist)\nA rotation can be represented by a rotation axis ˆn and an angle θ, or equivalently by a 3D\nvector ω = θˆn. Figure 2.5 shows how we can compute the equivalent rotation. First, we\nproject the vector v onto the axis ˆn to obtain\nv∥= ˆn(ˆn · v) = (ˆnˆnT )v,\n(2.29)\nwhich is the component of v that is not affected by the rotation. Next, we compute the\nperpendicular residual of v from ˆn,\nv⊥= v −v∥= (I −ˆnˆnT )v.\n(2.30)\n1 In robotics, this is sometimes referred to as gimbal lock.",
  "64": "42\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWe can rotate this vector by 90◦using the cross product,\nv× = ˆn × v = [ˆn]×v,\n(2.31)\nwhere [ˆn]× is the matrix form of the cross product operator with the vector ˆn = (ˆnx, ˆny, ˆnz),\n[ˆn]× =\n\n\n0\n−ˆnz\nˆny\nˆnz\n0\n−ˆnx\n−ˆny\nˆnx\n0\n\n.\n(2.32)\nNote that rotating this vector by another 90◦is equivalent to taking the cross product again,\nv×× = ˆn × v× = [ˆn]2\n×v = −v⊥,\nand hence\nv∥= v −v⊥= v + v×× = (I + [ˆn]2\n×)v.\nWe can now compute the in-plane component of the rotated vector u as\nu⊥= cos θv⊥+ sin θv× = (sin θ[ˆn]× −cos θ[ˆn]2\n×)v.\nPutting all these terms together, we obtain the ﬁnal rotated vector as\nu = u⊥+ v∥= (I + sin θ[ˆn]× + (1 −cos θ)[ˆn]2\n×)v.\n(2.33)\nWe can therefore write the rotation matrix corresponding to a rotation by θ around an axis ˆn\nas\nR(ˆn, θ) = I + sin θ[ˆn]× + (1 −cos θ)[ˆn]2\n×,\n(2.34)\nwhich is known as Rodriguez’s formula (Ayache 1989).\nThe product of the axis ˆn and angle θ, ω = θˆn = (ωx, ωy, ωz), is a minimal represen-\ntation for a 3D rotation. Rotations through common angles such as multiples of 90◦can be\nrepresented exactly (and converted to exact matrices) if θ is stored in degrees. Unfortunately,\nthis representation is not unique, since we can always add a multiple of 360◦(2π radians) to\nθ and get the same rotation matrix. As well, (ˆn, θ) and (−ˆn, −θ) represent the same rotation.\nHowever, for small rotations (e.g., corrections to rotations), this is an excellent choice.\nIn particular, for small (inﬁnitesimal or instantaneous) rotations and θ expressed in radians,\nRodriguez’s formula simpliﬁes to\nR(ω) ≈I + sin θ[ˆn]× ≈I + [θˆn]× =\n\n\n1\n−ωz\nωy\nωz\n1\n−ωx\n−ωy\nωx\n1\n\n,\n(2.35)",
  "65": "2.1 Geometric primitives and transformations\n43\nwhich gives a nice linearized relationship between the rotation parameters ω and R. We can\nalso write R(ω)v ≈v + ω × v, which is handy when we want to compute the derivative of\nRv with respect to ω,\n∂Rv\n∂ωT = −[v]× =\n\n\n0\nz\n−y\n−z\n0\nx\ny\n−x\n0\n\n.\n(2.36)\nAnother way to derive a rotation through a ﬁnite angle is called the exponential twist\n(Murray, Li, and Sastry 1994). A rotation by an angle θ is equivalent to k rotations through\nθ/k. In the limit as k →∞, we obtain\nR(ˆn, θ) = lim\nk→∞(I + 1\nk [θˆn]×)k = exp [ω]×.\n(2.37)\nIf we expand the matrix exponential as a Taylor series (using the identity [ˆn]k+2\n×\n= −[ˆn]k\n×,\nk > 0, and again assuming θ is in radians),\nexp [ω]×\n=\nI + θ[ˆn]× + θ2\n2 [ˆn]2\n× + θ3\n3! [ˆn]3\n× + · · ·\n=\nI + (θ −θ3\n3! + · · ·)[ˆn]× + (θ2\n2 −θ3\n4! + · · ·)[ˆn]2\n×\n=\nI + sin θ[ˆn]× + (1 −cos θ)[ˆn]2\n×,\n(2.38)\nwhich yields the familiar Rodriguez’s formula.\nUnit quaternions\nThe unit quaternion representation is closely related to the angle/axis representation. A unit\nquaternion is a unit length 4-vector whose components can be written as q = (qx, qy, qz, qw)\nor q = (x, y, z, w) for short. Unit quaternions live on the unit sphere ∥q∥= 1 and antipodal\n(opposite sign) quaternions, q and −q, represent the same rotation (Figure 2.6). Other than\nthis ambiguity (dual covering), the unit quaternion representation of a rotation is unique.\nFurthermore, the representation is continuous, i.e., as rotation matrices vary continuously,\none can ﬁnd a continuous quaternion representation, although the path on the quaternion\nsphere may wrap all the way around before returning to the “origin” qo = (0, 0, 0, 1). For\nthese and other reasons given below, quaternions are a very popular representation for pose\nand for pose interpolation in computer graphics (Shoemake 1985).\nQuaternions can be derived from the axis/angle representation through the formula\nq = (v, w) = (sin θ\n2 ˆn, cos θ\n2),\n(2.39)",
  "66": "44\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nz\nx\nw\n║q║=1\ny\nq0\nq1\nq2\n-q2\nFigure 2.6 Unit quaternions live on the unit sphere ∥q∥= 1. This ﬁgure shows a smooth\ntrajectory through the three quaternions q0, q1, and q2. The antipodal point to q2, namely\n−q2, represents the same rotation as q2.\nwhere ˆn and θ are the rotation axis and angle. Using the trigonometric identities sin θ =\n2 sin θ\n2 cos θ\n2 and (1 −cos θ) = 2 sin2 θ\n2, Rodriguez’s formula can be converted to\nR(ˆn, θ)\n=\nI + sin θ[ˆn]× + (1 −cos θ)[ˆn]2\n×\n=\nI + 2w[v]× + 2[v]2\n×.\n(2.40)\nThis suggests a quick way to rotate a vector v by a quaternion using a series of cross products,\nscalings, and additions. To obtain a formula for R(q) as a function of (x, y, z, w), recall that\n[v]× =\n\n\n0\n−z\ny\nz\n0\n−x\n−y\nx\n0\n\nand [v]2\n× =\n\n\n−y2 −z2\nxy\nxz\nxy\n−x2 −z2\nyz\nxz\nyz\n−x2 −y2\n\n.\nWe thus obtain\nR(q) =\n\n\n1 −2(y2 + z2)\n2(xy −zw)\n2(xz + yw)\n2(xy + zw)\n1 −2(x2 + z2)\n2(yz −xw)\n2(xz −yw)\n2(yz + xw)\n1 −2(x2 + y2)\n\n.\n(2.41)\nThe diagonal terms can be made more symmetrical by replacing 1 −2(y2 + z2) with (x2 +\nw2 −y2 −z2), etc.\nThe nicest aspect of unit quaternions is that there is a simple algebra for composing rota-\ntions expressed as unit quaternions. Given two quaternions q0 = (v0, w0) and q1 = (v1, w1),\nthe quaternion multiply operator is deﬁned as\nq2 = q0q1 = (v0 × v1 + w0v1 + w1v0, w0w1 −v0 · v1),\n(2.42)",
  "67": "2.1 Geometric primitives and transformations\n45\nwith the property that R(q2) = R(q0)R(q1). Note that quaternion multiplication is not\ncommutative, just as 3D rotations and matrix multiplications are not.\nTaking the inverse of a quaternion is easy: Just ﬂip the sign of v or w (but not both!).\n(You can verify this has the desired effect of transposing the R matrix in (2.41).) Thus, we\ncan also deﬁne quaternion division as\nq2 = q0/q1 = q0q−1\n1\n= (v0 × v1 + w0v1 −w1v0, −w0w1 −v0 · v1).\n(2.43)\nThis is useful when the incremental rotation between two rotations is desired.\nIn particular, if we want to determine a rotation that is partway between two given rota-\ntions, we can compute the incremental rotation, take a fraction of the angle, and compute the\nnew rotation. This procedure is called spherical linear interpolation or slerp for short (Shoe-\nmake 1985) and is given in Algorithm 2.1. Note that Shoemake presents two formulas other\nthan the one given here. The ﬁrst exponentiates qr by alpha before multiplying the original\nquaternion,\nq2 = qα\nr q0,\n(2.44)\nwhile the second treats the quaternions as 4-vectors on a sphere and uses\nq2 = sin(1 −α)θ\nsin θ\nq0 + sin αθ\nsin θ q1,\n(2.45)\nwhere θ = cos−1(q0 · q1) and the dot product is directly between the quaternion 4-vectors.\nAll of these formulas give comparable results, although care should be taken when q0 and q1\nare close together, which is why I prefer to use an arctangent to establish the rotation angle.\nWhich rotation representation is better?\nThe choice of representation for 3D rotations depends partly on the application.\nThe axis/angle representation is minimal, and hence does not require any additional con-\nstraints on the parameters (no need to re-normalize after each update). If the angle is ex-\npressed in degrees, it is easier to understand the pose (say, 90◦twist around x-axis), and also\neasier to express exact rotations. When the angle is in radians, the derivatives of R with\nrespect to ω can easily be computed (2.36).\nQuaternions, on the other hand, are better if you want to keep track of a smoothly moving\ncamera, since there are no discontinuities in the representation. It is also easier to interpolate\nbetween rotations and to chain rigid transformations (Murray, Li, and Sastry 1994; Bregler\nand Malik 1998).\nMy usual preference is to use quaternions, but to update their estimates using an incre-\nmental rotation, as described in Section 6.2.2.",
  "68": "46\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprocedure slerp(q0, q1, α):\n1. qr = q1/q0 = (vr, wr)\n2. if wr < 0 then qr ←−qr\n3. θr = 2 tan−1(∥vr∥/wr)\n4. ˆnr = N(vr) = vr/∥vr∥\n5. θα = α θr\n6. qα = (sin θα\n2 ˆnr, cos θα\n2 )\n7. return q2 = qαq0\nAlgorithm 2.1 Spherical linear interpolation (slerp). The axis and total angle are ﬁrst com-\nputed from the quaternion ratio. (This computation can be lifted outside an inner loop that\ngenerates a set of interpolated position for animation.) An incremental quaternion is then\ncomputed and multiplied by the starting rotation quaternion.\n2.1.5 3D to 2D projections\nNow that we know how to represent 2D and 3D geometric primitives and how to transform\nthem spatially, we need to specify how 3D primitives are projected onto the image plane. We\ncan do this using a linear 3D to 2D projection matrix. The simplest model is orthography,\nwhich requires no division to get the ﬁnal (inhomogeneous) result. The more commonly used\nmodel is perspective, since this more accurately models the behavior of real cameras.\nOrthography and para-perspective\nAn orthographic projection simply drops the z component of the three-dimensional coordi-\nnate p to obtain the 2D point x. (In this section, we use p to denote 3D points and x to denote\n2D points.) This can be written as\nx = [I2×2|0] p.\n(2.46)\nIf we are using homogeneous (projective) coordinates, we can write\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n˜p,\n(2.47)",
  "69": "2.1 Geometric primitives and transformations\n47\n(a) 3D view\n(b) orthography\n(c) scaled orthography\n(d) para-perspective\n(e) perspective\n(f) object-centered\nFigure 2.7 Commonly used projection models: (a) 3D view of world, (b) orthography, (c)\nscaled orthography, (d) para-perspective, (e) perspective, (f) object-centered. Each diagram\nshows a top-down view of the projection. Note how parallel lines on the ground plane and\nbox sides remain parallel in the non-perspective projections.",
  "70": "48\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ni.e., we drop the z component but keep the w component. Orthography is an approximate\nmodel for long focal length (telephoto) lenses and objects whose depth is shallow relative\nto their distance to the camera (Sawhney and Hanson 1991). It is exact only for telecentric\nlenses (Baker and Nayar 1999, 2001).\nIn practice, world coordinates (which may measure dimensions in meters) need to be\nscaled to ﬁt onto an image sensor (physically measured in millimeters, but ultimately mea-\nsured in pixels). For this reason, scaled orthography is actually more commonly used,\nx = [sI2×2|0] p.\n(2.48)\nThis model is equivalent to ﬁrst projecting the world points onto a local fronto-parallel image\nplane and then scaling this image using regular perspective projection. The scaling can be the\nsame for all parts of the scene (Figure 2.7b) or it can be different for objects that are being\nmodeled independently (Figure 2.7c). More importantly, the scaling can vary from frame to\nframe when estimating structure from motion, which can better model the scale change that\noccurs as an object approaches the camera.\nScaled orthography is a popular model for reconstructing the 3D shape of objects far away\nfrom the camera, since it greatly simpliﬁes certain computations. For example, pose (camera\norientation) can be estimated using simple least squares (Section 6.2.1). Under orthography,\nstructure and motion can simultaneously be estimated using factorization (singular value de-\ncomposition), as discussed in Section 7.3 (Tomasi and Kanade 1992).\nA closely related projection model is para-perspective (Aloimonos 1990; Poelman and\nKanade 1997). In this model, object points are again ﬁrst projected onto a local reference\nparallel to the image plane. However, rather than being projected orthogonally to this plane,\nthey are projected parallel to the line of sight to the object center (Figure 2.7d). This is\nfollowed by the usual projection onto the ﬁnal image plane, which again amounts to a scaling.\nThe combination of these two projections is therefore afﬁne and can be written as\n˜x =\n\n\na00\na01\na02\na03\na10\na11\na12\na13\n0\n0\n0\n1\n\n˜p.\n(2.49)\nNote how parallel lines in 3D remain parallel after projection in Figure 2.7b–d. Para-perspective\nprovides a more accurate projection model than scaled orthography, without incurring the\nadded complexity of per-pixel perspective division, which invalidates traditional factoriza-\ntion methods (Poelman and Kanade 1997).\nPerspective\nThe most commonly used projection in computer graphics and computer vision is true 3D\nperspective (Figure 2.7e). Here, points are projected onto the image plane by dividing them",
  "71": "2.1 Geometric primitives and transformations\n49\nby their z component. Using inhomogeneous coordinates, this can be written as\n¯x = Pz(p) =\n\n\nx/z\ny/z\n1\n\n.\n(2.50)\nIn homogeneous coordinates, the projection has a simple linear form,\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n˜p,\n(2.51)\ni.e., we drop the w component of p. Thus, after projection, it is not possible to recover the\ndistance of the 3D point from the image, which makes sense for a 2D imaging sensor.\nA form often seen in computer graphics systems is a two-step projection that ﬁrst projects\n3D coordinates into normalized device coordinates in the range (x, y, z) ∈[−1, −1] ×\n[−1, 1] × [0, 1], and then rescales these coordinates to integer pixel coordinates using a view-\nport transformation (Watt 1995; OpenGL-ARB 1997).\nThe (initial) perspective projection\nis then represented using a 4 × 4 matrix\n˜x =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n−zfar/zrange\nznearzfar/zrange\n0\n0\n1\n0\n\n˜p,\n(2.52)\nwhere znear and zfar are the near and far z clipping planes and zrange = zfar −znear. Note\nthat the ﬁrst two rows are actually scaled by the focal length and the aspect ratio so that\nvisible rays are mapped to (x, y, z) ∈[−1, −1]2. The reason for keeping the third row, rather\nthan dropping it, is that visibility operations, such as z-buffering, require a depth for every\ngraphical element that is being rendered.\nIf we set znear = 1, zfar →∞, and switch the sign of the third row, the third element\nof the normalized screen vector becomes the inverse depth, i.e., the disparity (Okutomi and\nKanade 1993). This can be quite convenient in many cases since, for cameras moving around\noutdoors, the inverse depth to the camera is often a more well-conditioned parameterization\nthan direct 3D distance.\nWhile a regular 2D image sensor has no way of measuring distance to a surface point,\nrange sensors (Section 12.2) and stereo matching algorithms (Chapter 11) can compute such\nvalues. It is then convenient to be able to map from a sensor-based depth or disparity value d\ndirectly back to a 3D location using the inverse of a 4 × 4 matrix (Section 2.1.5). We can do\nthis if we represent perspective projection using a full-rank 4 × 4 matrix, as in (2.64).",
  "72": "50\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\ncs\nyc\nxs\nys\nsx\nsy\npc\np\nOc\nFigure 2.8 Projection of a 3D camera-centered point pc onto the sensor planes at location\np. Oc is the camera center (nodal point), cs is the 3D origin of the sensor plane coordinate\nsystem, and sx and sy are the pixel spacings.\nCamera intrinsics\nOnce we have projected a 3D point through an ideal pinhole using a projection matrix, we\nmust still transform the resulting coordinates according to the pixel sensor spacing and the\nrelative position of the sensor plane to the origin. Figure 2.8 shows an illustration of the\ngeometry involved. In this section, we ﬁrst present a mapping from 2D pixel coordinates to\n3D rays using a sensor homography M s, since this is easier to explain in terms of physically\nmeasurable quantities. We then relate these quantities to the more commonly used camera in-\ntrinsic matrix K, which is used to map 3D camera-centered points pc to 2D pixel coordinates\n˜xs.\nImage sensors return pixel values indexed by integer pixel coordinates (xs, ys), often\nwith the coordinates starting at the upper-left corner of the image and moving down and to\nthe right. (This convention is not obeyed by all imaging libraries, but the adjustment for\nother coordinate systems is straightforward.) To map pixel centers to 3D coordinates, we ﬁrst\nscale the (xs, ys) values by the pixel spacings (sx, sy) (sometimes expressed in microns for\nsolid-state sensors) and then describe the orientation of the sensor array relative to the camera\nprojection center Oc with an origin cs and a 3D rotation Rs (Figure 2.8).\nThe combined 2D to 3D projection can then be written as\np =\nh\nRs\ncs\ni\n\n\nsx\n0\n0\n0\nsy\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\nxs\nys\n1\n\n= M s¯xs.\n(2.53)\nThe ﬁrst two columns of the 3 × 3 matrix M s are the 3D vectors corresponding to unit steps\nin the image pixel array along the xs and ys directions, while the third column is the 3D\nimage array origin cs.",
  "73": "2.1 Geometric primitives and transformations\n51\nThe matrix M s is parameterized by eight unknowns: the three parameters describing\nthe rotation Rs, the three parameters describing the translation cs, and the two scale factors\n(sx, sy). Note that we ignore here the possibility of skew between the two axes on the image\nplane, since solid-state manufacturing techniques render this negligible. In practice, unless\nwe have accurate external knowledge of the sensor spacing or sensor orientation, there are\nonly seven degrees of freedom, since the distance of the sensor from the origin cannot be\nteased apart from the sensor spacing, based on external image measurement alone.\nHowever, estimating a camera model M s with the required seven degrees of freedom\n(i.e., where the ﬁrst two columns are orthogonal after an appropriate re-scaling) is impractical,\nso most practitioners assume a general 3 × 3 homogeneous matrix form.\nThe relationship between the 3D pixel center p and the 3D camera-centered point pc is\ngiven by an unknown scaling s, p = spc. We can therefore write the complete projection\nbetween pc and a homogeneous version of the pixel address ˜xs as\n˜xs = αM −1\ns pc = Kpc.\n(2.54)\nThe 3 × 3 matrix K is called the calibration matrix and describes the camera intrinsics (as\nopposed to the camera’s orientation in space, which are called the extrinsics).\nFrom the above discussion, we see that K has seven degrees of freedom in theory and\neight degrees of freedom (the full dimensionality of a 3×3 homogeneous matrix) in practice.\nWhy, then, do most textbooks on 3D computer vision and multi-view geometry (Faugeras\n1993; Hartley and Zisserman 2004; Faugeras and Luong 2001) treat K as an upper-triangular\nmatrix with ﬁve degrees of freedom?\nWhile this is usually not made explicit in these books, it is because we cannot recover\nthe full K matrix based on external measurement alone. When calibrating a camera (Chap-\nter 6) based on external 3D points or other measurements (Tsai 1987), we end up estimating\nthe intrinsic (K) and extrinsic (R, t) camera parameters simultaneously using a series of\nmeasurements,\n˜xs = K\nh\nR\nt\ni\npw = P pw,\n(2.55)\nwhere pw are known 3D world coordinates and\nP = K[R|t]\n(2.56)\nis known as the camera matrix. Inspecting this equation, we see that we can post-multiply\nK by R1 and pre-multiply [R|t] by RT\n1 , and still end up with a valid calibration. Thus, it\nis impossible based on image measurements alone to know the true orientation of the sensor\nand the true camera intrinsics.\nThe choice of an upper-triangular form for K seems to be conventional. Given a full\n3 × 4 camera matrix P = K[R|t], we can compute an upper-triangular K matrix using QR",
  "74": "52\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzc\nxc\n0\nyc\nxs\nys\nW-1\nH-1\n(cx,cy)\n0\nf\nFigure 2.9\nSimpliﬁed camera intrinsics showing the focal length f and the optical center\n(cx, cy). The image width and height are W and H.\nfactorization (Golub and Van Loan 1996). (Note the unfortunate clash of terminologies: In\nmatrix algebra textbooks, R represents an upper-triangular (right of the diagonal) matrix; in\ncomputer vision, R is an orthogonal rotation.)\nThere are several ways to write the upper-triangular form of K. One possibility is\nK =\n\n\nfx\ns\ncx\n0\nfy\ncy\n0\n0\n1\n\n,\n(2.57)\nwhich uses independent focal lengths fx and fy for the sensor x and y dimensions. The entry\ns encodes any possible skew between the sensor axes due to the sensor not being mounted\nperpendicular to the optical axis and (cx, cy) denotes the optical center expressed in pixel\ncoordinates. Another possibility is\nK =\n\n\nf\ns\ncx\n0\naf\ncy\n0\n0\n1\n\n,\n(2.58)\nwhere the aspect ratio a has been made explicit and a common focal length f is used.\nIn practice, for many applications an even simpler form can be obtained by setting a = 1\nand s = 0,\nK =\n\n\nf\n0\ncx\n0\nf\ncy\n0\n0\n1\n\n.\n(2.59)\nOften, setting the origin at roughly the center of the image, e.g., (cx, cy) = (W/2, H/2),\nwhere W and H are the image height and width, can result in a perfectly usable camera\nmodel with a single unknown, i.e., the focal length f.",
  "75": "2.1 Geometric primitives and transformations\n53\nW/2\nf\nθ/2\n(x,y,1)\n(X,Y,Z)\nZ\nFigure 2.10\nCentral projection, showing the relationship between the 3D and 2D coordi-\nnates, p and x, as well as the relationship between the focal length f, image width W, and\nthe ﬁeld of view θ.\nFigure 2.9 shows how these quantities can be visualized as part of a simpliﬁed imaging\nmodel. Note that now we have placed the image plane in front of the nodal point (projection\ncenter of the lens). The sense of the y axis has also been ﬂipped to get a coordinate system\ncompatible with the way that most imaging libraries treat the vertical (row) coordinate. Cer-\ntain graphics libraries, such as Direct3D, use a left-handed coordinate system, which can lead\nto some confusion.\nA note on focal lengths\nThe issue of how to express focal lengths is one that often causes confusion in implementing\ncomputer vision algorithms and discussing their results. This is because the focal length\ndepends on the units used to measure pixels.\nIf we number pixel coordinates using integer values, say [0, W)×[0, H), the focal length\nf and camera center (cx, cy) in (2.59) can be expressed as pixel values. How do these quan-\ntities relate to the more familiar focal lengths used by photographers?\nFigure 2.10 illustrates the relationship between the focal length f, the sensor width W,\nand the ﬁeld of view θ, which obey the formula\ntan θ\n2 = W\n2f\nor\nf = W\n2\n\u0014\ntan θ\n2\n\u0015−1\n.\n(2.60)\nFor conventional ﬁlm cameras, W = 35mm, and hence f is also expressed in millimeters.\nSince we work with digital images, it is more convenient to express W in pixels so that the\nfocal length f can be used directly in the calibration matrix K as in (2.59).\nAnother possibility is to scale the pixel coordinates so that they go from [−1, 1) along\nthe longer image dimension and [−a−1, a−1) along the shorter axis, where a ≥1 is the\nimage aspect ratio (as opposed to the sensor cell aspect ratio introduced earlier). This can be",
  "76": "54\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\naccomplished using modiﬁed normalized device coordinates,\nx′\ns = (2xs −W)/S and y′\ns = (2ys −H)/S,\nwhere\nS = max(W, H).\n(2.61)\nThis has the advantage that the focal length f and optical center (cx, cy) become independent\nof the image resolution, which can be useful when using multi-resolution, image-processing\nalgorithms, such as image pyramids (Section 3.5).2 The use of S instead of W also makes the\nfocal length the same for landscape (horizontal) and portrait (vertical) pictures, as is the case\nin 35mm photography. (In some computer graphics textbooks and systems, normalized device\ncoordinates go from [−1, 1] × [−1, 1], which requires the use of two different focal lengths\nto describe the camera intrinsics (Watt 1995; OpenGL-ARB 1997).) Setting S = W = 2 in\n(2.60), we obtain the simpler (unitless) relationship\nf −1 = tan θ\n2.\n(2.62)\nThe conversion between the various focal length representations is straightforward, e.g.,\nto go from a unitless f to one expressed in pixels, multiply by W/2, while to convert from an\nf expressed in pixels to the equivalent 35mm focal length, multiply by 35/W.\nCamera matrix\nNow that we have shown how to parameterize the calibration matrix K, we can put the\ncamera intrinsics and extrinsics together to obtain a single 3 × 4 camera matrix\nP = K\nh\nR\nt\ni\n.\n(2.63)\nIt is sometimes preferable to use an invertible 4 × 4 matrix, which can be obtained by not\ndropping the last row in the P matrix,\n˜\nP =\n\"\nK\n0\n0T\n1\n# \"\nR\nt\n0T\n1\n#\n= ˜\nKE,\n(2.64)\nwhere E is a 3D rigid-body (Euclidean) transformation and ˜\nK is the full-rank calibration\nmatrix. The 4 × 4 camera matrix ˜\nP can be used to map directly from 3D world coordinates\n¯pw = (xw, yw, zw, 1) to screen coordinates (plus disparity), xs = (xs, ys, 1, d),\nxs ∼˜\nP ¯pw,\n(2.65)\nwhere ∼indicates equality up to scale. Note that after multiplication by ˜\nP , the vector is\ndivided by the third element of the vector to obtain the normalized form xs = (xs, ys, 1, d).\n2 To make the conversion truly accurate after a downsampling step in a pyramid, ﬂoating point values of W and\nH would have to be maintained since they can become non-integral if they are ever odd at a larger resolution in the\npyramid.",
  "77": "2.1 Geometric primitives and transformations\n55\nC\n(xs,ys,d)\nZ\nimage plane\nd=1.0 d=0.67 d=0.5\nd = inverse depth\n(xw,yw,zw)\nd\nz\nC\n(xs,ys,d)\nZ\nimage plane\nd=0.5\nd=0\nd=-0.25\nd = projective depth\n(xw,yw,zw)\nz\nplane\nparallax\nFigure 2.11 Regular disparity (inverse depth) and projective depth (parallax from a reference\nplane).\nPlane plus parallax (projective depth)\nIn general, when using the 4 × 4 matrix ˜\nP , we have the freedom to remap the last row to\nwhatever suits our purpose (rather than just being the “standard” interpretation of disparity as\ninverse depth). Let us re-write the last row of ˜\nP as p3 = s3[ˆn0|c0], where ∥ˆn0∥= 1. We\nthen have the equation\nd = s3\nz (ˆn0 · pw + c0),\n(2.66)\nwhere z = p2 · ¯pw = rz · (pw −c) is the distance of pw from the camera center C (2.25)\nalong the optical axis Z (Figure 2.11). Thus, we can interpret d as the projective disparity\nor projective depth of a 3D scene point pw from the reference plane ˆn0 · pw + c0 = 0\n(Szeliski and Coughlan 1997; Szeliski and Golland 1999; Shade, Gortler, He et al. 1998;\nBaker, Szeliski, and Anandan 1998). (The projective depth is also sometimes called parallax\nin reconstruction algorithms that use the term plane plus parallax (Kumar, Anandan, and\nHanna 1994; Sawhney 1994).) Setting ˆn0 = 0 and c0 = 1, i.e., putting the reference plane\nat inﬁnity, results in the more standard d = 1/z version of disparity (Okutomi and Kanade\n1993).\nAnother way to see this is to invert the ˜\nP matrix so that we can map pixels plus disparity\ndirectly back to 3D points,\n˜pw = ˜\nP\n−1xs.\n(2.67)\nIn general, we can choose ˜\nP to have whatever form is convenient, i.e., to sample space us-\ning an arbitrary projection. This can come in particularly handy when setting up multi-view\nstereo reconstruction algorithms, since it allows us to sweep a series of planes (Section 11.1.2)\nthrough space with a variable (projective) sampling that best matches the sensed image mo-\ntions (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).",
  "78": "56\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z,1)\nx1 = (x1,y1,1,d1)\nx0 = (x0,y0,1,d0)\n~\n~\nM10\nn0·p+c0= 0\nx1 = (x1,y1,1)\nx0 = (x0,y0,1)\n~\n~\nH10\n^\n.\n(a)\n(b)\nFigure 2.12 A point is projected into two images: (a) relationship between the 3D point co-\nordinate (X, Y, Z, 1) and the 2D projected point (x, y, 1, d); (b) planar homography induced\nby points all lying on a common plane ˆn0 · p + c0 = 0.\nMapping from one camera to another\nWhat happens when we take two images of a 3D scene from different camera positions or\norientations (Figure 2.12a)? Using the full rank 4 × 4 camera matrix ˜\nP = ˜\nKE from (2.64),\nwe can write the projection from world to screen coordinates as\n˜x0 ∼˜\nK0E0p = ˜\nP 0p.\n(2.68)\nAssuming that we know the z-buffer or disparity value d0 for a pixel in one image, we can\ncompute the 3D point location p using\np ∼E−1\n0\n˜\nK\n−1\n0 ˜x0\n(2.69)\nand then project it into another image yielding\n˜x1 ∼˜\nK1E1p = ˜\nK1E1E−1\n0\n˜\nK\n−1\n0 ˜x0 = ˜\nP 1 ˜\nP\n−1\n0 ˜x0 = M 10˜x0.\n(2.70)\nUnfortunately, we do not usually have access to the depth coordinates of pixels in a regular\nphotographic image. However, for a planar scene, as discussed above in (2.66), we can\nreplace the last row of P 0 in (2.64) with a general plane equation, ˆn0 · p + c0 that maps\npoints on the plane to d0 = 0 values (Figure 2.12b). Thus, if we set d0 = 0, we can ignore\nthe last column of M 10 in (2.70) and also its last row, since we do not care about the ﬁnal\nz-buffer depth. The mapping equation (2.70) thus reduces to\n˜x1 ∼˜\nH10˜x0,\n(2.71)\nwhere ˜\nH10 is a general 3 × 3 homography matrix and ˜x1 and ˜x0 are now 2D homogeneous\ncoordinates (i.e., 3-vectors) (Szeliski 1996).This justiﬁes the use of the 8-parameter homog-\nraphy as a general alignment model for mosaics of planar scenes (Mann and Picard 1994;\nSzeliski 1996).",
  "79": "2.1 Geometric primitives and transformations\n57\nThe other special case where we do not need to know depth to perform inter-camera\nmapping is when the camera is undergoing pure rotation (Section 9.1.3), i.e., when t0 = t1.\nIn this case, we can write\n˜x1 ∼K1R1R−1\n0 K−1\n0 ˜x0 = K1R10K−1\n0 ˜x0,\n(2.72)\nwhich again can be represented with a 3 × 3 homography. If we assume that the calibration\nmatrices have known aspect ratios and centers of projection (2.59), this homography can be\nparameterized by the rotation amount and the two unknown focal lengths. This particular\nformulation is commonly used in image-stitching applications (Section 9.1.3).\nObject-centered projection\nWhen working with long focal length lenses, it often becomes difﬁcult to reliably estimate\nthe focal length from image measurements alone. This is because the focal length and the\ndistance to the object are highly correlated and it becomes difﬁcult to tease these two effects\napart. For example, the change in scale of an object viewed through a zoom telephoto lens\ncan either be due to a zoom change or a motion towards the user. (This effect was put to\ndramatic use in some of Alfred Hitchcock’s ﬁlm Vertigo, where the simultaneous change of\nzoom and camera motion produces a disquieting effect.)\nThis ambiguity becomes clearer if we write out the projection equation corresponding to\nthe simple calibration matrix K (2.59),\nxs\n=\nf rx · p + tx\nrz · p + tz\n+ cx\n(2.73)\nys\n=\nf ry · p + ty\nrz · p + tz\n+ cy,\n(2.74)\nwhere rx, ry, and rz are the three rows of R. If the distance to the object center tz ≫∥p∥\n(the size of the object), the denominator is approximately tz and the overall scale of the\nprojected object depends on the ratio of f to tz. It therefore becomes difﬁcult to disentangle\nthese two quantities.\nTo see this more clearly, let ηz = t−1\nz\nand s = ηzf. We can then re-write the above\nequations as\nxs\n=\ns rx · p + tx\n1 + ηzrz · p + cx\n(2.75)\nys\n=\ns ry · p + ty\n1 + ηzrz · p + cy\n(2.76)\n(Szeliski and Kang 1994; Pighin, Hecker, Lischinski et al. 1998). The scale of the projection\ns can be reliably estimated if we are looking at a known object (i.e., the 3D coordinates p",
  "80": "58\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nare known). The inverse distance ηz is now mostly decoupled from the estimates of s and\ncan be estimated from the amount of foreshortening as the object rotates. Furthermore, as\nthe lens becomes longer, i.e., the projection model becomes orthographic, there is no need to\nreplace a perspective imaging model with an orthographic one, since the same equation can\nbe used, with ηz →0 (as opposed to f and tz both going to inﬁnity). This allows us to form\na natural link between orthographic reconstruction techniques such as factorization and their\nprojective/perspective counterparts (Section 7.3).\n2.1.6 Lens distortions\nThe above imaging models all assume that cameras obey a linear projection model where\nstraight lines in the world result in straight lines in the image. (This follows as a natural\nconsequence of linear matrix operations being applied to homogeneous coordinates.) Unfor-\ntunately, many wide-angle lenses have noticeable radial distortion, which manifests itself as\na visible curvature in the projection of straight lines. (See Section 2.2.3 for a more detailed\ndiscussion of lens optics, including chromatic aberration.) Unless this distortion is taken into\naccount, it becomes impossible to create highly accurate photorealistic reconstructions. For\nexample, image mosaics constructed without taking radial distortion into account will often\nexhibit blurring due to the mis-registration of corresponding features before pixel blending\n(Chapter 9).\nFortunately, compensating for radial distortion is not that difﬁcult in practice. For most\nlenses, a simple quartic model of distortion can produce good results. Let (xc, yc) be the\npixel coordinates obtained after perspective division but before scaling by focal length f and\nshifting by the optical center (cx, cy), i.e.,\nxc\n=\nrx · p + tx\nrz · p + tz\nyc\n=\nry · p + ty\nrz · p + tz\n.\n(2.77)\nThe radial distortion model says that coordinates in the observed images are displaced away\n(barrel distortion) or towards (pincushion distortion) the image center by an amount propor-\ntional to their radial distance (Figure 2.13a–b).3\nThe simplest radial distortion models use\nlow-order polynomials, e.g.,\nˆxc\n=\nxc(1 + κ1r2\nc + κ2r4\nc)\nˆyc\n=\nyc(1 + κ1r2\nc + κ2r4\nc),\n(2.78)\n3 Anamorphic lenses, which are widely used in feature ﬁlm production, do not follow this radial distortion model.\nInstead, they can be thought of, to a ﬁrst approximation, as inducing different vertical and horizontal scalings, i.e.,\nnon-square pixels.",
  "81": "2.1 Geometric primitives and transformations\n59\n(a)\n(b)\n(c)\nFigure 2.13 Radial lens distortions: (a) barrel, (b) pincushion, and (c) ﬁsheye. The ﬁsheye\nimage spans almost 180◦from side-to-side.\nwhere r2\nc = x2\nc + y2\nc and κ1 and κ2 are called the radial distortion parameters.4 After the\nradial distortion step, the ﬁnal pixel coordinates can be computed using\nxs\n=\nfx′\nc + cx\nys\n=\nfy′\nc + cy.\n(2.79)\nA variety of techniques can be used to estimate the radial distortion parameters for a given\nlens, as discussed in Section 6.3.5.\nSometimes the above simpliﬁed model does not model the true distortions produced by\ncomplex lenses accurately enough (especially at very wide angles). A more complete ana-\nlytic model also includes tangential distortions and decentering distortions (Slama 1980), but\nthese distortions are not covered in this book.\nFisheye lenses (Figure 2.13c) require a model that differs from traditional polynomial\nmodels of radial distortion. Fisheye lenses behave, to a ﬁrst approximation, as equi-distance\nprojectors of angles away from the optical axis (Xiong and Turkowski 1997), which is the\nsame as the polar projection described by Equations (9.22–9.24). Xiong and Turkowski\n(1997) describe how this model can be extended with the addition of an extra quadratic cor-\nrection in φ and how the unknown parameters (center of projection, scaling factor s, etc.)\ncan be estimated from a set of overlapping ﬁsheye images using a direct (intensity-based)\nnon-linear minimization algorithm.\nFor even larger, less regular distortions, a parametric distortion model using splines may\nbe necessary (Goshtasby 1989). If the lens does not have a single center of projection, it\n4 Sometimes the relationship between xc and ˆxc is expressed the other way around, i.e., xc = ˆxc(1 + κ1ˆr2\nc +\nκ2ˆr4\nc). This is convenient if we map image pixels into (warped) rays by dividing through by f. We can then undistort\nthe rays and have true 3D rays in space.",
  "82": "60\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmay become necessary to model the 3D line (as opposed to direction) corresponding to each\npixel separately (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Sautot et al.\n1992; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm, Trudeau et\nal. 2009). Some of these techniques are described in more detail in Section 6.3.5, which\ndiscusses how to calibrate lens distortions.\nThere is one subtle issue associated with the simple radial distortion model that is often\nglossed over. We have introduced a non-linearity between the perspective projection and ﬁnal\nsensor array projection steps. Therefore, we cannot, in general, post-multiply an arbitrary 3×\n3 matrix K with a rotation to put it into upper-triangular form and absorb this into the global\nrotation. However, this situation is not as bad as it may at ﬁrst appear. For many applications,\nkeeping the simpliﬁed diagonal form of (2.59) is still an adequate model. Furthermore, if we\ncorrect radial and other distortions to an accuracy where straight lines are preserved, we have\nessentially converted the sensor back into a linear imager and the previous decomposition still\napplies.\n2.2 Photometric image formation\nIn modeling the image formation process, we have described how 3D geometric features in\nthe world are projected into 2D features in an image. However, images are not composed of\n2D features. Instead, they are made up of discrete color or intensity values. Where do these\nvalues come from? How do they relate to the lighting in the environment, surface properties\nand geometry, camera optics, and sensor properties (Figure 2.14)? In this section, we develop\na set of models to describe these interactions and formulate a generative process of image\nformation. A more detailed treatment of these topics can be found in other textbooks on\ncomputer graphics and image synthesis (Glassner 1995; Weyrich, Lawrence, Lensch et al.\n2008; Foley, van Dam, Feiner et al. 1995; Watt 1995; Cohen and Wallace 1993; Sillion and\nPuech 1994).\n2.2.1 Lighting\nImages cannot exist without light. To produce an image, the scene must be illuminated with\none or more light sources. (Certain modalities such as ﬂuorescent microscopy and X-ray\ntomography do not ﬁt this model, but we do not deal with them in this book.) Light sources\ncan generally be divided into point and area light sources.\nA point light source originates at a single location in space (e.g., a small light bulb),\npotentially at inﬁnity (e.g., the sun). (Note that for some applications such as modeling soft\nshadows (penumbras), the sun may have to be treated as an area light source.) In addition to\nits location, a point light source has an intensity and a color spectrum, i.e., a distribution over",
  "83": "2.2 Photometric image formation\n61\nn^\nsurface\nlight \nsource\nimage plane\nsensor \nplane\noptics\nFigure 2.14\nA simpliﬁed model of photometric image formation. Light is emitted by one\nor more light sources and is then reﬂected from an object’s surface. A portion of this light is\ndirected towards the camera. This simpliﬁed model ignores multiple reﬂections, which often\noccur in real-world scenes.\nwavelengths L(λ). The intensity of a light source falls off with the square of the distance\nbetween the source and the object being lit, because the same light is being spread over a\nlarger (spherical) area. A light source may also have a directional falloff (dependence), but\nwe ignore this in our simpliﬁed model.\nArea light sources are more complicated. A simple area light source such as a ﬂuorescent\nceiling light ﬁxture with a diffuser can be modeled as a ﬁnite rectangular area emitting light\nequally in all directions (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995).\nWhen the distribution is strongly directional, a four-dimensional lightﬁeld can be used instead\n(Ashdown 1993).\nA more complex light distribution that approximates, say, the incident illumination on an\nobject sitting in an outdoor courtyard, can often be represented using an environment map\n(Greene 1986) (originally called a reﬂection map (Blinn and Newell 1976)). This representa-\ntion maps incident light directions ˆv to color values (or wavelengths, λ),\nL(ˆv; λ),\n(2.80)\nand is equivalent to assuming that all light sources are at inﬁnity. Environment maps can be\nrepresented as a collection of cubical faces (Greene 1986), as a single longitude–latitude map\n(Blinn and Newell 1976), or as the image of a reﬂecting sphere (Watt 1995). A convenient\nway to get a rough model of a real-world environment map is to take an image of a reﬂective\nmirrored sphere and to unwrap this image onto the desired environment map (Debevec 1998).\nWatt (1995) gives a nice discussion of environment mapping, including the formulas needed\nto map directions to pixels for the three most commonly used representations.",
  "84": "62\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nn^\nvi\ndx\nn^\nvr\n^\ndy^\nθi\nφi\nφr\nθr\n^\n^\n(a)\n(b)\nFigure 2.15\n(a) Light scatters when it hits a surface. (b) The bidirectional reﬂectance\ndistribution function (BRDF) f(θi, φi, θr, φr) is parameterized by the angles that the inci-\ndent, ˆvi, and reﬂected, ˆvr, light ray directions make with the local surface coordinate frame\n( ˆdx, ˆdy, ˆn).\n2.2.2 Reﬂectance and shading\nWhen light hits an object’s surface, it is scattered and reﬂected (Figure 2.15a). Many different\nmodels have been developed to describe this interaction. In this section, we ﬁrst describe the\nmost general form, the bidirectional reﬂectance distribution function, and then look at some\nmore specialized models, including the diffuse, specular, and Phong shading models. We also\ndiscuss how these models can be used to compute the global illumination corresponding to a\nscene.\nThe Bidirectional Reﬂectance Distribution Function (BRDF)\nThe most general model of light scattering is the bidirectional reﬂectance distribution func-\ntion (BRDF).5 Relative to some local coordinate frame on the surface, the BRDF is a four-\ndimensional function that describes how much of each wavelength arriving at an incident\ndirection ˆvi is emitted in a reﬂected direction ˆvr (Figure 2.15b). The function can be written\nin terms of the angles of the incident and reﬂected directions relative to the surface frame as\nfr(θi, φi, θr, φr; λ).\n(2.81)\nThe BRDF is reciprocal, i.e., because of the physics of light transport, you can interchange\nthe roles of ˆvi and ˆvr and still get the same answer (this is sometimes called Helmholtz\nreciprocity).\n5 Actually, even more general models of light transport exist, including some that model spatial variation along\nthe surface, sub-surface scattering, and atmospheric effects—see Section 12.7.1—(Dorsey, Rushmeier, and Sillion\n2007; Weyrich, Lawrence, Lensch et al. 2008).",
  "85": "2.2 Photometric image formation\n63\nMost surfaces are isotropic, i.e., there are no preferred directions on the surface as far\nas light transport is concerned. (The exceptions are anisotropic surfaces such as brushed\n(scratched) aluminum, where the reﬂectance depends on the light orientation relative to the\ndirection of the scratches.) For an isotropic material, we can simplify the BRDF to\nfr(θi, θr, |φr −φi|; λ) or fr(ˆvi, ˆvr, ˆn; λ),\n(2.82)\nsince the quantities θi, θr and φr −φi can be computed from the directions ˆvi, ˆvr, and ˆn.\nTo calculate the amount of light exiting a surface point p in a direction ˆvr under a given\nlighting condition, we integrate the product of the incoming light Li(ˆvi; λ) with the BRDF\n(some authors call this step a convolution). Taking into account the foreshortening factor\ncos+ θi, we obtain\nLr(ˆvr; λ) =\nZ\nLi(ˆvi; λ)fr(ˆvi, ˆvr, ˆn; λ) cos+ θi dˆvi,\n(2.83)\nwhere\ncos+ θi = max(0, cos θi).\n(2.84)\nIf the light sources are discrete (a ﬁnite number of point light sources), we can replace the\nintegral with a summation,\nLr(ˆvr; λ) =\nX\ni\nLi(λ)fr(ˆvi, ˆvr, ˆn; λ) cos+ θi.\n(2.85)\nBRDFs for a given surface can be obtained through physical modeling (Torrance and\nSparrow 1967; Cook and Torrance 1982; Glassner 1995), heuristic modeling (Phong 1975), or\nthrough empirical observation (Ward 1992; Westin, Arvo, and Torrance 1992; Dana, van Gin-\nneken, Nayar et al. 1999; Dorsey, Rushmeier, and Sillion 2007; Weyrich, Lawrence, Lensch\net al. 2008).6 Typical BRDFs can often be split into their diffuse and specular components,\nas described below.\nDiffuse reﬂection\nThe diffuse component (also known as Lambertian or matte reﬂection) scatters light uni-\nformly in all directions and is the phenomenon we most normally associate with shading,\ne.g., the smooth (non-shiny) variation of intensity with surface normal that is seen when ob-\nserving a statue (Figure 2.16). Diffuse reﬂection also often imparts a strong body color to\nthe light since it is caused by selective absorption and re-emission of light inside the object’s\nmaterial (Shafer 1985; Glassner 1995).\n6 See http://www1.cs.columbia.edu/CAVE/software/curet/ for a database of some empirically sampled BRDFs.",
  "86": "64\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 2.16\nThis close-up of a statue shows both diffuse (smooth shading) and specular\n(shiny highlight) reﬂection, as well as darkening in the grooves and creases due to reduced\nlight visibility and interreﬂections. (Photo courtesy of the Caltech Vision Lab, http://www.\nvision.caltech.edu/archive.html.)\nWhile light is scattered uniformly in all directions, i.e., the BRDF is constant,\nfd(ˆvi, ˆvr, ˆn; λ) = fd(λ),\n(2.86)\nthe amount of light depends on the angle between the incident light direction and the surface\nnormal θi. This is because the surface area exposed to a given amount of light becomes larger\nat oblique angles, becoming completely self-shadowed as the outgoing surface normal points\naway from the light (Figure 2.17a). (Think about how you orient yourself towards the sun or\nﬁreplace to get maximum warmth and how a ﬂashlight projected obliquely against a wall is\nless bright than one pointing directly at it.) The shading equation for diffuse reﬂection can\nthus be written as\nLd(ˆvr; λ) =\nX\ni\nLi(λ)fd(λ) cos+ θi =\nX\ni\nLi(λ)fd(λ)[ˆvi · ˆn]+,\n(2.87)\nwhere\n[ˆvi · ˆn]+ = max(0, ˆvi · ˆn).\n(2.88)\nSpecular reﬂection\nThe second major component of a typical BRDF is specular (gloss or highlight) reﬂection,\nwhich depends strongly on the direction of the outgoing light. Consider light reﬂecting off a\nmirrored surface (Figure 2.17b). Incident light rays are reﬂected in a direction that is rotated\nby 180◦around the surface normal ˆn. Using the same notation as in Equations (2.29–2.30),",
  "87": "2.2 Photometric image formation\n65\nvi•n = 1\n^  ^\n0 < vi•n < 1\n^  ^\nvi•n < 0\n^  ^\nvi•n = 0\n^  ^\nvi\nv┴\nn^\nv║\n-v┴\nsi\n180°\nv║\n^\n^\n(a)\n(b)\nFigure 2.17 (a) The diminution of returned light caused by foreshortening depends on ˆvi·ˆn,\nthe cosine of the angle between the incident light direction ˆvi and the surface normal ˆn. (b)\nMirror (specular) reﬂection: The incident light ray direction ˆvi is reﬂected onto the specular\ndirection ˆsi around the surface normal ˆn.\nwe can compute the specular reﬂection direction ˆsi as\nˆsi = v∥−v⊥= (2ˆnˆnT −I)vi.\n(2.89)\nThe amount of light reﬂected in a given direction ˆvr thus depends on the angle θs =\ncos−1(ˆvr · ˆsi) between the view direction ˆvr and the specular direction ˆsi. For example, the\nPhong (1975) model uses a power of the cosine of the angle,\nfs(θs; λ) = ks(λ) coske θs,\n(2.90)\nwhile the Torrance and Sparrow (1967) micro-facet model uses a Gaussian,\nfs(θs; λ) = ks(λ) exp(−c2\nsθ2\ns).\n(2.91)\nLarger exponents ke (or inverse Gaussian widths cs) correspond to more specular surfaces\nwith distinct highlights, while smaller exponents better model materials with softer gloss.\nPhong shading\nPhong (1975) combined the diffuse and specular components of reﬂection with another term,\nwhich he called the ambient illumination. This term accounts for the fact that objects are\ngenerally illuminated not only by point light sources but also by a general diffuse illumination\ncorresponding to inter-reﬂection (e.g., the walls in a room) or distant sources, such as the",
  "88": "66\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n-90\n-80\n-70\n-60\n-50\n-40\n-30\n-20\n-10\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nAmbient\nDiffuse\nExp=10\nExp=100\nExp=1000\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAmbient\nDiffuse\nExp=10\nExp=100\nExp=1000\n(a)\n(b)\nFigure 2.18 Cross-section through a Phong shading model BRDF for a ﬁxed incident illu-\nmination direction: (a) component values as a function of angle away from surface normal;\n(b) polar plot. The value of the Phong exponent ke is indicated by the “Exp” labels and the\nlight source is at an angle of 30◦away from the normal.\nblue sky. In the Phong model, the ambient term does not depend on surface orientation, but\ndepends on the color of both the ambient illumination La(λ) and the object ka(λ),\nfa(λ) = ka(λ)La(λ).\n(2.92)\nPutting all of these terms together, we arrive at the Phong shading model,\nLr(ˆvr; λ) = ka(λ)La(λ) + kd(λ)\nX\ni\nLi(λ)[ˆvi · ˆn]+ + ks(λ)\nX\ni\nLi(λ)(ˆvr · ˆsi)ke. (2.93)\nFigure 2.18 shows a typical set of Phong shading model components as a function of the\nangle away from the surface normal (in a plane containing both the lighting direction and the\nviewer).\nTypically, the ambient and diffuse reﬂection color distributions ka(λ) and kd(λ) are the\nsame, since they are both due to sub-surface scattering (body reﬂection) inside the surface\nmaterial (Shafer 1985). The specular reﬂection distribution ks(λ) is often uniform (white),\nsince it is caused by interface reﬂections that do not change the light color. (The exception\nto this are metallic materials, such as copper, as opposed to the more common dielectric\nmaterials, such as plastics.)\nThe ambient illumination La(λ) often has a different color cast from the direct light\nsources Li(λ), e.g., it may be blue for a sunny outdoor scene or yellow for an interior lit\nwith candles or incandescent lights. (The presence of ambient sky illumination in shadowed\nareas is what often causes shadows to appear bluer than the corresponding lit portions of a\nscene). Note also that the diffuse component of the Phong model (or of any shading model)\ndepends on the angle of the incoming light source ˆvi, while the specular component depends\non the relative angle between the viewer vr and the specular reﬂection direction ˆsi (which\nitself depends on the incoming light direction ˆvi and the surface normal ˆn).",
  "89": "2.2 Photometric image formation\n67\nThe Phong shading model has been superseded in terms of physical accuracy by a number\nof more recently developed models in computer graphics, including the model developed by\nCook and Torrance (1982) based on the original micro-facet model of Torrance and Sparrow\n(1967). Until recently, most computer graphics hardware implemented the Phong model but\nthe recent advent of programmable pixel shaders makes the use of more complex models\nfeasible.\nDi-chromatic reﬂection model\nThe Torrance and Sparrow (1967) model of reﬂection also forms the basis of Shafer’s (1985)\ndi-chromatic reﬂection model, which states that the apparent color of a uniform material lit\nfrom a single source depends on the sum of two terms,\nLr(ˆvr; λ)\n=\nLi(ˆvr, ˆvi, ˆn; λ) + Lb(ˆvr, ˆvi, ˆn; λ)\n(2.94)\n=\nci(λ)mi(ˆvr, ˆvi, ˆn) + cb(λ)mb(ˆvr, ˆvi, ˆn),\n(2.95)\ni.e., the radiance of the light reﬂected at the interface, Li, and the radiance reﬂected at the sur-\nface body, Lb. Each of these, in turn, is a simple product between a relative power spectrum\nc(λ), which depends only on wavelength, and a magnitude m(ˆvr, ˆvi, ˆn), which depends only\non geometry. (This model can easily be derived from a generalized version of Phong’s model\nby assuming a single light source and no ambient illumination, and re-arranging terms.) The\ndi-chromatic model has been successfully used in computer vision to segment specular col-\nored objects with large variations in shading (Klinker 1993) and more recently has inspired\nlocal two-color models for applications such Bayer pattern demosaicing (Bennett, Uytten-\ndaele, Zitnick et al. 2006).\nGlobal illumination (ray tracing and radiosity)\nThe simple shading model presented thus far assumes that light rays leave the light sources,\nbounce off surfaces visible to the camera, thereby changing in intensity or color, and arrive\nat the camera. In reality, light sources can be shadowed by occluders and rays can bounce\nmultiple times around a scene while making their trip from a light source to the camera.\nTwo methods have traditionally been used to model such effects. If the scene is mostly\nspecular (the classic example being scenes made of glass objects and mirrored or highly pol-\nished balls), the preferred approach is ray tracing or path tracing (Glassner 1995; Akenine-\nM¨oller and Haines 2002; Shirley 2005), which follows individual rays from the camera across\nmultiple bounces towards the light sources (or vice versa). If the scene is composed mostly\nof uniform albedo simple geometry illuminators and surfaces, radiosity (global illumination)\ntechniques are preferred (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995).",
  "90": "68\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nCombinations of the two techniques have also been developed (Wallace, Cohen, and Green-\nberg 1987), as well as more general light transport techniques for simulating effects such as\nthe caustics cast by rippling water.\nThe basic ray tracing algorithm associates a light ray with each pixel in the camera im-\nage and ﬁnds its intersection with the nearest surface. A primary contribution can then be\ncomputed using the simple shading equations presented previously (e.g., Equation (2.93))\nfor all light sources that are visible for that surface element. (An alternative technique for\ncomputing which surfaces are illuminated by a light source is to compute a shadow map,\nor shadow buffer, i.e., a rendering of the scene from the light source’s perspective, and then\ncompare the depth of pixels being rendered with the map (Williams 1983; Akenine-M¨oller\nand Haines 2002).) Additional secondary rays can then be cast along the specular direction\ntowards other objects in the scene, keeping track of any attenuation or color change that the\nspecular reﬂection induces.\nRadiosity works by associating lightness values with rectangular surface areas in the scene\n(including area light sources). The amount of light interchanged between any two (mutually\nvisible) areas in the scene can be captured as a form factor, which depends on their relative\norientation and surface reﬂectance properties, as well as the 1/r2 fall-off as light is distributed\nover a larger effective sphere the further away it is (Cohen and Wallace 1993; Sillion and\nPuech 1994; Glassner 1995). A large linear system can then be set up to solve for the ﬁnal\nlightness of each area patch, using the light sources as the forcing function (right hand side).\nOnce the system has been solved, the scene can be rendered from any desired point of view.\nUnder certain circumstances, it is possible to recover the global illumination in a scene from\nphotographs using computer vision techniques (Yu, Debevec, Malik et al. 1999).\nThe basic radiosity algorithm does not take into account certain near ﬁeld effects, such\nas the darkening inside corners and scratches, or the limited ambient illumination caused\nby partial shadowing from other surfaces. Such effects have been exploited in a number of\ncomputer vision algorithms (Nayar, Ikeuchi, and Kanade 1991; Langer and Zucker 1994).\nWhile all of these global illumination effects can have a strong effect on the appearance\nof a scene, and hence its 3D interpretation, they are not covered in more detail in this book.\n(But see Section 12.7.1 for a discussion of recovering BRDFs from real scenes and objects.)\n2.2.3 Optics\nOnce the light from a scene reaches the camera, it must still pass through the lens before\nreaching the sensor (analog ﬁlm or digital silicon). For many applications, it sufﬁces to\ntreat the lens as an ideal pinhole that simply projects all rays through a common center of\nprojection (Figures 2.8 and 2.9).\nHowever, if we want to deal with issues such as focus, exposure, vignetting, and aber-",
  "91": "2.2 Photometric image formation\n69\nzi=102 mm\nf = 100 mm\nW=35mm\nzo=5 m\nf.o.v.\nc\nΔzi\nP\nd\nFigure 2.19\nA thin lens of focal length f focuses the light from a plane a distance zo in front\nof the lens at a distance zi behind the lens, where\n1\nzo + 1\nzi = 1\nf . If the focal plane (vertical\ngray line next to c) is moved forward, the images are no longer in focus and the circle of\nconfusion c (small thick line segments) depends on the distance of the image plane motion\n∆zi relative to the lens aperture diameter d. The ﬁeld of view (f.o.v.) depends on the ratio\nbetween the sensor width W and the focal length f (or, more precisely, the focusing distance\nzi, which is usually quite close to f).\nration, we need to develop a more sophisticated model, which is where the study of optics\ncomes in (M¨oller 1988; Hecht 2001; Ray 2002).\nFigure 2.19 shows a diagram of the most basic lens model, i.e., the thin lens composed\nof a single piece of glass with very low, equal curvature on both sides. According to the\nlens law (which can be derived using simple geometric arguments on light ray refraction), the\nrelationship between the distance to an object zo and the distance behind the lens at which a\nfocused image is formed zi can be expressed as\n1\nzo\n+ 1\nzi\n= 1\nf ,\n(2.96)\nwhere f is called the focal length of the lens. If we let zo →∞, i.e., we adjust the lens (move\nthe image plane) so that objects at inﬁnity are in focus, we get zi = f, which is why we can\nthink of a lens of focal length f as being equivalent (to a ﬁrst approximation) to a pinhole a\ndistance f from the focal plane (Figure 2.10), whose ﬁeld of view is given by (2.60).\nIf the focal plane is moved away from its proper in-focus setting of zi (e.g., by twisting\nthe focus ring on the lens), objects at zo are no longer in focus, as shown by the gray plane in\nFigure 2.19. The amount of mis-focus is measured by the circle of confusion c (shown as short\nthick blue line segments on the gray plane).7 The equation for the circle of confusion can be\nderived using similar triangles; it depends on the distance of travel in the focal plane ∆zi\nrelative to the original focus distance zi and the diameter of the aperture d (see Exercise 2.4).\n7 If the aperture is not completely circular, e.g., if it is caused by a hexagonal diaphragm, it is sometimes possible\nto see this effect in the actual blur function (Levin, Fergus, Durand et al. 2007; Joshi, Szeliski, and Kriegman 2008)\nor in the “glints” that are seen when shooting into the sun.",
  "92": "70\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 2.20 Regular and zoom lens depth of ﬁeld indicators.\nThe allowable depth variation in the scene that limits the circle of confusion to an accept-\nable number is commonly called the depth of ﬁeld and is a function of both the focus distance\nand the aperture, as shown diagrammatically by many lens markings (Figure 2.20). Since this\ndepth of ﬁeld depends on the aperture diameter d, we also have to know how this varies with\nthe commonly displayed f-number, which is usually denoted as f/# or N and is deﬁned as\nf/# = N = f\nd ,\n(2.97)\nwhere the focal length f and the aperture diameter d are measured in the same unit (say,\nmillimeters).\nThe usual way to write the f-number is to replace the # in f/# with the actual number,\ni.e., f/1.4, f/2, f/2.8, . . . , f/22. (Alternatively, we can say N = 1.4, etc.) An easy way to\ninterpret these numbers is to notice that dividing the focal length by the f-number gives us the\ndiameter d, so these are just formulas for the aperture diameter.8\nNotice that the usual progression for f-numbers is in full stops, which are multiples of\n√\n2,\nsince this corresponds to doubling the area of the entrance pupil each time a smaller f-number\nis selected. (This doubling is also called changing the exposure by one exposure value or EV.\nIt has the same effect on the amount of light reaching the sensor as doubling the exposure\nduration, e.g., from 1/125 to 1/250, see Exercise 2.5.)\nNow that you know how to convert between f-numbers and aperture diameters, you can\nconstruct your own plots for the depth of ﬁeld as a function of focal length f, circle of\nconfusion c, and focus distance zo, as explained in Exercise 2.4 and see how well these match\nwhat you observe on actual lenses, such as those shown in Figure 2.20.\nOf course, real lenses are not inﬁnitely thin and therefore suffer from geometric aber-\nrations, unless compound elements are used to correct for them. The classic ﬁve Seidel\naberrations, which arise when using third-order optics, include spherical aberration, coma,\nastigmatism, curvature of ﬁeld, and distortion (M¨oller 1988; Hecht 2001; Ray 2002).\n8 This also explains why, with zoom lenses, the f-number varies with the current zoom (focal length) setting.",
  "93": "2.2 Photometric image formation\n71\nzi’=103mm\nf’ = 101mm\nzo=5m\nP\nd\nc\nFigure 2.21\nIn a lens subject to chromatic aberration, light at different wavelengths (e.g.,\nthe red and blur arrows) is focused with a different focal length f ′ and hence a different depth\nz′\ni, resulting in both a geometric (in-plane) displacement and a loss of focus.\nChromatic aberration\nBecause the index of refraction for glass varies slightly as a function of wavelength, sim-\nple lenses suffer from chromatic aberration, which is the tendency for light of different\ncolors to focus at slightly different distances (and hence also with slightly different mag-\nniﬁcation factors), as shown in Figure 2.21. The wavelength-dependent magniﬁcation fac-\ntor, i.e., the transverse chromatic aberration, can be modeled as a per-color radial distortion\n(Section 2.1.6) and, hence, calibrated using the techniques described in Section 6.3.5. The\nwavelength-dependent blur caused by longitudinal chromatic aberration can be calibrated\nusing techniques described in Section 10.1.4. Unfortunately, the blur induced by longitudinal\naberration can be harder to undo, as higher frequencies can get strongly attenuated and hence\nhard to recover.\nIn order to reduce chromatic and other kinds of aberrations, most photographic lenses\ntoday are compound lenses made of different glass elements (with different coatings). Such\nlenses can no longer be modeled as having a single nodal point P through which all of the\nrays must pass (when approximating the lens with a pinhole model). Instead, these lenses\nhave both a front nodal point, through which the rays enter the lens, and a rear nodal point,\nthrough which they leave on their way to the sensor. In practice, only the location of the front\nnodal point is of interest when performing careful camera calibration, e.g., when determining\nthe point around which to rotate to capture a parallax-free panorama (see Section 9.1.3).\nNot all lenses, however, can be modeled as having a single nodal point. In particular, very\nwide-angle lenses such as ﬁsheye lenses (Section 2.1.6) and certain catadioptric imaging\nsystems consisting of lenses and curved mirrors (Baker and Nayar 1999) do not have a single\npoint through which all of the acquired light rays pass. In such cases, it is preferable to\nexplicitly construct a mapping function (look-up table) between pixel coordinates and 3D\nrays in space (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Sautot et al.",
  "94": "72\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nzi=102mm\nf = 100mm\nzo=5m\nδi\nd\nδo\nα\nα\nα\nP\nJ\nI\nO\nQ\nro\nFigure 2.22\nThe amount of light hitting a pixel of surface area δi depends on the square of\nthe ratio of the aperture diameter d to the focal length f, as well as the fourth power of the\noff-axis angle α cosine, cos4 α.\n1992; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm, Trudeau et\nal. 2009), as mentioned in Section 2.1.6.\nVignetting\nAnother property of real-world lenses is vignetting, which is the tendency for the brightness\nof the image to fall off towards the edge of the image.\nTwo kinds of phenomena usually contribute to this effect (Ray 2002). The ﬁrst is called\nnatural vignetting and is due to the foreshortening in the object surface, projected pixel, and\nlens aperture, as shown in Figure 2.22. Consider the light leaving the object surface patch\nof size δo located at an off-axis angle α. Because this patch is foreshortened with respect\nto the camera lens, the amount of light reaching the lens is reduced by a factor cos α. The\namount of light reaching the lens is also subject to the usual 1/r2 fall-off; in this case, the\ndistance ro = zo/ cos α. The actual area of the aperture through which the light passes\nis foreshortened by an additional factor cos α, i.e., the aperture as seen from point O is an\nellipse of dimensions d×d cos α. Putting all of these factors together, we see that the amount\nof light leaving O and passing through the aperture on its way to the image pixel located at I\nis proportional to\nδo cos α\nr2o\nπ\n\u0012d\n2\n\u00132\ncos α = δoπ\n4\nd2\nz2o\ncos4 α.\n(2.98)\nSince triangles ∆OPQ and ∆IPJ are similar, the projected areas of of the object surface δo\nand image pixel δi are in the same (squared) ratio as zo : zi,\nδo\nδi = z2\no\nz2\ni\n.\n(2.99)\nPutting these together, we obtain the ﬁnal relationship between the amount of light reaching",
  "95": "2.3 The digital camera\n73\npixel i and the aperture diameter d, the focusing distance zi ≈f, and the off-axis angle α,\nδoπ\n4\nd2\nz2o\ncos4 α = δiπ\n4\nd2\nz2\ni\ncos4 α ≈δiπ\n4\n\u0012 d\nf\n\u00132\ncos4 α,\n(2.100)\nwhich is called the fundamental radiometric relation between the scene radiance L and the\nlight (irradiance) E reaching the pixel sensor,\nE = Lπ\n4\n\u0012 d\nf\n\u00132\ncos4 α,\n(2.101)\n(Horn 1986; Nalwa 1993; Hecht 2001; Ray 2002). Notice in this equation how the amount of\nlight depends on the pixel surface area (which is why the smaller sensors in point-and-shoot\ncameras are so much noisier than digital single lens reﬂex (SLR) cameras), the inverse square\nof the f-stop N = f/d (2.97), and the fourth power of the cos4 α off-axis fall-off, which is\nthe natural vignetting term.\nThe other major kind of vignetting, called mechanical vignetting, is caused by the internal\nocclusion of rays near the periphery of lens elements in a compound lens, and cannot easily\nbe described mathematically without performing a full ray-tracing of the actual lens design.9\nHowever, unlike natural vignetting, mechanical vignetting can be decreased by reducing the\ncamera aperture (increasing the f-number). It can also be calibrated (along with natural vi-\ngnetting) using special devices such as integrating spheres, uniformly illuminated targets, or\ncamera rotation, as discussed in Section 10.1.3.\n2.3 The digital camera\nAfter starting from one or more light sources, reﬂecting off one or more surfaces in the world,\nand passing through the camera’s optics (lenses), light ﬁnally reaches the imaging sensor.\nHow are the photons arriving at this sensor converted into the digital (R, G, B) values that\nwe observe when we look at a digital image? In this section, we develop a simple model\nthat accounts for the most important effects such as exposure (gain and shutter speed), non-\nlinear mappings, sampling and aliasing, and noise. Figure 2.23, which is based on camera\nmodels developed by Healey and Kondepudy (1994); Tsin, Ramesh, and Kanade (2001); Liu,\nSzeliski, Kang et al. (2008), shows a simple version of the processing stages that occur in\nmodern digital cameras. Chakrabarti, Scharstein, and Zickler (2009) developed a sophisti-\ncated 24-parameter model that is an even better match to the processing performed in today’s\ncameras.\n9 There are some empirical models that work well in practice (Kang and Weiss 2000; Zheng, Lin, and Kang\n2006).",
  "96": "74\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 2.23 Image sensing pipeline, showing the various sources of noise as well as typical\ndigital post-processing steps.\nLight falling on an imaging sensor is usually picked up by an active sensing area, inte-\ngrated for the duration of the exposure (usually expressed as the shutter speed in a fraction of\na second, e.g.,\n1\n125,\n1\n60,\n1\n30), and then passed to a set of sense ampliﬁers . The two main kinds\nof sensor used in digital still and video cameras today are charge-coupled device (CCD) and\ncomplementary metal oxide on silicon (CMOS).\nIn a CCD, photons are accumulated in each active well during the exposure time. Then,\nin a transfer phase, the charges are transferred from well to well in a kind of “bucket brigade”\nuntil they are deposited at the sense ampliﬁers, which amplify the signal and pass it to\nan analog-to-digital converter (ADC).10 Older CCD sensors were prone to blooming, when\ncharges from one over-exposed pixel spilled into adjacent ones, but most newer CCDs have\nanti-blooming technology (“troughs” into which the excess charge can spill).\nIn CMOS, the photons hitting the sensor directly affect the conductivity (or gain) of a\nphotodetector, which can be selectively gated to control exposure duration, and locally am-\npliﬁed before being read out using a multiplexing scheme.\nTraditionally, CCD sensors\noutperformed CMOS in quality sensitive applications, such as digital SLRs, while CMOS\nwas better for low-power applications, but today CMOS is used in most digital cameras.\nThe main factors affecting the performance of a digital image sensor are the shutter speed,\nsampling pitch, ﬁll factor, chip size, analog gain, sensor noise, and the resolution (and quality)\n10 In digital still cameras, a complete frame is captured and then read out sequentially at once. However, if video\nis being captured, a rolling shutter, which exposes and transfers each line separately, is often used. In older video\ncameras, the even ﬁelds (lines) were scanned ﬁrst, followed by the odd ﬁelds, in a process that is called interlacing.",
  "97": "2.3 The digital camera\n75\nof the analog-to-digital converter. Many of the actual values for these parameters can be read\nfrom the EXIF tags embedded with digital images. while others can be obtained from the\ncamera manufacturers’ speciﬁcation sheets or from camera review or calibration Web sites.11\nShutter speed.\nThe shutter speed (exposure time) directly controls the amount of light\nreaching the sensor and, hence, determines if images are under- or over-exposed. (For bright\nscenes, where a large aperture or slow shutter speed are desired to get a shallow depth of ﬁeld\nor motion blur, neutral density ﬁlters are sometimes used by photographers.) For dynamic\nscenes, the shutter speed also determines the amount of motion blur in the resulting picture.\nUsually, a higher shutter speed (less motion blur) makes subsequent analysis easier (see Sec-\ntion 10.3 for techniques to remove such blur). However, when video is being captured for\ndisplay, some motion blur may be desirable to avoid stroboscopic effects.\nSampling pitch.\nThe sampling pitch is the physical spacing between adjacent sensor cells\non the imaging chip. A sensor with a smaller sampling pitch has a higher sampling density and\nhence provides a higher resolution (in terms of pixels) for a given active chip area. However,\na smaller pitch also means that each sensor has a smaller area and cannot accumulate as many\nphotons; this makes it not as light sensitive and more prone to noise.\nFill factor.\nThe ﬁll factor is the active sensing area size as a fraction of the theoretically\navailable sensing area (the product of the horizontal and vertical sampling pitches). Higher\nﬁll factors are usually preferable, as they result in more light capture and less aliasing (see\nSection 2.3.1). However, this must be balanced with the need to place additional electronics\nbetween the active sense areas.\nThe ﬁll factor of a camera can be determined empirically\nusing a photometric camera calibration process (see Section 10.1.4).\nChip size.\nVideo and point-and-shoot cameras have traditionally used small chip areas ( 1\n4-\ninch to 1\n2-inch sensors12), while digital SLR cameras try to come closer to the traditional size\nof a 35mm ﬁlm frame.13 When overall device size is not important, having a larger chip\nsize is preferable, since each sensor cell can be more photo-sensitive. (For compact cameras,\na smaller chip means that all of the optics can be shrunk down proportionately.) However,\n11 http://www.clarkvision.com/imagedetail/digital.sensor.performance.summary/ .\n12 These numbers refer to the “tube diameter” of the old vidicon tubes used in video cameras (http://www.\ndpreview.com/learn/?/Glossary/Camera System/sensor sizes 01.htm). The 1/2.5” sensor on the Canon SD800 cam-\nera actually measures 5.76mm × 4.29mm, i.e., a sixth of the size (on side) of a 35mm full-frame (36mm × 24mm)\nDSLR sensor.\n13 When a DSLR chip does not ﬁll the 35mm full frame, it results in a multiplier effect on the lens focal length.\nFor example, a chip that is only 0.6 the dimension of a 35mm frame will make a 50mm lens image the same angular\nextent as a 50/0.6 = 50 × 1.6 =80mm lens, as demonstrated in (2.60).",
  "98": "76\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nlarger chips are more expensive to produce, not only because fewer chips can be packed into\neach wafer, but also because the probability of a chip defect goes up linearly with the chip\narea.\nAnalog gain.\nBefore analog-to-digital conversion, the sensed signal is usually boosted by\na sense ampliﬁer. In video cameras, the gain on these ampliﬁers was traditionally controlled\nby automatic gain control (AGC) logic, which would adjust these values to obtain a good\noverall exposure. In newer digital still cameras, the user now has some additional control\nover this gain through the ISO setting, which is typically expressed in ISO standard units\nsuch as 100, 200, or 400. Since the automated exposure control in most cameras also adjusts\nthe aperture and shutter speed, setting the ISO manually removes one degree of freedom from\nthe camera’s control, just as manually specifying aperture and shutter speed does. In theory, a\nhigher gain allows the camera to perform better under low light conditions (less motion blur\ndue to long exposure times when the aperture is already maxed out). In practice, however,\nhigher ISO settings usually amplify the sensor noise.\nSensor noise.\nThroughout the whole sensing process, noise is added from various sources,\nwhich may include ﬁxed pattern noise, dark current noise, shot noise, ampliﬁer noise and\nquantization noise (Healey and Kondepudy 1994; Tsin, Ramesh, and Kanade 2001). The\nﬁnal amount of noise present in a sampled image depends on all of these quantities, as well\nas the incoming light (controlled by the scene radiance and aperture), the exposure time, and\nthe sensor gain. Also, for low light conditions where the noise is due to low photon counts, a\nPoisson model of noise may be more appropriate than a Gaussian model.\nAs discussed in more detail in Section 10.1.1, Liu, Szeliski, Kang et al. (2008) use this\nmodel, along with an empirical database of camera response functions (CRFs) obtained by\nGrossberg and Nayar (2004), to estimate the noise level function (NLF) for a given image,\nwhich predicts the overall noise variance at a given pixel as a function of its brightness (a\nseparate NLF is estimated for each color channel). An alternative approach, when you have\naccess to the camera before taking pictures, is to pre-calibrate the NLF by taking repeated\nshots of a scene containing a variety of colors and luminances, such as the Macbeth Color\nChart shown in Figure 10.3b (McCamy, Marcus, and Davidson 1976). (When estimating\nthe variance, be sure to throw away or downweight pixels with large gradients, as small\nshifts between exposures will affect the sensed values at such pixels.) Unfortunately, the pre-\ncalibration process may have to be repeated for different exposure times and gain settings\nbecause of the complex interactions occurring within the sensing system.\nIn practice, most computer vision algorithms, such as image denoising, edge detection,\nand stereo matching, all beneﬁt from at least a rudimentary estimate of the noise level. Barring\nthe ability to pre-calibrate the camera or to take repeated shots of the same scene, the simplest",
  "99": "2.3 The digital camera\n77\napproach is to look for regions of near-constant value and to estimate the noise variance in\nsuch regions (Liu, Szeliski, Kang et al. 2008).\nADC resolution.\nThe ﬁnal step in the analog processing chain occurring within an imaging\nsensor is the analog to digital conversion (ADC). While a variety of techniques can be used\nto implement this process, the two quantities of interest are the resolution of this process\n(how many bits it yields) and its noise level (how many of these bits are useful in practice).\nFor most cameras, the number of bits quoted (eight bits for compressed JPEG images and a\nnominal 16 bits for the RAW formats provided by some DSLRs) exceeds the actual number\nof usable bits. The best way to tell is to simply calibrate the noise of a given sensor, e.g.,\nby taking repeated shots of the same scene and plotting the estimated noise as a function of\nbrightness (Exercise 2.6).\nDigital post-processing.\nOnce the irradiance values arriving at the sensor have been con-\nverted to digital bits, most cameras perform a variety of digital signal processing (DSP)\noperations to enhance the image before compressing and storing the pixel values. These in-\nclude color ﬁlter array (CFA) demosaicing, white point setting, and mapping of the luminance\nvalues through a gamma function to increase the perceived dynamic range of the signal. We\ncover these topics in Section 2.3.2 but, before we do, we return to the topic of aliasing, which\nwas mentioned in connection with sensor array ﬁll factors.\n2.3.1 Sampling and aliasing\nWhat happens when a ﬁeld of light impinging on the image sensor falls onto the active sense\nareas in the imaging chip? The photons arriving at each active cell are integrated and then\ndigitized. However, if the ﬁll factor on the chip is small and the signal is not otherwise\nband-limited, visually unpleasing aliasing can occur.\nTo explore the phenomenon of aliasing, let us ﬁrst look at a one-dimensional signal (Fig-\nure 2.24), in which we have two sine waves, one at a frequency of f = 3/4 and the other at\nf = 5/4. If we sample these two signals at a frequency of f = 2, we see that they produce\nthe same samples (shown in black), and so we say that they are aliased.14 Why is this a bad\neffect? In essence, we can no longer reconstruct the original signal, since we do not know\nwhich of the two original frequencies was present.\nIn fact, Shannon’s Sampling Theorem shows that the minimum sampling (Oppenheim\nand Schafer 1996; Oppenheim, Schafer, and Buck 1999) rate required to reconstruct a signal\n14 An alias is an alternate name for someone, so the sampled signal corresponds to two different aliases.",
  "100": "78\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n*\nf = 3/4\nf = 5/4\n=\nFigure 2.24\nAliasing of a one-dimensional signal: The blue sine wave at f = 3/4 and the\nred sine wave at f = 5/4 have the same digital samples, when sampled at f = 2. Even after\nconvolution with a 100% ﬁll factor box ﬁlter, the two signals, while no longer of the same\nmagnitude, are still aliased in the sense that the sampled red signal looks like an inverted\nlower magnitude version of the blue signal. (The image on the right is scaled up for better\nvisibility. The actual sine magnitudes are 30% and −18% of their original values.)\nfrom its instantaneous samples must be at least twice the highest frequency,15\nfs ≥2fmax.\n(2.102)\nThe maximum frequency in a signal is known as the Nyquist frequency and the inverse of the\nminimum sampling frequency rs = 1/fs is known as the Nyquist rate.\nHowever, you may ask, since an imaging chip actually averages the light ﬁeld over a\nﬁnite area, are the results on point sampling still applicable? Averaging over the sensor area\ndoes tend to attenuate some of the higher frequencies. However, even if the ﬁll factor is\n100%, as in the right image of Figure 2.24, frequencies above the Nyquist limit (half the\nsampling frequency) still produce an aliased signal, although with a smaller magnitude than\nthe corresponding band-limited signals.\nA more convincing argument as to why aliasing is bad can be seen by downsampling\na signal using a poor quality ﬁlter such as a box (square) ﬁlter. Figure 2.25 shows a high-\nfrequency chirp image (so called because the frequencies increase over time), along with the\nresults of sampling it with a 25% ﬁll-factor area sensor, a 100% ﬁll-factor sensor, and a high-\nquality 9-tap ﬁlter. Additional examples of downsampling (decimation) ﬁlters can be found\nin Section 3.5.2 and Figure 3.30.\nThe best way to predict the amount of aliasing that an imaging system (or even an image\nprocessing algorithm) will produce is to estimate the point spread function (PSF), which\nrepresents the response of a particular pixel sensor to an ideal point light source. The PSF\nis a combination (convolution) of the blur induced by the optical system (lens) and the ﬁnite\nintegration area of a chip sensor.16\n15 The actual theorem states that fs must be at least twice the signal bandwidth but, since we are not dealing with\nmodulated signals such as radio waves during image capture, the maximum frequency sufﬁces.\n16 Imaging chips usually interpose an optical anti-aliasing ﬁlter just before the imaging chip to reduce or control\nthe amount of aliasing.",
  "101": "2.3 The digital camera\n79\n(a)\n(b)\n(c)\n(d)\nFigure 2.25\nAliasing of a two-dimensional signal: (a) original full-resolution image; (b)\ndownsampled 4× with a 25% ﬁll factor box ﬁlter; (c) downsampled 4× with a 100% ﬁll\nfactor box ﬁlter; (d) downsampled 4× with a high-quality 9-tap ﬁlter. Notice how the higher\nfrequencies are aliased into visible frequencies with the lower quality ﬁlters, while the 9-tap\nﬁlter completely removes these higher frequencies.\nIf we know the blur function of the lens and the ﬁll factor (sensor area shape and spacing)\nfor the imaging chip (plus, optionally, the response of the anti-aliasing ﬁlter), we can convolve\nthese (as described in Section 3.2) to obtain the PSF. Figure 2.26a shows the one-dimensional\ncross-section of a PSF for a lens whose blur function is assumed to be a disc of a radius\nequal to the pixel spacing s plus a sensing chip whose horizontal ﬁll factor is 80%. Taking\nthe Fourier transform of this PSF (Section 3.4), we obtain the modulation transfer function\n(MTF), from which we can estimate the amount of aliasing as the area of the Fourier magni-\ntude outside the f ≤fs Nyquist frequency.17 If we de-focus the lens so that the blur function\nhas a radius of 2s (Figure 2.26c), we see that the amount of aliasing decreases signiﬁcantly,\nbut so does the amount of image detail (frequencies closer to f = fs).\nUnder laboratory conditions, the PSF can be estimated (to pixel precision) by looking at a\npoint light source such as a pin hole in a black piece of cardboard lit from behind. However,\nthis PSF (the actual image of the pin hole) is only accurate to a pixel resolution and, while\nit can model larger blur (such as blur caused by defocus), it cannot model the sub-pixel\nshape of the PSF and predict the amount of aliasing. An alternative technique, described in\nSection 10.1.4, is to look at a calibration pattern (e.g., one consisting of slanted step edges\n(Reichenbach, Park, and Narayanswamy 1991; Williams and Burns 2001; Joshi, Szeliski, and\nKriegman 2008)) whose ideal appearance can be re-synthesized to sub-pixel precision.\nIn addition to occurring during image acquisition, aliasing can also be introduced in var-\nious image processing operations, such as resampling, upsampling, and downsampling. Sec-\ntions 3.4 and 3.5.2 discuss these issues and show how careful selection of ﬁlters can reduce\n17 The complex Fourier transform of the PSF is actually called the optical transfer function (OTF) (Williams\n1999). Its magnitude is called the modulation transfer function (MTF) and its phase is called the phase transfer\nfunction (PTF).",
  "102": "80\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n(a)\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n(c)\n(d)\nFigure 2.26\nSample point spread functions (PSF): The diameter of the blur disc (blue) in\n(a) is equal to half the pixel spacing, while the diameter in (c) is twice the pixel spacing. The\nhorizontal ﬁll factor of the sensing chip is 80% and is shown in brown. The convolution of\nthese two kernels gives the point spread function, shown in green. The Fourier response of\nthe PSF (the MTF) is plotted in (b) and (d). The area above the Nyquist frequency where\naliasing occurs is shown in red.\nthe amount of aliasing that operations inject.\n2.3.2 Color\nIn Section 2.2, we saw how lighting and surface reﬂections are functions of wavelength.\nWhen the incoming light hits the imaging sensor, light from different parts of the spectrum is\nsomehow integrated into the discrete red, green, and blue (RGB) color values that we see in\na digital image. How does this process work and how can we analyze and manipulate color\nvalues?\nYou probably recall from your childhood days the magical process of mixing paint colors\nto obtain new ones. You may recall that blue+yellow makes green, red+blue makes purple,\nand red+green makes brown. If you revisited this topic at a later age, you may have learned\nthat the proper subtractive primaries are actually cyan (a light blue-green), magenta (pink),\nand yellow (Figure 2.27b), although black is also often used in four-color printing (CMYK).\n(If you ever subsequently took any painting classes, you learned that colors can have even",
  "103": "2.3 The digital camera\n81\n(a)\n(b)\nFigure 2.27 Primary and secondary colors: (a) additive colors red, green, and blue can be\nmixed to produce cyan, magenta, yellow, and white; (b) subtractive colors cyan, magenta,\nand yellow can be mixed to produce red, green, blue, and black.\nmore fanciful names, such as alizarin crimson, cerulean blue, and chartreuse.) The subtractive\ncolors are called subtractive because pigments in the paint absorb certain wavelengths in the\ncolor spectrum.\nLater on, you may have learned about the additive primary colors (red, green, and blue)\nand how they can be added (with a slide projector or on a computer monitor) to produce cyan,\nmagenta, yellow, white, and all the other colors we typically see on our TV sets and monitors\n(Figure 2.27a).\nThrough what process is it possible for two different colors, such as red and green, to\ninteract to produce a third color like yellow? Are the wavelengths somehow mixed up to\nproduce a new wavelength?\nYou probably know that the correct answer has nothing to do with physically mixing\nwavelengths. Instead, the existence of three primaries is a result of the tri-stimulus (or tri-\nchromatic) nature of the human visual system, since we have three different kinds of cone,\neach of which responds selectively to a different portion of the color spectrum (Glassner 1995;\nWyszecki and Stiles 2000; Fairchild 2005; Reinhard, Ward, Pattanaik et al. 2005; Livingstone\n2008).18 Note that for machine vision applications, such as remote sensing and terrain clas-\nsiﬁcation, it is preferable to use many more wavelengths. Similarly, surveillance applications\ncan often beneﬁt from sensing in the near-infrared (NIR) range.\nCIE RGB and XYZ\nTo test and quantify the tri-chromatic theory of perception, we can attempt to reproduce all\nmonochromatic (single wavelength) colors as a mixture of three suitably chosen primaries.\n18 See also Mark Fairchild’s Web page, http://www.cis.rit.edu/fairchild/WhyIsColor/books links.html.",
  "104": "82\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n360\n400\n440\n480\n520\n560\n600\n640\n680\n720\n760\nr\ng\nb\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n360\n400\n440\n480\n520\n560\n600\n640\n680\n720\n760\nx\ny\nz\n(a)\n(b)\nFigure 2.28\nStandard CIE color matching functions: (a) ¯r(λ), ¯g(λ), ¯b(λ) color spectra\nobtained from matching pure colors to the R=700.0nm, G=546.1nm, and B=435.8nm pri-\nmaries; (b) ¯x(λ), ¯y(λ), ¯z(λ) color matching functions, which are linear combinations of the\n(¯r(λ), ¯g(λ),¯b(λ)) spectra.\n(Pure wavelength light can be obtained using either a prism or specially manufactured color\nﬁlters.) In the 1930s, the Commission Internationale d’Eclairage (CIE) standardized the RGB\nrepresentation by performing such color matching experiments using the primary colors of\nred (700.0nm wavelength), green (546.1nm), and blue (435.8nm).\nFigure 2.28 shows the results of performing these experiments with a standard observer,\ni.e., averaging perceptual results over a large number of subjects. You will notice that for\ncertain pure spectra in the blue–green range, a negative amount of red light has to be added,\ni.e., a certain amount of red has to be added to the color being matched in order to get a color\nmatch. These results also provided a simple explanation for the existence of metamers, which\nare colors with different spectra that are perceptually indistinguishable. Note that two fabrics\nor paint colors that are metamers under one light may no longer be so under different lighting.\nBecause of the problem associated with mixing negative light, the CIE also developed a\nnew color space called XYZ, which contains all of the pure spectral colors within its positive\noctant. (It also maps the Y axis to the luminance, i.e., perceived relative brightness, and maps\npure white to a diagonal (equal-valued) vector.) The transformation from RGB to XYZ is\ngiven by\n\n\nX\nY\nZ\n\n=\n1\n0.17697\n\n\n0.49\n0.31\n0.20\n0.17697\n0.81240\n0.01063\n0.00\n0.01\n0.99\n\n\n\n\nR\nG\nB\n\n.\n(2.103)\nWhile the ofﬁcial deﬁnition of the CIE XYZ standard has the matrix normalized so that the\nY value corresponding to pure red is 1, a more commonly used form is to omit the leading",
  "105": "2.3 The digital camera\n83\nFigure 2.29 CIE chromaticity diagram, showing colors and their corresponding (x, y) val-\nues. Pure spectral colors are arranged around the outside of the curve.\nfraction, so that the second row adds up to one, i.e., the RGB triplet (1, 1, 1) maps to a Y value\nof 1. Linearly blending the (¯r(λ), ¯g(λ),¯b(λ)) curves in Figure 2.28a according to (2.103), we\nobtain the resulting (¯x(λ), ¯y(λ), ¯z(λ)) curves shown in Figure 2.28b. Notice how all three\nspectra (color matching functions) now have only positive values and how the ¯y(λ) curve\nmatches that of the luminance perceived by humans.\nIf we divide the XYZ values by the sum of X+Y+Z, we obtain the chromaticity coordi-\nnates\nx =\nX\nX + Y + Z , y =\nY\nX + Y + Z , z =\nZ\nX + Y + Z ,\n(2.104)\nwhich sum up to 1. The chromaticity coordinates discard the absolute intensity of a given\ncolor sample and just represent its pure color. If we sweep the monochromatic color λ pa-\nrameter in Figure 2.28b from λ = 380nm to λ = 800nm, we obtain the familiar chromaticity\ndiagram shown in Figure 2.29. This ﬁgure shows the (x, y) value for every color value per-\nceivable by most humans. (Of course, the CMYK reproduction process in this book does not\nactually span the whole gamut of perceivable colors.) The outer curved rim represents where\nall of the pure monochromatic color values map in (x, y) space, while the lower straight line,\nwhich connects the two endpoints, is known as the purple line.\nA convenient representation for color values, when we want to tease apart luminance\nand chromaticity, is therefore Yxy (luminance plus the two most distinctive chrominance\ncomponents).\nL*a*b* color space\nWhile the XYZ color space has many convenient properties, including the ability to separate\nluminance from chrominance, it does not actually predict how well humans perceive differ-\nences in color or luminance.",
  "106": "84\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBecause the response of the human visual system is roughly logarithmic (we can perceive\nrelative luminance differences of about 1%), the CIE deﬁned a non-linear re-mapping of the\nXYZ space called L*a*b* (also sometimes called CIELAB), where differences in luminance\nor chrominance are more perceptually uniform.19\nThe L* component of lightness is deﬁned as\nL∗= 116f\n\u0012 Y\nYn\n\u0013\n,\n(2.105)\nwhere Yn is the luminance value for nominal white (Fairchild 2005) and\nf(t) =\n(\nt1/3\nt > δ3\nt/(3δ2) + 2δ/3\nelse,\n(2.106)\nis a ﬁnite-slope approximation to the cube root with δ = 6/29. The resulting 0 . . . 100 scale\nroughly measures equal amounts of lightness perceptibility.\nIn a similar fashion, the a* and b* components are deﬁned as\na∗= 500\n\u0014\nf\n\u0012 X\nXn\n\u0013\n−f\n\u0012 Y\nYn\n\u0013\u0015\nand b∗= 200\n\u0014\nf\n\u0012 Y\nYn\n\u0013\n−f\n\u0012 Z\nZn\n\u0013\u0015\n,\n(2.107)\nwhere again, (Xn, Yn, Zn) is the measured white point. Figure 2.32i–k show the L*a*b*\nrepresentation for a sample color image.\nColor cameras\nWhile the preceding discussion tells us how we can uniquely describe the perceived tri-\nstimulus description of any color (spectral distribution), it does not tell us how RGB still\nand video cameras actually work. Do they just measure the amount of light at the nominal\nwavelengths of red (700.0nm), green (546.1nm), and blue (435.8nm)? Do color monitors just\nemit exactly these wavelengths and, if so, how can they emit negative red light to reproduce\ncolors in the cyan range?\nIn fact, the design of RGB video cameras has historically been based around the availabil-\nity of colored phosphors that go into television sets. When standard-deﬁnition color television\nwas invented (NTSC), a mapping was deﬁned between the RGB values that would drive the\nthree color guns in the cathode ray tube (CRT) and the XYZ values that unambiguously de-\nﬁne perceived color (this standard was called ITU-R BT.601). With the advent of HDTV and\nnewer monitors, a new standard called ITU-R BT.709 was created, which speciﬁes the XYZ\n19 Another perceptually motivated color space called L*u*v* was developed and standardized simultaneously\n(Fairchild 2005).",
  "107": "2.3 The digital camera\n85\nvalues of each of the color primaries,\n\n\nX\nY\nZ\n\n=\n\n\n0.412453\n0.357580\n0.180423\n0.212671\n0.715160\n0.072169\n0.019334\n0.119193\n0.950227\n\n\n\n\nR709\nG709\nB709\n\n.\n(2.108)\nIn practice, each color camera integrates light according to the spectral response function\nof its red, green, and blue sensors,\nR\n=\nZ\nL(λ)SR(λ)dλ,\nG\n=\nZ\nL(λ)SG(λ)dλ,\n(2.109)\nB\n=\nZ\nL(λ)SB(λ)dλ,\nwhere L(λ) is the incoming spectrum of light at a given pixel and {SR(λ), SG(λ), SB(λ)}\nare the red, green, and blue spectral sensitivities of the corresponding sensors.\nCan we tell what spectral sensitivities the cameras actually have? Unless the camera\nmanufacturer provides us with this data or we observe the response of the camera to a whole\nspectrum of monochromatic lights, these sensitivities are not speciﬁed by a standard such as\nBT.709. Instead, all that matters is that the tri-stimulus values for a given color produce the\nspeciﬁed RGB values. The manufacturer is free to use sensors with sensitivities that do not\nmatch the standard XYZ deﬁnitions, so long as they can later be converted (through a linear\ntransform) to the standard colors.\nSimilarly, while TV and computer monitors are supposed to produce RGB values as spec-\niﬁed by Equation (2.108), there is no reason that they cannot use digital logic to transform the\nincoming RGB values into different signals to drive each of the color channels. Properly cal-\nibrated monitors make this information available to software applications that perform color\nmanagement, so that colors in real life, on the screen, and on the printer all match as closely\nas possible.\nColor ﬁlter arrays\nWhile early color TV cameras used three vidicons (tubes) to perform their sensing and later\ncameras used three separate RGB sensing chips, most of today’s digital still and video cam-\neras cameras use a color ﬁlter array (CFA), where alternating sensors are covered by different\ncolored ﬁlters.20\n20 A newer chip design by Foveon (http://www.foveon.com) stacks the red, green, and blue sensors beneath each\nother, but it has not yet gained widespread adoption.",
  "108": "86\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nrgB\nrGb\nrgB\nrGb\nrGb\nRgb\nrGb\nRgb\nrgB\nrGb\nrgB\nrGb\nrGb\nRgb\nrGb\nRgb\nB\nG\nB\nG\nG\nR\nG\nR\nG\nB\nG\nR\nG\nR\nB\nG\nFigure 2.30 Bayer RGB pattern: (a) color ﬁlter array layout; (b) interpolated pixel values,\nwith unknown (guessed) values shown as lower case.\nThe most commonly used pattern in color cameras today is the Bayer pattern (Bayer\n1976), which places green ﬁlters over half of the sensors (in a checkerboard pattern), and red\nand blue ﬁlters over the remaining ones (Figure 2.30). The reason that there are twice as many\ngreen ﬁlters as red and blue is because the luminance signal is mostly determined by green\nvalues and the visual system is much more sensitive to high frequency detail in luminance\nthan in chrominance (a fact that is exploited in color image compression—see Section 2.3.3).\nThe process of interpolating the missing color values so that we have valid RGB values for\nall the pixels is known as demosaicing and is covered in detail in Section 10.3.1.\nSimilarly, color LCD monitors typically use alternating stripes of red, green, and blue\nﬁlters placed in front of each liquid crystal active area to simulate the experience of a full color\ndisplay. As before, because the visual system has higher resolution (acuity) in luminance than\nchrominance, it is possible to digitally pre-ﬁlter RGB (and monochrome) images to enhance\nthe perception of crispness (Betrisey, Blinn, Dresevic et al. 2000; Platt 2000).\nColor balance\nBefore encoding the sensed RGB values, most cameras perform some kind of color balancing\noperation in an attempt to move the white point of a given image closer to pure white (equal\nRGB values). If the color system and the illumination are the same (the BT.709 system uses\nthe daylight illuminant D65 as its reference white), the change may be minimal. However,\nif the illuminant is strongly colored, such as incandescent indoor lighting (which generally\nresults in a yellow or orange hue), the compensation can be quite signiﬁcant.\nA simple way to perform color correction is to multiply each of the RGB values by a\ndifferent factor (i.e., to apply a diagonal matrix transform to the RGB color space). More\ncomplicated transforms, which are sometimes the result of mapping to XYZ space and back,",
  "109": "2.3 The digital camera\n87\nY\nY’\nY’ = Y1/γ\nY’\nY\nY = Y’γ\nquantization \nnoise\nvisible \nnoise\nFigure 2.31 Gamma compression: (a) The relationship between the input signal luminance\nY and the transmitted signal Y ′ is given by Y ′ = Y 1/γ. (b) At the receiver, the signal Y ′ is\nexponentiated by the factor γ, ˆY = Y ′γ. Noise introduced during transmission is squashed in\nthe dark regions, which corresponds to the more noise-sensitive region of the visual system.\nactually perform a color twist, i.e., they use a general 3 × 3 color transform matrix.21 Exer-\ncise 2.9 has you explore some of these issues.\nGamma\nIn the early days of black and white television, the phosphors in the CRT used to display\nthe TV signal responded non-linearly to their input voltage. The relationship between the\nvoltage and the resulting brightness was characterized by a number called gamma (γ), since\nthe formula was roughly\nB = V γ,\n(2.110)\nwith a γ of about 2.2. To compensate for this effect, the electronics in the TV camera would\npre-map the sensed luminance Y through an inverse gamma,\nY ′ = Y\n1\nγ ,\n(2.111)\nwith a typical value of 1\nγ = 0.45.\nThe mapping of the signal through this non-linearity before transmission had a beneﬁcial\nside effect: noise added during transmission (remember, these were analog days!) would be\nreduced (after applying the gamma at the receiver) in the darker regions of the signal where\nit was more visible (Figure 2.31).22 (Remember that our visual system is roughly sensitive to\nrelative differences in luminance.)\n21 Those of you old enough to remember the early days of color television will naturally think of the hue adjustment\nknob on the television set, which could produce truly bizarre results.\n22 A related technique called companding was the basis of the Dolby noise reduction systems used with audio\ntapes.",
  "110": "88\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen color television was invented, it was decided to separately pass the red, green, and\nblue signals through the same gamma non-linearity before combining them for encoding.\nToday, even though we no longer have analog noise in our transmission systems, signals are\nstill quantized during compression (see Section 2.3.3), so applying inverse gamma to sensed\nvalues is still useful.\nUnfortunately, for both computer vision and computer graphics, the presence of gamma\nin images is often problematic. For example, the proper simulation of radiometric phenomena\nsuch as shading (see Section 2.2 and Equation (2.87)) occurs in a linear radiance space. Once\nall of the computations have been performed, the appropriate gamma should be applied before\ndisplay. Unfortunately, many computer graphics systems (such as shading models) operate\ndirectly on RGB values and display these values directly. (Fortunately, newer color imaging\nstandards such as the 16-bit scRGB use a linear space, which makes this less of a problem\n(Glassner 1995).)\nIn computer vision, the situation can be even more daunting. The accurate determination\nof surface normals, using a technique such as photometric stereo (Section 12.1.1) or even a\nsimpler operation such as accurate image deblurring, require that the measurements be in a\nlinear space of intensities. Therefore, it is imperative when performing detailed quantitative\ncomputations such as these to ﬁrst undo the gamma and the per-image color re-balancing\nin the sensed color values. Chakrabarti, Scharstein, and Zickler (2009) develop a sophisti-\ncated 24-parameter model that is a good match to the processing performed by today’s digital\ncameras; they also provide a database of color images you can use for your own testing.23\nFor other vision applications, however, such as feature detection or the matching of sig-\nnals in stereo and motion estimation, this linearization step is often not necessary. In fact,\ndetermining whether it is necessary to undo gamma can take some careful thinking, e.g., in\nthe case of compensating for exposure variations in image stitching (see Exercise 2.7).\nIf all of these processing steps sound confusing to model, they are. Exercise 2.10 has you\ntry to tease apart some of these phenomena using empirical investigation, i.e., taking pictures\nof color charts and comparing the RAW and JPEG compressed color values.\nOther color spaces\nWhile RGB and XYZ are the primary color spaces used to describe the spectral content (and\nhence tri-stimulus response) of color signals, a variety of other representations have been\ndeveloped both in video and still image coding and in computer graphics.\nThe earliest color representation developed for video transmission was the YIQ standard\ndeveloped for NTSC video in North America and the closely related YUV standard developed\nfor PAL in Europe. In both of these cases, it was desired to have a luma channel Y (so called\n23 http://vision.middlebury.edu/color/.",
  "111": "2.3 The digital camera\n89\nsince it only roughly mimics true luminance) that would be comparable to the regular black-\nand-white TV signal, along with two lower frequency chroma channels.\nIn both systems, the Y signal (or more appropriately, the Y’ luma signal since it is gamma\ncompressed) is obtained from\nY ′\n601 = 0.299R′ + 0.587G′ + 0.114B′,\n(2.112)\nwhere R’G’B’ is the triplet of gamma-compressed color components. When using the newer\ncolor deﬁnitions for HDTV in BT.709, the formula is\nY ′\n709 = 0.2125R′ + 0.7154G′ + 0.0721B′.\n(2.113)\nThe UV components are derived from scaled versions of (B′−Y ′) and (R′−Y ′), namely,\nU = 0.492111(B′ −Y ′) and V = 0.877283(R′ −Y ′),\n(2.114)\nwhereas the IQ components are the UV components rotated through an angle of 33◦. In\ncomposite (NTSC and PAL) video, the chroma signals were then low-pass ﬁltered horizon-\ntally before being modulated and superimposed on top of the Y’ luma signal. Backward\ncompatibility was achieved by having older black-and-white TV sets effectively ignore the\nhigh-frequency chroma signal (because of slow electronics) or, at worst, superimposing it as\na high-frequency pattern on top of the main signal.\nWhile these conversions were important in the early days of computer vision, when frame\ngrabbers would directly digitize the composite TV signal, today all digital video and still\nimage compression standards are based on the newer YCbCr conversion. YCbCr is closely\nrelated to YUV (the Cb and Cr signals carry the blue and red color difference signals and have\nmore useful mnemonics than UV) but uses different scale factors to ﬁt within the eight-bit\nrange available with digital signals.\nFor video, the Y’ signal is re-scaled to ﬁt within the [16 . . . 235] range of values, while\nthe Cb and Cr signals are scaled to ﬁt within [16 . . . 240] (Gomes and Velho 1997; Fairchild\n2005). For still images, the JPEG standard uses the full eight-bit range with no reserved\nvalues,\n\n\nY ′\nCb\nCr\n\n=\n\n\n0.299\n0.587\n0.114\n−0.168736\n−0.331264\n0.5\n0.5\n−0.418688\n−0.081312\n\n\n\n\nR′\nG′\nB′\n\n+\n\n\n0\n128\n128\n\n,\n(2.115)\nwhere the R’G’B’ values are the eight-bit gamma-compressed color components (i.e., the\nactual RGB values we obtain when we open up or display a JPEG image). For most appli-\ncations, this formula is not that important, since your image reading software will directly",
  "112": "90\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprovide you with the eight-bit gamma-compressed R’G’B’ values. However, if you are trying\nto do careful image deblocking (Exercise 3.30), this information may be useful.\nAnother color space you may come across is hue, saturation, value (HSV), which is a pro-\njection of the RGB color cube onto a non-linear chroma angle, a radial saturation percentage,\nand a luminance-inspired value. In more detail, value is deﬁned as either the mean or maxi-\nmum color value, saturation is deﬁned as scaled distance from the diagonal, and hue is deﬁned\nas the direction around a color wheel (the exact formulas are described by Hall (1989); Foley,\nvan Dam, Feiner et al. (1995)). Such a decomposition is quite natural in graphics applications\nsuch as color picking (it approximates the Munsell chart for color description). Figure 2.32l–\nn shows an HSV representation of a sample color image, where saturation is encoded using a\ngray scale (saturated = darker) and hue is depicted as a color.\nIf you want your computer vision algorithm to only affect the value (luminance) of an\nimage and not its saturation or hue, a simpler solution is to use either the Y xy (luminance +\nchromaticity) coordinates deﬁned in (2.104) or the even simpler color ratios,\nr =\nR\nR + G + B , g =\nG\nR + G + B , b =\nB\nR + G + B\n(2.116)\n(Figure 2.32e–h). After manipulating the luma (2.112), e.g., through the process of histogram\nequalization (Section 3.1.4), you can multiply each color ratio by the ratio of the new to old\nluma to obtain an adjusted RGB triplet.\nWhile all of these color systems may sound confusing, in the end, it often may not mat-\nter that much which one you use. Poynton, in his Color FAQ, http://www.poynton.com/\nColorFAQ.html, notes that the perceptually motivated L*a*b* system is qualitatively similar\nto the gamma-compressed R’G’B’ system we mostly deal with, since both have a fractional\npower scaling (which approximates a logarithmic response) between the actual intensity val-\nues and the numbers being manipulated. As in all cases, think carefully about what you are\ntrying to accomplish before deciding on a technique to use.24\n2.3.3 Compression\nThe last stage in a camera’s processing pipeline is usually some form of image compression\n(unless you are using a lossless compression scheme such as camera RAW or PNG).\nAll color video and image compression algorithms start by converting the signal into\nYCbCr (or some closely related variant), so that they can compress the luminance signal with\nhigher ﬁdelity than the chrominance signal. (Recall that the human visual system has poorer\n24 If you are at a loss for questions at a conference, you can always ask why the speaker did not use a perceptual\ncolor space, such as L*a*b*. Conversely, if they did use L*a*b*, you can ask if they have any concrete evidence that\nthis works better than regular colors.",
  "113": "2.3 The digital camera\n91\n(a) RGB\n(b) R\n(c) G\n(d) B\n(e) rgb\n(f) r\n(g) g\n(h) b\n(i) L*\n(j) a*\n(k) b*\n(l) H\n(m) S\n(n) V\nFigure 2.32 Color space transformations: (a–d) RGB; (e–h) rgb. (i–k) L*a*b*; (l–n) HSV.\nNote that the rgb, L*a*b*, and HSV values are all re-scaled to ﬁt the dynamic range of the\nprinted page.\nfrequency response to color than to luminance changes.) In video, it is common to subsam-\nple Cb and Cr by a factor of two horizontally; with still images (JPEG), the subsampling\n(averaging) occurs both horizontally and vertically.\nOnce the luminance and chrominance images have been appropriately subsampled and\nseparated into individual images, they are then passed to a block transform stage. The most\ncommon technique used here is the discrete cosine transform (DCT), which is a real-valued\nvariant of the discrete Fourier transform (DFT) (see Section 3.4.3). The DCT is a reasonable\napproximation to the Karhunen–Lo`eve or eigenvalue decomposition of natural image patches,\ni.e., the decomposition that simultaneously packs the most energy into the ﬁrst coefﬁcients\nand diagonalizes the joint covariance matrix among the pixels (makes transform coefﬁcients",
  "114": "92\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 2.33 Image compressed with JPEG at three quality settings. Note how the amount\nof block artifact and high-frequency aliasing (“mosquito noise”) increases from left to right.\nstatistically independent). Both MPEG and JPEG use 8 × 8 DCT transforms (Wallace 1991;\nLe Gall 1991), although newer variants use smaller 4×4 blocks or alternative transformations,\nsuch as wavelets (Taubman and Marcellin 2002) and lapped transforms (Malvar 1990, 1998,\n2000) are now used.\nAfter transform coding, the coefﬁcient values are quantized into a set of small integer\nvalues that can be coded using a variable bit length scheme such as a Huffman code or an\narithmetic code (Wallace 1991). (The DC (lowest frequency) coefﬁcients are also adaptively\npredicted from the previous block’s DC values. The term “DC” comes from “direct current”,\ni.e., the non-sinusoidal or non-alternating part of a signal.) The step size in the quantization\nis the main variable controlled by the quality setting on the JPEG ﬁle (Figure 2.33).\nWith video, it is also usual to perform block-based motion compensation, i.e., to encode\nthe difference between each block and a predicted set of pixel values obtained from a shifted\nblock in the previous frame. (The exception is the motion-JPEG scheme used in older DV\ncamcorders, which is nothing more than a series of individually JPEG compressed image\nframes.) While basic MPEG uses 16 × 16 motion compensation blocks with integer motion\nvalues (Le Gall 1991), newer standards use adaptively sized block, sub-pixel motions, and\nthe ability to reference blocks from older frames. In order to recover more gracefully from\nfailures and to allow for random access to the video stream, predicted P frames are interleaved\namong independently coded I frames. (Bi-directional B frames are also sometimes used.)\nThe quality of a compression algorithm is usually reported using its peak signal-to-noise\nratio (PSNR), which is derived from the average mean square error,\nMSE = 1\nn\nX\nx\nh\nI(x) −ˆI(x)\ni2\n,\n(2.117)\nwhere I(x) is the original uncompressed image and ˆI(x) is its compressed counterpart, or\nequivalently, the root mean square error (RMS error), which is deﬁned as\nRMS =\n√\nMSE.\n(2.118)",
  "115": "2.4 Additional reading\n93\nThe PSNR is deﬁned as\nPSNR = 10 log10\nI2\nmax\nMSE = 20 log10\nImax\nRMS ,\n(2.119)\nwhere Imax is the maximum signal extent, e.g., 255 for eight-bit images.\nWhile this is just a high-level sketch of how image compression works, it is useful to\nunderstand so that the artifacts introduced by such techniques can be compensated for in\nvarious computer vision applications.\n2.4 Additional reading\nAs we mentioned at the beginning of this chapter, it provides but a brief summary of a very\nrich and deep set of topics, traditionally covered in a number of separate ﬁelds.\nA more thorough introduction to the geometry of points, lines, planes, and projections\ncan be found in textbooks on multi-view geometry (Hartley and Zisserman 2004; Faugeras\nand Luong 2001) and computer graphics (Foley, van Dam, Feiner et al. 1995; Watt 1995;\nOpenGL-ARB 1997). Topics covered in more depth include higher-order primitives such as\nquadrics, conics, and cubics, as well as three-view and multi-view geometry.\nThe image formation (synthesis) process is traditionally taught as part of a computer\ngraphics curriculum (Foley, van Dam, Feiner et al. 1995; Glassner 1995; Watt 1995; Shirley\n2005) but it is also studied in physics-based computer vision (Wolff, Shafer, and Healey\n1992a).\nThe behavior of camera lens systems is studied in optics (M¨oller 1988; Hecht 2001; Ray\n2002).\nSome good books on color theory have been written by Healey and Shafer (1992); Wyszecki\nand Stiles (2000); Fairchild (2005), with Livingstone (2008) providing a more fun and infor-\nmal introduction to the topic of color perception. Mark Fairchild’s page of color books and\nlinks25 lists many other sources.\nTopics relating to sampling and aliasing are covered in textbooks on signal and image\nprocessing (Crane 1997; J¨ahne 1997; Oppenheim and Schafer 1996; Oppenheim, Schafer,\nand Buck 1999; Pratt 2007; Russ 2007; Burger and Burge 2008; Gonzales and Woods 2008).\n2.5 Exercises\nA note to students: This chapter is relatively light on exercises since it contains mostly\nbackground material and not that many usable techniques. If you really want to understand\n25 http://www.cis.rit.edu/fairchild/WhyIsColor/books links.html.",
  "116": "94\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmulti-view geometry in a thorough way, I encourage you to read and do the exercises provided\nby Hartley and Zisserman (2004). Similarly, if you want some exercises related to the image\nformation process, Glassner’s (1995) book is full of challenging problems.\nEx 2.1: Least squares intersection point and line ﬁtting—advanced\nEquation (2.4) shows\nhow the intersection of two 2D lines can be expressed as their cross product, assuming the\nlines are expressed as homogeneous coordinates.\n1. If you are given more than two lines and want to ﬁnd a point ˜x that minimizes the sum\nof squared distances to each line,\nD =\nX\ni\n(˜x · ˜li)2,\n(2.120)\nhow can you compute this quantity? (Hint: Write the dot product as ˜xT˜li and turn the\nsquared quantity into a quadratic form, ˜xT A˜x.)\n2. To ﬁt a line to a bunch of points, you can compute the centroid (mean) of the points\nas well as the covariance matrix of the points around this mean. Show that the line\npassing through the centroid along the major axis of the covariance ellipsoid (largest\neigenvector) minimizes the sum of squared distances to the points.\n3. These two approaches are fundamentally different, even though projective duality tells\nus that points and lines are interchangeable. Why are these two algorithms so appar-\nently different? Are they actually minimizing different objectives?\nEx 2.2: 2D transform editor\nWrite a program that lets you interactively create a set of\nrectangles and then modify their “pose” (2D transform). You should implement the following\nsteps:\n1. Open an empty window (“canvas”).\n2. Shift drag (rubber-band) to create a new rectangle.\n3. Select the deformation mode (motion model): translation, rigid, similarity, afﬁne, or\nperspective.\n4. Drag any corner of the outline to change its transformation.\nThis exercise should be built on a set of pixel coordinate and transformation classes, either\nimplemented by yourself or from a software library. Persistence of the created representation\n(save and load) should also be supported (for each rectangle, save its transformation).",
  "117": "2.5 Exercises\n95\nEx 2.3: 3D viewer\nWrite a simple viewer for 3D points, lines, and polygons. Import a set\nof point and line commands (primitives) as well as a viewing transform. Interactively modify\nthe object or camera transform. This viewer can be an extension of the one you created in\n(Exercise 2.2). Simply replace the viewing transformations with their 3D equivalents.\n(Optional) Add a z-buffer to do hidden surface removal for polygons.\n(Optional) Use a 3D drawing package and just write the viewer control.\nEx 2.4: Focus distance and depth of ﬁeld\nFigure out how the focus distance and depth of\nﬁeld indicators on a lens are determined.\n1. Compute and plot the focus distance zo as a function of the distance traveled from the\nfocal length ∆zi = f −zi for a lens of focal length f (say, 100mm). Does this explain\nthe hyperbolic progression of focus distances you see on a typical lens (Figure 2.20)?\n2. Compute the depth of ﬁeld (minimum and maximum focus distances) for a given focus\nsetting zo as a function of the circle of confusion diameter c (make it a fraction of\nthe sensor width), the focal length f, and the f-stop number N (which relates to the\naperture diameter d). Does this explain the usual depth of ﬁeld markings on a lens that\nbracket the in-focus marker, as in Figure 2.20a?\n3. Now consider a zoom lens with a varying focal length f. Assume that as you zoom,\nthe lens stays in focus, i.e., the distance from the rear nodal point to the sensor plane\nzi adjusts itself automatically for a ﬁxed focus distance zo. How do the depth of ﬁeld\nindicators vary as a function of focal length? Can you reproduce a two-dimensional\nplot that mimics the curved depth of ﬁeld lines seen on the lens in Figure 2.20b?\nEx 2.5: F-numbers and shutter speeds\nList the common f-numbers and shutter speeds\nthat your camera provides. On older model SLRs, they are visible on the lens and shut-\nter speed dials. On newer cameras, you have to look at the electronic viewﬁnder (or LCD\nscreen/indicator) as you manually adjust exposures.\n1. Do these form geometric progressions; if so, what are the ratios? How do these relate\nto exposure values (EVs)?\n2. If your camera has shutter speeds of 1\n60 and\n1\n125, do you think that these two speeds are\nexactly a factor of two apart or a factor of 125/60 = 2.083 apart?\n3. How accurate do you think these numbers are? Can you devise some way to measure\nexactly how the aperture affects how much light reaches the sensor and what the exact\nexposure times actually are?",
  "118": "96\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 2.6: Noise level calibration\nEstimate the amount of noise in your camera by taking re-\npeated shots of a scene with the camera mounted on a tripod. (Purchasing a remote shutter\nrelease is a good investment if you own a DSLR.) Alternatively, take a scene with constant\ncolor regions (such as a color checker chart) and estimate the variance by ﬁtting a smooth\nfunction to each color region and then taking differences from the predicted function.\n1. Plot your estimated variance as a function of level for each of your color channels\nseparately.\n2. Change the ISO setting on your camera; if you cannot do that, reduce the overall light\nin your scene (turn off lights, draw the curtains, wait until dusk). Does the amount of\nnoise vary a lot with ISO/gain?\n3. Compare your camera to another one at a different price point or year of make. Is\nthere evidence to suggest that “you get what you pay for”? Does the quality of digital\ncameras seem to be improving over time?\nEx 2.7: Gamma correction in image stitching\nHere’s a relatively simple puzzle. Assume\nyou are given two images that are part of a panorama that you want to stitch (see Chapter 9).\nThe two images were taken with different exposures, so you want to adjust the RGB values\nso that they match along the seam line. Is it necessary to undo the gamma in the color values\nin order to achieve this?\nEx 2.8: Skin color detection\nDevise a simple skin color detector (Forsyth and Fleck 1999;\nJones and Rehg 2001; Vezhnevets, Sazonov, and Andreeva 2003; Kakumanu, Makrogiannis,\nand Bourbakis 2007) based on chromaticity or other color properties.\n1. Take a variety of photographs of people and calculate the xy chromaticity values for\neach pixel.\n2. Crop the photos or otherwise indicate with a painting tool which pixels are likely to be\nskin (e.g. face and arms).\n3. Calculate a color (chromaticity) distribution for these pixels. You can use something as\nsimple as a mean and covariance measure or as complicated as a mean-shift segmenta-\ntion algorithm (see Section 5.3.2). You can optionally use non-skin pixels to model the\nbackground distribution.\n4. Use your computed distribution to ﬁnd the skin regions in an image. One easy way to\nvisualize this is to paint all non-skin pixels a given color, such as white or black.\n5. How sensitive is your algorithm to color balance (scene lighting)?",
  "119": "2.5 Exercises\n97\n6. Does a simpler chromaticity measurement, such as a color ratio (2.116), work just as\nwell?\nEx 2.9: White point balancing—tricky\nA common (in-camera or post-processing) tech-\nnique for performing white point adjustment is to take a picture of a white piece of paper and\nto adjust the RGB values of an image to make this a neutral color.\n1. Describe how you would adjust the RGB values in an image given a sample “white\ncolor” of (Rw, Gw, Bw) to make this color neutral (without changing the exposure too\nmuch).\n2. Does your transformation involve a simple (per-channel) scaling of the RGB values or\ndo you need a full 3 × 3 color twist matrix (or something else)?\n3. Convert your RGB values to XYZ. Does the appropriate correction now only depend\non the XY (or xy) values? If so, when you convert back to RGB space, do you need a\nfull 3 × 3 color twist matrix to achieve the same effect?\n4. If you used pure diagonal scaling in the direct RGB mode but end up with a twist if you\nwork in XYZ space, how do you explain this apparent dichotomy? Which approach is\ncorrect? (Or is it possible that neither approach is actually correct?)\nIf you want to ﬁnd out what your camera actually does, continue on to the next exercise.\nEx 2.10: In-camera color processing—challenging\nIf your camera supports a RAW pixel\nmode, take a pair of RAW and JPEG images, and see if you can infer what the camera is doing\nwhen it converts the RAW pixel values to the ﬁnal color-corrected and gamma-compressed\neight-bit JPEG pixel values.\n1. Deduce the pattern in your color ﬁlter array from the correspondence between co-\nlocated RAW and color-mapped pixel values. Use a color checker chart at this stage\nif it makes your life easier. You may ﬁnd it helpful to split the RAW image into four\nseparate images (subsampling even and odd columns and rows) and to treat each of\nthese new images as a “virtual” sensor.\n2. Evaluate the quality of the demosaicing algorithm by taking pictures of challenging\nscenes which contain strong color edges (such as those shown in in Section 10.3.1).\n3. If you can take the same exact picture after changing the color balance values in your\ncamera, compare how these settings affect this processing.\n4. Compare your results against those presented by Chakrabarti, Scharstein, and Zickler\n(2009) or use the data available in their database of color images.26\n26 http://vision.middlebury.edu/color/.",
  "120": "98\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "121": "Chapter 3\nImage processing\n3.1\nPoint operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\n3.1.1\nPixel transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n103\n3.1.2\nColor transforms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n104\n3.1.3\nCompositing and matting . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n3.1.4\nHistogram equalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n107\n3.1.5\nApplication: Tonal adjustment\n. . . . . . . . . . . . . . . . . . . . . . . . .\n111\n3.2\nLinear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n111\n3.2.1\nSeparable ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n115\n3.2.2\nExamples of linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . .\n117\n3.2.3\nBand-pass and steerable ﬁlters\n. . . . . . . . . . . . . . . . . . . . . . . . .\n118\n3.3\nMore neighborhood operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n122\n3.3.1\nNon-linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n122\n3.3.2\nMorphology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n127\n3.3.3\nDistance transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n129\n3.3.4\nConnected components . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n131\n3.4\nFourier transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n132\n3.4.1\nFourier transform pairs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n136\n3.4.2\nTwo-dimensional Fourier transforms . . . . . . . . . . . . . . . . . . . . . .\n140\n3.4.3\nWiener ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n140\n3.4.4\nApplication: Sharpening, blur, and noise removal . . . . . . . . . . . . . . . .\n144\n3.5\nPyramids and wavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n144\n3.5.1\nInterpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n145\n3.5.2\nDecimation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n148\n3.5.3\nMulti-resolution representations . . . . . . . . . . . . . . . . . . . . . . . . .\n150\n3.5.4\nWavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n154\n3.5.5\nApplication: Image blending\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n160\n3.6\nGeometric transformations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n162\n3.6.1\nParametric transformations . . . . . . . . . . . . . . . . . . . . . . . . . . .\n163\n3.6.2\nMesh-based warping\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n170\n3.6.3\nApplication: Feature-based morphing . . . . . . . . . . . . . . . . . . . . . .\n173\n3.7\nGlobal optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n174\n3.7.1\nRegularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n174\n3.7.2\nMarkov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n180\n3.7.3\nApplication: Image restoration\n. . . . . . . . . . . . . . . . . . . . . . . . .\n192\n3.8\nAdditional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n192\n3.9\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n194",
  "122": "100\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3.1\nSome common image processing operations: (a) original image; (b) increased\ncontrast; (c) change in hue; (d) “posterized” (quantized colors); (e) blurred; (f) rotated.",
  "123": "3.1 Point operators\n101\nNow that we have seen how images are formed through the interaction of 3D scene elements,\nlighting, and camera optics and sensors, let us look at the ﬁrst stage in most computer vision\napplications, namely the use of image processing to preprocess the image and convert it into\na form suitable for further analysis. Examples of such operations include exposure correction\nand color balancing, the reduction of image noise, increasing sharpness, or straightening the\nimage by rotating it (Figure 3.1). While some may consider image processing to be outside\nthe purview of computer vision, most computer vision applications, such as computational\nphotography and even recognition, require care in designing the image processing stages in\norder to achieve acceptable results.\nIn this chapter, we review standard image processing operators that map pixel values from\none image to another. Image processing is often taught in electrical engineering departments\nas a follow-on course to an introductory course in signal processing (Oppenheim and Schafer\n1996; Oppenheim, Schafer, and Buck 1999). There are several popular textbooks for image\nprocessing (Crane 1997; Gomes and Velho 1997; J¨ahne 1997; Pratt 2007; Russ 2007; Burger\nand Burge 2008; Gonzales and Woods 2008).\nWe begin this chapter with the simplest kind of image transforms, namely those that\nmanipulate each pixel independently of its neighbors (Section 3.1). Such transforms are of-\nten called point operators or point processes. Next, we examine neighborhood (area-based)\noperators, where each new pixel’s value depends on a small number of neighboring input\nvalues (Sections 3.2 and 3.3). A convenient tool to analyze (and sometimes accelerate) such\nneighborhood operations is the Fourier Transform, which we cover in Section 3.4. Neighbor-\nhood operators can be cascaded to form image pyramids and wavelets, which are useful for\nanalyzing images at a variety of resolutions (scales) and for accelerating certain operations\n(Section 3.5). Another important class of global operators are geometric transformations,\nsuch as rotations, shears, and perspective deformations (Section 3.6). Finally, we introduce\nglobal optimization approaches to image processing, which involve the minimization of an\nenergy functional or, equivalently, optimal estimation using Bayesian Markov random ﬁeld\nmodels (Section 3.7).\n3.1 Point operators\nThe simplest kinds of image processing transforms are point operators, where each output\npixel’s value depends on only the corresponding input pixel value (plus, potentially, some\nglobally collected information or parameters). Examples of such operators include brightness\nand contrast adjustments (Figure 3.2) as well as color correction and transformations. In the\nimage processing literature, such operations are also known as point processes (Crane 1997).\nWe begin this section with a quick review of simple point operators such as brightness",
  "124": "102\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3.2 Some local image processing operations: (a) original image along with its three\ncolor (per-channel) histograms; (b) brightness increased (additive offset, b = 16); (c) contrast\nincreased (multiplicative gain, a = 1.1); (d) gamma (partially) linearized (γ = 1.2); (e) full\nhistogram equalization; (f) partial histogram equalization.",
  "125": "3.1 Point operators\n103\n45\n60\n98\n127 132 133 137 133\n46\n65\n98\n123 126 128 131 133\n47\n65\n96\n115 119 123 135 137\n47\n63\n91\n107 113 122 138 134\n50\n59\n80\n97\n110 123 133 134\n49\n53\n68\n83\n97\n113 128 133\n50\n50\n58\n70\n84\n102 116 126\n50\n50\n52\n58\n69\n86\n101 120\n1\n3\n5\n7\n9\n11\n13\n15\nS1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\nS9\nS10\nS11\nS12\nS13\nS14\nS15\nS16\n0\n20\n40\n60\n80\n100\n120\n140\n160\nrange\ndomain\ndomain\n(a)\n(b)\n(c)\n(d)\nFigure 3.3 Visualizing image data: (a) original image; (b) cropped portion and scanline plot\nusing an image inspection tool; (c) grid of numbers; (d) surface plot. For ﬁgures (c)–(d), the\nimage was ﬁrst converted to grayscale.\nscaling and image addition. Next, we discuss how colors in images can be manipulated.\nWe then present image compositing and matting operations, which play an important role\nin computational photography (Chapter 10) and computer graphics applications. Finally, we\ndescribe the more global process of histogram equalization. We close with an example appli-\ncation that manipulates tonal values (exposure and contrast) to improve image appearance.\n3.1.1 Pixel transforms\nA general image processing operator is a function that takes one or more input images and\nproduces an output image. In the continuous domain, this can be denoted as\ng(x) = h(f(x)) or g(x) = h(f0(x), . . . , fn(x)),\n(3.1)\nwhere x is in the D-dimensional domain of the functions (usually D = 2 for images) and the\nfunctions f and g operate over some range, which can either be scalar or vector-valued, e.g.,\nfor color images or 2D motion. For discrete (sampled) images, the domain consists of a ﬁnite\nnumber of pixel locations, x = (i, j), and we can write\ng(i, j) = h(f(i, j)).\n(3.2)\nFigure 3.3 shows how an image can be represented either by its color (appearance), as a grid\nof numbers, or as a two-dimensional function (surface plot).\nTwo commonly used point processes are multiplication and addition with a constant,\ng(x) = af(x) + b.\n(3.3)\nThe parameters a > 0 and b are often called the gain and bias parameters; sometimes these\nparameters are said to control contrast and brightness, respectively (Figures 3.2b–c).1 The\n1 An image’s luminance characteristics can also be summarized by its key (average luminanance) and range\n(Kopf, Uyttendaele, Deussen et al. 2007).",
  "126": "104\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nbias and gain parameters can also be spatially varying,\ng(x) = a(x)f(x) + b(x),\n(3.4)\ne.g., when simulating the graded density ﬁlter used by photographers to selectively darken\nthe sky or when modeling vignetting in an optical system.\nMultiplicative gain (both global and spatially varying) is a linear operation, since it obeys\nthe superposition principle,\nh(f0 + f1) = h(f0) + h(f1).\n(3.5)\n(We will have more to say about linear shift invariant operators in Section 3.2.) Operators\nsuch as image squaring (which is often used to get a local estimate of the energy in a band-\npass ﬁltered signal, see Section 3.5) are not linear.\nAnother commonly used dyadic (two-input) operator is the linear blend operator,\ng(x) = (1 −α)f0(x) + αf1(x).\n(3.6)\nBy varying α from 0 →1, this operator can be used to perform a temporal cross-dissolve\nbetween two images or videos, as seen in slide shows and ﬁlm production, or as a component\nof image morphing algorithms (Section 3.6.3).\nOne highly used non-linear transform that is often applied to images before further pro-\ncessing is gamma correction, which is used to remove the non-linear mapping between input\nradiance and quantized pixel values (Section 2.3.2).\nTo invert the gamma mapping applied\nby the sensor, we can use\ng(x) = [f(x)]1/γ ,\n(3.7)\nwhere a gamma value of γ ≈2.2 is a reasonable ﬁt for most digital cameras.\n3.1.2 Color transforms\nWhile color images can be treated as arbitrary vector-valued functions or collections of inde-\npendent bands, it usually makes sense to think about them as highly correlated signals with\nstrong connections to the image formation process (Section 2.2), sensor design (Section 2.3),\nand human perception (Section 2.3.2). Consider, for example, brightening a picture by adding\na constant value to all three channels, as shown in Figure 3.2b. Can you tell if this achieves the\ndesired effect of making the image look brighter? Can you see any undesirable side-effects\nor artifacts?\nIn fact, adding the same value to each color channel not only increases the apparent in-\ntensity of each pixel, it can also affect the pixel’s hue and saturation. How can we deﬁne and\nmanipulate such quantities in order to achieve the desired perceptual effects?",
  "127": "3.1 Point operators\n105\n(a)\n(b)\n(c)\n(d)\nFigure 3.4 Image matting and compositing (Chuang, Curless, Salesin et al. 2001) c⃝2001\nIEEE: (a) source image; (b) extracted foreground object F; (c) alpha matte α shown in\ngrayscale; (d) new composite C.\nAs discussed in Section 2.3.2, chromaticity coordinates (2.104) or even simpler color ra-\ntios (2.116) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figure\n2.32g–i shows some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 × 3 color twist transform matrix.\nExercises 2.9 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn\n1996), while the process of inserting it into another image (without visible artifacts) is called\ncompositing (Porter and Duff 1984; Blinn 1994a).\nThe intermediate representation used for the foreground object between these two stages\nis called an alpha-matted color image (Figure 3.4b–c). In addition to the three color RGB\nchannels, an alpha-matted image contains a fourth alpha channel α (or A) that describes the\nrelative amount of opacity or fractional coverage at each pixel (Figures 3.4c and 3.5b). The\nopacity is the opposite of the transparency. Pixels within the object are fully opaque (α = 1),\nwhile pixels fully outside the object are transparent (α = 0). Pixels on the boundary of the\nobject vary smoothly between these two extremes, which hides the perceptual visible jaggies",
  "128": "106\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n×\n(1−\n)\n+\n=\nB\nα\nαF\nC\n(a)\n(b)\n(c)\n(d)\nFigure 3.5\nCompositing equation C = (1 −α)B + αF. The images are taken from a\nclose-up of the region of the hair in the upper right part of the lion in Figure 3.4.\nthat occur if only binary opacities are used.\nTo composite a new (or foreground) image on top of an old (background) image, the over\noperator, ﬁrst proposed by Porter and Duff (1984) and then studied extensively by Blinn\n(1994a; 1994b), is used,\nC = (1 −α)B + αF.\n(3.8)\nThis operator attenuates the inﬂuence of the background image B by a factor (1 −α) and\nthen adds in the color (and opacity) values corresponding to the foreground layer F, as shown\nin Figure 3.5.\nIn many situations, it is convenient to represent the foreground colors in pre-multiplied\nform, i.e., to store (and manipulate) the αF values directly. As Blinn (1994b) shows, the\npre-multiplied RGBA representation is preferred for several reasons, including the ability\nto blur or resample (e.g., rotate) alpha-matted images without any additional complications\n(just treating each RGBA band independently). However, when matting using local color\nconsistency (Ruzon and Tomasi 2000; Chuang, Curless, Salesin et al. 2001), the pure un-\nmultiplied foreground colors F are used, since these remain constant (or vary slowly) in the\nvicinity of the object edge.\nThe over operation is not the only kind of compositing operation that can be used. Porter\nand Duff (1984) describe a number of additional operations that can be useful in photo editing\nand visual effects applications. In this book, we concern ourselves with only one additional,\ncommonly occurring case (but see Exercise 3.2).\nWhen light reﬂects off clean transparent glass, the light passing through the glass and\nthe light reﬂecting off the glass are simply added together (Figure 3.6). This model is use-\nful in the analysis of transparent motion (Black and Anandan 1996; Szeliski, Avidan, and\nAnandan 2000), which occurs when such scenes are observed from a moving camera (see\nSection 8.5.2).\nThe actual process of matting, i.e., recovering the foreground, background, and alpha\nmatte values from one or more images, has a rich history, which we study in Section 10.4.",
  "129": "3.1 Point operators\n107\nFigure 3.6 An example of light reﬂecting off the transparent glass of a picture frame (Black\nand Anandan 1996) c⃝1996 Elsevier. You can clearly see the woman’s portrait inside the\npicture frame superimposed with the reﬂection of a man’s face off the glass.\nSmith and Blinn (1996) have a nice survey of traditional blue-screen matting techniques,\nwhile Toyama, Krumm, Brumitt et al. (1999) review difference matting. More recently, there\nhas been a lot of activity in computational photography relating to natural image matting\n(Ruzon and Tomasi 2000; Chuang, Curless, Salesin et al. 2001; Wang and Cohen 2007a),\nwhich attempts to extract the mattes from a single natural image (Figure 3.4a) or from ex-\ntended video sequences (Chuang, Agarwala, Curless et al. 2002). All of these techniques are\ndescribed in more detail in Section 10.4.\n3.1.4 Histogram equalization\nWhile the brightness and gain controls described in Section 3.1.1 can improve the appearance\nof an image, how can we automatically determine their best values? One approach might\nbe to look at the darkest and brightest pixel values in an image and map them to pure black\nand pure white. Another approach might be to ﬁnd the average value in the image, push it\ntowards middle gray, and expand the range so that it more closely ﬁlls the displayable values\n(Kopf, Uyttendaele, Deussen et al. 2007).\nHow can we visualize the set of lightness values in an image in order to test some of\nthese heuristics? The answer is to plot the histogram of the individual color channels and\nluminance values, as shown in Figure 3.7b.2 From this distribution, we can compute relevant\nstatistics such as the minimum, maximum, and average intensity values. Notice that the image\nin Figure 3.7a has both an excess of dark values and light values, but that the mid-range values\nare largely under-populated. Would it not be better if we could simultaneously brighten some\n2 The histogram is simply the count of the number of pixels at each gray level value. For an eight-bit image, an\naccumulation table with 256 entries is needed. For higher bit depths, a table with the appropriate number of entries\n(probably fewer than the full number of gray levels) should be used.",
  "130": "108\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n0\n50\n100\n150\n200\n250\nB\nG\nR\nY\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\n0\n50\n100\n150\n200\n250\nB\nG\nR\nY\n(a)\n(b)\n(c)\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\nB\nG\nR\nY\n(d)\n(e)\n(f)\nFigure 3.7 Histogram analysis and equalization: (a) original image (b) color channel and in-\ntensity (luminance) histograms; (c) cumulative distribution functions; (d) equalization (trans-\nfer) functions; (e) full histogram equalization; (f) partial histogram equalization.\ndark values and darken some light values, while still using the full extent of the available\ndynamic range? Can you think of a mapping that might do this?\nOne popular answer to this question is to perform histogram equalization, i.e., to ﬁnd\nan intensity mapping function f(I) such that the resulting histogram is ﬂat. The trick to\nﬁnding such a mapping is the same one that people use to generate random samples from\na probability density function, which is to ﬁrst compute the cumulative distribution function\nshown in Figure 3.7c.\nThink of the original histogram h(I) as the distribution of grades in a class after some\nexam. How can we map a particular grade to its corresponding percentile, so that students at\nthe 75% percentile range scored better than 3/4 of their classmates? The answer is to integrate\nthe distribution h(I) to obtain the cumulative distribution c(I),\nc(I) = 1\nN\nI\nX\ni=0\nh(i) = c(I −1) + 1\nN h(I),\n(3.9)\nwhere N is the number of pixels in the image or students in the class. For any given grade or\nintensity, we can look up its corresponding percentile c(I) and determine the ﬁnal value that\npixel should take. When working with eight-bit pixel values, the I and c axes are rescaled\nfrom [0, 255].",
  "131": "3.1 Point operators\n109\n(a)\n(b)\n(c)\nFigure 3.8 Locally adaptive histogram equalization: (a) original image; (b) block histogram\nequalization; (c) full locally adaptive equalization.\nFigure 3.7d shows the result of applying f(I) = c(I) to the original image. As we\ncan see, the resulting histogram is ﬂat; so is the resulting image (it is “ﬂat” in the sense\nof a lack of contrast and being muddy looking). One way to compensate for this is to only\npartially compensate for the histogram unevenness, e.g., by using a mapping function f(I) =\nαc(I) + (1 −α)I, which is a linear blend between the cumulative distribution function and\nthe identity transform (a straight line). As you can see in Figure 3.7e, the resulting image\nmaintains more of its original grayscale distribution while having a more appealing balance.\nAnother potential problem with histogram equalization (or, in general, image brightening)\nis that noise in dark regions can be ampliﬁed and become more visible. Exercise 3.6 suggests\nsome possible ways to mitigate this, as well as alternative techniques to maintain contrast and\n“punch” in the original images (Larson, Rushmeier, and Piatko 1997; Stark 2000).\nLocally adaptive histogram equalization\nWhile global histogram equalization can be useful, for some images it might be preferable\nto apply different kinds of equalization in different regions. Consider for example the image\nin Figure 3.8a, which has a wide range of luminance values. Instead of computing a single\ncurve, what if we were to subdivide the image into M ×M pixel blocks and perform separate\nhistogram equalization in each sub-block? As you can see in Figure 3.8b, the resulting image\nexhibits a lot of blocking artifacts, i.e., intensity discontinuities at block boundaries.\nOne way to eliminate blocking artifacts is to use a moving window, i.e., to recompute the\nhistogram for every M × M block centered at each pixel. This process can be quite slow\n(M 2 operations per pixel), although with clever programming only the histogram entries\ncorresponding to the pixels entering and leaving the block (in a raster scan across the image)\nneed to be updated (M operations per pixel). Note that this operation is an example of the\nnon-linear neighborhood operations we study in more detail in Section 3.3.1.\nA more efﬁcient approach is to compute non-overlapped block-based equalization func-\ntions as before, but to then smoothly interpolate the transfer functions as we move between",
  "132": "110\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nt\ns\nt\ns\n(a)\n(b)\nFigure 3.9 Local histogram interpolation using relative (s, t) coordinates: (a) block-based\nhistograms, with block centers shown as circles; (b) corner-based “spline” histograms. Pixels\nare located on grid intersections. The black square pixel’s transfer function is interpolated\nfrom the four adjacent lookup tables (gray arrows) using the computed (s, t) values. Block\nboundaries are shown as dashed lines.\nblocks. This technique is known as adaptive histogram equalization (AHE) and its contrast-\nlimited (gain-limited) version is known as CLAHE (Pizer, Amburn, Austin et al. 1987).3 The\nweighting function for a given pixel (i, j) can be computed as a function of its horizontal\nand vertical position (s, t) within a block, as shown in Figure 3.9a. To blend the four lookup\nfunctions {f00, . . . , f11}, a bilinear blending function,\nfs,t(I) = (1 −s)(1 −t)f00(I) + s(1 −t)f10(I) + (1 −s)tf01(I) + stf11(I)\n(3.10)\ncan be used. (See Section 3.5.2 for higher-order generalizations of such spline functions.)\nNote that instead of blending the four lookup tables for each output pixel (which would be\nquite slow), we can instead blend the results of mapping a given pixel through the four neigh-\nboring lookups.\nA variant on this algorithm is to place the lookup tables at the corners of each M × M\nblock (see Figure 3.9b and Exercise 3.7). In addition to blending four lookups to compute the\nﬁnal value, we can also distribute each input pixel into four adjacent lookup tables during the\nhistogram accumulation phase (notice that the gray arrows in Figure 3.9b point both ways),\ni.e.,\nhk,l(I(i, j)) += w(i, j, k, l),\n(3.11)\nwhere w(i, j, k, l) is the bilinear weighting function between pixel (i, j) and lookup table\n(k, l). This is an example of soft histogramming, which is used in a variety of other applica-\n3This algorithm is implemented in the MATLAB adapthist function.",
  "133": "3.2 Linear ﬁltering\n111\ntions, including the construction of SIFT feature descriptors (Section 4.1.3) and vocabulary\ntrees (Section 14.3.2).\n3.1.5 Application: Tonal adjustment\nOne of the most widely used applications of point-wise image processing operators is the\nmanipulation of contrast or tone in photographs, to make them look either more attractive or\nmore interpretable. You can get a good sense of the range of operations possible by opening\nup any photo manipulation tool and trying out a variety of contrast, brightness, and color\nmanipulation options, as shown in Figures 3.2 and 3.7.\nExercises 3.1, 3.5, and 3.6 have you implement some of these operations, in order to\nbecome familiar with basic image processing operators. More sophisticated techniques for\ntonal adjustment (Reinhard, Ward, Pattanaik et al. 2005; Bae, Paris, and Durand 2006) are\ndescribed in the section on high dynamic range tone mapping (Section 10.2.1).\n3.2 Linear ﬁltering\nLocally adaptive histogram equalization is an example of a neighborhood operator or local\noperator, which uses a collection of pixel values in the vicinity of a given pixel to deter-\nmine its ﬁnal output value (Figure 3.10). In addition to performing local tone adjustment,\nneighborhood operators can be used to ﬁlter images in order to add soft blur, sharpen de-\ntails, accentuate edges, or remove noise (Figure 3.11b–d). In this section, we look at linear\nﬁltering operators, which involve weighted combinations of pixels in small neighborhoods.\nIn Section 3.3, we look at non-linear operators such as morphological ﬁlters and distance\ntransforms.\nThe most commonly used type of neighborhood operator is a linear ﬁlter, in which an\noutput pixel’s value is determined as a weighted sum of input pixel values (Figure 3.10),\ng(i, j) =\nX\nk,l\nf(i + k, j + l)h(k, l).\n(3.12)\nThe entries in the weight kernel or mask h(k, l) are often called the ﬁlter coefﬁcients. The\nabove correlation operator can be more compactly notated as\ng = f ⊗h.\n(3.13)\nA common variant on this formula is\ng(i, j) =\nX\nk,l\nf(i −k, j −l)h(k, l) =\nX\nk,l\nf(k, l)h(i −k, j −l),\n(3.14)",
  "134": "112\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n45\n60\n98\n127 132 133 137 133\n46\n65\n98\n123 126 128 131 133\n69\n95\n116 125 129 132\n47\n65\n96\n115 119 123 135 137\n0.1\n0.1\n0.1\n68\n92\n110 120 126 132\n47\n63\n91\n107 113 122 138 134\n*\n0.1\n0.2\n0.1\n=\n66\n86\n104 114 124 132\n50\n59\n80\n97\n110 123 133 134\n0.1\n0.1\n0.1\n62\n78\n94\n108 120 129\n49\n53\n68\n83\n97\n113 128 133\n57\n69\n83\n98\n112 124\n50\n50\n58\n70\n84\n102 116 126\n53\n60\n71\n85\n100 114\n50\n50\n52\n58\n69\n86\n101 120\nf (x,y )\nh (x,y )\ng (x,y )\nFigure 3.10 Neighborhood ﬁltering (convolution): The image on the left is convolved with\nthe ﬁlter in the middle to yield the image on the right. The light blue pixels indicate the source\nneighborhood for the light green destination pixel.\nwhere the sign of the offsets in f has been reversed. This is called the convolution operator,\ng = f ∗h,\n(3.15)\nand h is then called the impulse response function.4 The reason for this name is that the kernel\nfunction, h, convolved with an impulse signal, δ(i, j) (an image that is 0 everywhere except\nat the origin) reproduces itself, h ∗δ = h, whereas correlation produces the reﬂected signal.\n(Try this yourself to verify that it is so.)\nIn fact, Equation (3.14) can be interpreted as the superposition (addition) of shifted im-\npulse response functions h(i−k, j −l) multiplied by the input pixel values f(k, l). Convolu-\ntion has additional nice properties, e.g., it is both commutative and associative. As well, the\nFourier transform of two convolved images is the product of their individual Fourier trans-\nforms (Section 3.4).\nBoth correlation and convolution are linear shift-invariant (LSI) operators, which obey\nboth the superposition principle (3.5),\nh ◦(f0 + f1) = h ◦f0 + h ◦f1,\n(3.16)\nand the shift invariance principle,\ng(i, j) = f(i + k, j + l) ⇔(h ◦g)(i, j) = (h ◦f)(i + k, j + l),\n(3.17)\nwhich means that shifting a signal commutes with applying the operator (◦stands for the LSI\noperator). Another way to think of shift invariance is that the operator “behaves the same\neverywhere”.\n4 The continuous version of convolution can be written as g(x) = R\nf(x −u)h(u)du.",
  "135": "3.2 Linear ﬁltering\n113\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 3.11 Some neighborhood operations: (a) original image; (b) blurred; (c) sharpened;\n(d) smoothed with edge-preserving ﬁlter; (e) binary image; (f) dilated; (g) distance transform;\n(h) connected components. For the dilation and connected components, black (ink) pixels are\nassumed to be active, i.e., to have a value of 1 in Equations (3.41–3.45).",
  "136": "114\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n72\n88\n62\n52\n37 ∗\n1/4\n1/2\n1/4\n⇔\n1\n4\n\n\n2\n1\n.\n.\n.\n1\n2\n1\n.\n.\n.\n1\n2\n1\n.\n.\n.\n1\n2\n1\n.\n.\n.\n1\n2\n\n\n\n\n72\n88\n62\n52\n37\n\n\nFigure 3.12\nOne-dimensional signal convolution as a sparse matrix-vector multiply, g =\nHf.\nOccasionally, a shift-variant version of correlation or convolution may be used, e.g.,\ng(i, j) =\nX\nk,l\nf(i −k, j −l)h(k, l; i, j),\n(3.18)\nwhere h(k, l; i, j) is the convolution kernel at pixel (i, j). For example, such a spatially\nvarying kernel can be used to model blur in an image due to variable depth-dependent defocus.\nCorrelation and convolution can both be written as a matrix-vector multiply, if we ﬁrst\nconvert the two-dimensional images f(i, j) and g(i, j) into raster-ordered vectors f and g,\ng = Hf,\n(3.19)\nwhere the (sparse) H matrix contains the convolution kernels. Figure 3.12 shows how a\none-dimensional convolution can be represented in matrix-vector form.\nPadding (border effects)\nThe astute reader will notice that the matrix multiply shown in Figure 3.12 suffers from\nboundary effects, i.e., the results of ﬁltering the image in this form will lead to a darkening of\nthe corner pixels. This is because the original image is effectively being padded with 0 values\nwherever the convolution kernel extends beyond the original image boundaries.\nTo compensate for this, a number of alternative padding or extension modes have been\ndeveloped (Figure 3.13):\n• zero: set all pixels outside the source image to 0 (a good choice for alpha-matted cutout\nimages);\n• constant (border color): set all pixels outside the source image to a speciﬁed border\nvalue;\n• clamp (replicate or clamp to edge): repeat edge pixels indeﬁnitely;\n• (cyclic) wrap (repeat or tile): loop “around” the image in a “toroidal” conﬁguration;",
  "137": "3.2 Linear ﬁltering\n115\nzero\nwrap\nclamp\nmirror\nblurred zero\nnormalized zero\nblurred clamp\nblurred mirror\nFigure 3.13 Border padding (top row) and the results of blurring the padded image (bottom\nrow). The normalized zero image is the result of dividing (normalizing) the blurred zero-\npadded RGBA image by its corresponding soft alpha value.\n• mirror: reﬂect pixels across the image edge;\n• extend: extend the signal by subtracting the mirrored version of the signal from the\nedge pixel value.\nIn the computer graphics literature (Akenine-M¨oller and Haines 2002, p. 124), these mech-\nanisms are known as the wrapping mode (OpenGL) or texture addressing mode (Direct3D).\nThe formulas for each of these modes are left to the reader (Exercise 3.8).\nFigure 3.13 shows the effects of padding an image with each of the above mechanisms and\nthen blurring the resulting padded image. As you can see, zero padding darkens the edges,\nclamp (replication) padding propagates border values inward, mirror (reﬂection) padding pre-\nserves colors near the borders. Extension padding (not shown) keeps the border pixels ﬁxed\n(during blur).\nAn alternative to padding is to blur the zero-padded RGBA image and to then divide the\nresulting image by its alpha value to remove the darkening effect. The results can be quite\ngood, as seen in the normalized zero image in Figure 3.13.\n3.2.1 Separable ﬁltering\nThe process of performing a convolution requires K2 (multiply-add) operations per pixel,\nwhere K is the size (width or height) of the convolution kernel, e.g., the box ﬁlter in Fig-",
  "138": "116\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1\nK2\n1\n1\n· · ·\n1\n1\n1\n· · ·\n1\n...\n...\n1\n...\n1\n1\n· · ·\n1\n1\n16\n1\n2\n1\n2\n4\n2\n1\n2\n1\n1\n256\n1\n4\n6\n4\n1\n4\n16\n24\n16\n4\n6\n24\n36\n24\n6\n4\n16\n24\n16\n4\n1\n4\n6\n4\n1\n1\n8\n−1\n0\n1\n−2\n0\n2\n−1\n0\n1\n1\n4\n1\n−2\n1\n−2\n4\n−2\n1\n−2\n1\n1\nK\n1\n1\n· · ·\n1\n1\n4\n1\n2\n1\n1\n16\n1\n4\n6\n4\n1\n1\n2\n−1\n0\n1\n1\n2\n1\n−2\n1\n(a) box, K = 5\n(b) bilinear\n(c) “Gaussian”\n(d) Sobel\n(e) corner\nFigure 3.14\nSeparable linear ﬁlters: For each image (a)–(e), we show the 2D ﬁlter kernel\n(top), the corresponding horizontal 1D kernel (middle), and the ﬁltered image (bottom). The\nﬁltered Sobel and corner images are signed, scaled up by 2× and 4×, respectively, and added\nto a gray offset before display.\nure 3.14a. In many cases, this operation can be signiﬁcantly sped up by ﬁrst performing a\none-dimensional horizontal convolution followed by a one-dimensional vertical convolution\n(which requires a total of 2K operations per pixel). A convolution kernel for which this is\npossible is said to be separable.\nIt is easy to show that the two-dimensional kernel K corresponding to successive con-\nvolution with a horizontal kernel h and a vertical kernel v is the outer product of the two\nkernels,\nK = vhT\n(3.20)\n(see Figure 3.14 for some examples). Because of the increased efﬁciency, the design of\nconvolution kernels for computer vision applications is often inﬂuenced by their separability.\nHow can we tell if a given kernel K is indeed separable? This can often be done by\ninspection or by looking at the analytic form of the kernel (Freeman and Adelson 1991). A\nmore direct method is to treat the 2D kernel as a 2D matrix K and to take its singular value\ndecomposition (SVD),\nK =\nX\ni\nσiuivT\ni\n(3.21)\n(see Appendix A.1.1 for the deﬁnition of the SVD). If only the ﬁrst singular value σ0 is\nnon-zero, the kernel is separable and √σ0u0 and √σ0vT\n0 provide the vertical and horizontal",
  "139": "3.2 Linear ﬁltering\n117\nkernels (Perona 1995). For example, the Laplacian of Gaussian kernel (3.26 and 4.23) can be\nimplemented as the sum of two separable ﬁlters (4.24) (Wiejak, Buxton, and Buxton 1985).\nWhat if your kernel is not separable and yet you still want a faster way to implement\nit? Perona (1995), who ﬁrst made the link between kernel separability and SVD, suggests\nusing more terms in the (3.21) series, i.e., summing up a number of separable convolutions.\nWhether this is worth doing or not depends on the relative sizes of K and the number of sig-\nniﬁcant singular values, as well as other considerations, such as cache coherency and memory\nlocality.\n3.2.2 Examples of linear ﬁltering\nNow that we have described the process for performing linear ﬁltering, let us examine a\nnumber of frequently used ﬁlters.\nThe simplest ﬁlter to implement is the moving average or box ﬁlter, which simply averages\nthe pixel values in a K ×K window. This is equivalent to convolving the image with a kernel\nof all ones and then scaling (Figure 3.14a). For large kernels, a more efﬁcient implementation\nis to slide a moving window across each scanline (in a separable ﬁlter) while adding the\nnewest pixel and subtracting the oldest pixel from the running sum. This is related to the\nconcept of summed area tables, which we describe shortly.\nA smoother image can be obtained by separably convolving the image with a piecewise\nlinear “tent” function (also known as a Bartlett ﬁlter). Figure 3.14b shows a 3 × 3 version\nof this ﬁlter, which is called the bilinear kernel, since it is the outer product of two linear\n(ﬁrst-order) splines (see Section 3.5.2).\nConvolving the linear tent function with itself yields the cubic approximating spline,\nwhich is called the “Gaussian” kernel (Figure 3.14c) in Burt and Adelson’s (1983a) Lapla-\ncian pyramid representation (Section 3.5). Note that approximate Gaussian kernels can also\nbe obtained by iterated convolution with box ﬁlters (Wells 1986). In applications where the\nﬁlters really need to be rotationally symmetric, carefully tuned versions of sampled Gaussians\nshould be used (Freeman and Adelson 1991) (Exercise 3.10).\nThe kernels we just discussed are all examples of blurring (smoothing) or low-pass ker-\nnels (since they pass through the lower frequencies while attenuating higher frequencies).\nHow good are they at doing this? In Section 3.4, we use frequency-space Fourier analysis to\nexamine the exact frequency response of these ﬁlters. We also introduce the sinc ((sin x)/x)\nﬁlter, which performs ideal low-pass ﬁltering.\nIn practice, smoothing kernels are often used to reduce high-frequency noise. We have\nmuch more to say about using variants on smoothing to remove noise later (see Sections 3.3.1,\n3.4, and 3.7).\nSurprisingly, smoothing kernels can also be used to sharpen images using a process called",
  "140": "118\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nunsharp masking.\nSince blurring the image reduces high frequencies, adding some of the\ndifference between the original and the blurred image makes it sharper,\ngsharp = f + γ(f −hblur ∗f).\n(3.22)\nIn fact, before the advent of digital photography, this was the standard way to sharpen images\nin the darkroom: create a blurred (“positive”) negative from the original negative by mis-\nfocusing, then overlay the two negatives before printing the ﬁnal image, which corresponds\nto\ngunsharp = f(1 −γhblur ∗f).\n(3.23)\nThis is no longer a linear ﬁlter but it still works well.\nLinear ﬁltering can also be used as a pre-processing stage to edge extraction (Section 4.2)\nand interest point detection (Section 4.1) algorithms. Figure 3.14d shows a simple 3 × 3 edge\nextractor called the Sobel operator, which is a separable combination of a horizontal central\ndifference (so called because the horizontal derivative is centered on the pixel) and a vertical\ntent ﬁlter (to smooth the results). As you can see in the image below the kernel, this ﬁlter\neffectively emphasizes horizontal edges.\nThe simple corner detector (Figure 3.14e) looks for simultaneous horizontal and vertical\nsecond derivatives. As you can see however, it responds not only to the corners of the square,\nbut also along diagonal edges. Better corner detectors, or at least interest point detectors that\nare more rotationally invariant, are described in Section 4.1.\n3.2.3 Band-pass and steerable ﬁlters\nThe Sobel and corner operators are simple examples of band-pass and oriented ﬁlters. More\nsophisticated kernels can be created by ﬁrst smoothing the image with a (unit area) Gaussian\nﬁlter,\nG(x, y; σ) =\n1\n2πσ2 e−x2+y2\n2σ2 ,\n(3.24)\nand then taking the ﬁrst or second derivatives (Marr 1982; Witkin 1983; Freeman and Adelson\n1991). Such ﬁlters are known collectively as band-pass ﬁlters, since they ﬁlter out both low\nand high frequencies.\nThe (undirected) second derivative of a two-dimensional image,\n∇2f = ∂2f\n∂x2 + ∂2y\n∂y2 ,\n(3.25)\nis known as the Laplacian operator. Blurring an image with a Gaussian and then taking its\nLaplacian is equivalent to convolving directly with the Laplacian of Gaussian (LoG) ﬁlter,\n∇2G(x, y; σ) =\n\u0012x2 + y2\nσ4\n−2\nσ2\n\u0013\nG(x, y; σ),\n(3.26)",
  "141": "3.2 Linear ﬁltering\n119\n(a)\n(b)\n(c)\nFigure 3.15 Second-order steerable ﬁlter (Freeman 1992) c⃝1992 IEEE: (a) original image\nof Einstein; (b) orientation map computed from the second-order oriented energy; (c) original\nimage with oriented structures enhanced.\nwhich has certain nice scale-space properties (Witkin 1983; Witkin, Terzopoulos, and Kass\n1986). The ﬁve-point Laplacian is just a compact approximation to this more sophisticated\nﬁlter.\nLikewise, the Sobel operator is a simple approximation to a directional or oriented ﬁlter,\nwhich can obtained by smoothing with a Gaussian (or some other ﬁlter) and then taking a\ndirectional derivative ∇ˆu =\n∂\n∂ˆu, which is obtained by taking the dot product between the\ngradient ﬁeld ∇and a unit direction ˆu = (cos θ, sin θ),\nˆu · ∇(G ∗f) = ∇ˆu(G ∗f) = (∇ˆuG) ∗f.\n(3.27)\nThe smoothed directional derivative ﬁlter,\nGˆu = uGx + vGy = u∂G\n∂x + v ∂G\n∂y ,\n(3.28)\nwhere ˆu = (u, v), is an example of a steerable ﬁlter, since the value of an image convolved\nwith Gˆu can be computed by ﬁrst convolving with the pair of ﬁlters (Gx, Gy) and then\nsteering the ﬁlter (potentially locally) by multiplying this gradient ﬁeld with a unit vector ˆu\n(Freeman and Adelson 1991). The advantage of this approach is that a whole family of ﬁlters\ncan be evaluated with very little cost.\nHow about steering a directional second derivative ﬁlter ∇ˆu · ∇ˆuGˆu, which is the result\nof taking a (smoothed) directional derivative and then taking the directional derivative again?\nFor example, Gxx is the second directional derivative in the x direction.\nAt ﬁrst glance, it would appear that the steering trick will not work, since for every di-\nrection ˆu, we need to compute a different ﬁrst directional derivative. Somewhat surprisingly,\nFreeman and Adelson (1991) showed that, for directional Gaussian derivatives, it is possible",
  "142": "120\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 3.16\nFourth-order steerable ﬁlter (Freeman and Adelson 1991) c⃝1991 IEEE: (a)\ntest image containing bars (lines) and step edges at different orientations; (b) average oriented\nenergy; (c) dominant orientation; (d) oriented energy as a function of angle (polar plot).\nto steer any order of derivative with a relatively small number of basis functions. For example,\nonly three basis functions are required for the second-order directional derivative,\nGˆuˆu = u2Gxx + 2uvGxy + v2Gyy.\n(3.29)\nFurthermore, each of the basis ﬁlters, while not itself necessarily separable, can be computed\nusing a linear combination of a small number of separable ﬁlters (Freeman and Adelson\n1991).\nThis remarkable result makes it possible to construct directional derivative ﬁlters of in-\ncreasingly greater directional selectivity, i.e., ﬁlters that only respond to edges that have\nstrong local consistency in orientation (Figure 3.15). Furthermore, higher order steerable\nﬁlters can respond to potentially more than a single edge orientation at a given location, and\nthey can respond to both bar edges (thin lines) and the classic step edges (Figure 3.16). In\norder to do this, however, full Hilbert transform pairs need to be used for second-order and\nhigher ﬁlters, as described in (Freeman and Adelson 1991).\nSteerable ﬁlters are often used to construct both feature descriptors (Section 4.1.3) and\nedge detectors (Section 4.2). While the ﬁlters developed by Freeman and Adelson (1991)\nare best suited for detecting linear (edge-like) structures, more recent work by Koethe (2003)\nshows how a combined 2 × 2 boundary tensor can be used to encode both edge and junction\n(“corner”) features. Exercise 3.12 has you implement such steerable ﬁlters and apply them to\nﬁnding both edge and corner features.\nSummed area table (integral image)\nIf an image is going to be repeatedly convolved with different box ﬁlters (and especially ﬁlters\nof different sizes at different locations), you can precompute the summed area table (Crow",
  "143": "3.2 Linear ﬁltering\n121\n3\n2\n7\n2\n3\n3\n5\n12\n14\n17\n3\n5\n12 14\n17\n1\n5\n1\n3\n4\n4\n11\n19\n24\n31\n4\n11\n19\n24\n31\n5\n1\n3\n5\n1\n9\n17\n28\n38\n46\n9\n17\n28\n38\n46\n4\n3\n2\n1\n6\n13\n24\n37\n48\n62\n13\n24\n37\n48\n62\n2\n4\n1\n4\n8\n15\n30\n44\n59\n81\n15\n30\n44\n59\n81\n (a)  S = 24\n (b)  s =\n28\n (c)  S = 24\nFigure 3.17 Summed area tables: (a) original image; (b) summed area table; (c) computation\nof area sum. Each value in the summed area table s(i, j) (red) is computed recursively from\nits three adjacent (blue) neighbors (3.31). Area sums S (green) are computed by combining\nthe four values at the rectangle corners (purple) (3.32). Positive values are shown in bold and\nnegative values in italics.\n1984), which is just the running sum of all the pixel values from the origin,\ns(i, j) =\ni\nX\nk=0\nj\nX\nl=0\nf(k, l).\n(3.30)\nThis can be efﬁciently computed using a recursive (raster-scan) algorithm,\ns(i, j) = s(i −1, j) + s(i, j −1) −s(i −1, j −1) + f(i, j).\n(3.31)\nThe image s(i, j) is also often called an integral image (see Figure 3.17) and can actually be\ncomputed using only two additions per pixel if separate row sums are used (Viola and Jones\n2004). To ﬁnd the summed area (integral) inside a rectangle [i0, i1] × [j0, j1], we simply\ncombine four samples from the summed area table,\nS(i0 . . . i1, j0 . . . j1) =\ni1\nX\ni=i0\nj1\nX\nj=j0\ns(i1, j1) −s(i1, j0 −1) −s(i0 −1, j1) + s(i0 −1, j0 −1).\n(3.32)\nA potential disadvantage of summed area tables is that they require log M + log N extra bits\nin the accumulation image compared to the original image, where M and N are the image\nwidth and height. Extensions of summed area tables can also be used to approximate other\nconvolution kernels (Wolberg (1990, Section 6.5.2) contains a review).\nIn computer vision, summed area tables have been used in face detection (Viola and\nJones 2004) to compute simple multi-scale low-level features. Such features, which consist of\nadjacent rectangles of positive and negative values, are also known as boxlets (Simard, Bottou,",
  "144": "122\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nHaffner et al. 1998). In principle, summed area tables could also be used to compute the sums\nin the sum of squared differences (SSD) stereo and motion algorithms (Section 11.4). In\npractice, separable moving average ﬁlters are usually preferred (Kanade, Yoshida, Oda et al.\n1996), unless many different window shapes and sizes are being considered (Veksler 2003).\nRecursive ﬁltering\nThe incremental formula (3.31) for the summed area is an example of a recursive ﬁlter, i.e.,\none whose values depends on previous ﬁlter outputs. In the signal processing literature, such\nﬁlters are known as inﬁnite impulse response (IIR), since the output of the ﬁlter to an impulse\n(single non-zero value) goes on forever. For example, for a summed area table, an impulse\ngenerates an inﬁnite rectangle of 1s below and to the right of the impulse. The ﬁlters we have\npreviously studied in this chapter, which involve the image with a ﬁnite extent kernel, are\nknown as ﬁnite impulse response (FIR).\nTwo-dimensional IIR ﬁlters and recursive formulas are sometimes used to compute quan-\ntities that involve large area interactions, such as two-dimensional distance functions (Sec-\ntion 3.3.3) and connected components (Section 3.3.4).\nMore commonly, however, IIR ﬁlters are used inside one-dimensional separable ﬁltering\nstages to compute large-extent smoothing kernels, such as efﬁcient approximations to Gaus-\nsians and edge ﬁlters (Deriche 1990; Nielsen, Florack, and Deriche 1997).\nPyramid-based\nalgorithms (Section 3.5) can also be used to perform such large-area smoothing computations.\n3.3 More neighborhood operators\nAs we have just seen, linear ﬁlters can perform a wide variety of image transformations.\nHowever non-linear ﬁlters, such as edge-preserving median or bilateral ﬁlters, can sometimes\nperform even better. Other examples of neighborhood operators include morphological oper-\nators that operate on binary images, as well as semi-global operators that compute distance\ntransforms and ﬁnd connected components in binary images (Figure 3.11f–h).\n3.3.1 Non-linear ﬁltering\nThe ﬁlters we have looked at so far have all been linear, i.e., their response to a sum of two\nsignals is the same as the sum of the individual responses. This is equivalent to saying that\neach output pixel is a weighted summation of some number of input pixels (3.19). Linear\nﬁlters are easier to compose and are amenable to frequency response analysis (Section 3.4).\nIn many cases, however, better performance can be obtained by using a non-linear com-\nbination of neighboring pixels. Consider for example the image in Figure 3.18e, where the",
  "145": "3.3 More neighborhood operators\n123\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 3.18 Median and bilateral ﬁltering: (a) original image with Gaussian noise; (b) Gaus-\nsian ﬁltered; (c) median ﬁltered; (d) bilaterally ﬁltered; (e) original image with shot noise; (f)\nGaussian ﬁltered; (g) median ﬁltered; (h) bilaterally ﬁltered. Note that the bilateral ﬁlter fails\nto remove the shot noise because the noisy pixels are too different from their neighbors.\n.\n2\n1\n0\n1\n2\n1\n2\n1\n2\n4\n1\n2\n1\n2\n4\n2\n0.1 0.3 0.4 0.3 0.1\n0.0 0.0 0.0 0.0 0.2\n2\n1\n3\n5\n8\n2\n1\n3\n5\n8\n1\n0.3 0.6 0.8 0.6 0.3\n0.0 0.0 0.0 0.4 0.8\n1\n3\n7\n6\n9\n1\n3\n7\n6\n9\n0\n0.4 0.8 1.0 0.8 0.4\n0.0 0.0 1.0 0.8 0.4\n3\n4\n8\n6\n7\n3\n4\n8\n6\n7\n1\n0.3 0.6 0.8 0.6 0.3\n0.0 0.2 0.8 0.8 1.0\n4\n5\n7\n8\n9\n4\n5\n7\n8\n9\n2\n0.1 0.3 0.4 0.3 0.1\n0.2 0.4 1.0 0.8 0.4\n(a) median =\n4\n(b) α-mean= 4.6\n(c) domain filter\n(d) range filter\nFigure 3.19 Median and bilateral ﬁltering: (a) median pixel (green); (b) selected α-trimmed\nmean pixels; (c) domain ﬁlter (numbers along edge are pixel distances); (d) range ﬁlter.",
  "146": "124\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nnoise, rather than being Gaussian, is shot noise, i.e., it occasionally has very large values. In\nthis case, regular blurring with a Gaussian ﬁlter fails to remove the noisy pixels and instead\nturns them into softer (but still visible) spots (Figure 3.18f).\nMedian ﬁltering\nA better ﬁlter to use in this case is the median ﬁlter, which selects the median value from each\npixel’s neighborhood (Figure 3.19a). Median values can be computed in expected linear time\nusing a randomized select algorithm (Cormen 2001) and incremental variants have also been\ndeveloped by Tomasi and Manduchi (1998) and Bovik (2000, Section 3.2). Since the shot\nnoise value usually lies well outside the true values in the neighborhood, the median ﬁlter is\nable to ﬁlter away such bad pixels (Figure 3.18c).\nOne downside of the median ﬁlter, in addition to its moderate computational cost, is that\nsince it selects only one input pixel value to replace each output pixel, it is not as efﬁcient at\naveraging away regular Gaussian noise (Huber 1981; Hampel, Ronchetti, Rousseeuw et al.\n1986; Stewart 1999). A better choice may be the α-trimmed mean (Lee and Redner 1990)\n(Crane 1997, p. 109), which averages together all of the pixels except for the α fraction that\nare the smallest and the largest (Figure 3.19b).\nAnother possibility is to compute a weighted median, in which each pixel is used a num-\nber of times depending on its distance from the center. This turns out to be equivalent to\nminimizing the weighted objective function\nX\nk,l\nw(k, l)|f(i + k, j + l) −g(i, j)|p,\n(3.33)\nwhere g(i, j) is the desired output value and p = 1 for the weighted median. The value p = 2\nis the usual weighted mean, which is equivalent to correlation (3.12) after normalizing by the\nsum of the weights (Bovik 2000, Section 3.2) (Haralick and Shapiro 1992, Section 7.2.6).\nThe weighted mean also has deep connections to other methods in robust statistics (see Ap-\npendix B.3), such as inﬂuence functions (Huber 1981; Hampel, Ronchetti, Rousseeuw et al.\n1986).\nNon-linear smoothing has another, perhaps even more important property, especially\nsince shot noise is rare in today’s cameras. Such ﬁltering is more edge preserving, i.e., it\nhas less tendency to soften edges while ﬁltering away high-frequency noise.\nConsider the noisy image in Figure 3.18a. In order to remove most of the noise, the\nGaussian ﬁlter is forced to smooth away high-frequency detail, which is most noticeable near\nstrong edges. Median ﬁltering does better but, as mentioned before, does not do as good\na job at smoothing away from discontinuities. See (Tomasi and Manduchi 1998) for some\nadditional references to edge-preserving smoothing techniques.",
  "147": "3.3 More neighborhood operators\n125\nWhile we could try to use the α-trimmed mean or weighted median, these techniques still\nhave a tendency to round sharp corners, since the majority of pixels in the smoothing area\ncome from the background distribution.\nBilateral ﬁltering\nWhat if we were to combine the idea of a weighted ﬁlter kernel with a better version of outlier\nrejection? What if instead of rejecting a ﬁxed percentage α, we simply reject (in a soft way)\npixels whose values differ too much from the central pixel value? This is the essential idea in\nbilateral ﬁltering, which was ﬁrst popularized in the computer vision community by Tomasi\nand Manduchi (1998). Chen, Paris, and Durand (2007) and Paris, Kornprobst, Tumblin et al.\n(2008) cite similar earlier work (Aurich and Weule 1995; Smith and Brady 1997) as well as\nthe wealth of subsequent applications in computer vision and computational photography.\nIn the bilateral ﬁlter, the output pixel value depends on a weighted combination of neigh-\nboring pixel values\ng(i, j) =\nP\nk,l f(k, l)w(i, j, k, l)\nP\nk,l w(i, j, k, l)\n.\n(3.34)\nThe weighting coefﬁcient w(i, j, k, l) depends on the product of a domain kernel (Figure 3.19c),\nd(i, j, k, l) = exp\n\u0012\n−(i −k)2 + (j −l)2\n2σ2\nd\n\u0013\n,\n(3.35)\nand a data-dependent range kernel (Figure 3.19d),\nr(i, j, k, l) = exp\n\u0012\n−∥f(i, j) −f(k, l)∥2\n2σ2r\n\u0013\n.\n(3.36)\nWhen multiplied together, these yield the data-dependent bilateral weight function\nw(i, j, k, l) = exp\n\u0012\n−(i −k)2 + (j −l)2\n2σ2\nd\n−∥f(i, j) −f(k, l)∥2\n2σ2r\n\u0013\n.\n(3.37)\nFigure 3.20 shows an example of the bilateral ﬁltering of a noisy step edge. Note how the do-\nmain kernel is the usual Gaussian, the range kernel measures appearance (intensity) similarity\nto the center pixel, and the bilateral ﬁlter kernel is a product of these two.\nNotice that the range ﬁlter (3.36) uses the vector distance between the center and the\nneighboring pixel. This is important in color images, since an edge in any one of the color\nbands signals a change in material and hence the need to downweight a pixel’s inﬂuence.5\n5 Tomasi and Manduchi (1998) show that using the vector distance (as opposed to ﬁltering each color band\nseparately) reduces color fringing effects. They also recommend taking the color difference in the more perceptually\nuniform CIELAB color space (see Section 2.3.2).",
  "148": "126\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3.20\nBilateral ﬁltering (Durand and Dorsey 2002) c⃝2002 ACM: (a) noisy step\nedge input; (b) domain ﬁlter (Gaussian); (c) range ﬁlter (similarity to center pixel value); (d)\nbilateral ﬁlter; (e) ﬁltered step edge output; (f) 3D distance between pixels.\nSince bilateral ﬁltering is quite slow compared to regular separable ﬁltering, a number\nof acceleration techniques have been developed (Durand and Dorsey 2002; Paris and Durand\n2006; Chen, Paris, and Durand 2007; Paris, Kornprobst, Tumblin et al. 2008). Unfortunately,\nthese techniques tend to use more memory than regular ﬁltering and are hence not directly\napplicable to ﬁltering full-color images.\nIterated adaptive smoothing and anisotropic diffusion\nBilateral (and other) ﬁlters can also be applied in an iterative fashion, especially if an appear-\nance more like a “cartoon” is desired (Tomasi and Manduchi 1998). When iterated ﬁltering\nis applied, a much smaller neighborhood can often be used.\nConsider, for example, using only the four nearest neighbors, i.e., restricting |k −i|+|l −\nj| ≤1 in (3.34). Observe that\nd(i, j, k, l)\n=\nexp\n\u0012\n−(i −k)2 + (j −l)2\n2σ2\nd\n\u0013\n(3.38)\n=\n(\n1,\n|k −i| + |l −j| = 0,\nλ = e−1/2σ2\nd,\n|k −i| + |l −j| = 1.\n(3.39)",
  "149": "3.3 More neighborhood operators\n127\nWe can thus re-write (3.34) as\nf (t+1)(i, j)\n=\nf (t)(i, j) + η P\nk,l f (t)(k, l)r(i, j, k, l)\n1 + η P\nk,l r(i, j, k, l)\n(3.40)\n=\nf (t)(i, j) +\nη\n1 + ηR\nX\nk,l\nr(i, j, k, l)[f (t)(k, l) −f (t)(i, j)],\nwhere R = P\n(k,l) r(i, j, k, l), (k, l) are the N4 neighbors of (i, j), and we have made the\niterative nature of the ﬁltering explicit.\nAs Barash (2002) notes, (3.40) is the same as the discrete anisotropic diffusion equation\nﬁrst proposed by Perona and Malik (1990b).6 Since its original introduction, anisotropic dif-\nfusion has been extended and applied to a wide range of problems (Nielsen, Florack, and De-\nriche 1997; Black, Sapiro, Marimont et al. 1998; Weickert, ter Haar Romeny, and Viergever\n1998; Weickert 1998). It has also been shown to be closely related to other adaptive smooth-\ning techniques (Saint-Marc, Chen, and Medioni 1991; Barash 2002; Barash and Comaniciu\n2004) as well as Bayesian regularization with a non-linear smoothness term that can be de-\nrived from image statistics (Scharr, Black, and Haussecker 2003).\nIn its general form, the range kernel r(i, j, k, l) = r(∥f(i, j)−f(k, l)∥), which is usually\ncalled the gain or edge-stopping function, or diffusion coefﬁcient, can be any monotonically\nincreasing function with r′(x) →0 as x →∞. Black, Sapiro, Marimont et al. (1998) show\nhow anisotropic diffusion is equivalent to minimizing a robust penalty function on the image\ngradients, which we discuss in Sections 3.7.1 and 3.7.2). Scharr, Black, and Haussecker\n(2003) show how the edge-stopping function can be derived in a principled manner from\nlocal image statistics. They also extend the diffusion neighborhood from N4 to N8, which\nallows them to create a diffusion operator that is both rotationally invariant and incorporates\ninformation about the eigenvalues of the local structure tensor.\nNote that, without a bias term towards the original image, anisotropic diffusion and itera-\ntive adaptive smoothing converge to a constant image. Unless a small number of iterations is\nused (e.g., for speed), it is usually preferable to formulate the smoothing problem as a joint\nminimization of a smoothness term and a data ﬁdelity term, as discussed in Sections 3.7.1\nand 3.7.2 and by Scharr, Black, and Haussecker (2003), which introduce such a bias in a\nprincipled manner.\n3.3.2 Morphology\nWhile non-linear ﬁlters are often used to enhance grayscale and color images, they are also\nused extensively to process binary images. Such images often occur after a thresholding\n6 The 1/(1 + ηR) factor is not present in anisotropic diffusion but becomes negligible as η →0.",
  "150": "128\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3.21\nBinary image morphology: (a) original image; (b) dilation; (c) erosion; (d)\nmajority; (e) opening; (f) closing. The structuring element for all examples is a 5 × 5 square.\nThe effects of majority are a subtle rounding of sharp corners. Opening fails to eliminate the\ndot, since it is not wide enough.\noperation,\nθ(f, t) =\n(\n1\nif f ≥t,\n0\nelse,\n(3.41)\ne.g., converting a scanned grayscale document into a binary image for further processing such\nas optical character recognition.\nThe most common binary image operations are called morphological operations, since\nthey change the shape of the underlying binary objects (Ritter and Wilson 2000, Chapter 7).\nTo perform such an operation, we ﬁrst convolve the binary image with a binary structuring\nelement and then select a binary output value depending on the thresholded result of the\nconvolution. (This is not the usual way in which these operations are described, but I ﬁnd it\na nice simple way to unify the processes.) The structuring element can be any shape, from\na simple 3 × 3 box ﬁlter, to more complicated disc structures. It can even correspond to a\nparticular shape that is being sought for in the image.\nFigure 3.21 shows a close-up of the convolution of a binary image f with a 3 × 3 struc-\nturing element s and the resulting images for the operations described below. Let\nc = f ⊗s\n(3.42)\nbe the integer-valued count of the number of 1s inside each structuring element as it is scanned\nover the image and S be the size of the structuring element (number of pixels). The standard\noperations used in binary morphology include:\n• dilation: dilate(f, s) = θ(c, 1);\n• erosion: erode(f, s) = θ(c, S);\n• majority: maj(f, s) = θ(c, S/2);\n• opening: open(f, s) = dilate(erode(f, s), s);",
  "151": "3.3 More neighborhood operators\n129\n• closing: close(f, s) = erode(dilate(f, s), s).\nAs we can see from Figure 3.21, dilation grows (thickens) objects consisting of 1s, while\nerosion shrinks (thins) them. The opening and closing operations tend to leave large regions\nand smooth boundaries unaffected, while removing small objects or holes and smoothing\nboundaries.\nWhile we will not use mathematical morphology much in the rest of this book, it is a\nhandy tool to have around whenever you need to clean up some thresholded images. You\ncan ﬁnd additional details on morphology in other textbooks on computer vision and image\nprocessing (Haralick and Shapiro 1992, Section 5.2) (Bovik 2000, Section 2.2) (Ritter and\nWilson 2000, Section 7) as well as articles and books speciﬁcally on this topic (Serra 1982;\nSerra and Vincent 1992; Yuille, Vincent, and Geiger 1992; Soille 2006).\n3.3.3 Distance transforms\nThe distance transform is useful in quickly precomputing the distance to a curve or set of\npoints using a two-pass raster algorithm (Rosenfeld and Pfaltz 1966; Danielsson 1980; Borge-\nfors 1986; Paglieroni 1992; Breu, Gil, Kirkpatrick et al. 1995; Felzenszwalb and Huttenlocher\n2004a; Fabbri, Costa, Torelli et al. 2008). It has many applications, including level sets (Sec-\ntion 5.1.4), fast chamfer matching (binary image alignment) (Huttenlocher, Klanderman, and\nRucklidge 1993), feathering in image stitching and blending (Section 9.3.2), and nearest point\nalignment (Section 12.2.1).\nThe distance transform D(i, j) of a binary image b(i, j) is deﬁned as follows. Let d(k, l)\nbe some distance metric between pixel offsets. Two commonly used metrics include the city\nblock or Manhattan distance\nd1(k, l) = |k| + |l|\n(3.43)\nand the Euclidean distance\nd2(k, l) =\np\nk2 + l2.\n(3.44)\nThe distance transform is then deﬁned as\nD(i, j) =\nmin\nk,l:b(k,l)=0 d(i −k, j −l),\n(3.45)\ni.e., it is the distance to the nearest background pixel whose value is 0.\nThe D1 city block distance transform can be efﬁciently computed using a forward and\nbackward pass of a simple raster-scan algorithm, as shown in Figure 3.22. During the forward\npass, each non-zero pixel in b is replaced by the minimum of 1 + the distance of its north or\nwest neighbor. During the backward pass, the same occurs, except that the minimum is both\nover the current value D and 1 + the distance of the south and east neighbors (Figure 3.22).",
  "152": "130\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n.\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n1\n2\n0\n0\n0\n0\n1\n1\n2\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1\n2\n2\n3\n1\n0\n0\n1\n2\n2\n3\n1\n0\n0\n1\n2\n2\n2\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1\n2\n3\n0\n1\n2\n2\n1\n1\n0\n0\n1\n2\n2\n1\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n2\n1\n0\n0\n0\n0\n1\n2\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n(a)\n(b)\n(c)\n(d)\nFigure 3.22\nCity block distance transform: (a) original binary image; (b) top to bottom\n(forward) raster sweep: green values are used to compute the orange value; (c) bottom to top\n(backward) raster sweep: green values are merged with old orange value; (d) ﬁnal distance\ntransform.\nEfﬁciently computing the Euclidean distance transform is more complicated. Here, just\nkeeping the minimum scalar distance to the boundary during the two passes is not sufﬁcient.\nInstead, a vector-valued distance consisting of both the x and y coordinates of the distance\nto the boundary must be kept and compared using the squared distance (hypotenuse) rule. As\nwell, larger search regions need to be used to obtain reasonable results. Rather than explaining\nthe algorithm (Danielsson 1980; Borgefors 1986) in more detail, we leave it as an exercise\nfor the motivated reader (Exercise 3.13).\nFigure 3.11g shows a distance transform computed from a binary image. Notice how\nthe values grow away from the black (ink) regions and form ridges in the white area of the\noriginal image. Because of this linear growth from the starting boundary pixels, the distance\ntransform is also sometimes known as the grassﬁre transform, since it describes the time at\nwhich a ﬁre starting inside the black region would consume any given pixel, or a chamfer,\nbecause it resembles similar shapes used in woodworking and industrial design. The ridges\nin the distance transform become the skeleton (or medial axis transform (MAT)) of the region\nwhere the transform is computed, and consist of pixels that are of equal distance to two (or\nmore) boundaries (Tek and Kimia 2003; Sebastian and Kimia 2005).\nA useful extension of the basic distance transform is the signed distance transform, which\ncomputes distances to boundary pixels for all the pixels (Lavall´ee and Szeliski 1995). The\nsimplest way to create this is to compute the distance transforms for both the original bi-\nnary image and its complement and to negate one of them before combining. Because such\ndistance ﬁelds tend to be smooth, it is possible to store them more compactly (with mini-\nmal loss in relative accuracy) using a spline deﬁned over a quadtree or octree data structure\n(Lavall´ee and Szeliski 1995; Szeliski and Lavall´ee 1996; Frisken, Perry, Rockwood et al.\n2000). Such precomputed signed distance transforms can be extremely useful in efﬁciently\naligning and merging 2D curves and 3D surfaces (Huttenlocher, Klanderman, and Rucklidge",
  "153": "3.3 More neighborhood operators\n131\n(a)\n(b)\n(c)\nFigure 3.23 Connected component computation: (a) original grayscale image; (b) horizontal\nruns (nodes) connected by vertical (graph) edges (dashed blue)—runs are pseudocolored with\nunique colors inherited from parent nodes; (c) re-coloring after merging adjacent segments.\n1993; Szeliski and Lavall´ee 1996; Curless and Levoy 1996), especially if the vectorial version\nof the distance transform, i.e., a pointer from each pixel or voxel to the nearest boundary or\nsurface element, is stored and interpolated. Signed distance ﬁelds are also an essential com-\nponent of level set evolution (Section 5.1.4), where they are called characteristic functions.\n3.3.4 Connected components\nAnother useful semi-global image operation is ﬁnding connected components, which are de-\nﬁned as regions of adjacent pixels that have the same input value (or label). (In the remainder\nof this section, consider pixels to be adjacent if they are immediate N4 neighbors and they\nhave the same input value.) Connected components can be used in a variety of applications,\nsuch as ﬁnding individual letters in a scanned document or ﬁnding objects (say, cells) in a\nthresholded image and computing their area statistics.\nConsider the grayscale image in Figure 3.23a. There are four connected components in\nthis ﬁgure: the outermost set of white pixels, the large ring of gray pixels, the white enclosed\nregion, and the single gray pixel. These are shown pseudocolored in Figure 3.23c as pink,\ngreen, blue, and brown.\nTo compute the connected components of an image, we ﬁrst (conceptually) split the image\ninto horizontal runs of adjacent pixels, and then color the runs with unique labels, re-using\nthe labels of vertically adjacent runs whenever possible. In a second phase, adjacent runs of\ndifferent colors are then merged.\nWhile this description is a little sketchy, it should be enough to enable a motivated stu-\ndent to implement this algorithm (Exercise 3.14). Haralick and Shapiro (1992, Section 2.3)\ngive a much longer description of various connected component algorithms, including ones",
  "154": "132\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nthat avoid the creation of a potentially large re-coloring (equivalence) table. Well-debugged\nconnected component algorithms are also available in most image processing libraries.\nOnce a binary or multi-valued image has been segmented into its connected components,\nit is often useful to compute the area statistics for each individual region R. Such statistics\ninclude:\n• the area (number of pixels);\n• the perimeter (number of boundary pixels);\n• the centroid (average x and y values);\n• the second moments,\nM =\nX\n(x,y)∈R\n\"\nx −x\ny −y\n# h\nx −x\ny −y\ni\n,\n(3.46)\nfrom which the major and minor axis orientation and lengths can be computed using\neigenvalue analysis.7\nThese statistics can then be used for further processing, e.g., for sorting the regions by the area\nsize (to consider the largest regions ﬁrst) or for preliminary matching of regions in different\nimages.\n3.4 Fourier transforms\nIn Section 3.2, we mentioned that Fourier analysis could be used to analyze the frequency\ncharacteristics of various ﬁlters. In this section, we explain both how Fourier analysis lets us\ndetermine these characteristics (or equivalently, the frequency content of an image) and how\nusing the Fast Fourier Transform (FFT) lets us perform large-kernel convolutions in time that\nis independent of the kernel’s size. More comprehensive introductions to Fourier transforms\nare provided by Bracewell (1986); Glassner (1995); Oppenheim and Schafer (1996); Oppen-\nheim, Schafer, and Buck (1999).\nHow can we analyze what a given ﬁlter does to high, medium, and low frequencies? The\nanswer is to simply pass a sinusoid of known frequency through the ﬁlter and to observe by\nhow much it is attenuated. Let\ns(x) = sin(2πfx + φi) = sin(ωx + φi)\n(3.47)\n7 Moments can also be computed using Green’s theorem applied to the boundary pixels (Yang and Albregtsen\n1996).",
  "155": "3.4 Fourier transforms\n133\ns(x)\no(x)\nh(x)\ns\no\nx\nx\nA\nφ\nFigure 3.24\nThe Fourier Transform as the response of a ﬁlter h(x) to an input sinusoid\ns(x) = ejωx yielding an output sinusoid o(x) = h(x) ∗s(x) = Aejωx+φ.\nbe the input sinusoid whose frequency is f, angular frequency is ω = 2πf, and phase is φi.\nNote that in this section, we use the variables x and y to denote the spatial coordinates of an\nimage, rather than i and j as in the previous sections. This is both because the letters i and j\nare used for the imaginary number (the usage depends on whether you are reading complex\nvariables or electrical engineering literature) and because it is clearer how to distinguish the\nhorizontal (x) and vertical (y) components in frequency space. In this section, we use the\nletter j for the imaginary number, since that is the form more commonly found in the signal\nprocessing literature (Bracewell 1986; Oppenheim and Schafer 1996; Oppenheim, Schafer,\nand Buck 1999).\nIf we convolve the sinusoidal signal s(x) with a ﬁlter whose impulse response is h(x),\nwe get another sinusoid of the same frequency but different magnitude A and phase φo,\no(x) = h(x) ∗s(x) = A sin(ωx + φo),\n(3.48)\nas shown in Figure 3.24. To see that this is the case, remember that a convolution can be\nexpressed as a weighted summation of shifted input signals (3.14) and that the summation of\na bunch of shifted sinusoids of the same frequency is just a single sinusoid at that frequency.8\nThe new magnitude A is called the gain or magnitude of the ﬁlter, while the phase difference\n∆φ = φo −φi is called the shift or phase.\nIn fact, a more compact notation is to use the complex-valued sinusoid\ns(x) = ejωx = cos ωx + j sin ωx.\n(3.49)\nIn that case, we can simply write,\no(x) = h(x) ∗s(x) = Aejωx+φ.\n(3.50)\n8 If h is a general (non-linear) transform, additional harmonic frequencies are introduced. This was traditionally\nthe bane of audiophiles, who insisted on equipment with no harmonic distortion. Now that digital audio has intro-\nduced pure distortion-free sound, some audiophiles are buying retro tube ampliﬁers or digital signal processors that\nsimulate such distortions because of their “warmer sound”.",
  "156": "134\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe Fourier transform is simply a tabulation of the magnitude and phase response at each\nfrequency,\nH(ω) = F {h(x)} = Aejφ,\n(3.51)\ni.e., it is the response to a complex sinusoid of frequency ω passed through the ﬁlter h(x).\nThe Fourier transform pair is also often written as\nh(x) F\n↔H(ω).\n(3.52)\nUnfortunately, (3.51) does not give an actual formula for computing the Fourier transform.\nInstead, it gives a recipe, i.e., convolve the ﬁlter with a sinusoid, observe the magnitude and\nphase shift, repeat. Fortunately, closed form equations for the Fourier transform exist both in\nthe continuous domain,\nH(ω) =\nZ ∞\n−∞\nh(x)e−jωxdx,\n(3.53)\nand in the discrete domain,\nH(k) = 1\nN\nN−1\nX\nx=0\nh(x)e−j 2πkx\nN ,\n(3.54)\nwhere N is the length of the signal or region of analysis. These formulas apply both to ﬁlters,\nsuch as h(x), and to signals or images, such as s(x) or g(x).\nThe discrete form of the Fourier transform (3.54) is known as the Discrete Fourier Trans-\nform (DFT). Note that while (3.54) can be evaluated for any value of k, it only makes sense\nfor values in the range k ∈[−N\n2 , N\n2 ]. This is because larger values of k alias with lower\nfrequencies and hence provide no additional information, as explained in the discussion on\naliasing in Section 2.3.1.\nAt face value, the DFT takes O(N 2) operations (multiply-adds) to evaluate. Fortunately,\nthere exists a faster algorithm called the Fast Fourier Transform (FFT), which requires only\nO(N log2 N) operations (Bracewell 1986; Oppenheim, Schafer, and Buck 1999). We do not\nexplain the details of the algorithm here, except to say that it involves a series of log2 N\nstages, where each stage performs small 2×2 transforms (matrix multiplications with known\ncoefﬁcients) followed by some semi-global permutations. (You will often see the term but-\nterﬂy applied to these stages because of the pictorial shape of the signal processing graphs\ninvolved.) Implementations for the FFT can be found in most numerical and signal processing\nlibraries.\nNow that we have deﬁned the Fourier transform, what are some of its properties and how\ncan they be used? Table 3.1 lists a number of useful properties, which we describe in a little\nmore detail below:",
  "157": "3.4 Fourier transforms\n135\nProperty\nSignal\nTransform\nsuperposition\nf1(x) + f2(x)\nF1(ω) + F2(ω)\nshift\nf(x −x0)\nF(ω)e−jωx0\nreversal\nf(−x)\nF ∗(ω)\nconvolution\nf(x) ∗h(x)\nF(ω)H(ω)\ncorrelation\nf(x) ⊗h(x)\nF(ω)H∗(ω)\nmultiplication\nf(x)h(x)\nF(ω) ∗H(ω)\ndifferentiation\nf ′(x)\njωF(ω)\ndomain scaling\nf(ax)\n1/aF(ω/a)\nreal images\nf(x) = f ∗(x)\n⇔\nF(ω) = F(−ω)\nParseval’s Theorem\nP\nx[f(x)]2\n=\nP\nω[F(ω)]2\nTable 3.1\nSome useful properties of Fourier transforms. The original transform pair is\nF(ω) = F{f(x)}.\n• Superposition: The Fourier transform of a sum of signals is the sum of their Fourier\ntransforms. Thus, the Fourier transform is a linear operator.\n• Shift: The Fourier transform of a shifted signal is the transform of the original signal\nmultiplied by a linear phase shift (complex sinusoid).\n• Reversal: The Fourier transform of a reversed signal is the complex conjugate of the\nsignal’s transform.\n• Convolution: The Fourier transform of a pair of convolved signals is the product of\ntheir transforms.\n• Correlation: The Fourier transform of a correlation is the product of the ﬁrst transform\ntimes the complex conjugate of the second one.\n• Multiplication: The Fourier transform of the product of two signals is the convolution\nof their transforms.\n• Differentiation: The Fourier transform of the derivative of a signal is that signal’s\ntransform multiplied by the frequency. In other words, differentiation linearly empha-\nsizes (magniﬁes) higher frequencies.\n• Domain scaling: The Fourier transform of a stretched signal is the equivalently com-\npressed (and scaled) version of the original transform and vice versa.",
  "158": "136\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n• Real images: The Fourier transform of a real-valued signal is symmetric around the\norigin. This fact can be used to save space and to double the speed of image FFTs\nby packing alternating scanlines into the real and imaginary parts of the signal being\ntransformed.\n• Parseval’s Theorem: The energy (sum of squared values) of a signal is the same as\nthe energy of its Fourier transform.\nAll of these properties are relatively straightforward to prove (see Exercise 3.15) and they will\ncome in handy later in the book, e.g., when designing optimum Wiener ﬁlters (Section 3.4.3)\nor performing fast image correlations (Section 8.1.2).\n3.4.1 Fourier transform pairs\nNow that we have these properties in place, let us look at the Fourier transform pairs of some\ncommonly occurring ﬁlters and signals, as listed in Table 3.2. In more detail, these pairs are\nas follows:\n• Impulse: The impulse response has a constant (all frequency) transform.\n• Shifted impulse: The shifted impulse has unit magnitude and linear phase.\n• Box ﬁlter: The box (moving average) ﬁlter\nbox(x) =\n(\n1\nif |x| ≤1\n0\nelse\n(3.55)\nhas a sinc Fourier transform,\nsinc(ω) = sin ω\nω\n,\n(3.56)\nwhich has an inﬁnite number of side lobes. Conversely, the sinc ﬁlter is an ideal low-\npass ﬁlter. For a non-unit box, the width of the box a and the spacing of the zero\ncrossings in the sinc 1/a are inversely proportional.\n• Tent: The piecewise linear tent function,\ntent(x) = max(0, 1 −|x|),\n(3.57)\nhas a sinc2 Fourier transform.\n• Gaussian: The (unit area) Gaussian of width σ,\nG(x; σ) =\n1\n√\n2πσ e−x2\n2σ2 ,\n(3.58)\nhas a (unit height) Gaussian of width σ−1 as its Fourier transform.",
  "159": "3.4 Fourier transforms\n137\nName\nSignal\nTransform\nimpulse\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\nδ(x)\n⇔\n1\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\nshifted\nimpulse\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\nδ(x −u)\n⇔\ne−jωu\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\nbox ﬁlter\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\nbox(x/a)\n⇔\nasinc(aω)\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\ntent\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\ntent(x/a)\n⇔\nasinc2(aω)\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\nGaussian\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\nG(x; σ)\n⇔\n√\n2π\nσ G(ω; σ−1)\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\nLaplacian\nof Gaussian\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\n( x2\nσ4 −\n1\nσ2 )G(x; σ)\n⇔\n−\n√\n2π\nσ ω2G(ω; σ−1)\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\nGabor\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\ncos(ω0x)G(x; σ)\n⇔\n√\n2π\nσ G(ω ± ω0; σ−1)\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\nunsharp\nmask\n-0.5\n0.0\n0.5\n1.0\n1.5\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\n(1 + γ)δ(x)\n−γG(x; σ)\n⇔\n(1 + γ)−\n√\n2πγ\nσ\nG(ω; σ−1)\n-0.5\n0.0\n0.5\n1.0\n1.5\n-0.5000\n0.0000\n0.5000\nwindowed\nsinc\n-0.5\n0.0\n0.5\n1.0\n-1.0000\n-0.5000\n0.0000\n0.5000\n1.0000\nrcos(x/(aW))\nsinc(x/a)\n⇔\n(see Figure 3.29)\n-0.5\n0.0\n0.5\n1.0\n-0.5000\n0.0000\n0.5000\nTable 3.2 Some useful (continuous) Fourier transform pairs: The dashed line in the Fourier\ntransform of the shifted impulse indicates its (linear) phase. All other transforms have zero\nphase (they are real-valued). Note that the ﬁgures are not necessarily drawn to scale but\nare drawn to illustrate the general shape and characteristics of the ﬁlter or its response. In\nparticular, the Laplacian of Gaussian is drawn inverted because it resembles more a “Mexican\nhat”, as it is sometimes called.",
  "160": "138\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n• Laplacian of Gaussian: The second derivative of a Gaussian of width σ,\nLoG(x; σ) = (x2\nσ4 −1\nσ2 )G(x; σ)\n(3.59)\nhas a band-pass response of\n−\n√\n2π\nσ\nω2G(ω; σ−1)\n(3.60)\nas its Fourier transform.\n• Gabor: The even Gabor function, which is the product of a cosine of frequency ω0 and\na Gaussian of width σ, has as its transform the sum of the two Gaussians of width σ−1\ncentered at ω = ±ω0. The odd Gabor function, which uses a sine, is the difference\nof two such Gaussians. Gabor functions are often used for oriented and band-pass\nﬁltering, since they can be more frequency selective than Gaussian derivatives.\n• Unsharp mask: The unsharp mask introduced in (3.22) has as its transform a unit\nresponse with a slight boost at higher frequencies.\n• Windowed sinc: The windowed (masked) sinc function shown in Table 3.2 has a re-\nsponse function that approximates an ideal low-pass ﬁlter better and better as additional\nside lobes are added (W is increased). Figure 3.29 shows the shapes of these such ﬁl-\nters along with their Fourier transforms. For these examples, we use a one-lobe raised\ncosine,\nrcos(x) = 1\n2(1 + cos πx)box(x),\n(3.61)\nalso known as the Hann window, as the windowing function. Wolberg (1990) and\nOppenheim, Schafer, and Buck (1999) discuss additional windowing functions, which\ninclude the Lanczos window, the positive ﬁrst lobe of a sinc function.\nWe can also compute the Fourier transforms for the small discrete kernels shown in Fig-\nure 3.14 (see Table 3.3). Notice how the moving average ﬁlters do not uniformly dampen\nhigher frequencies and hence can lead to ringing artifacts. The binomial ﬁlter (Gomes and\nVelho 1997) used as the “Gaussian” in Burt and Adelson’s (1983a) Laplacian pyramid (see\nSection 3.5), does a decent job of separating the high and low frequencies, but still leaves\na fair amount of high-frequency detail, which can lead to aliasing after downsampling. The\nSobel edge detector at ﬁrst linearly accentuates frequencies, but then decays at higher fre-\nquencies, and hence has trouble detecting ﬁne-scale edges, e.g., adjacent black and white\ncolumns. We look at additional examples of small kernel Fourier transforms in Section 3.5.2,\nwhere we study better kernels for pre-ﬁltering before decimation (size reduction).",
  "161": "3.4 Fourier transforms\n139\nName\nKernel\nTransform\nPlot\nbox-3\n1\n3\n1\n1\n1\n1\n3(1 + 2 cos ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nbox-5\n1\n5\n1\n1\n1\n1\n1\n1\n5(1 + 2 cos ω + 2 cos 2ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nlinear\n1\n4\n1\n2\n1\n1\n2(1 + cos ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nbinomial\n1\n16\n1\n4\n6\n4\n1\n1\n4(1 + cos ω)2\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nSobel\n1\n2\n−1\n0\n1\nsin ω\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\ncorner\n1\n2\n−1\n2\n−1\n1\n2(1 −cos ω)\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nTable 3.3 Fourier transforms of the separable kernels shown in Figure 3.14.",
  "162": "140\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3.4.2 Two-dimensional Fourier transforms\nThe formulas and insights we have developed for one-dimensional signals and their trans-\nforms translate directly to two-dimensional images. Here, instead of just specifying a hor-\nizontal or vertical frequency ωx or ωy, we can create an oriented sinusoid of frequency\n(ωx, ωy),\ns(x, y) = sin(ωxx + ωyy).\n(3.62)\nThe corresponding two-dimensional Fourier transforms are then\nH(ωx, ωy) =\nZ ∞\n−∞\nZ ∞\n−∞\nh(x, y)e−j(ωxx+ωyy)dx dy,\n(3.63)\nand in the discrete domain,\nH(kx, ky) =\n1\nMN\nM−1\nX\nx=0\nN−1\nX\ny=0\nh(x, y)e−j2π\nkxx+kyy\nMN\n,\n(3.64)\nwhere M and N are the width and height of the image.\nAll of the Fourier transform properties from Table 3.1 carry over to two dimensions if\nwe replace the scalar variables x, ω, x0 and a with their 2D vector counterparts x = (x, y),\nω = (ωx, ωy), x0 = (x0, y0), and a = (ax, ay), and use vector inner products instead of\nmultiplications.\n3.4.3 Wiener ﬁltering\nWhile the Fourier transform is a useful tool for analyzing the frequency characteristics of a\nﬁlter kernel or image, it can also be used to analyze the frequency spectrum of a whole class\nof images.\nA simple model for images is to assume that they are random noise ﬁelds whose expected\nmagnitude at each frequency is given by this power spectrum Ps(ωx, ωy), i.e.,\n\n[S(ωx, ωy)]2\u000b\n= Ps(ωx, ωy),\n(3.65)\nwhere the angle brackets ⟨·⟩denote the expected (mean) value of a random variable.9 To\ngenerate such an image, we simply create a random Gaussian noise image S(ωx, ωy) where\neach “pixel” is a zero-mean Gaussian10 of variance Ps(ωx, ωy) and then take its inverse FFT.\nThe observation that signal spectra capture a ﬁrst-order description of spatial statistics\nis widely used in signal and image processing. In particular, assuming that an image is a\n9 The notation E[·] is also commonly used.\n10 We set the DC (i.e., constant) component at S(0, 0) to the mean grey level. See Algorithm C.1 in Appendix C.2\nfor code to generate Gaussian noise.",
  "163": "3.4 Fourier transforms\n141\nsample from a correlated Gaussian random noise ﬁeld combined with a statistical model of\nthe measurement process yields an optimum restoration ﬁlter known as the Wiener ﬁlter.11\nTo derive the Wiener ﬁlter, we analyze each frequency component of a signal’s Fourier\ntransform independently. The noisy image formation process can be written as\no(x, y) = s(x, y) + n(x, y),\n(3.66)\nwhere s(x, y) is the (unknown) image we are trying to recover, n(x, y) is the additive noise\nsignal, and o(x, y) is the observed noisy image. Because of the linearity of the Fourier trans-\nform, we can write\nO(ωx, ωy) = S(ωx, ωy) + N(ωx, ωy),\n(3.67)\nwhere each quantity in the above equation is the Fourier transform of the corresponding\nimage.\nAt each frequency (ωx, ωy), we know from our image spectrum that the unknown trans-\nform component S(ωx, ωy) has a prior distribution which is a zero-mean Gaussian with vari-\nance Ps(ωx, ωy). We also have noisy measurement O(ωx, ωy) whose variance is Pn(ωx, ωy),\ni.e., the power spectrum of the noise, which is usually assumed to be constant (white),\nPn(ωx, ωy) = σ2\nn.\nAccording to Bayes’ Rule (Appendix B.4), the posterior estimate of S can be written as\np(S|O) = p(O|S)p(S)\np(O)\n,\n(3.68)\nwhere p(O) =\nR\nS p(O|S)p(S) is a normalizing constant used to make the p(S|O) distribution\nproper (integrate to 1). The prior distribution p(S) is given by\np(S) = e−(S−µ)2\n2Ps\n,\n(3.69)\nwhere µ is the expected mean at that frequency (0 everywhere except at the origin) and the\nmeasurement distribution P(O|S) is given by\np(S) = e−(S−O)2\n2Pn\n.\n(3.70)\nTaking the negative logarithm of both sides of (3.68) and setting µ = 0 for simplicity, we\nget\n−log p(S|O)\n=\n−log p(O|S) −log p(S) + C\n(3.71)\n=\n1/2P −1\nn (S −O)2 + 1/2P −1\ns\nS2 + C,\n(3.72)\n11 Wiener is pronounced “veener” since, in German, the “w” is pronounced “v”. Remember that next time you\norder “Wiener schnitzel”.",
  "164": "142\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nP\nn\nW\n(a)\n(b)\nFigure 3.25 One-dimensional Wiener ﬁlter: (a) power spectrum of signal Ps(f), noise level\nσ2, and Wiener ﬁlter transform W(f); (b) Wiener ﬁlter spatial kernel.\nwhich is the negative posterior log likelihood. The minimum of this quantity is easy to\ncompute,\nSopt =\nP −1\nn\nP −1\nn\n+ P −1\ns\nO =\nPs\nPs + Pn\nO =\n1\n1 + Pn/Ps\nO.\n(3.73)\nThe quantity\nW(ωx, ωy) =\n1\n1 + σ2n/Ps(ωx, ωy)\n(3.74)\nis the Fourier transform of the optimum Wiener ﬁlter needed to remove the noise from an\nimage whose power spectrum is Ps(ωx, ωy).\nNotice that this ﬁlter has the right qualitative properties, i.e., for low frequencies where\nPs ≫σ2\nn, it has unit gain, whereas for high frequencies, it attenuates the noise by a factor\nPs/σ2\nn. Figure 3.25 shows the one-dimensional transform W(f) and the corresponding ﬁlter\nkernel w(x) for the commonly assumed case of P(f) = f −2 (Field 1987). Exercise 3.16 has\nyou compare the Wiener ﬁlter as a denoising algorithm to hand-tuned Gaussian smoothing.\nThe methodology given above for deriving the Wiener ﬁlter can easily be extended to the\ncase where the observed image is a noisy blurred version of the original image,\no(x, y) = b(x, y) ∗s(x, y) + n(x, y),\n(3.75)\nwhere b(x, y) is the known blur kernel. Rather than deriving the corresponding Wiener ﬁl-\nter, we leave it as an exercise (Exercise 3.17), which also encourages you to compare your\nde-blurring results with unsharp masking and na¨ıve inverse ﬁltering. More sophisticated al-\ngorithms for blur removal are discussed in Sections 3.7 and 10.3.\nDiscrete cosine transform\nThe discrete cosine transform (DCT) is a variant of the Fourier transform particularly well-\nsuited to compressing images in a block-wise fashion. The one-dimensional DCT is com-\nputed by taking the dot product of each N-wide block of pixels with a set of cosines of",
  "165": "3.4 Fourier transforms\n143\n-1.00\n-0.75\n-0.50\n-0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 3.26 Discrete cosine transform (DCT) basis functions: The ﬁrst DC (i.e., constant)\nbasis is the horizontal blue line, the second is the brown half-cycle waveform, etc. These\nbases are widely used in image and video compression standards such as JPEG.\ndifferent frequencies,\nF(k) =\nN−1\nX\ni=0\ncos\n\u0012 π\nN (i + 1\n2)k\n\u0013\nf(i),\n(3.76)\nwhere k is the coefﬁcient (frequency) index, and the 1/2-pixel offset is used to make the\nbasis coefﬁcients symmetric (Wallace 1991). Some of the discrete cosine basis functions are\nshown in Figure 3.26. As you can see, the ﬁrst basis function (the straight blue line) encodes\nthe average DC value in the block of pixels, while the second encodes a slightly curvy version\nof the slope.\nIn turns out that the DCT is a good approximation to the optimal Karhunen–Lo`eve decom-\nposition of natural image statistics over small patches, which can be obtained by performing\na principal component analysis (PCA) of images, as described in Section 14.2.1. The KL-\ntransform de-correlates the signal optimally (assuming the signal is described by its spectrum)\nand thus, theoretically, leads to optimal compression.\nThe two-dimensional version of the DCT is deﬁned similarly,\nF(k, l) =\nN−1\nX\ni=0\nN−1\nX\nj=0\ncos\n\u0012 π\nN (i + 1\n2)k\n\u0013\ncos\n\u0012 π\nN (j + 1\n2)l\n\u0013\nf(i, j).\n(3.77)\nLike the 2D Fast Fourier Transform, the 2D DCT can be implemented separably, i.e., ﬁrst\ncomputing the DCT of each line in the block and then computing the DCT of each resulting\ncolumn. Like the FFT, each of the DCTs can also be computed in O(N log N) time.\nAs we mentioned in Section 2.3.3, the DCT is widely used in today’s image and video\ncompression algorithms, although it is slowly being supplanted by wavelet algorithms (Si-\nmoncelli and Adelson 1990b), as discussed in Section 3.5.4, and overlapped variants of the\nDCT (Malvar 1990, 1998, 2000), which are used in the new JPEG XR standard.12 These\n12 http://www.itu.int/rec/T-REC-T.832-200903-I/en.",
  "166": "144\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nnewer algorithms suffer less from the blocking artifacts (visible edge-aligned discontinuities)\nthat result from the pixels in each block (typically 8 × 8) being transformed and quantized\nindependently. See Exercise 3.30 for ideas on how to remove blocking artifacts from com-\npressed JPEG images.\n3.4.4 Application: Sharpening, blur, and noise removal\nAnother common application of image processing is the enhancement of images through the\nuse of sharpening and noise removal operations, which require some kind of neighborhood\nprocessing. Traditionally, these kinds of operation were performed using linear ﬁltering (see\nSections 3.2 and Section 3.4.3). Today, it is more common to use non-linear ﬁlters (Sec-\ntion 3.3.1), such as the weighted median or bilateral ﬁlter (3.34–3.37), anisotropic diffusion\n(3.39–3.40), or non-local means (Buades, Coll, and Morel 2008). Variational methods (Sec-\ntion 3.7.1), especially those using non-quadratic (robust) norms such as the L1 norm (which\nis called total variation), are also often used. Figure 3.19 shows some examples of linear and\nnon-linear ﬁlters being used to remove noise.\nWhen measuring the effectiveness of image denoising algorithms, it is common to report\nthe results as a peak signal-to-noise ratio (PSNR) measurement (2.119), where I(x) is the\noriginal (noise-free) image and ˆI(x) is the image after denoising; this is for the case where the\nnoisy image has been synthetically generated, so that the clean image is known. A better way\nto measure the quality is to use a perceptually based similarity metric, such as the structural\nsimilarity (SSIM) index (Wang, Bovik, Sheikh et al. 2004; Wang, Bovik, and Simoncelli\n2005).\nExercises 3.11, 3.16, 3.17, 3.21, and 3.28 have you implement some of these operations\nand compare their effectiveness. More sophisticated techniques for blur removal and the\nrelated task of super-resolution are discussed in Section 10.3.\n3.5 Pyramids and wavelets\nSo far in this chapter, all of the image transformations we have studied produce output images\nof the same size as the inputs. Often, however, we may wish to change the resolution of an\nimage before proceeding further. For example, we may need to interpolate a small image to\nmake its resolution match that of the output printer or computer screen. Alternatively, we\nmay want to reduce the size of an image to speed up the execution of an algorithm or to save\non storage space or transmission time.\nSometimes, we do not even know what the appropriate resolution for the image should\nbe. Consider, for example, the task of ﬁnding a face in an image (Section 14.1.1). Since we\ndo not know the scale at which the face will appear, we need to generate a whole pyramid",
  "167": "3.5 Pyramids and wavelets\n145\nof differently sized images and scan each one for possible faces. (Biological visual systems\nalso operate on a hierarchy of scales (Marr 1982).) Such a pyramid can also be very helpful\nin accelerating the search for an object by ﬁrst ﬁnding a smaller instance of that object at a\ncoarser level of the pyramid and then looking for the full resolution object only in the vicinity\nof coarse-level detections (Section 8.1.1). Finally, image pyramids are extremely useful for\nperforming multi-scale editing operations such as blending images while maintaining details.\nIn this section, we ﬁrst discuss good ﬁlters for changing image resolution, i.e., upsampling\n(interpolation, Section 3.5.1) and downsampling (decimation, Section 3.5.2). We then present\nthe concept of multi-resolution pyramids, which can be used to create a complete hierarchy\nof differently sized images and to enable a variety of applications (Section 3.5.3). A closely\nrelated concept is that of wavelets, which are a special kind of pyramid with higher frequency\nselectivity and other useful properties (Section 3.5.4). Finally, we present a useful application\nof pyramids, namely the blending of different images in a way that hides the seams between\nthe image boundaries (Section 3.5.5).\n3.5.1 Interpolation\nIn order to interpolate (or upsample) an image to a higher resolution, we need to select some\ninterpolation kernel with which to convolve the image,\ng(i, j) =\nX\nk,l\nf(k, l)h(i −rk, j −rl).\n(3.78)\nThis formula is related to the discrete convolution formula (3.14), except that we replace k\nand l in h() with rk and rl, where r is the upsampling rate. Figure 3.27a shows how to think\nof this process as the superposition of sample weighted interpolation kernels, one centered\nat each input sample k. An alternative mental model is shown in Figure 3.27b, where the\nkernel is centered at the output pixel value i (the two forms are equivalent). The latter form\nis sometimes called the polyphase ﬁlter form, since the kernel values h(i) can be stored as r\nseparate kernels, each of which is selected for convolution with the input samples depending\non the phase of i relative to the upsampled grid.\nWhat kinds of kernel make good interpolators? The answer depends on the application\nand the computation time involved. Any of the smoothing kernels shown in Tables 3.2 and 3.3\ncan be used after appropriate re-scaling.13 The linear interpolator (corresponding to the tent\nkernel) produces interpolating piecewise linear curves, which result in unappealing creases\nwhen applied to images (Figure 3.28a). The cubic B-spline, whose discrete 1/2-pixel sam-\npling appears as the binomial kernel in Table 3.3, is an approximating kernel (the interpolated\n13 The smoothing kernels in Table 3.3 have a unit area. To turn them into interpolating kernels, we simply scale\nthem up by the interpolation rate r.",
  "168": "146\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf(k-1)\nf(k+1)\nf(k)\ng(i)\nr·(k-1)\nr·(k+1)\nrk\ni\nf(k)h(i-rk)\nf(k-1)\nf(k+1)\nf(k)\ng(i)\nr·(k-1)\nr·(k+1)\nrk\ni\nh(i-rk)\n(a)\n(b)\nFigure 3.27\nSignal interpolation, g(i) = P\nk f(k)h(i −rk): (a) weighted summation of\ninput values; (b) polyphase ﬁlter interpretation.\nimage does not pass through the input data points) that produces soft images with reduced\nhigh-frequency detail. The equation for the cubic B-spline is easiest to derive by convolving\nthe tent function (linear B-spline) with itself.\nWhile most graphics cards use the bilinear kernel (optionally combined with a MIP-\nmap—see Section 3.5.3), most photo editing packages use bicubic interpolation. The cu-\nbic interpolant is a C1 (derivative-continuous) piecewise-cubic spline (the term “spline” is\nsynonymous with “piecewise-polynomial”)14 whose equation is\nh(x) =\n\n\n\n\n\n1 −(a + 3)x2 + (a + 2)|x|3\nif |x| < 1\na(|x| −1)(|x| −2)2\nif 1 ≤|x| < 2\n0\notherwise,\n(3.79)\nwhere a speciﬁes the derivative at x = 1 (Parker, Kenyon, and Troxel 1983). The value of\na is often set to −1, since this best matches the frequency characteristics of a sinc function\n(Figure 3.29). It also introduces a small amount of sharpening, which can be visually appeal-\ning. Unfortunately, this choice does not linearly interpolate straight lines (intensity ramps),\nso some visible ringing may occur. A better choice for large amounts of interpolation is prob-\nably a = −0.5, which produces a quadratic reproducing spline; it interpolates linear and\nquadratic functions exactly (Wolberg 1990, Section 5.4.3).\nFigure 3.29 shows the a = −1\nand a = −0.5 cubic interpolating kernel along with their Fourier transforms; Figure 3.28b\nand c shows them being applied to two-dimensional interpolation.\nSplines have long been used for function and data value interpolation because of the abil-\nity to precisely specify derivatives at control points and efﬁcient incremental algorithms for\ntheir evaluation (Bartels, Beatty, and Barsky 1987; Farin 1992, 1996). Splines are widely used\nin geometric modeling and computer-aided design (CAD) applications, although they have\n14 The term “spline” comes from the draughtsman’s workshop, where it was the name of a ﬂexible piece of wood\nor metal used to draw smooth curves.",
  "169": "3.5 Pyramids and wavelets\n147\n(a)\n(b)\n(c)\n(d)\nFigure 3.28\nTwo-dimensional image interpolation: (a) bilinear; (b) bicubic (a = −1); (c)\nbicubic (a = −0.5); (d) windowed sinc (nine taps).\n-0.5\n0.0\n0.5\n1.0\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\nwindowed-sinc\ntent\ncubic a=-0.5\ncubic a=-1\n-200\n-180\n-160\n-140\n-120\n-100\n-80\n-60\n-40\n-20\n0\n-0.5\n0.0\n0.5\nwindowed-sinc\ncubic a=-0.5\ntent\ncubic a=-1\n(a)\n(b)\nFigure 3.29 (a) Some windowed sinc functions and (b) their log Fourier transforms: raised-\ncosine windowed sinc in blue, cubic interpolators (a = −1 and a = −0.5) in green and\npurple, and tent function in brown. They are often used to perform high-accuracy low-pass\nﬁltering operations.",
  "170": "148\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nstarted being displaced by subdivision surfaces (Zorin, Schr¨oder, and Sweldens 1996; Peters\nand Reif 2008). In computer vision, splines are often used for elastic image deformations\n(Section 3.6.2), motion estimation (Section 8.3), and surface interpolation (Section 12.3). In\nfact, it is possible to carry out most image processing operations by representing images as\nsplines and manipulating them in a multi-resolution framework (Unser 1999).\nThe highest quality interpolator is generally believed to be the windowed sinc function\nbecause it both preserves details in the lower resolution image and avoids aliasing. (It is also\npossible to construct a C1 piecewise-cubic approximation to the windowed sinc by matching\nits derivatives at zero crossing (Szeliski and Ito 1986).) However, some people object to the\nexcessive ringing that can be introduced by the windowed sinc and to the repetitive nature\nof the ringing frequencies (see Figure 3.28d). For this reason, some photographers prefer\nto repeatedly interpolate images by a small fractional amount (this tends to de-correlate the\noriginal pixel grid with the ﬁnal image).\nAdditional possibilities include using the bilat-\neral ﬁlter as an interpolator (Kopf, Cohen, Lischinski et al. 2007), using global optimization\n(Section 3.6) or hallucinating details (Section 10.3).\n3.5.2 Decimation\nWhile interpolation can be used to increase the resolution of an image, decimation (downsam-\npling) is required to reduce the resolution.15 To perform decimation, we ﬁrst (conceptually)\nconvolve the image with a low-pass ﬁlter (to avoid aliasing) and then keep every rth sample.\nIn practice, we usually only evaluate the convolution at every rth sample,\ng(i, j) =\nX\nk,l\nf(k, l)h(ri −k, rj −l),\n(3.80)\nas shown in Figure 3.30. Note that the smoothing kernel h(k, l), in this case, is often a\nstretched and re-scaled version of an interpolation kernel. Alternatively, we can write\ng(i, j) = 1\nr\nX\nk,l\nf(k, l)h(i −k/r, j −l/r)\n(3.81)\nand keep the same kernel h(k, l) for both interpolation and decimation.\nOne commonly used (r = 2) decimation ﬁlter is the binomial ﬁlter introduced by Burt\nand Adelson (1983a). As shown in Table 3.3, this kernel does a decent job of separating\nthe high and low frequencies, but still leaves a fair amount of high-frequency detail, which\ncan lead to aliasing after downsampling. However, for applications such as image blending\n(discussed later in this section), this aliasing is of little concern.\n15 The term “decimation” has a gruesome etymology relating to the practice of killing every tenth soldier in\na Roman unit guilty of cowardice. It is generally used in signal processing to mean any downsampling or rate\nreduction operation.",
  "171": "3.5 Pyramids and wavelets\n149\n(a)\n(b)\nFigure 3.30 Signal decimation: (a) the original samples are (b) convolved with a low-pass\nﬁlter before being downsampled.\nIf, however, the downsampled images will be displayed directly to the user or, perhaps,\nblended with other resolutions (as in MIP-mapping, Section 3.5.3), a higher-quality ﬁlter is\ndesired. For high downsampling rates, the windowed sinc pre-ﬁlter is a good choice (Fig-\nure 3.29). However, for small downsampling rates, e.g., r = 2, more careful ﬁlter design is\nrequired.\nTable 3.4 shows a number of commonly used r = 2 downsampling ﬁlters, while Fig-\nure 3.31 shows their corresponding frequency responses. These ﬁlters include:\n• the linear [1, 2, 1] ﬁlter gives a relatively poor response;\n• the binomial [1, 4, 6, 4, 1] ﬁlter cuts off a lot of frequencies but is useful for computer\nvision analysis pyramids;\n• the cubic ﬁlters from (3.79); the a = −1 ﬁlter has a sharper fall-off than the a = −0.5\nﬁlter (Figure 3.31);\n|n|\nLinear\nBinomial\nCubic\na = −1\nCubic\na = −0.5\nWindowed\nsinc\nQMF-9\nJPEG\n2000\n0\n0.50\n0.3750\n0.5000\n0.50000\n0.4939\n0.5638\n0.6029\n1\n0.25\n0.2500\n0.3125\n0.28125\n0.2684\n0.2932\n0.2669\n2\n0.0625\n0.0000\n0.00000\n0.0000\n-0.0519\n-0.0782\n3\n-0.0625\n-0.03125\n-0.0153\n-0.0431\n-0.0169\n4\n0.0000\n0.0198\n0.0267\nTable 3.4\nFilter coefﬁcients for 2× decimation. These ﬁlters are of odd length, are sym-\nmetric, and are normalized to have unit DC gain (sum up to 1). See Figure 3.31 for their\nassociated frequency responses.",
  "172": "150\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nLinear\nBinomial\nCubic a=-1\nCubic a=-0.5\nWind. sinc\nQMF-9\nJPEG 2000\nFigure 3.31\nFrequency response for some 2× decimation ﬁlters. The cubic a = −1 ﬁlter\nhas the sharpest fall-off but also a bit of ringing; the wavelet analysis ﬁlters (QMF-9 and\nJPEG 2000), while useful for compression, have more aliasing.\n• a cosine-windowed sinc function (Table 3.2);\n• the QMF-9 ﬁlter of Simoncelli and Adelson (1990b) is used for wavelet denoising and\naliases a fair amount (note that the original ﬁlter coefﬁcients are normalized to\n√\n2 gain\nso they can be “self-inverting”);\n• the 9/7 analysis ﬁlter from JPEG 2000 (Taubman and Marcellin 2002).\nPlease see the original papers for the full-precision values of some of these coefﬁcients.\n3.5.3 Multi-resolution representations\nNow that we have described interpolation and decimation algorithms, we can build a complete\nimage pyramid (Figure 3.32). As we mentioned before, pyramids can be used to accelerate\ncoarse-to-ﬁne search algorithms, to look for objects or patterns at different scales, and to per-\nform multi-resolution blending operations. They are also widely used in computer graphics\nhardware and software to perform fractional-level decimation using the MIP-map, which we\ncover in Section 3.6.\nThe best known (and probably most widely used) pyramid in computer vision is Burt\nand Adelson’s (1983a) Laplacian pyramid. To construct the pyramid, we ﬁrst blur and sub-\nsample the original image by a factor of two and store this in the next level of the pyramid\n(Figure 3.33). Because adjacent levels in the pyramid are related by a sampling rate r = 2,\nthis kind of pyramid is known as an octave pyramid. Burt and Adelson originally proposed a",
  "173": "3.5 Pyramids and wavelets\n151\nfine\nmedium\ncoarse\nl = 0\nl = 1\nl = 2\nFigure 3.32\nA traditional image pyramid: each level has half the resolution (width and\nheight), and hence a quarter of the pixels, of its parent level.\nﬁve-tap kernel of the form\nc\nb\na\nb\nc ,\n(3.82)\nwith b = 1/4 and c = 1/4−a/2. In practice, a = 3/8, which results in the familiar binomial\nkernel,\n1\n16 1\n4\n6\n4\n1 ,\n(3.83)\nwhich is particularly easy to implement using shifts and adds. (This was important in the days\nwhen multipliers were expensive.) The reason they call their resulting pyramid a Gaussian\npyramid is that repeated convolutions of the binomial kernel converge to a Gaussian.16\nTo compute the Laplacian pyramid, Burt and Adelson ﬁrst interpolate a lower resolu-\ntion image to obtain a reconstructed low-pass version of the original image (Figure 3.34b).\nThey then subtract this low-pass version from the original to yield the band-pass “Laplacian”\nimage, which can be stored away for further processing. The resulting pyramid has perfect\nreconstruction, i.e., the Laplacian images plus the base-level Gaussian (L2 in Figure 3.34b)\nare sufﬁcient to exactly reconstruct the original image. Figure 3.33 shows the same com-\nputation in one dimension as a signal processing diagram, which completely captures the\ncomputations being performed during the analysis and re-synthesis stages.\nBurt and Adelson also describe a variant on the Laplacian pyramid, where the low-pass\nimage is taken from the original blurred image rather than the reconstructed pyramid (piping\nthe output of the L box directly to the subtraction in Figure 3.34b). This variant has less\n16 Then again, this is true for any smoothing kernel (Wells 1986).",
  "174": "152\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n¼\n⅜\n¼\n16\n1\n16\n1\n¼\n⅜\n¼\n16\n1\n16\n1\n½\n¾\n½\n½\n¾\n½\n⅛\n⅛\n⅛\n⅛\n(a)\n(b)\nFigure 3.33 The Gaussian pyramid shown as a signal processing diagram: The (a) analysis\nand (b) re-synthesis stages are shown as using similar computations. The white circles in-\ndicate zero values inserted by the ↑2 upsampling operation. Notice how the reconstruction\nﬁlter coefﬁcients are twice the analysis coefﬁcients. The computation is shown as ﬂowing\ndown the page, regardless of whether we are going from coarse to ﬁne or vice versa.\naliasing, since it avoids one downsampling and upsampling round-trip, but it is not self-\ninverting, since the Laplacian images are no longer adequate to reproduce the original image.\nAs with the Gaussian pyramid, the term Laplacian is a bit of a misnomer, since their\nband-pass images are really differences of (approximate) Gaussians, or DoGs,\nDoG{I; σ1, σ2} = Gσ1 ∗I −Gσ2 ∗I = (Gσ1 −Gσ2) ∗I.\n(3.84)\nA Laplacian of Gaussian (which we saw in (3.26)) is actually its second derivative,\nLoG{I; σ} = ∇2(Gσ ∗I) = (∇2Gσ) ∗I,\n(3.85)\nwhere\n∇2 = ∂2\n∂x2 + ∂2\n∂y2\n(3.86)\nis the Laplacian (operator) of a function. Figure 3.35 shows how the Differences of Gaussian\nand Laplacians of Gaussian look in both space and frequency.\nLaplacians of Gaussian have elegant mathematical properties, which have been widely\nstudied in the scale-space community (Witkin 1983; Witkin, Terzopoulos, and Kass 1986;\nLindeberg 1990; Nielsen, Florack, and Deriche 1997) and can be used for a variety of appli-\ncations including edge detection (Marr and Hildreth 1980; Perona and Malik 1990b), stereo\nmatching (Witkin, Terzopoulos, and Kass 1987), and image enhancement (Nielsen, Florack,\nand Deriche 1997).\nA less widely used variant is half-octave pyramids, shown in Figure 3.36a. These were\nﬁrst introduced to the vision community by Crowley and Stern (1984), who call them Dif-\nference of Low-Pass (DOLP) transforms. Because of the small scale change between adja-",
  "175": "3.5 Pyramids and wavelets\n153\nH\nL\n↓2\nH0\nH\nL\n↓2\nL1\nH1\nL2\nQ\nF\nI\n↑2\nF\nI\n↑2\nQ\nQ\nH0\nL1\nH1\nL2\nI\nO\n(a)\nL\n↓2\nH0\nL1\nQ\nI\n↑2\nH0\nL1\nI\n↑2\n–\n(b)\nFigure 3.34 The Laplacian pyramid: (a) The conceptual ﬂow of images through processing\nstages: images are high-pass and low-pass ﬁltered, and the low-pass ﬁltered images are pro-\ncessed in the next stage of the pyramid. During reconstruction, the interpolated image and the\n(optionally ﬁltered) high-pass image are added back together. The Q box indicates quantiza-\ntion or some other pyramid processing, e.g., noise removal by coring (setting small wavelet\nvalues to 0). (b) The actual computation of the high-pass ﬁlter involves ﬁrst interpolating the\ndownsampled low-pass image and then subtracting it. This results in perfect reconstruction\nwhen Q is the identity. The high-pass (or band-pass) images are typically called Laplacian\nimages, while the low-pass images are called Gaussian images.",
  "176": "154\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nspace:\n−\n=\nfrequency:\n−\n=\nlow-pass\nlower-pass\nFigure 3.35 The difference of two low-pass ﬁlters results in a band-pass ﬁlter. The dashed\nblue lines show the close ﬁt to a half-octave Laplacian of Gaussian.\ncent levels, the authors claim that coarse-to-ﬁne algorithms perform better. In the image-\nprocessing community, half-octave pyramids combined with checkerboard sampling grids\nare known as quincunx sampling (Feilner, Van De Ville, and Unser 2005). In detecting multi-\nscale features (Section 4.1.1), it is often common to use half-octave or even quarter-octave\npyramids (Lowe 2004; Triggs 2004). However, in this case, the subsampling only occurs\nat every octave level, i.e., the image is repeatedly blurred with wider Gaussians until a full\noctave of resolution change has been achieved (Figure 4.11).\n3.5.4 Wavelets\nWhile pyramids are used extensively in computer vision applications, some people use wavelet\ndecompositions as an alternative. Wavelets are ﬁlters that localize a signal in both space\nand frequency (like the Gabor ﬁlter in Table 3.2) and are deﬁned over a hierarchy of scales.\nWavelets provide a smooth way to decompose a signal into frequency components without\nblocking and are closely related to pyramids.\nWavelets were originally developed in the applied math and signal processing communi-\nties and were introduced to the computer vision community by Mallat (1989). Strang (1989);\nSimoncelli and Adelson (1990b); Rioul and Vetterli (1991); Chui (1992); Meyer (1993) all\nprovide nice introductions to the subject along with historical reviews, while Chui (1992) pro-\nvides a more comprehensive review and survey of applications. Sweldens (1997) describes\nthe more recent lifting approach to wavelets that we discuss shortly.\nWavelets are widely used in the computer graphics community to perform multi-resolution\ngeometric processing (Stollnitz, DeRose, and Salesin 1996) and have also been used in com-\nputer vision for similar applications (Szeliski 1990b; Pentland 1994; Gortler and Cohen 1995;\nYaou and Chang 1994; Lai and Vemuri 1997; Szeliski 2006b), as well as for multi-scale ori-\nented ﬁltering (Simoncelli, Freeman, Adelson et al. 1992) and denoising (Portilla, Strela,",
  "177": "3.5 Pyramids and wavelets\n155\nfine\nmedium\ncoarse\nl = 0\nl = 1\nl = 2\nl = 3\nl = 4\nfine\nmedium\ncoarse\nl = 0\nl = 1\nl = 2\n(a)\n(b)\nFigure 3.36\nMultiresolution pyramids: (a) pyramid with half-octave (quincunx) sampling\n(odd levels are colored gray for clarity). (b) wavelet pyramid—each wavelet level stores 3/4\nof the original pixels (usually the horizontal, vertical, and mixed gradients), so that the total\nnumber of wavelet coefﬁcients and original pixels is the same.\nWainwright et al. 2003).\nSince both image pyramids and wavelets decompose an image into multi-resolution de-\nscriptions that are localized in both space and frequency, how do they differ? The usual\nanswer is that traditional pyramids are overcomplete, i.e., they use more pixels than the orig-\ninal image to represent the decomposition, whereas wavelets provide a tight frame, i.e., they\nkeep the size of the decomposition the same as the image (Figure 3.36b). However, some\nwavelet families are, in fact, overcomplete in order to provide better shiftability or steering in\norientation (Simoncelli, Freeman, Adelson et al. 1992). A better distinction, therefore, might\nbe that wavelets are more orientation selective than regular band-pass pyramids.\nHow are two-dimensional wavelets constructed? Figure 3.37a shows a high-level dia-\ngram of one stage of the (recursive) coarse-to-ﬁne construction (analysis) pipeline alongside\nthe complementary re-construction (synthesis) stage. In this diagram, the high-pass ﬁlter\nfollowed by decimation keeps 3/4 of the original pixels, while 1/4 of the low-frequency coef-\nﬁcients are passed on to the next stage for further analysis. In practice, the ﬁltering is usually\nbroken down into two separable sub-stages, as shown in Figure 3.37b. The resulting three\nwavelet images are sometimes called the high–high (HH), high–low (HL), and low–high\n(LH) images. The high–low and low–high images accentuate the horizontal and vertical\nedges and gradients, while the high–high image contains the less frequently occurring mixed\nderivatives.\nHow are the high-pass H and low-pass L ﬁlters shown in Figure 3.37b chosen and how\ncan the corresponding reconstruction ﬁlters I and F be computed? Can ﬁlters be designed",
  "178": "156\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nH\nL\n↓2\n↓2\nL1\nQ\nF\nI\n↑2\n↑2\nLH0\nHH0\nHL0\nLH0\nHH0\nHL0\nL1\n(a)\nHh\n↓2h\nL1\nLH0\nHH0\nHL0\nHv\n↓2v\nLv\n↓2v\nLh\n↓2h\nHv\n↓2v\nLv\n↓2v\nL1\nLH0\nHH0\nHL0\nQ\nQ\nQ\nFh\n↑2h\nFv\n↑2v\nIv\n↑2v\nIh\n↑2h\nFv\n↑2v\nIv\n↑2v\n(b)\nFigure 3.37 Two-dimensional wavelet decomposition: (a) high-level diagram showing the\nlow-pass and high-pass transforms as single boxes; (b) separable implementation, which in-\nvolves ﬁrst performing the wavelet transform horizontally and then vertically. The I and F\nboxes are the interpolation and ﬁltering boxes required to re-synthesize the image from its\nwavelet components.\nthat all have ﬁnite impulse responses? This topic has been the main subject of study in the\nwavelet community for over two decades. The answer depends largely on the intended ap-\nplication, e.g., whether the wavelets are being used for compression, image analysis (feature\nﬁnding), or denoising. Simoncelli and Adelson (1990b) show (in Table 4.1) some good odd-\nlength quadrature mirror ﬁlter (QMF) coefﬁcients that seem to work well in practice.\nSince the design of wavelet ﬁlters is such a tricky art, is there perhaps a better way? In-\ndeed, a simpler procedure is to split the signal into its even and odd components and then\nperform trivially reversible ﬁltering operations on each sequence to produce what are called\nlifted wavelets (Figures 3.38 and 3.39). Sweldens (1996) gives a wonderfully understandable\nintroduction to the lifting scheme for second-generation wavelets, followed by a comprehen-\nsive review (Sweldens 1997).\nAs Figure 3.38 demonstrates, rather than ﬁrst ﬁltering the whole input sequence (image)",
  "179": "3.5 Pyramids and wavelets\n157\nH\nL\n↓2e\n↓2o\nL1\nQ\nF\nI\n↑2e\n↑2o\nH0\nL1\nH0\n(a)\nQ\n↓2o\n↓2e\nL\nC\nL1\nH0\nL1\nH0\n↑2o\n↑2e\nL\nC\n–\n–\n(b)\nFigure 3.38 One-dimensional wavelet transform: (a) usual high-pass + low-pass ﬁlters fol-\nlowed by odd (↓2o) and even (↓2e) downsampling; (b) lifted version, which ﬁrst selects the\nodd and even subsequences and then applies a low-pass prediction stage L and a high-pass\ncorrection stage C in an easily reversible manner.\nwith high-pass and low-pass ﬁlters and then keeping the odd and even sub-sequences, the\nlifting scheme ﬁrst splits the sequence into its even and odd sub-components. Filtering the\neven sequence with a low-pass ﬁlter L and subtracting the result from the even sequence\nis trivially reversible: simply perform the same ﬁltering and then add the result back in.\nFurthermore, this operation can be performed in place, resulting in signiﬁcant space savings.\nThe same applies to ﬁltering the even sequence with the correction ﬁlter C, which is used to\nensure that the even sequence is low-pass. A series of such lifting steps can be used to create\nmore complex ﬁlter responses with low computational cost and guaranteed reversibility.\nThis process can perhaps be more easily understood by considering the signal processing\ndiagram in Figure 3.39. During analysis, the average of the even values is subtracted from the\nodd value to obtain a high-pass wavelet coefﬁcient. However, the even samples still contain\nan aliased sample of the low-frequency signal. To compensate for this, a small amount of the\nhigh-pass wavelet is added back to the even sequence so that it is properly low-pass ﬁltered.\n(It is easy to show that the effective low-pass ﬁlter is [−1/8, 1/4, 3/4, 1/4, −1/8], which is in-",
  "180": "158\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n-½\n-½\n-½\n-½\n¼\n¼\n¼\n¼\nL0\nH0\nL1\nH1\nL2\n½\n½\n-¼\n-¼\n-¼\n-¼\nL0\nH0\nL1\nH1\nL2\n½\n½\n(a)\n(b)\nFigure 3.39 Lifted transform shown as a signal processing diagram: (a) The analysis stage\nﬁrst predicts the odd value from its even neighbors, stores the difference wavelet, and then\ncompensates the coarser even value by adding in a fraction of the wavelet. (b) The synthesis\nstage simply reverses the ﬂow of computation and the signs of some of the ﬁlters and op-\nerations. The light blue lines show what happens if we use four taps for the prediction and\ncorrection instead of just two.\ndeed a low-pass ﬁlter.) During synthesis, the same operations are reversed with a judicious\nchange in sign.\nOf course, we need not restrict ourselves to two-tap ﬁlters. Figure 3.39 shows as light\nblue arrows additional ﬁlter coefﬁcients that could optionally be added to the lifting scheme\nwithout affecting its reversibility. In fact, the low-pass and high-pass ﬁltering operations can\nbe interchanged, e.g., we could use a ﬁve-tap cubic low-pass ﬁlter on the odd sequence (plus\ncenter value) ﬁrst, followed by a four-tap cubic low-pass predictor to estimate the wavelet,\nalthough I have not seen this scheme written down.\nLifted wavelets are called second-generation wavelets because they can easily adapt to\nnon-regular sampling topologies, e.g., those that arise in computer graphics applications such\nas multi-resolution surface manipulation (Schr¨oder and Sweldens 1995). It also turns out that\nlifted weighted wavelets, i.e., wavelets whose coefﬁcients adapt to the underlying problem\nbeing solved (Fattal 2009), can be extremely effective for low-level image manipulation tasks\nand also for preconditioning the kinds of sparse linear systems that arise in the optimization-\nbased approaches to vision algorithms that we discuss in Section 3.7 (Szeliski 2006b).\nAn alternative to the widely used “separable” approach to wavelet construction, which de-\ncomposes each level into horizontal, vertical, and “cross” sub-bands, is to use a representation\nthat is more rotationally symmetric and orientationally selective and also avoids the aliasing\ninherent in sampling signals below their Nyquist frequency.17 Simoncelli, Freeman, Adelson\net al. (1992) introduce such a representation, which they call a pyramidal radial frequency\n17 Such aliasing can often be seen as the signal content moving between bands as the original signal is slowly\nshifted.",
  "181": "3.5 Pyramids and wavelets\n159\n(a)\n(b)\n(c)\n(d)\nFigure 3.40 Steerable shiftable multiscale transforms (Simoncelli, Freeman, Adelson et al.\n1992) c⃝1992 IEEE: (a) radial multi-scale frequency domain decomposition; (b) original\nimage; (c) a set of four steerable ﬁlters; (d) the radial multi-scale wavelet decomposition.\nimplementation of shiftable multi-scale transforms or, more succinctly, steerable pyramids.\nTheir representation is not only overcomplete (which eliminates the aliasing problem) but is\nalso orientationally selective and has identical analysis and synthesis basis functions, i.e., it is\nself-inverting, just like “regular” wavelets. As a result, this makes steerable pyramids a much\nmore useful basis for the structural analysis and matching tasks commonly used in computer\nvision.\nFigure 3.40a shows how such a decomposition looks in frequency space. Instead of re-\ncursively dividing the frequency domain into 2 × 2 squares, which results in checkerboard\nhigh frequencies, radial arcs are used instead. Figure 3.40b illustrates the resulting pyramid\nsub-bands. Even through the representation is overcomplete, i.e., there are more wavelet co-\nefﬁcients than input pixels, the additional frequency and orientation selectivity makes this\nrepresentation preferable for tasks such as texture analysis and synthesis (Portilla and Simon-\ncelli 2000) and image denoising (Portilla, Strela, Wainwright et al. 2003; Lyu and Simoncelli\n2009).",
  "182": "160\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 3.41 Laplacian pyramid blending (Burt and Adelson 1983b) c⃝1983 ACM: (a) orig-\ninal image of apple, (b) original image of orange, (c) regular splice, (d) pyramid blend.\n3.5.5 Application: Image blending\nOne of the most engaging and fun applications of the Laplacian pyramid presented in Sec-\ntion 3.5.3 is the creation of blended composite images, as shown in Figure 3.41 (Burt and\nAdelson 1983b). While splicing the apple and orange images together along the midline\nproduces a noticeable cut, splining them together (as Burt and Adelson (1983b) called their\nprocedure) creates a beautiful illusion of a truly hybrid fruit. The key to their approach is\nthat the low-frequency color variations between the red apple and the orange are smoothly\nblended, while the higher-frequency textures on each fruit are blended more quickly to avoid\n“ghosting” effects when two textures are overlaid.\nTo create the blended image, each source image is ﬁrst decomposed into its own Lapla-\ncian pyramid (Figure 3.42, left and middle columns). Each band is then multiplied by a\nsmooth weighting function whose extent is proportional to the pyramid level. The simplest\nand most general way to create these weights is to take a binary mask image (Figure 3.43c)\nand to construct a Gaussian pyramid from this mask. Each Laplacian pyramid image is then",
  "183": "3.5 Pyramids and wavelets\n161\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n(l)\nFigure 3.42 Laplacian pyramid blending details (Burt and Adelson 1983b) c⃝1983 ACM.\nThe ﬁrst three rows show the high, medium, and low frequency parts of the Laplacian pyramid\n(taken from levels 0, 2, and 4). The left and middle columns show the original apple and\norange images weighted by the smooth interpolation functions, while the right column shows\nthe averaged contributions.",
  "184": "162\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 3.43 Laplacian pyramid blend of two images of arbitrary shape (Burt and Adelson\n1983b) c⃝1983 ACM: (a) ﬁrst input image; (b) second input image; (c) region mask; (d)\nblended image.\nmultiplied by its corresponding Gaussian mask and the sum of these two weighted pyramids\nis then used to construct the ﬁnal image (Figure 3.42, right column).\nFigure 3.43 shows that this process can be applied to arbitrary mask images with sur-\nprising results. It is also straightforward to extend the pyramid blend to an arbitrary number\nof images whose pixel provenance is indicated by an integer-valued label image (see Exer-\ncise 3.20). This is particularly useful in image stitching and compositing applications, where\nthe exposures may vary between different images, as described in Section 9.3.4.\n3.6 Geometric transformations\nIn the previous sections, we saw how interpolation and decimation could be used to change\nthe resolution of an image. In this section, we look at how to perform more general transfor-\nmations, such as image rotations or general warps. In contrast to the point processes we saw\nin Section 3.1, where the function applied to an image transforms the range of the image,\ng(x) = h(f(x)),\n(3.87)",
  "185": "3.6 Geometric transformations\n163\nf\nx\nh\nf\nf\ng\nh\ng\nh\nh\ng\nx\nf\nx\ng\nx\nFigure 3.44 Image warping involves modifying the domain of an image function rather than\nits range.\ny\nx\nsimilarity\nEuclidean\naffine\nprojective\ntranslation\nFigure 3.45 Basic set of 2D geometric image transformations.\nhere we look at functions that transform the domain,\ng(x) = f(h(x))\n(3.88)\n(see Figure 3.44).\nWe begin by studying the global parametric 2D transformation ﬁrst introduced in Sec-\ntion 2.1.2. (Such a transformation is called parametric because it is controlled by a small\nnumber of parameters.) We then turn our attention to more local general deformations such as\nthose deﬁned on meshes (Section 3.6.2). Finally, we show how image warps can be combined\nwith cross-dissolves to create interesting morphs (in-between animations) in Section 3.6.3.\nFor readers interested in more details on these topics, there is an excellent survey by Heck-\nbert (1986) as well as very accessible textbooks by Wolberg (1990), Gomes, Darsa, Costa\net al. (1999) and Akenine-M¨oller and Haines (2002). Note that Heckbert’s survey is on tex-\nture mapping, which is how the computer graphics community refers to the topic of warping\nimages onto surfaces.\n3.6.1 Parametric transformations\nParametric transformations apply a global deformation to an image, where the behavior of the\ntransformation is controlled by a small number of parameters. Figure 3.45 shows a few ex-",
  "186": "164\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTransformation\nMatrix\n# DoF\nPreserves\nIcon\ntranslation\nh\nI\nt\ni\n2×3\n2\norientation\nrigid (Euclidean)\nh\nR\nt\ni\n2×3\n3\nlengths\n\u001a\n\u001a\n\u001a\u001a\nS\nS S\nS\nsimilarity\nh\nsR\nt\ni\n2×3\n4\nangles\n\u001a\n\u001a\nS S\nafﬁne\nh\nA\ni\n2×3\n6\nparallelism\n\u0002\u0002\n\u0002\u0002\nprojective\nh\n˜\nH\ni\n3×3\n8\nstraight lines\n``\n  \nTable 3.5 Hierarchy of 2D coordinate transformations. Each transformation also preserves\nthe properties listed in the rows below it, i.e., similarity preserves not only angles but also\nparallelism and straight lines. The 2×3 matrices are extended with a third [0T 1] row to form\na full 3 × 3 matrix for homogeneous coordinate transformations.\namples of such transformations, which are based on the 2D geometric transformations shown\nin Figure 2.4. The formulas for these transformations were originally given in Table 2.1 and\nare reproduced here in Table 3.5 for ease of reference.\nIn general, given a transformation speciﬁed by a formula x′ = h(x) and a source image\nf(x), how do we compute the values of the pixels in the new image g(x), as given in (3.88)?\nThink about this for a minute before proceeding and see if you can ﬁgure it out.\nIf you are like most people, you will come up with an algorithm that looks something like\nAlgorithm 3.1. This process is called forward warping or forward mapping and is shown in\nFigure 3.46a. Can you think of any problems with this approach?\nprocedure forwardWarp(f, h, out g):\nFor every pixel x in f(x)\n1. Compute the destination location x′ = h(x).\n2. Copy the pixel f(x) to g(x′).\nAlgorithm 3.1\nForward warping algorithm for transforming an image f(x) into an image\ng(x′) through the parametric transform x′ = h(x).",
  "187": "3.6 Geometric transformations\n165\nf(x)\ng(x’)\nx\nx’\nx’=h(x)\nf(x)\ng(x’)\nx\nx’\nx’=h(x)\n(a)\n(b)\nFigure 3.46\nForward warping algorithm: (a) a pixel f(x) is copied to its corresponding\nlocation x′ = h(x) in image g(x′); (b) detail of the source and destination pixel locations.\nIn fact, this approach suffers from several limitations. The process of copying a pixel\nf(x) to a location x′ in g is not well deﬁned when x′ has a non-integer value. What do we\ndo in such a case? What would you do?\nYou can round the value of x′ to the nearest integer coordinate and copy the pixel there,\nbut the resulting image has severe aliasing and pixels that jump around a lot when animating\nthe transformation. You can also “distribute” the value among its four nearest neighbors in\na weighted (bilinear) fashion, keeping track of the per-pixel weights and normalizing at the\nend. This technique is called splatting and is sometimes used for volume rendering in the\ngraphics community (Levoy and Whitted 1985; Levoy 1988; Westover 1989; Rusinkiewicz\nand Levoy 2000).\nUnfortunately, it suffers from both moderate amounts of aliasing and a\nfair amount of blur (loss of high-resolution detail).\nThe second major problem with forward warping is the appearance of cracks and holes,\nespecially when magnifying an image. Filling such holes with their nearby neighbors can\nlead to further aliasing and blurring.\nWhat can we do instead? A preferable solution is to use inverse warping (Algorithm 3.2),\nwhere each pixel in the destination image g(x′) is sampled from the original image f(x)\n(Figure 3.47).\nHow does this differ from the forward warping algorithm? For one thing, since ˆh(x′)\nis (presumably) deﬁned for all pixels in g(x′), we no longer have holes. More importantly,\nresampling an image at non-integer locations is a well-studied problem (general image inter-\npolation, see Section 3.5.2) and high-quality ﬁlters that control aliasing can be used.\nWhere does the function ˆh(x′) come from? Quite often, it can simply be computed as the\ninverse of h(x). In fact, all of the parametric transforms listed in Table 3.5 have closed form\nsolutions for the inverse transform: simply take the inverse of the 3 × 3 matrix specifying the\ntransform.\nIn other cases, it is preferable to formulate the problem of image warping as that of re-\nsampling a source image f(x) given a mapping x = ˆh(x′) from destination pixels x′ to\nsource pixels x. For example, in optical ﬂow (Section 8.4), we estimate the ﬂow ﬁeld as the",
  "188": "166\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprocedure inverseWarp(f, h, out g):\nFor every pixel x′ in g(x′)\n1. Compute the source location x = ˆh(x′)\n2. Resample f(x) at location x and copy to g(x′)\nAlgorithm 3.2 Inverse warping algorithm for creating an image g(x′) from an image f(x)\nusing the parametric transform x′ = h(x).\nf(x)\ng(x’)\nx\nx’\nx=h(x’)\n^ \nf(x)\ng(x’)\nx\nx’\nx=h(x’)\n^ \n(a)\n(b)\nFigure 3.47 Inverse warping algorithm: (a) a pixel g(x′) is sampled from its corresponding\nlocation x = ˆh(x′) in image f(x); (b) detail of the source and destination pixel locations.\nlocation of the source pixel which produced the current pixel whose ﬂow is being estimated,\nas opposed to computing the destination pixel to which it is going. Similarly, when correcting\nfor radial distortion (Section 2.1.6), we calibrate the lens by computing for each pixel in the\nﬁnal (undistorted) image the corresponding pixel location in the original (distorted) image.\nWhat kinds of interpolation ﬁlter are suitable for the resampling process? Any of the ﬁl-\nters we studied in Section 3.5.2 can be used, including nearest neighbor, bilinear, bicubic, and\nwindowed sinc functions. While bilinear is often used for speed (e.g., inside the inner loop\nof a patch-tracking algorithm, see Section 8.1.3), bicubic, and windowed sinc are preferable\nwhere visual quality is important.\nTo compute the value of f(x) at a non-integer location x, we simply apply our usual FIR\nresampling ﬁlter,\ng(x, y) =\nX\nk,l\nf(k, l)h(x −k, y −l),\n(3.89)\nwhere (x, y) are the sub-pixel coordinate values and h(x, y) is some interpolating or smooth-\ning kernel. Recall from Section 3.5.2 that when decimation is being performed, the smoothing\nkernel is stretched and re-scaled according to the downsampling rate r.\nUnfortunately, for a general (non-zoom) image transformation, the resampling rate r is\nnot well deﬁned. Consider a transformation that stretches the x dimensions while squashing",
  "189": "3.6 Geometric transformations\n167\nx\ny\nx’\ny’\nx\ny\nx’\ny’\nx\ny\nx’\ny’\nay’y\nay’x\nax’x\nax’y\n(a)\n(b)\n(c)\nmajor axis\nminor axis\nFigure 3.48\nAnisotropic texture ﬁltering: (a) Jacobian of transform A and the induced\nhorizontal and vertical resampling rates {ax′x, ax′y, ay′x, ay′y}; (b) elliptical footprint of an\nEWA smoothing kernel; (c) anisotropic ﬁltering using multiple samples along the major axis.\nImage pixels lie at line intersections.\nthe y dimensions. The resampling kernel should be performing regular interpolation along\nthe x dimension and smoothing (to anti-alias the blurred image) in the y direction. This gets\neven more complicated for the case of general afﬁne or perspective transforms.\nWhat can we do? Fortunately, Fourier analysis can help. The two-dimensional general-\nization of the one-dimensional domain scaling law given in Table 3.1 is\ng(Ax) ⇔|A|−1G(A−T f).\n(3.90)\nFor all of the transforms in Table 3.5 except perspective, the matrix A is already deﬁned.\nFor perspective transformations, the matrix A is the linearized derivative of the perspective\ntransformation (Figure 3.48a), i.e., the local afﬁne approximation to the stretching induced\nby the projection (Heckbert 1986; Wolberg 1990; Gomes, Darsa, Costa et al. 1999; Akenine-\nM¨oller and Haines 2002).\nTo prevent aliasing, we need to pre-ﬁlter the image f(x) with a ﬁlter whose frequency\nresponse is the projection of the ﬁnal desired spectrum through the A−T transform (Szeliski,\nWinder, and Uyttendaele 2010). In general (for non-zoom transforms), this ﬁlter is non-\nseparable and hence is very slow to compute. Therefore, a number of approximations to this\nﬁlter are used in practice, include MIP-mapping, elliptically weighted Gaussian averaging,\nand anisotropic ﬁltering (Akenine-M¨oller and Haines 2002).\nMIP-mapping\nMIP-mapping was ﬁrst proposed by Williams (1983) as a means to rapidly pre-ﬁlter images\nbeing used for texture mapping in computer graphics. A MIP-map18 is a standard image\n18 The term ‘MIP’ stands for multi in parvo, meaning ‘many in one’.",
  "190": "168\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\npyramid (Figure 3.32), where each level is pre-ﬁltered with a high-quality ﬁlter rather than\na poorer quality approximation, such as Burt and Adelson’s (1983b) ﬁve-tap binomial. To\nresample an image from a MIP-map, a scalar estimate of the resampling rate r is ﬁrst com-\nputed. For example, r can be the maximum of the absolute values in A (which suppresses\naliasing) or it can be the minimum (which reduces blurring). Akenine-M¨oller and Haines\n(2002) discuss these issues in more detail.\nOnce a resampling rate has been speciﬁed, a fractional pyramid level is computed using\nthe base 2 logarithm,\nl = log2 r.\n(3.91)\nOne simple solution is to resample the texture from the next higher or lower pyramid level,\ndepending on whether it is preferable to reduce aliasing or blur. A better solution is to re-\nsample both images and blend them linearly using the fractional component of l. Since most\nMIP-map implementations use bilinear resampling within each level, this approach is usu-\nally called trilinear MIP-mapping. Computer graphics rendering APIs, such as OpenGL and\nDirect3D, have parameters that can be used to select which variant of MIP-mapping (and of\nthe sampling rate r computation) should be used, depending on the desired tradeoff between\nspeed and quality. Exercise 3.22 has you examine some of these tradeoffs in more detail.\nElliptical Weighted Average\nThe Elliptical Weighted Average (EWA) ﬁlter invented by Greene and Heckbert (1986) is\nbased on the observation that the afﬁne mapping x = Ax′ deﬁnes a skewed two-dimensional\ncoordinate system in the vicinity of each source pixel x (Figure 3.48a). For every destina-\ntion pixel x′, the ellipsoidal projection of a small pixel grid in x′ onto x is computed (Fig-\nure 3.48b). This is then used to ﬁlter the source image g(x) with a Gaussian whose inverse\ncovariance matrix is this ellipsoid.\nDespite its reputation as a high-quality ﬁlter (Akenine-M¨oller and Haines 2002), we have\nfound in our work (Szeliski, Winder, and Uyttendaele 2010) that because a Gaussian kernel\nis used, the technique suffers simultaneously from both blurring and aliasing, compared to\nhigher-quality ﬁlters. The EWA is also quite slow, although faster variants based on MIP-\nmapping have been proposed (Szeliski, Winder, and Uyttendaele (2010) provide some addi-\ntional references).\nAnisotropic ﬁltering\nAn alternative approach to ﬁltering oriented textures, which is sometimes implemented in\ngraphics hardware (GPUs), is to use anisotropic ﬁltering (Barkans 1997; Akenine-M¨oller and\nHaines 2002). In this approach, several samples at different resolutions (fractional levels in\nthe MIP-map) are combined along the major axis of the EWA Gaussian (Figure 3.48c).",
  "191": "3.6 Geometric transformations\n169\nH2\ni\nf\nx\nx\nx\ni\nf’\ng1\ng2\ng3\nu\nF\nu\nG1\nu\nG2\nu\nG3\nu\nF’\nH1\ninterpolate\n* h1(x)\nwarp\nax+t\nfilter\n* h2(x)\nsample\n* δ(x)\n(f)\n(g)\n(h)\n(i)\n(j)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 3.49 One-dimensional signal resampling (Szeliski, Winder, and Uyttendaele 2010):\n(a) original sampled signal f(i); (b) interpolated signal g1(x); (c) warped signal g2(x); (d)\nﬁltered signal g3(x); (e) sampled signal f ′(i). The corresponding spectra are shown below\nthe signals, with the aliased portions shown in red.\nMulti-pass transforms\nThe optimal approach to warping images without excessive blurring or aliasing is to adap-\ntively pre-ﬁlter the source image at each pixel using an ideal low-pass ﬁlter, i.e., an oriented\nskewed sinc or low-order (e.g., cubic) approximation (Figure 3.48a). Figure 3.49 shows how\nthis works in one dimension. The signal is ﬁrst (theoretically) interpolated to a continuous\nwaveform, (ideally) low-pass ﬁltered to below the new Nyquist rate, and then re-sampled to\nthe ﬁnal desired resolution. In practice, the interpolation and decimation steps are concate-\nnated into a single polyphase digital ﬁltering operation (Szeliski, Winder, and Uyttendaele\n2010).\nFor parametric transforms, the oriented two-dimensional ﬁltering and resampling opera-\ntions can be approximated using a series of one-dimensional resampling and shearing trans-\nforms (Catmull and Smith 1980; Heckbert 1989; Wolberg 1990; Gomes, Darsa, Costa et al.\n1999; Szeliski, Winder, and Uyttendaele 2010). The advantage of using a series of one-\ndimensional transforms is that they are much more efﬁcient (in terms of basic arithmetic\noperations) than large, non-separable, two-dimensional ﬁlter kernels.\nIn order to prevent aliasing, however, it may be necessary to upsample in the opposite di-\nrection before applying a shearing transformation (Szeliski, Winder, and Uyttendaele 2010).\nFigure 3.50 shows this process for a rotation, where a vertical upsampling stage is added be-\nfore the horizontal shearing (and upsampling) stage. The upper image shows the appearance\nof the letter being rotated, while the lower image shows its corresponding Fourier transform.",
  "192": "170\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nvertical shear\n+ downsample\n(a)\n(b)\n(c)\n(d)\nvertical \nupsample\nhorizontal shear\n+ upsample\nhorizontal  \ndownsample\n(e)\nFigure 3.50 Four-pass rotation (Szeliski, Winder, and Uyttendaele 2010): (a) original pixel\ngrid, image, and its Fourier transform; (b) vertical upsampling; (c) horizontal shear and up-\nsampling; (d) vertical shear and downsampling; (e) horizontal downsampling. The general\nafﬁne case looks similar except that the ﬁrst two stages perform general resampling.\n3.6.2 Mesh-based warping\nWhile parametric transforms speciﬁed by a small number of global parameters have many\nuses, local deformations with more degrees of freedom are often required.\nConsider, for example, changing the appearance of a face from a frown to a smile (Fig-\nure 3.51a). What is needed in this case is to curve the corners of the mouth upwards while\nleaving the rest of the face intact.19 To perform such a transformation, different amounts of\nmotion are required in different parts of the image. Figure 3.51 shows some of the commonly\nused approaches.\nThe ﬁrst approach, shown in Figure 3.51a–b, is to specify a sparse set of corresponding\npoints. The displacement of these points can then be interpolated to a dense displacement ﬁeld\n(Chapter 8) using a variety of techniques (Nielson 1993). One possibility is to triangulate\nthe set of points in one image (de Berg, Cheong, van Kreveld et al. 2006; Litwinowicz and\nWilliams 1994; Buck, Finkelstein, Jacobs et al. 2000) and to use an afﬁne motion model\n(Table 3.5), speciﬁed by the three triangle vertices, inside each triangle. If the destination\n19 Rowland and Perrett (1995); Pighin, Hecker, Lischinski et al. (1998); Blanz and Vetter (1999); Leyvand, Cohen-\nOr, Dror et al. (2008) show more sophisticated examples of changing facial expression and appearance.",
  "193": "3.6 Geometric transformations\n171\n(a)\n(b)\n(c)\n(d)\nFigure 3.51 Image warping alternatives (Gomes, Darsa, Costa et al. 1999) c⃝1999 Morgan\nKaufmann: (a) sparse control points −→deformation grid; (b) denser set of control point\ncorrespondences; (c) oriented line correspondences; (d) uniform quadrilateral grid.\nimage is triangulated according to the new vertex locations, an inverse warping algorithm\n(Figure 3.47) can be used. If the source image is triangulated and used as a texture map,\ncomputer graphics rendering algorithms can be used to draw the new image (but care must\nbe taken along triangle edges to avoid potential aliasing).\nAlternative methods for interpolating a sparse set of displacements include moving nearby\nquadrilateral mesh vertices, as shown in Figure 3.51a, using variational (energy minimizing)\ninterpolants such as regularization (Litwinowicz and Williams 1994), see Section 3.7.1, or\nusing locally weighted (radial basis function) combinations of displacements (Nielson 1993).\n(See (Section 12.3.1) for additional scattered data interpolation techniques.) If quadrilateral\nmeshes are used, it may be desirable to interpolate displacements down to individual pixel\nvalues using a smooth interpolant such as a quadratic B-spline (Farin 1996; Lee, Wolberg,\nChwa et al. 1996).20\nIn some cases, e.g., if a dense depth map has been estimated for an image (Shade, Gortler,\nHe et al. 1998), we only know the forward displacement for each pixel. As mentioned before,\ndrawing source pixels at their destination location, i.e., forward warping (Figure 3.46), suffers\nfrom several potential problems, including aliasing and the appearance of small cracks. An\nalternative technique in this case is to forward warp the displacement ﬁeld (or depth map) to\n20 Note that the block-based motion models used by many video compression standards (Le Gall 1991) can be\nthought of as a 0th-order (piecewise-constant) displacement ﬁeld.",
  "194": "172\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 3.52 Line-based image warping (Beier and Neely 1992) c⃝1992 ACM: (a) distance\ncomputation and position transfer; (b) rendering algorithm; (c) two intermediate warps used\nfor morphing.\nits new location, ﬁll small holes in the resulting map, and then use inverse warping to perform\nthe resampling (Shade, Gortler, He et al. 1998). The reason that this generally works better\nthan forward warping is that displacement ﬁelds tend to be much smoother than images, so\nthe aliasing introduced during the forward warping of the displacement ﬁeld is much less\nnoticeable.\nA second approach to specifying displacements for local deformations is to use corre-\nsponding oriented line segments (Beier and Neely 1992), as shown in Figures 3.51c and 3.52.\nPixels along each line segment are transferred from source to destination exactly as speciﬁed,\nand other pixels are warped using a smooth interpolation of these displacements. Each line\nsegment correspondence speciﬁes a translation, rotation, and scaling, i.e., a similarity trans-\nform (Table 3.5), for pixels in its vicinity, as shown in Figure 3.52a. Line segments inﬂuence\nthe overall displacement of the image using a weighting function that depends on the mini-\nmum distance to the line segment (v in Figure 3.52a if u ∈[0, 1], else the shorter of the two\ndistances to P and Q).\nFor each pixel X, the target location X′ for each line correspondence is computed along\nwith a weight that depends on the distance and the line segment length (Figure 3.52b). The\nweighted average of all target locations X′\ni then becomes the ﬁnal destination location. Note\nthat while Beier and Neely describe this algorithm as a forward warp, an equivalent algorithm\ncan be written by sequencing through the destination pixels. The resulting warps are not\nidentical because line lengths or distances to lines may be different. Exercise 3.23 has you\nimplement the Beier–Neely (line-based) warp and compare it to a number of other local\ndeformation methods.\nYet another way of specifying correspondences in order to create image warps is to use\nsnakes (Section 5.1.1) combined with B-splines (Lee, Wolberg, Chwa et al. 1996). This tech-\nnique is used in Apple’s Shake software and is popular in the medical imaging community.",
  "195": "3.6 Geometric transformations\n173\nFigure 3.53 Image morphing (Gomes, Darsa, Costa et al. 1999) c⃝1999 Morgan Kaufmann.\nTop row: if the two images are just blended, visible ghosting results. Bottom row: both\nimages are ﬁrst warped to the same intermediate location (e.g., halfway towards the other\nimage) and the resulting warped images are then blended resulting in a seamless morph.\nOne ﬁnal possibility for specifying displacement ﬁelds is to use a mesh speciﬁcally\nadapted to the underlying image content, as shown in Figure 3.51d. Specifying such meshes\nby hand can involve a fair amount of work; Gomes, Darsa, Costa et al. (1999) describe an\ninteractive system for doing this. Once the two meshes have been speciﬁed, intermediate\nwarps can be generated using linear interpolation and the displacements at mesh nodes can\nbe interpolated using splines.\n3.6.3 Application: Feature-based morphing\nWhile warps can be used to change the appearance of or to animate a single image, even\nmore powerful effects can be obtained by warping and blending two or more images using a\nprocess now commonly known as morphing (Beier and Neely 1992; Lee, Wolberg, Chwa et\nal. 1996; Gomes, Darsa, Costa et al. 1999).\nFigure 3.53 shows the essence of image morphing. Instead of simply cross-dissolving\nbetween two images, which leads to ghosting as shown in the top row, each image is warped\ntoward the other image before blending, as shown in the bottom row. If the correspondences\nhave been set up well (using any of the techniques shown in Figure 3.51), corresponding\nfeatures are aligned and no ghosting results.\nThe above process is repeated for each intermediate frame being generated during a",
  "196": "174\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmorph, using different blends (and amounts of deformation) at each interval. Let t ∈[0, 1] be\nthe time parameter that describes the sequence of interpolated frames. The weighting func-\ntions for the two warped images in the blend go as (1 −t) and t. Conversely, the amount of\nmotion that image 0 undergoes at time t is t of the total amount of motion that is speciﬁed\nby the correspondences. However, some care must be taken in deﬁning what it means to par-\ntially warp an image towards a destination, especially if the desired motion is far from linear\n(Sederberg, Gao, Wang et al. 1993). Exercise 3.25 has you implement a morphing algorithm\nand test it out under such challenging conditions.\n3.7 Global optimization\nSo far in this chapter, we have covered a large number of image processing operators that\ntake as input one or more images and produce some ﬁltered or transformed version of these\nimages. In many applications, it is more useful to ﬁrst formulate the goals of the desired\ntransformation using some optimization criterion and then ﬁnd or infer the solution that best\nmeets this criterion.\nIn this ﬁnal section, we present two different (but closely related) variants on this idea.\nThe ﬁrst, which is often called regularization or variational methods (Section 3.7.1), con-\nstructs a continuous global energy function that describes the desired characteristics of the\nsolution and then ﬁnds a minimum energy solution using sparse linear systems or related\niterative techniques. The second formulates the problem using Bayesian statistics, model-\ning both the noisy measurement process that produced the input images as well as prior\nassumptions about the solution space, which are often encoded using a Markov random ﬁeld\n(Section 3.7.2).\nExamples of such problems include surface interpolation from scattered data (Figure 3.54),\nimage denoising and the restoration of missing regions (Figure 3.57), and the segmentation\nof images into foreground and background regions (Figure 3.61).\n3.7.1 Regularization\nThe theory of regularization was ﬁrst developed by statisticians trying to ﬁt models to data\nthat severely underconstrained the solution space (Tikhonov and Arsenin 1977; Engl, Hanke,\nand Neubauer 1996). Consider, for example, ﬁnding a smooth surface that passes through\n(or near) a set of measured data points (Figure 3.54). Such a problem is described as ill-\nposed because many possible surfaces can ﬁt this data. Since small changes in the input can\nsometimes lead to large changes in the ﬁt (e.g., if we use polynomial interpolation), such\nproblems are also often ill-conditioned. Since we are trying to recover the unknown function\nf(x, y) from which the data point d(xi, yi) were sampled, such problems are also often called",
  "197": "3.7 Global optimization\n175\n(a)\n(b)\nFigure 3.54 A simple surface interpolation problem: (a) nine data points of various height\nscattered on a grid; (b) second-order, controlled-continuity, thin-plate spline interpolator, with\na tear along its left edge and a crease along its right (Szeliski 1989) c⃝1989 Springer.\ninverse problems. Many computer vision tasks can be viewed as inverse problems, since we\nare trying to recover a full description of the 3D world from a limited set of images.\nIn order to quantify what it means to ﬁnd a smooth solution, we can deﬁne a norm on\nthe solution space. For one-dimensional functions f(x), we can integrate the squared ﬁrst\nderivative of the function,\nE1 =\nZ\nf 2\nx(x) dx\n(3.92)\nor perhaps integrate the squared second derivative,\nE2 =\nZ\nf 2\nxx(x) dx.\n(3.93)\n(Here, we use subscripts to denote differentiation.) Such energy measures are examples of\nfunctionals, which are operators that map functions to scalar values. They are also often called\nvariational methods, because they measure the variation (non-smoothness) in a function.\nIn two dimensions (e.g., for images, ﬂow ﬁelds, or surfaces), the corresponding smooth-\nness functionals are\nE1 =\nZ\nf 2\nx(x, y) + f 2\ny (x, y) dx dy =\nZ\n∥∇f(x, y)∥2 dx dy\n(3.94)\nand\nE2 =\nZ\nf 2\nxx(x, y) + 2f 2\nxy(x, y) + f 2\nyy(x, y) dx dy,\n(3.95)\nwhere the mixed 2f 2\nxy term is needed to make the measure rotationally invariant (Grimson\n1983).\nThe ﬁrst derivative norm is often called the membrane, since interpolating a set of data\npoints using this measure results in a tent-like structure. (In fact, this formula is a small-\ndeﬂection approximation to the surface area, which is what soap bubbles minimize.) The",
  "198": "176\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nsecond-order norm is called the thin-plate spline, since it approximates the behavior of thin\nplates (e.g., ﬂexible steel) under small deformations. A blend of the two is called the thin-\nplate spline under tension; versions of these formulas where each derivative term is mul-\ntiplied by a local weighting function are called controlled-continuity splines (Terzopoulos\n1988). Figure 3.54 shows a simple example of a controlled-continuity interpolator ﬁt to nine\nscattered data points. In practice, it is more common to ﬁnd ﬁrst-order smoothness terms\nused with images and ﬂow ﬁelds (Section 8.4) and second-order smoothness associated with\nsurfaces (Section 12.3.1).\nIn addition to the smoothness term, regularization also requires a data term (or data\npenalty). For scattered data interpolation (Nielson 1993), the data term measures the dis-\ntance between the function f(x, y) and a set of data points di = d(xi, yi),\nEd =\nX\ni\n[f(xi, yi) −di]2.\n(3.96)\nFor a problem like noise removal, a continuous version of this measure can be used,\nEd =\nZ\n[f(x, y) −d(x, y)]2 dx dy.\n(3.97)\nTo obtain a global energy that can be minimized, the two energy terms are usually added\ntogether,\nE = Ed + λEs,\n(3.98)\nwhere Es is the smoothness penalty (E1, E2 or some weighted blend) and λ is the regulariza-\ntion parameter, which controls how smooth the solution should be.\nIn order to ﬁnd the minimum of this continuous problem, the function f(x, y) is usually\nﬁrst discretized on a regular grid.21 The most principled way to perform this discretization is\nto use ﬁnite element analysis, i.e., to approximate the function with a piecewise continuous\nspline, and then perform the analytic integration (Bathe 2007).\nFortunately, for both the ﬁrst-order and second-order smoothness functionals, the judi-\ncious selection of appropriate ﬁnite elements results in particularly simple discrete forms\n(Terzopoulos 1983). The corresponding discrete smoothness energy functions become\nE1\n=\nX\ni,j\nsx(i, j)[f(i + 1, j) −f(i, j) −gx(i, j)]2\n(3.99)\n+ sy(i, j)[f(i, j + 1) −f(i, j) −gy(i, j)]2\nand\nE2\n=\nh−2 X\ni,j\ncx(i, j)[f(i + 1, j) −2f(i, j) + f(i −1, j)]2\n(3.100)\n21 The alternative of using kernel basis functions centered on the data points (Boult and Kender 1986; Nielson\n1993) is discussed in more detail in Section 12.3.1.",
  "199": "3.7 Global optimization\n177\n+ 2cm(i, j)[f(i + 1, j + 1) −f(i + 1, j) −f(i, j + 1) + f(i, j)]2\n+ cy(i, j)[f(i, j + 1) −2f(i, j) + f(i, j −1)]2,\nwhere h is the size of the ﬁnite element grid. The h factor is only important if the energy is\nbeing discretized at a variety of resolutions, as in coarse-to-ﬁne or multigrid techniques.\nThe optional smoothness weights sx(i, j) and sy(i, j) control the location of horizon-\ntal and vertical tears (or weaknesses) in the surface. For other problems, such as coloriza-\ntion (Levin, Lischinski, and Weiss 2004) and interactive tone mapping (Lischinski, Farbman,\nUyttendaele et al. 2006a), they control the smoothness in the interpolated chroma or expo-\nsure ﬁeld and are often set inversely proportional to the local luminance gradient strength.\nFor second-order problems, the crease variables cx(i, j), cm(i, j), and cy(i, j) control the\nlocations of creases in the surface (Terzopoulos 1988; Szeliski 1990a).\nThe data values gx(i, j) and gy(i, j) are gradient data terms (constraints) used by al-\ngorithms, such as photometric stereo (Section 12.1.1), HDR tone mapping (Section 10.2.1)\n(Fattal, Lischinski, and Werman 2002), Poisson blending (Section 9.3.4) (P´erez, Gangnet,\nand Blake 2003), and gradient-domain blending (Section 9.3.4) (Levin, Zomet, Peleg et al.\n2004). They are set to zero when just discretizing the conventional ﬁrst-order smoothness\nfunctional (3.94).\nThe two-dimensional discrete data energy is written as\nEd =\nX\ni,j\nw(i, j)[f(i, j) −d(i, j)]2,\n(3.101)\nwhere the local weights w(i, j) control how strongly the data constraint is enforced. These\nvalues are set to zero where there is no data and can be set to the inverse variance of the data\nmeasurements when there is data (as discussed by Szeliski (1989) and in Section 3.7.2).\nThe total energy of the discretized problem can now be written as a quadratic form\nE = Ed + λEs = xT Ax −2xT b + c,\n(3.102)\nwhere x = [f(0, 0) . . . f(m −1, n −1)] is called the state vector.22\nThe sparse symmetric positive-deﬁnite matrix A is called the Hessian since it encodes the\nsecond derivative of the energy function.23 For the one-dimensional, ﬁrst-order problem, A\nis tridiagonal; for the two-dimensional, ﬁrst-order problem, it is multi-banded with ﬁve non-\nzero entries per row. We call b the weighted data vector. Minimizing the above quadratic\nform is equivalent to solving the sparse linear system\nAx = b,\n(3.103)\n22 We use x instead of f because this is the more common form in the numerical analysis literature (Golub and\nVan Loan 1996).\n23 In numerical analysis, A is called the coefﬁcient matrix (Saad 2003); in ﬁnite element analysis (Bathe 2007), it\nis called the stiffness matrix.",
  "200": "178\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure 3.55\nGraphical model interpretation of ﬁrst-order regularization. The white circles\nare the unknowns f(i, j) while the dark circles are the input data d(i, j). In the resistive grid\ninterpretation, the d and f values encode input and output voltages and the black squares\ndenote resistors whose conductance is set to sx(i, j), sy(i, j), and w(i, j). In the spring-mass\nsystem analogy, the circles denote elevations and the black squares denote springs. The same\ngraphical model can be used to depict a ﬁrst-order Markov random ﬁeld (Figure 3.56).\nwhich can be done using a variety of sparse matrix techniques, such as multigrid (Briggs,\nHenson, and McCormick 2000) and hierarchical preconditioners (Szeliski 2006b), as de-\nscribed in Appendix A.5.\nWhile regularization was ﬁrst introduced to the vision community by Poggio, Torre, and\nKoch (1985) and Terzopoulos (1986b) for problems such as surface interpolation, it was\nquickly adopted by other vision researchers for such varied problems as edge detection (Sec-\ntion 4.2), optical ﬂow (Section 8.4), and shape from shading (Section 12.1) (Poggio, Torre,\nand Koch 1985; Horn and Brooks 1986; Terzopoulos 1986b; Bertero, Poggio, and Torre 1988;\nBrox, Bruhn, Papenberg et al. 2004). Poggio, Torre, and Koch (1985) also showed how the\ndiscrete energy deﬁned by Equations (3.100–3.101) could be implemented in a resistive grid,\nas shown in Figure 3.55. In computational photography (Chapter 10), regularization and its\nvariants are commonly used to solve problems such as high-dynamic range tone mapping\n(Fattal, Lischinski, and Werman 2002; Lischinski, Farbman, Uyttendaele et al. 2006a), Pois-\nson and gradient-domain blending (P´erez, Gangnet, and Blake 2003; Levin, Zomet, Peleg et\nal. 2004; Agarwala, Dontcheva, Agrawala et al. 2004), colorization (Levin, Lischinski, and\nWeiss 2004), and natural image matting (Levin, Lischinski, and Weiss 2008).\nRobust regularization\nWhile regularization is most commonly formulated using quadratic (L2) norms (compare\nwith the squared derivatives in (3.92–3.95) and squared differences in (3.100–3.101)), it can",
  "201": "3.7 Global optimization\n179\nalso be formulated using non-quadratic robust penalty functions (Appendix B.3). For exam-\nple, (3.100) can be generalized to\nE1r\n=\nX\ni,j\nsx(i, j)ρ(f(i + 1, j) −f(i, j))\n(3.104)\n+ sy(i, j)ρ(f(i, j + 1) −f(i, j)),\nwhere ρ(x) is some monotonically increasing penalty function. For example, the family of\nnorms ρ(x) = |x|p is called p-norms. When p < 2, the resulting smoothness terms become\nmore piecewise continuous than totally smooth, which can better model the discontinuous\nnature of images, ﬂow ﬁelds, and 3D surfaces.\nAn early example of robust regularization is the graduated non-convexity (GNC) algo-\nrithm introduced by Blake and Zisserman (1987). Here, the norms on the data and derivatives\nare clamped to a maximum value\nρ(x) = min(x2, V ).\n(3.105)\nBecause the resulting problem is highly non-convex (it has many local minima), a continua-\ntion method is proposed, where a quadratic norm (which is convex) is gradually replaced by\nthe non-convex robust norm (Allgower and Georg 2003). (Around the same time, Terzopou-\nlos (1988) was also using continuation to infer the tear and crease variables in his surface\ninterpolation problems.)\nToday, it is more common to use the L1 (p = 1) norm, which is often called total variation\n(Chan, Osher, and Shen 2001; Tschumperl´e and Deriche 2005; Tschumperl´e 2006; Kaftory,\nSchechner, and Zeevi 2007). Other norms, for which the inﬂuence (derivative) more quickly\ndecays to zero, are presented by Black and Rangarajan (1996); Black, Sapiro, Marimont et\nal. (1998) and discussed in Appendix B.3.\nEven more recently, hyper-Laplacian norms with p < 1 have gained popularity, based\non the observation that the log-likelihood distribution of image derivatives follows a p ≈\n0.5 −0.8 slope and is therefore a hyper-Laplacian distribution (Simoncelli 1999; Levin and\nWeiss 2007; Weiss and Freeman 2007; Krishnan and Fergus 2009). Such norms have an even\nstronger tendency to prefer large discontinuities over small ones. See the related discussion\nin Section 3.7.2 (3.114).\nWhile least squares regularized problems using L2 norms can be solved using linear sys-\ntems, other p-norms require different iterative techniques, such as iteratively reweighted least\nsquares (IRLS), Levenberg–Marquardt, or alternation between local non-linear subproblems\nand global quadratic regularization (Krishnan and Fergus 2009). Such techniques are dis-\ncussed in Section 6.1.3 and Appendices A.3 and B.3.",
  "202": "180\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3.7.2 Markov random ﬁelds\nAs we have just seen, regularization, which involves the minimization of energy functionals\ndeﬁned over (piecewise) continuous functions, can be used to formulate and solve a variety\nof low-level computer vision problems. An alternative technique is to formulate a Bayesian\nmodel, which separately models the noisy image formation (measurement) process, as well\nas assuming a statistical prior model over the solution space. In this section, we look at\npriors based on Markov random ﬁelds, whose log-likelihood can be described using local\nneighborhood interaction (or penalty) terms (Kindermann and Snell 1980; Geman and Geman\n1984; Marroquin, Mitter, and Poggio 1987; Li 1995; Szeliski, Zabih, Scharstein et al. 2008).\nThe use of Bayesian modeling has several potential advantages over regularization (see\nalso Appendix B). The ability to model measurement processes statistically enables us to\nextract the maximum information possible from each measurement, rather than just guessing\nwhat weighting to give the data. Similarly, the parameters of the prior distribution can often\nbe learned by observing samples from the class we are modeling (Roth and Black 2007a;\nTappen 2007; Li and Huttenlocher 2008). Furthermore, because our model is probabilistic,\nit is possible to estimate (in principle) complete probability distributions over the unknowns\nbeing recovered and, in particular, to model the uncertainty in the solution, which can be\nuseful in latter processing stages. Finally, Markov random ﬁeld models can be deﬁned over\ndiscrete variables, such as image labels (where the variables have no proper ordering), for\nwhich regularization does not apply.\nRecall from (3.68) in Section 3.4.3 (or see Appendix B.4) that, according to Bayes’ Rule,\nthe posterior distribution for a given set of measurements y, p(y|x), combined with a prior\np(x) over the unknowns x, is given by\np(x|y) = p(y|x)p(x)\np(y)\n,\n(3.106)\nwhere p(y) =\nR\nx p(y|x)p(x) is a normalizing constant used to make the p(x|y) distribution\nproper (integrate to 1). Taking the negative logarithm of both sides of (3.106), we get\n−log p(x|y) = −log p(y|x) −log p(x) + C,\n(3.107)\nwhich is the negative posterior log likelihood.\nTo ﬁnd the most likely (maximum a posteriori or MAP) solution x given some measure-\nments y, we simply minimize this negative log likelihood, which can also be thought of as an\nenergy,\nE(x, y) = Ed(x, y) + Ep(x).\n(3.108)\n(We drop the constant C because its value does not matter during energy minimization.) The\nﬁrst term Ed(x, y) is the data energy or data penalty; it measures the negative log likelihood",
  "203": "3.7 Global optimization\n181\nthat the data were observed given the unknown state x. The second term Ep(x) is the prior\nenergy; it plays a role analogous to the smoothness energy in regularization. Note that the\nMAP estimate may not always be desirable, since it selects the “peak” in the posterior dis-\ntribution rather than some more stable statistic—see the discussion in Appendix B.2 and by\nLevin, Weiss, Durand et al. (2009).\nFor image processing applications, the unknowns x are the set of output pixels\nx = [f(0, 0) . . . f(m −1, n −1)],\nand the data are (in the simplest case) the input pixels\ny = [d(0, 0) . . . d(m −1, n −1)]\nas shown in Figure 3.56.\nFor a Markov random ﬁeld, the probability p(x) is a Gibbs or Boltzmann distribution,\nwhose negative log likelihood (according to the Hammersley–Clifford theorem) can be writ-\nten as a sum of pairwise interaction potentials,\nEp(x) =\nX\n{(i,j),(k,l)}∈N\nVi,j,k,l(f(i, j), f(k, l)),\n(3.109)\nwhere N(i, j) denotes the neighbors of pixel (i, j). In fact, the general version of the theorem\nsays that the energy may have to be evaluated over a larger set of cliques, which depend on\nthe order of the Markov random ﬁeld (Kindermann and Snell 1980; Geman and Geman 1984;\nBishop 2006; Kohli, Ladick´y, and Torr 2009; Kohli, Kumar, and Torr 2009).\nThe most commonly used neighborhood in Markov random ﬁeld modeling is the N4\nneighborhood, where each pixel in the ﬁeld f(i, j) interacts only with its immediate neigh-\nbors. The model in Figure 3.56, which we previously used in Figure 3.55 to illustrate the\ndiscrete version of ﬁrst-order regularization, shows an N4 MRF. The sx(i, j) and sy(i, j)\nblack boxes denote arbitrary interaction potentials between adjacent nodes in the random\nﬁeld and the w(i, j) denote the data penalty functions. These square nodes can also be inter-\npreted as factors in a factor graph version of the (undirected) graphical model (Bishop 2006),\nwhich is another name for interaction potentials. (Strictly speaking, the factors are (improper)\nprobability functions whose product is the (un-normalized) posterior distribution.)\nAs we will see in (3.112–3.113), there is a close relationship between these interaction\npotentials and the discretized versions of regularized image restoration problems. Thus, to\na ﬁrst approximation, we can view energy minimization being performed when solving a\nregularized problem and the maximum a posteriori inference being performed in an MRF as\nequivalent.\nWhile N4 neighborhoods are most commonly used, in some applications N8 (or even\nhigher order) neighborhoods perform better at tasks such as image segmentation because",
  "204": "182\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure 3.56\nGraphical model for an N4 neighborhood Markov random ﬁeld. (The blue\nedges are added for an N8 neighborhood.) The white circles are the unknowns f(i, j), while\nthe dark circles are the input data d(i, j). The sx(i, j) and sy(i, j) black boxes denote arbi-\ntrary interaction potentials between adjacent nodes in the random ﬁeld, and the w(i, j) denote\nthe data penalty functions. The same graphical model can be used to depict a discrete version\nof a ﬁrst-order regularization problem (Figure 3.55).\nthey can better model discontinuities at different orientations (Boykov and Kolmogorov 2003;\nRother, Kohli, Feng et al. 2009; Kohli, Ladick´y, and Torr 2009; Kohli, Kumar, and Torr 2009).\nBinary MRFs\nThe simplest possible example of a Markov random ﬁeld is a binary ﬁeld. Examples of such\nﬁelds include 1-bit (black and white) scanned document images as well as images segmented\ninto foreground and background regions.\nTo denoise a scanned image, we set the data penalty to reﬂect the agreement between the\nscanned and ﬁnal images,\nEd(i, j) = wδ(f(i, j), d(i, j))\n(3.110)\nand the smoothness penalty to reﬂect the agreement between neighboring pixels\nEp(i, j) = Ex(i, j) + Ey(i, j) = sδ(f(i, j), f(i + 1, j)) + sδ(f(i, j), f(i, j + 1)). (3.111)\nOnce we have formulated the energy, how do we minimize it? The simplest approach is\nto perform gradient descent, ﬂipping one state at a time if it produces a lower energy. This ap-\nproach is known as contextual classiﬁcation (Kittler and F¨oglein 1984), iterated conditional\nmodes (ICM) (Besag 1986), or highest conﬁdence ﬁrst (HCF) (Chou and Brown 1990) if the\npixel with the largest energy decrease is selected ﬁrst.\nUnfortunately, these downhill methods tend to get easily stuck in local minima. An al-\nternative approach is to add some randomness to the process, which is known as stochastic",
  "205": "3.7 Global optimization\n183\ngradient descent (Metropolis, Rosenbluth, Rosenbluth et al. 1953; Geman and Geman 1984).\nWhen the amount of noise is decreased over time, this technique is known as simulated an-\nnealing (Kirkpatrick, Gelatt, and Vecchi 1983; Carnevali, Coletti, and Patarnello 1985; Wol-\nberg and Pavlidis 1985; Swendsen and Wang 1987) and was ﬁrst popularized in computer\nvision by Geman and Geman (1984) and later applied to stereo matching by Barnard (1989),\namong others.\nEven this technique, however, does not perform that well (Boykov, Veksler, and Zabih\n2001). For binary images, a much better technique, introduced to the computer vision com-\nmunity by Boykov, Veksler, and Zabih (2001) is to re-formulate the energy minimization as\na max-ﬂow/min-cut graph optimization problem (Greig, Porteous, and Seheult 1989). This\ntechnique has informally come to be known as graph cuts in the computer vision community\n(Boykov and Kolmogorov 2010). For simple energy functions, e.g., those where the penalty\nfor non-identical neighboring pixels is a constant, this algorithm is guaranteed to produce the\nglobal minimum. Kolmogorov and Zabih (2004) formally characterize the class of binary\nenergy potentials (regularity conditions) for which these results hold, while newer work by\nKomodakis, Tziritas, and Paragios (2008) and Rother, Kolmogorov, Lempitsky et al. (2007)\nprovide good algorithms for the cases when they do not.\nIn addition to the above mentioned techniques, a number of other optimization approaches\nhave been developed for MRF energy minimization, such as (loopy) belief propagation and\ndynamic programming (for one-dimensional problems). These are discussed in more detail\nin Appendix B.5 as well as the comparative survey paper by Szeliski, Zabih, Scharstein et al.\n(2008).\nOrdinal-valued MRFs\nIn addition to binary images, Markov random ﬁelds can be applied to ordinal-valued labels\nsuch as grayscale images or depth maps. The term ”ordinal” indicates that the labels have an\nimplied ordering, e.g., that higher values are lighter pixels. In the next section, we look at\nunordered labels, such as source image labels for image compositing.\nIn many cases, it is common to extend the binary data and smoothness prior terms as\nEd(i, j) = w(i, j)ρd(f(i, j) −d(i, j))\n(3.112)\nand\nEp(i, j) = sx(i, j)ρp(f(i, j) −f(i + 1, j)) + sy(i, j)ρp(f(i, j) −f(i, j + 1)),\n(3.113)\nwhich are robust generalizations of the quadratic penalty terms (3.101) and (3.100), ﬁrst\nintroduced in (3.105). As before, the w(i, j), sx(i, j) and sy(i, j) weights can be used to\nlocally control the data weighting and the horizontal and vertical smoothness. Instead of",
  "206": "184\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 3.57\nGrayscale image denoising and inpainting: (a) original image; (b) image\ncorrupted by noise and with missing data (black bar); (c) image restored using loopy be-\nlief propagation; (d) image restored using expansion move graph cuts. Images are from\nhttp://vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharstein et al. 2008).\nusing a quadratic penalty, however, a general monotonically increasing penalty function ρ()\nis used. (Different functions can be used for the data and smoothness terms.) For example,\nρp can be a hyper-Laplacian penalty\nρp(d) = |d|p, p < 1,\n(3.114)\nwhich better encodes the distribution of gradients (mainly edges) in an image than either a\nquadratic or linear (total variation) penalty.24\nLevin and Weiss (2007) use such a penalty\nto separate a transmitted and reﬂected image (Figure 8.17) by encouraging gradients to lie in\none or the other image, but not both. More recently, Levin, Fergus, Durand et al. (2007) use\nthe hyper-Laplacian as a prior for image deconvolution (deblurring) and Krishnan and Fergus\n(2009) develop a faster algorithm for solving such problems. For the data penalty, ρd can be\nquadratic (to model Gaussian noise) or the log of a contaminated Gaussian (Appendix B.3).\nWhen ρp is a quadratic function, the resulting Markov random ﬁeld is called a Gaussian\nMarkov random ﬁeld (GMRF) and its minimum can be found by sparse linear system solving\n(3.103). When the weighting functions are uniform, the GMRF becomes a special case of\nWiener ﬁltering (Section 3.4.3). Allowing the weighting functions to depend on the input\nimage (a special kind of conditional random ﬁeld, which we describe below) enables quite\nsophisticated image processing algorithms to be performed, including colorization (Levin,\nLischinski, and Weiss 2004), interactive tone mapping (Lischinski, Farbman, Uyttendaele et\nal. 2006a), natural image matting (Levin, Lischinski, and Weiss 2008), and image restoration\n(Tappen, Liu, Freeman et al. 2007).\n24 Note that, unlike a quadratic penalty, the sum of the horizontal and vertical derivative p-norms is not rotationally\ninvariant. A better approach may be to locally estimate the gradient direction and to impose different norms on the\nperpendicular and parallel components, which Roth and Black (2007b) call a steerable random ﬁeld.",
  "207": "3.7 Global optimization\n185\n(a) initial labeling\n(b) standard move\n(c) α-β-swap\n(d) α-expansion\nFigure 3.58\nMulti-level graph optimization from (Boykov, Veksler, and Zabih 2001) c⃝\n2001 IEEE: (a) initial problem conﬁguration; (b) the standard move only changes one pixel;\n(c) the α-β-swap optimally exchanges all α and β-labeled pixels; (d) the α-expansion move\noptimally selects among current pixel values and the α label.\nWhen ρd or ρp are non-quadratic functions, gradient descent techniques such as non-\nlinear least squares or iteratively re-weighted least squares can sometimes be used (Ap-\npendix A.3). However, if the search space has lots of local minima, as is the case for stereo\nmatching (Barnard 1989; Boykov, Veksler, and Zabih 2001), more sophisticated techniques\nare required.\nThe extension of graph cut techniques to multi-valued problems was ﬁrst proposed by\nBoykov, Veksler, and Zabih (2001). In their paper, they develop two different algorithms,\ncalled the swap move and the expansion move, which iterate among a series of binary labeling\nsub-problems to ﬁnd a good solution (Figure 3.58). Note that a global solution is generally not\nachievable, as the problem is provably NP-hard for general energy functions. Because both\nthese algorithms use a binary MRF optimization inside their inner loop, they are subject to the\nkind of constraints on the energy functions that occur in the binary labeling case (Kolmogorov\nand Zabih 2004). Appendix B.5.4 discusses these algorithms in more detail, along with some\nmore recently developed approaches to this problem.\nAnother MRF inference technique is belief propagation (BP). While belief propagation\nwas originally developed for inference over trees, where it is exact (Pearl 1988), it has more\nrecently been applied to graphs with loops such as Markov random ﬁelds (Freeman, Pasz-\ntor, and Carmichael 2000; Yedidia, Freeman, and Weiss 2001). In fact, some of the better\nperforming stereo-matching algorithms use loopy belief propagation (LBP) to perform their\ninference (Sun, Zheng, and Shum 2003). LBP is discussed in more detail in Appendix B.5.3\nas well as the comparative survey paper on MRF optimization (Szeliski, Zabih, Scharstein et\nal. 2008).\nFigure 3.57 shows an example of image denoising and inpainting (hole ﬁlling) using a\nnon-quadratic energy function (non-Gaussian MRF). The original image has been corrupted\nby noise and a portion of the data has been removed (the black bar). In this case, the loopy",
  "208": "186\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nd (i, j+1)\nFigure 3.59\nGraphical model for a Markov random ﬁeld with a more complex measurement\nmodel. The additional colored edges show how combinations of unknown values (say, in a\nsharp image) produce the measured values (a noisy blurred image). The resulting graphical\nmodel is still a classic MRF and is just as easy to sample from, but some inference algorithms\n(e.g., those based on graph cuts) may not be applicable because of the increased network\ncomplexity, since state changes during the inference become more entangled and the posterior\nMRF has much larger cliques.\nbelief propagation algorithm computes a slightly lower energy and also a smoother image\nthan the alpha-expansion graph cut algorithm.\nOf course, the above formula (3.113) for the smoothness term Ep(i, j) just shows the\nsimplest case. In more recent work, Roth and Black (2009) propose a Field of Experts (FoE)\nmodel, which sums up a large number of exponentiated local ﬁlter outputs to arrive at the\nsmoothness penalty. Weiss and Freeman (2007) analyze this approach and compare it to the\nsimpler hyper-Laplacian model of natural image statistics. Lyu and Simoncelli (2009) use\nGaussian Scale Mixtures (GSMs) to construct an inhomogeneous multi-scale MRF, with one\n(positive exponential) GMRF modulating the variance (amplitude) of another Gaussian MRF.\nIt is also possible to extend the measurement model to make the sampled (noise-corrupted)\ninput pixels correspond to blends of unknown (latent) image pixels, as in Figure 3.59. This is\nthe commonly occurring case when trying to de-blur an image. While this kind of a model is\nstill a traditional generative Markov random ﬁeld, ﬁnding an optimal solution can be difﬁcult\nbecause the clique sizes get larger. In such situations, gradient descent techniques, such\nas iteratively reweighted least squares, can be used (Joshi, Zitnick, Szeliski et al. 2009).\nExercise 3.31 has you explore some of these issues.",
  "209": "3.7 Global optimization\n187\nFigure 3.60\nAn unordered label MRF (Agarwala, Dontcheva, Agrawala et al. 2004) c⃝\n2004 ACM: Strokes in each of the source images on the left are used as constraints on an\nMRF optimization, which is solved using graph cuts. The resulting multi-valued label ﬁeld is\nshown as a color overlay in the middle image, and the ﬁnal composite is shown on the right.\nUnordered labels\nAnother case with multi-valued labels where Markov random ﬁelds are often applied are\nunordered labels, i.e., labels where there is no semantic meaning to the numerical difference\nbetween the values of two labels. For example, if we are classifying terrain from aerial\nimagery, it makes no sense to take the numeric difference between the labels assigned to\nforest, ﬁeld, water, and pavement. In fact, the adjacencies of these various kinds of terrain\neach have different likelihoods, so it makes more sense to use a prior of the form\nEp(i, j) = sx(i, j)V (l(i, j), l(i + 1, j)) + sy(i, j)V (l(i, j), l(i, j + 1)),\n(3.115)\nwhere V (l0, l1) is a general compatibility or potential function. (Note that we have also\nreplaced f(i, j) with l(i, j) to make it clearer that these are labels rather than discrete function\nsamples.) An alternative way to write this prior energy (Boykov, Veksler, and Zabih 2001;\nSzeliski, Zabih, Scharstein et al. 2008) is\nEp =\nX\n(p,q)∈N\nVp,q(lp, lq),\n(3.116)\nwhere the (p, q) are neighboring pixels and a spatially varying potential function Vp,q is eval-\nuated for each neighboring pair.\nAn important application of unordered MRF labeling is seam ﬁnding in image composit-\ning (Davis 1998; Agarwala, Dontcheva, Agrawala et al. 2004) (see Figure 3.60, which is\nexplained in more detail in Section 9.3.2). Here, the compatibility Vp,q(lp, lq) measures the\nquality of the visual appearance that would result from placing a pixel p from image lp next\nto a pixel q from image lq. As with most MRFs, we assume that Vp,q(l, l) = 0, i.e., it is per-\nfectly ﬁne to choose contiguous pixels from the same image. For different labels, however,",
  "210": "188\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 3.61\nImage segmentation (Boykov and Funka-Lea 2006) c⃝2006 Springer: The user\ndraws a few red strokes in the foreground object and a few blue ones in the background. The\nsystem computes color distributions for the foreground and background and solves a binary\nMRF. The smoothness weights are modulated by the intensity gradients (edges), which makes\nthis a conditional random ﬁeld (CRF).\nthe compatibility Vp,q(lp, lq) may depend on the values of the underlying pixels Ilp(p) and\nIlq(q).\nConsider, for example, where one image I0 is all sky blue, i.e., I0(p) = I0(q) = B, while\nthe other image I1 has a transition from sky blue, I1(p) = B, to forest green, I1(q) = G.\nI0 :\np\nq\np\nq\n: I1\nIn this case, Vp,q(1, 0) = 0 (the colors agree), while Vp,q(0, 1) > 0 (the colors disagree).\nConditional random ﬁelds\nIn a classic Bayesian model (3.106–3.108),\np(x|y) ∝p(y|x)p(x),\n(3.117)\nthe prior distribution p(x) is independent of the observations y. Sometimes, however, it is\nuseful to modify our prior assumptions, say about the smoothness of the ﬁeld we are trying\nto estimate, in response to the sensed data. Whether this makes sense from a probability\nviewpoint is something we discuss once we have explained the new model.\nConsider the interactive image segmentation problem shown in Figure 3.61 (Boykov and\nFunka-Lea 2006). In this application, the user draws foreground (red) and background (blue)\nstrokes, and the system then solves a binary MRF labeling problem to estimate the extent of\nthe foreground object. In addition to minimizing a data term, which measures the pointwise\nsimilarity between pixel colors and the inferred region distributions (Section 5.5), the MRF",
  "211": "3.7 Global optimization\n189\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure 3.62\nGraphical model for a conditional random ﬁeld (CRF). The additional green\nedges show how combinations of sensed data inﬂuence the smoothness in the underlying\nMRF prior model, i.e., sx(i, j) and sy(i, j) in (3.113) depend on adjacent d(i, j) values.\nThese additional links (factors) enable the smoothness to depend on the input data. However,\nthey make sampling from this MRF more complex.\nis modiﬁed so that the smoothness terms sx(x, y) and sy(x, y) in Figure 3.56 and (3.113)\ndepend on the magnitude of the gradient between adjacent pixels.25\nSince the smoothness term now depends on the data, Bayes’ Rule (3.117) no longer ap-\nplies. Instead, we use a direct model for the posterior distribution p(x|y), whose negative log\nlikelihood can be written as\nE(x|y)\n=\nEd(x, y) + Es(x, y)\n=\nX\np\nVp(xp, y) +\nX\n(p,q)∈N\nVp,q(xp, xq, y),\n(3.118)\nusing the notation introduced in (3.116). The resulting probability distribution is called a\nconditional random ﬁeld (CRF) and was ﬁrst introduced to the computer vision ﬁeld by Ku-\nmar and Hebert (2003), based on earlier work in text modeling by Lafferty, McCallum, and\nPereira (2001).\nFigure 3.62 shows a graphical model where the smoothness terms depend on the data\nvalues. In this particular model, each smoothness term depends only on its adjacent pair of\ndata values, i.e., terms are of the form Vp,q(xp, xq, yp, yq) in (3.118).\nThe idea of modifying smoothness terms in response to input data is not new. For ex-\nample, Boykov and Jolly (2001) used this idea for interactive segmentation, as shown in\nFigure 3.61, and it is now widely used in image segmentation (Section 5.5) (Blake, Rother,\n25 An alternative formulation that also uses detected edges to modulate the smoothness of a depth or motion ﬁeld\nand hence to integrate multiple lower level vision modules is presented by Poggio, Gamble, and Little (1988).",
  "212": "190\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nd (i, j+1)\nFigure 3.63\nGraphical model for a discriminative random ﬁeld (DRF). The additional green\nedges show how combinations of sensed data, e.g., d(i, j + 1), inﬂuence the data term for\nf(i, j). The generative model is therefore more complex, i.e., we cannot just apply a simple\nfunction to the unknown variables and add noise.\nBrown et al. 2004; Rother, Kolmogorov, and Blake 2004), denoising (Tappen, Liu, Freeman\net al. 2007), and object recognition (Section 14.4.3) (Winn and Shotton 2006; Shotton, Winn,\nRother et al. 2009).\nIn stereo matching, the idea of encouraging disparity discontinuities to coincide with\nintensity edges goes back even further to the early days of optimization and MRF-based\nalgorithms (Poggio, Gamble, and Little 1988; Fua 1993; Bobick and Intille 1999; Boykov,\nVeksler, and Zabih 2001) and is discussed in more detail in (Section 11.5).\nIn addition to using smoothness terms that adapt to the input data, Kumar and Hebert\n(2003) also compute a neighborhood function over the input data for each Vp(xp, y) term,\nas illustrated in Figure 3.63, instead of using the classic unary MRF data term Vp(xp, yp)\nshown in Figure 3.56.26 Because such neighborhood functions can be thought of as dis-\ncriminant functions (a term widely used in machine learning (Bishop 2006)), they call the\nresulting graphical model a discriminative random ﬁeld (DRF). In their paper, Kumar and\nHebert (2006) show that DRFs outperform similar CRFs on a number of applications, such\nas structure detection (Figure 3.64) and binary image denoising.\nHere again, one could argue that previous stereo correspondence algorithms also look at\na neighborhood of input data, either explicitly, because they compute correlation measures\n(Criminisi, Cross, Blake et al. 2006) as data terms, or implicitly, because even pixel-wise\ndisparity costs look at several pixels in either the left or right image (Barnard 1989; Boykov,\nVeksler, and Zabih 2001).\n26 Kumar and Hebert (2006) call the unary potentials Vp(xp, y) association potentials and the pairwise potentials\nVp,q(xp, yq, y) interaction potentials.",
  "213": "3.7 Global optimization\n191\nFigure 3.64\nStructure detection results using an MRF (left) and a DRF (right) (Kumar and\nHebert 2006) c⃝2006 Springer.\nWhat, then are the advantages and disadvantages of using conditional or discriminative\nrandom ﬁelds instead of MRFs?\nClassic Bayesian inference (MRF) assumes that the prior distribution of the data is in-\ndependent of the measurements. This makes a lot of sense: if you see a pair of sixes when\nyou ﬁrst throw a pair of dice, it would be unwise to assume that they will always show up\nthereafter. However, if after playing for a long time you detect a statistically signiﬁcant bias,\nyou may want to adjust your prior. What CRFs do, in essence, is to select or modify the prior\nmodel based on observed data. This can be viewed as making a partial inference over addi-\ntional hidden variables or correlations between the unknowns (say, a label, depth, or clean\nimage) and the knowns (observed images).\nIn some cases, the CRF approach makes a lot of sense and is, in fact, the only plausi-\nble way to proceed. For example, in grayscale image colorization (Section 10.3.2) (Levin,\nLischinski, and Weiss 2004), the best way to transfer the continuity information from the\ninput grayscale image to the unknown color image is to modify local smoothness constraints.\nSimilarly, for simultaneous segmentation and recognition (Winn and Shotton 2006; Shotton,\nWinn, Rother et al. 2009), it makes a lot of sense to permit strong color edges to inﬂuence\nthe semantic image label continuities.\nIn other cases, such as image denoising, the situation is more subtle.\nUsing a non-\nquadratic (robust) smoothness term as in (3.113) plays a qualitatively similar role to setting\nthe smoothness based on local gradient information in a Gaussian MRF (GMRF) (Tappen,\nLiu, Freeman et al. 2007). (In more recent work, Tanaka and Okutomi (2008) use a larger\nneighborhood and full covariance matrix on a related Gaussian MRF.) The advantage of Gaus-\nsian MRFs, when the smoothness can be correctly inferred, is that the resulting quadratic\nenergy can be minimized in a single step. However, for situations where the discontinuities\nare not self-evident in the input data, such as for piecewise-smooth sparse data interpolation\n(Blake and Zisserman 1987; Terzopoulos 1988), classic robust smoothness energy minimiza-",
  "214": "192\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntion may be preferable. Thus, as with most computer vision algorithms, a careful analysis of\nthe problem at hand and desired robustness and computation constraints may be required to\nchoose the best technique.\nPerhaps the biggest advantage of CRFs and DRFs, as argued by Kumar and Hebert (2006),\nTappen, Liu, Freeman et al. (2007) and Blake, Rother, Brown et al. (2004), is that learning the\nmodel parameters is sometimes easier. While learning parameters in MRFs and their variants\nis not a topic that we cover in this book, interested readers can ﬁnd more details in recently\npublished articles (Kumar and Hebert 2006; Roth and Black 2007a; Tappen, Liu, Freeman et\nal. 2007; Tappen 2007; Li and Huttenlocher 2008).\n3.7.3 Application: Image restoration\nIn Section 3.4.4, we saw how two-dimensional linear and non-linear ﬁlters can be used to\nremove noise or enhance sharpness in images. Sometimes, however, images are degraded by\nlarger problems, such as scratches and blotches (Kokaram 2004). In this case, Bayesian meth-\nods such as MRFs, which can model spatially varying per-pixel measurement noise, can be\nused instead. An alternative is to use hole ﬁlling or inpainting techniques (Bertalmio, Sapiro,\nCaselles et al. 2000; Bertalmio, Vese, Sapiro et al. 2003; Criminisi, P´erez, and Toyama 2004),\nas discussed in Sections 5.1.4 and 10.5.1.\nFigure 3.57 shows an example of image denoising and inpainting (hole ﬁlling) using a\nMarkov random ﬁeld. The original image has been corrupted by noise and a portion of the\ndata has been removed. In this case, the loopy belief propagation algorithm computes a\nslightly lower energy and also a smoother image than the alpha-expansion graph cut algo-\nrithm.\n3.8 Additional reading\nIf you are interested in exploring the topic of image processing in more depth, some popular\ntextbooks have been written by Lim (1990); Crane (1997); Gomes and Velho (1997); J¨ahne\n(1997); Pratt (2007); Russ (2007); Burger and Burge (2008); Gonzales and Woods (2008).\nThe pre-eminent conference and journal in this ﬁeld are the IEEE Conference on Image Pro-\ncesssing and the IEEE Transactions on Image Processing.\nFor image compositing operators, the seminal reference is by Porter and Duff (1984)\nwhile Blinn (1994a,b) provides a more detailed tutorial. For image compositing, Smith and\nBlinn (1996) were the ﬁrst to bring this topic to the attention of the graphics community,\nwhile Wang and Cohen (2007a) provide a recent in-depth survey.\nIn the realm of linear ﬁltering, Freeman and Adelson (1991) provide a great introduc-\ntion to separable and steerable oriented band-pass ﬁlters, while Perona (1995) shows how to",
  "215": "3.8 Additional reading\n193\napproximate any ﬁlter as a sum of separable components.\nThe literature on non-linear ﬁltering is quite wide and varied; it includes such topics as\nbilateral ﬁltering (Tomasi and Manduchi 1998; Durand and Dorsey 2002; Paris and Durand\n2006; Chen, Paris, and Durand 2007; Paris, Kornprobst, Tumblin et al. 2008), related itera-\ntive algorithms (Saint-Marc, Chen, and Medioni 1991; Nielsen, Florack, and Deriche 1997;\nBlack, Sapiro, Marimont et al. 1998; Weickert, ter Haar Romeny, and Viergever 1998; Weick-\nert 1998; Barash 2002; Scharr, Black, and Haussecker 2003; Barash and Comaniciu 2004),\nand variational approaches (Chan, Osher, and Shen 2001; Tschumperl´e and Deriche 2005;\nTschumperl´e 2006; Kaftory, Schechner, and Zeevi 2007).\nGood references to image morphology include (Haralick and Shapiro 1992, Section 5.2;\nBovik 2000, Section 2.2; Ritter and Wilson 2000, Section 7; Serra 1982; Serra and Vincent\n1992; Yuille, Vincent, and Geiger 1992; Soille 2006).\nThe classic papers for image pyramids and pyramid blending are by Burt and Adelson\n(1983a,b). Wavelets were ﬁrst introduced to the computer vision community by Mallat (1989)\nand good tutorial and review papers and books are available (Strang 1989; Simoncelli and\nAdelson 1990b; Rioul and Vetterli 1991; Chui 1992; Meyer 1993; Sweldens 1997). Wavelets\nare widely used in the computer graphics community to perform multi-resolution geomet-\nric processing (Stollnitz, DeRose, and Salesin 1996) and have been used in computer vision\nfor similar applications (Szeliski 1990b; Pentland 1994; Gortler and Cohen 1995; Yaou and\nChang 1994; Lai and Vemuri 1997; Szeliski 2006b), as well as for multi-scale oriented ﬁlter-\ning (Simoncelli, Freeman, Adelson et al. 1992) and denoising (Portilla, Strela, Wainwright et\nal. 2003).\nWhile image pyramids (Section 3.5.3) are usually constructed using linear ﬁltering op-\nerators, some recent work has started investigating non-linear ﬁlters, since these can better\npreserve details and other salient features. Some representative papers in the computer vision\nliterature are by Gluckman (2006a,b); Lyu and Simoncelli (2008) and in computational pho-\ntography by Bae, Paris, and Durand (2006); Farbman, Fattal, Lischinski et al. (2008); Fattal\n(2009).\nHigh-quality algorithms for image warping and resampling are covered both in the im-\nage processing literature (Wolberg 1990; Dodgson 1992; Gomes, Darsa, Costa et al. 1999;\nSzeliski, Winder, and Uyttendaele 2010) and in computer graphics (Williams 1983; Heckbert\n1986; Barkans 1997; Akenine-M¨oller and Haines 2002), where they go under the name of\ntexture mapping. Combination of image warping and image blending techniques are used to\nenable morphing between images, which is covered in a series of seminal papers and books\n(Beier and Neely 1992; Gomes, Darsa, Costa et al. 1999).\nThe regularization approach to computer vision problems was ﬁrst introduced to the vi-\nsion community by Poggio, Torre, and Koch (1985) and Terzopoulos (1986a,b, 1988) and\ncontinues to be a popular framework for formulating and solving low-level vision problems",
  "216": "194\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Ju, Black, and Jepson 1996; Nielsen, Florack, and Deriche 1997; Nordstr¨om 1990; Brox,\nBruhn, Papenberg et al. 2004; Levin, Lischinski, and Weiss 2008). More detailed mathe-\nmatical treatment and additional applications can be found in the applied mathematics and\nstatistics literature (Tikhonov and Arsenin 1977; Engl, Hanke, and Neubauer 1996).\nThe literature on Markov random ﬁelds is truly immense, with publications in related\nﬁelds such as optimization and control theory of which few vision practitioners are even\naware. A good guide to the latest techniques is the book edited by Blake, Kohli, and Rother\n(2010). Other recent articles that contain nice literature reviews or experimental compar-\nisons include (Boykov and Funka-Lea 2006; Szeliski, Zabih, Scharstein et al. 2008; Kumar,\nVeksler, and Torr 2010).\nThe seminal paper on Markov random ﬁelds is the work of Geman and Geman (1984),\nwho introduced this formalism to computer vision researchers and also introduced the no-\ntion of line processes, additional binary variables that control whether smoothness penalties\nare enforced or not. Black and Rangarajan (1996) showed how independent line processes\ncould be replaced with robust pairwise potentials; Boykov, Veksler, and Zabih (2001) devel-\noped iterative binary, graph cut algorithms for optimizing multi-label MRFs; Kolmogorov\nand Zabih (2004) characterized the class of binary energy potentials required for these tech-\nniques to work; and Freeman, Pasztor, and Carmichael (2000) popularized the use of loopy\nbelief propagation for MRF inference. Many more additional references can be found in\nSections 3.7.2 and 5.5, and Appendix B.5.\n3.9 Exercises\nEx 3.1: Color balance\nWrite a simple application to change the color balance of an image\nby multiplying each color value by a different user-speciﬁed constant. If you want to get\nfancy, you can make this application interactive, with sliders.\n1. Do you get different results if you take out the gamma transformation before or after\ndoing the multiplication? Why or why not?\n2. Take the same picture with your digital camera using different color balance settings\n(most cameras control the color balance from one of the menus). Can you recover what\nthe color balance ratios are between the different settings? You may need to put your\ncamera on a tripod and align the images manually or automatically to make this work.\nAlternatively, use a color checker chart (Figure 10.3b), as discussed in Sections 2.3 and\n10.1.1.\n3. If you have access to the RAW image for the camera, perform the demosaicing yourself\n(Section 10.3.1) or downsample the image resolution to get a “true” RGB image. Does",
  "217": "3.9 Exercises\n195\nyour camera perform a simple linear mapping between RAW values and the color-\nbalanced values in a JPEG? Some high-end cameras have a RAW+JPEG mode, which\nmakes this comparison much easier.\n4. Can you think of any reason why you might want to perform a color twist (Sec-\ntion 3.1.2) on the images? See also Exercise 2.9 for some related ideas.\nEx 3.2: Compositing and reﬂections\nSection 3.1.3 describes the process of compositing\nan alpha-matted image on top of another. Answer the following questions and optionally\nvalidate them experimentally:\n1. Most captured images have gamma correction applied to them. Does this invalidate the\nbasic compositing equation (3.8); if so, how should it be ﬁxed?\n2. The additive (pure reﬂection) model may have limitations. What happens if the glass is\ntinted, especially to a non-gray hue? How about if the glass is dirty or smudged? How\ncould you model wavy glass or other kinds of refractive objects?\nEx 3.3: Blue screen matting\nSet up a blue or green background, e.g., by buying a large\npiece of colored posterboard. Take a picture of the empty background, and then of the back-\nground with a new object in front of it. Pull the matte using the difference between each\ncolored pixel and its assumed corresponding background pixel, using one of the techniques\ndescribed in Section 3.1.3) or by Smith and Blinn (1996).\nEx 3.4: Difference keying\nImplement a difference keying algorithm (see Section 3.1.3)\n(Toyama, Krumm, Brumitt et al. 1999), consisting of the following steps:\n1. Compute the mean and variance (or median and robust variance) at each pixel in an\n“empty” video sequence.\n2. For each new frame, classify each pixel as foreground or background (set the back-\nground pixels to RGBA=0).\n3. (Optional) Compute the alpha channel and composite over a new background.\n4. (Optional) Clean up the image using morphology (Section 3.3.1), label the connected\ncomponents (Section 3.3.4), compute their centroids, and track them from frame to\nframe. Use this to build a “people counter”.\nEx 3.5: Photo effects\nWrite a variety of photo enhancement or effects ﬁlters: contrast, so-\nlarization (quantization), etc. Which ones are useful (perform sensible corrections) and which\nones are more creative (create unusual images)?",
  "218": "196\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 3.6: Histogram equalization\nCompute the gray level (luminance) histogram for an im-\nage and equalize it so that the tones look better (and the image is less sensitive to exposure\nsettings). You may want to use the following steps:\n1. Convert the color image to luminance (Section 3.1.2).\n2. Compute the histogram, the cumulative distribution, and the compensation transfer\nfunction (Section 3.1.4).\n3. (Optional) Try to increase the “punch” in the image by ensuring that a certain fraction\nof pixels (say, 5%) are mapped to pure black and white.\n4. (Optional) Limit the local gain f ′(I) in the transfer function. One way to do this is to\nlimit f(I) < γI or f ′(I) < γ while performing the accumulation (3.9), keeping any\nunaccumulated values “in reserve”. (I’ll let you ﬁgure out the exact details.)\n5. Compensate the luminance channel through the lookup table and re-generate the color\nimage using color ratios (2.116).\n6. (Optional) Color values that are clipped in the original image, i.e., have one or more\nsaturated color channels, may appear unnatural when remapped to a non-clipped value.\nExtend your algorithm to handle this case in some useful way.\nEx 3.7: Local histogram equalization\nCompute the gray level (luminance) histograms for\neach patch, but add to vertices based on distance (a spline).\n1. Build on Exercise 3.6 (luminance computation).\n2. Distribute values (counts) to adjacent vertices (bilinear).\n3. Convert to CDF (look-up functions).\n4. (Optional) Use low-pass ﬁltering of CDFs.\n5. Interpolate adjacent CDFs for ﬁnal lookup.\nEx 3.8: Padding for neighborhood operations\nWrite down the formulas for computing\nthe padded pixel values ˜f(i, j) as a function of the original pixel values f(k, l) and the image\nwidth and height (M, N) for each of the padding modes shown in Figure 3.13. For example,\nfor replication (clamping),\n˜f(i, j) = f(k, l),\nk = max(0, min(M −1, i)),\nl = max(0, min(N −1, j)),\n(Hint: you may want to use the min, max, mod, and absolute value operators in addition to\nthe regular arithmetic operators.)",
  "219": "3.9 Exercises\n197\n• Describe in more detail the advantages and disadvantages of these various modes.\n• (Optional) Check what your graphics card does by drawing a texture-mapped rectangle\nwhere the texture coordinates lie beyond the [0.0, 1.0] range and using different texture\nclamping modes.\nEx 3.9: Separable ﬁlters\nImplement convolution with a separable kernel. The input should\nbe a grayscale or color image along with the horizontal and vertical kernels. Make sure\nyou support the padding mechanisms developed in the previous exercise. You will need this\nfunctionality for some of the later exercises. If you already have access to separable ﬁltering\nin an image processing package you are using (such as IPL), skip this exercise.\n• (Optional) Use Pietro Perona’s (1995) technique to approximate convolution as a sum\nof a number of separable kernels. Let the user specify the number of kernels and report\nback some sensible metric of the approximation ﬁdelity.\nEx 3.10: Discrete Gaussian ﬁlters\nDiscuss the following issues with implementing a dis-\ncrete Gaussian ﬁlter:\n• If you just sample the equation of a continuous Gaussian ﬁlter at discrete locations,\nwill you get the desired properties, e.g., will the coefﬁcients sum up to 0? Similarly, if\nyou sample a derivative of a Gaussian, do the samples sum up to 0 or have vanishing\nhigher-order moments?\n• Would it be preferable to take the original signal, interpolate it with a sinc, blur with a\ncontinuous Gaussian, then pre-ﬁlter with a sinc before re-sampling? Is there a simpler\nway to do this in the frequency domain?\n• Would it make more sense to produce a Gaussian frequency response in the Fourier\ndomain and to then take an inverse FFT to obtain a discrete ﬁlter?\n• How does truncation of the ﬁlter change its frequency response? Does it introduce any\nadditional artifacts?\n• Are the resulting two-dimensional ﬁlters as rotationally invariant as their continuous\nanalogs? Is there some way to improve this? In fact, can any 2D discrete (separable or\nnon-separable) ﬁlter be truly rotationally invariant?\nEx 3.11: Sharpening, blur, and noise removal\nImplement some softening, sharpening, and\nnon-linear diffusion (selective sharpening or noise removal) ﬁlters, such as Gaussian, median,\nand bilateral (Section 3.3.1), as discussed in Section 3.4.4.\nTake blurry or noisy images (shooting in low light is a good way to get both) and try to\nimprove their appearance and legibility.",
  "220": "198\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 3.12: Steerable ﬁlters\nImplement Freeman and Adelson’s (1991) steerable ﬁlter algo-\nrithm. The input should be a grayscale or color image and the output should be a multi-banded\nimage consisting of G0◦\n1 and G90◦\n1\n. The coefﬁcients for the ﬁlters can be found in the paper\nby Freeman and Adelson (1991).\nTest the various order ﬁlters on a number of images of your choice and see if you can\nreliably ﬁnd corner and intersection features. These ﬁlters will be quite useful later to detect\nelongated structures, such as lines (Section 4.3).\nEx 3.13: Distance transform\nImplement some (raster-scan) algorithms for city block and\nEuclidean distance transforms. Can you do it without peeking at the literature (Danielsson\n1980; Borgefors 1986)? If so, what problems did you come across and resolve?\nLater on, you can use the distance functions you compute to perform feathering during\nimage stitching (Section 9.3.2).\nEx 3.14: Connected components\nImplement one of the connected component algorithms\nfrom Section 3.3.4 or Section 2.3 from Haralick and Shapiro’s book (1992) and discuss its\ncomputational complexity.\n• Threshold or quantize an image to obtain a variety of input labels and then compute the\narea statistics for the regions that you ﬁnd.\n• Use the connected components that you have found to track or match regions in differ-\nent images or video frames.\nEx 3.15: Fourier transform\nProve the properties of the Fourier transform listed in Ta-\nble 3.1 and derive the formulas for the Fourier transforms listed in Tables 3.2 and 3.3. These\nexercises are very useful if you want to become comfortable working with Fourier transforms,\nwhich is a very useful skill when analyzing and designing the behavior and efﬁciency of many\ncomputer vision algorithms.\nEx 3.16: Wiener ﬁltering\nEstimate the frequency spectrum of your personal photo collec-\ntion and use it to perform Wiener ﬁltering on a few images with varying degrees of noise.\n1. Collect a few hundred of your images by re-scaling them to ﬁt within a 512 × 512\nwindow and cropping them.\n2. Take their Fourier transforms, throw away the phase information, and average together\nall of the spectra.\n3. Pick two of your favorite images and add varying amounts of Gaussian noise, σn ∈\n{1, 2, 5, 10, 20} gray levels.",
  "221": "3.9 Exercises\n199\n4. For each combination of image and noise, determine by eye which width of a Gaussian\nblurring ﬁlter σs gives the best denoised result. You will have to make a subjective\ndecision between sharpness and noise.\n5. Compute the Wiener ﬁltered version of all the noised images and compare them against\nyour hand-tuned Gaussian-smoothed images.\n6. (Optional) Do your image spectra have a lot of energy concentrated along the horizontal\nand vertical axes (fx = 0 and fy = 0)? Can you think of an explanation for this? Does\nrotating your image samples by 45◦move this energy to the diagonals? If not, could it\nbe due to edge effects in the Fourier transform? Can you suggest some techniques for\nreducing such effects?\nEx 3.17: Deblurring using Wiener ﬁltering\nUse Wiener ﬁltering to deblur some images.\n1. Modify the Wiener ﬁlter derivation (3.66–3.74) to incorporate blur (3.75).\n2. Discuss the resulting Wiener ﬁlter in terms of its noise suppression and frequency\nboosting characteristics.\n3. Assuming that the blur kernel is Gaussian and the image spectrum follows an inverse\nfrequency law, compute the frequency response of the Wiener ﬁlter, and compare it to\nthe unsharp mask.\n4. Synthetically blur two of your sample images with Gaussian blur kernels of different\nradii, add noise, and then perform Wiener ﬁltering.\n5. Repeat the above experiment with a “pillbox” (disc) blurring kernel, which is charac-\nteristic of a ﬁnite aperture lens (Section 2.2.3). Compare these results to Gaussian blur\nkernels (be sure to inspect your frequency plots).\n6. It has been suggested that regular apertures are anathema to de-blurring because they\nintroduce zeros in the sensed frequency spectrum (Veeraraghavan, Raskar, Agrawal et\nal. 2007). Show that this is indeed an issue if no prior model is assumed for the signal,\ni.e., P −1\ns\nl1. If a reasonable power spectrum is assumed, is this still a problem (do we\nstill get banding or ringing artifacts)?\nEx 3.18: High-quality image resampling\nImplement several of the low-pass ﬁlters pre-\nsented in Section 3.5.2 and also the discussion of the windowed sinc shown in Table 3.2 and\nFigure 3.29. Feel free to implement other ﬁlters (Wolberg 1990; Unser 1999).\nApply your ﬁlters to continuously resize an image, both magnifying (interpolating) and\nminifying (decimating) it; compare the resulting animations for several ﬁlters. Use both a",
  "222": "200\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 3.65 Sample images for testing the quality of resampling algorithms: (a) a synthetic\nchirp; (b) and (c) some high-frequency images from the image compression community.\nsynthetic chirp image (Figure 3.65a) and natural images with lots of high-frequency detail\n(Figure 3.65b-c).27\nYou may ﬁnd it helpful to write a simple visualization program that continuously plays the\nanimations for two or more ﬁlters at once and that let you “blink” between different results.\nDiscuss the merits and deﬁciencies of each ﬁlter, as well as its tradeoff between speed and\nquality.\nEx 3.19: Pyramids\nConstruct an image pyramid. The inputs should be a grayscale or color\nimage, a separable ﬁlter kernel, and the number of desired levels. Implement at least the\nfollowing kernels:\n• 2 × 2 block ﬁltering;\n• Burt and Adelson’s binomial kernel 1/16(1, 4, 6, 4, 1) (Burt and Adelson 1983a);\n• a high-quality seven- or nine-tap ﬁlter.\nCompare the visual quality of the various decimation ﬁlters. Also, shift your input image by\n1 to 4 pixels and compare the resulting decimated (quarter size) image sequence.\nEx 3.20: Pyramid blending\nWrite a program that takes as input two color images and a\nbinary mask image and produces the Laplacian pyramid blend of the two images.\n1. Construct the Laplacian pyramid for each image.\n2. Construct the Gaussian pyramid for the two mask images (the input image and its\ncomplement).\n27 These particular images are available on the book’s Web site.",
  "223": "3.9 Exercises\n201\n3. Multiply each Laplacian image by its corresponding mask and sum the images (see\nFigure 3.43).\n4. Reconstruct the ﬁnal image from the blended Laplacian pyramid.\nGeneralize your algorithm to input n images and a label image with values 1 . . . n (the value\n0 can be reserved for “no input”). Discuss whether the weighted summation stage (step 3)\nneeds to keep track of the total weight for renormalization, or whether the math just works\nout. Use your algorithm either to blend two differently exposed image (to avoid under- and\nover-exposed regions) or to make a creative blend of two different scenes.\nEx 3.21: Wavelet construction and applications\nImplement one of the wavelet families\ndescribed in Section 3.5.4 or by Simoncelli and Adelson (1990b), as well as the basic Lapla-\ncian pyramid (Exercise 3.19). Apply the resulting representations to one of the following two\ntasks:\n• Compression: Compute the entropy in each band for the different wavelet implemen-\ntations, assuming a given quantization level (say, 1/4 gray level, to keep the rounding\nerror acceptable). Quantize the wavelet coefﬁcients and reconstruct the original im-\nages. Which technique performs better? (See (Simoncelli and Adelson 1990b) or any\nof the multitude of wavelet compression papers for some typical results.)\n• Denoising. After computing the wavelets, suppress small values using coring, i.e., set\nsmall values to zero using a piecewise linear or other C0 function. Compare the results\nof your denoising using different wavelet and pyramid representations.\nEx 3.22: Parametric image warping\nWrite the code to do afﬁne and perspective image\nwarps (optionally bilinear as well). Try a variety of interpolants and report on their visual\nquality. In particular, discuss the following:\n• In a MIP-map, selecting only the coarser level adjacent to the computed fractional\nlevel will produce a blurrier image, while selecting the ﬁner level will lead to aliasing.\nExplain why this is so and discuss whether blending an aliased and a blurred image\n(tri-linear MIP-mapping) is a good idea.\n• When the ratio of the horizontal and vertical resampling rates becomes very different\n(anisotropic), the MIP-map performs even worse. Suggest some approaches to reduce\nsuch problems.\nEx 3.23: Local image warping\nOpen an image and deform its appearance in one of the\nfollowing ways:",
  "224": "202\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Click on a number of pixels and move (drag) them to new locations. Interpolate the\nresulting sparse displacement ﬁeld to obtain a dense motion ﬁeld (Sections 3.6.2 and\n3.5.1).\n2. Draw a number of lines in the image. Move the endpoints of the lines to specify their\nnew positions and use the Beier–Neely interpolation algorithm (Beier and Neely 1992),\ndiscussed in Section 3.6.2, to get a dense motion ﬁeld.\n3. Overlay a spline control grid and move one grid point at a time (optionally select the\nlevel of the deformation).\n4. Have a dense per-pixel ﬂow ﬁeld and use a soft “paintbrush” to design a horizontal and\nvertical velocity ﬁeld.\n5. (Optional): Prove whether the Beier–Neely warp does or does not reduce to a sparse\npoint-based deformation as the line segments become shorter (reduce to points).\nEx 3.24: Forward warping\nGiven a displacement ﬁeld from the previous exercise, write a\nforward warping algorithm:\n1. Write a forward warper using splatting, either nearest neighbor or soft accumulation\n(Section 3.6.1).\n2. Write a two-pass algorithm, which forward warps the displacement ﬁeld, ﬁlls in small\nholes, and then uses inverse warping (Shade, Gortler, He et al. 1998).\n3. Compare the quality of these two algorithms.\nEx 3.25: Feature-based morphing\nExtend the warping code you wrote in Exercise 3.23\nto import two different images and specify correspondences (point, line, or mesh-based) be-\ntween the two images.\n1. Create a morph by partially warping the images towards each other and cross-dissolving\n(Section 3.6.3).\n2. Try using your morphing algorithm to perform an image rotation and discuss whether\nit behaves the way you want it to.\nEx 3.26: 2D image editor\nExtend the program you wrote in Exercise 2.2 to import images\nand let you create a “collage” of pictures. You should implement the following steps:\n1. Open up a new image (in a separate window).",
  "225": "3.9 Exercises\n203\nFigure 3.66 There is a faint image of a rainbow visible in the right hand side of this picture.\nCan you think of a way to enhance it (Exercise 3.29)?\n2. Shift drag (rubber-band) to crop a subregion (or select whole image).\n3. Paste into the current canvas.\n4. Select the deformation mode (motion model): translation, rigid, similarity, afﬁne, or\nperspective.\n5. Drag any corner of the outline to change its transformation.\n6. (Optional) Change the relative ordering of the images and which image is currently\nbeing manipulated.\nThe user should see the composition of the various images’ pieces on top of each other.\nThis exercise should be built on the image transformation classes supported in the soft-\nware library. Persistence of the created representation (save and load) should also be sup-\nported (for each image, save its transformation).\nEx 3.27: 3D texture-mapped viewer\nExtend the viewer you created in Exercise 2.3 to in-\nclude texture-mapped polygon rendering. Augment each polygon with (u, v, w) coordinates\ninto an image.\nEx 3.28: Image denoising\nImplement at least two of the various image denoising tech-\nniques described in this chapter and compare them on both synthetically noised image se-\nquences and real-world (low-light) sequences. Does the performance of the algorithm de-\npend on the correct choice of noise level estimate? Can you draw any conclusions as to\nwhich techniques work better?",
  "226": "204\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 3.29: Rainbow enhancer—challenging\nTake a picture containing a rainbow, such as\nFigure 3.66, and enhance the strength (saturation) of the rainbow.\n1. Draw an arc in the image delineating the extent of the rainbow.\n2. Fit an additive rainbow function (explain why it is additive) to this arc (it is best to work\nwith linearized pixel values), using the spectrum as the cross section, and estimating\nthe width of the arc and the amount of color being added. This is the trickiest part of\nthe problem, as you need to tease apart the (low-frequency) rainbow pattern and the\nnatural image hiding behind it.\n3. Amplify the rainbow signal and add it back into the image, re-applying the gamma\nfunction if necessary to produce the ﬁnal image.\nEx 3.30: Image deblocking—challenging\nNow that you have some good techniques to\ndistinguish signal from noise, develop a technique to remove the blocking artifacts that occur\nwith JPEG at high compression settings (Section 2.3.3). Your technique can be as simple\nas looking for unexpected edges along block boundaries, to looking at the quantization step\nas a projection of a convex region of the transform coefﬁcient space onto the corresponding\nquantized values.\n1. Does the knowledge of the compression factor, which is available in the JPEG header\ninformation, help you perform better deblocking?\n2. Because the quantization occurs in the DCT transformed YCbCr space (2.115), it may\nbe preferable to perform the analysis in this space. On the other hand, image priors\nmake more sense in an RGB space (or do they?). Decide how you will approach this\ndichotomy and discuss your choice.\n3. While you are at it, since the YCbCr conversion is followed by a chrominance subsam-\npling stage (before the DCT), see if you can restore some of the lost high-frequency\nchrominance signal using one of the better restoration techniques discussed in this\nchapter.\n4. If your camera has a RAW + JPEG mode, how close can you come to the noise-free\ntrue pixel values? (This suggestion may not be that useful, since cameras generally use\nreasonably high quality settings for their RAW + JPEG models.)\nEx 3.31: Inference in de-blurring—challenging\nWrite down the graphical model corre-\nsponding to Figure 3.59 for a non-blind image deblurring problem, i.e., one where the blur\nkernel is known ahead of time.\nWhat kind of efﬁcient inference (optimization) algorithms can you think of for solving\nsuch problems?",
  "227": "Chapter 4\nFeature detection and matching\n4.1\nPoints and patches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n4.1.1\nFeature detectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n4.1.2\nFeature descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n4.1.3\nFeature matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n4.1.4\nFeature tracking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n4.1.5\nApplication: Performance-driven animation . . . . . . . . . . . . . . 237\n4.2\nEdges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n4.2.1\nEdge detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n4.2.2\nEdge linking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n4.2.3\nApplication: Edge editing and enhancement . . . . . . . . . . . . . . 249\n4.3\nLines\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n4.3.1\nSuccessive approximation\n. . . . . . . . . . . . . . . . . . . . . . . 250\n4.3.2\nHough transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\n4.3.3\nVanishing points\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n4.3.4\nApplication: Rectangle detection . . . . . . . . . . . . . . . . . . . . 257\n4.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n4.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259",
  "228": "206\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 4.1 A variety of feature detectors and descriptors can be used to analyze, describe and\nmatch images: (a) point-like interest operators (Brown, Szeliski, and Winder 2005) c⃝2005\nIEEE; (b) region-like interest operators (Matas, Chum, Urban et al. 2004) c⃝2004 Elsevier;\n(c) edges (Elder and Goldberg 2001) c⃝2001 IEEE; (d) straight lines (Sinha, Steedly, Szeliski\net al. 2008) c⃝2008 ACM.",
  "229": "4.1 Points and patches\n207\nFeature detection and matching are an essential component of many computer vision appli-\ncations. Consider the two pairs of images shown in Figure 4.2. For the ﬁrst pair, we may\nwish to align the two images so that they can be seamlessly stitched into a composite mosaic\n(Chapter 9). For the second pair, we may wish to establish a dense set of correspondences so\nthat a 3D model can be constructed or an in-between view can be generated (Chapter 11). In\neither case, what kinds of features should you detect and then match in order to establish such\nan alignment or set of correspondences? Think about this for a few moments before reading\non.\nThe ﬁrst kind of feature that you may notice are speciﬁc locations in the images, such as\nmountain peaks, building corners, doorways, or interestingly shaped patches of snow. These\nkinds of localized feature are often called keypoint features or interest points (or even corners)\nand are often described by the appearance of patches of pixels surrounding the point location\n(Section 4.1). Another class of important features are edges, e.g., the proﬁle of mountains\nagainst the sky, (Section 4.2). These kinds of features can be matched based on their orien-\ntation and local appearance (edge proﬁles) and can also be good indicators of object bound-\naries and occlusion events in image sequences. Edges can be grouped into longer curves and\nstraight line segments, which can be directly matched or analyzed to ﬁnd vanishing points\nand hence internal and external camera parameters (Section 4.3).\nIn this chapter, we describe some practical approaches to detecting such features and\nalso discuss how feature correspondences can be established across different images. Point\nfeatures are now used in such a wide variety of applications that it is good practice to read and\nimplement some of the algorithms from (Section 4.1). Edges and lines provide information\nthat is complementary to both keypoint and region-based descriptors and are well-suited to\ndescribing object boundaries and man-made objects. These alternative descriptors, while\nextremely useful, can be skipped in a short introductory course.\n4.1 Points and patches\nPoint features can be used to ﬁnd a sparse set of corresponding locations in different im-\nages, often as a pre-cursor to computing camera pose (Chapter 7), which is a prerequisite for\ncomputing a denser set of correspondences using stereo matching (Chapter 11). Such corre-\nspondences can also be used to align different images, e.g., when stitching image mosaics or\nperforming video stabilization (Chapter 9). They are also used extensively to perform object\ninstance and category recognition (Sections 14.3 and 14.4). A key advantage of keypoints\nis that they permit matching even in the presence of clutter (occlusion) and large scale and\norientation changes.\nFeature-based correspondence techniques have been used since the early days of stereo",
  "230": "208\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.2\nTwo pairs of images to be matched. What kinds of feature might one use to\nestablish a set of correspondences between these images?\nmatching (Hannah 1974; Moravec 1983; Hannah 1988) and have more recently gained pop-\nularity for image-stitching applications (Zoghlami, Faugeras, and Deriche 1997; Brown and\nLowe 2007) as well as fully automated 3D modeling (Beardsley, Torr, and Zisserman 1996;\nSchaffalitzky and Zisserman 2002; Brown and Lowe 2003; Snavely, Seitz, and Szeliski 2006).\nThere are two main approaches to ﬁnding feature points and their correspondences. The\nﬁrst is to ﬁnd features in one image that can be accurately tracked using a local search tech-\nnique, such as correlation or least squares (Section 4.1.4). The second is to independently\ndetect features in all the images under consideration and then match features based on their\nlocal appearance (Section 4.1.3). The former approach is more suitable when images are\ntaken from nearby viewpoints or in rapid succession (e.g., video sequences), while the lat-\nter is more suitable when a large amount of motion or appearance change is expected, e.g.,\nin stitching together panoramas (Brown and Lowe 2007), establishing correspondences in\nwide baseline stereo (Schaffalitzky and Zisserman 2002), or performing object recognition\n(Fergus, Perona, and Zisserman 2007).\nIn this section, we split the keypoint detection and matching pipeline into four separate\nstages. During the feature detection (extraction) stage (Section 4.1.1), each image is searched\nfor locations that are likely to match well in other images. At the feature description stage\n(Section 4.1.2), each region around detected keypoint locations is converted into a more com-\npact and stable (invariant) descriptor that can be matched against other descriptors. The",
  "231": "4.1 Points and patches\n209\nFigure 4.3\nImage pairs with extracted patches below. Notice how some patches can be\nlocalized or matched with higher accuracy than others.\nfeature matching stage (Section 4.1.3) efﬁciently searches for likely matching candidates in\nother images. The feature tracking stage (Section 4.1.4) is an alternative to the third stage\nthat only searches a small neighborhood around each detected feature and is therefore more\nsuitable for video processing.\nA wonderful example of all of these stages can be found in David Lowe’s (2004) paper,\nwhich describes the development and reﬁnement of his Scale Invariant Feature Transform\n(SIFT). Comprehensive descriptions of alternative techniques can be found in a series of\nsurvey and evaluation papers covering both feature detection (Schmid, Mohr, and Bauck-\nhage 2000; Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuytelaars and Mikolajczyk 2007)\nand feature descriptors (Mikolajczyk and Schmid 2005). Shi and Tomasi (1994) and Triggs\n(2004) also provide nice reviews of feature detection techniques.\n4.1.1 Feature detectors\nHow can we ﬁnd image locations where we can reliably ﬁnd correspondences with other\nimages, i.e., what are good features to track (Shi and Tomasi 1994; Triggs 2004)? Look again\nat the image pair shown in Figure 4.3 and at the three sample patches to see how well they\nmight be matched or tracked. As you may notice, textureless patches are nearly impossible\nto localize. Patches with large contrast changes (gradients) are easier to localize, although\nstraight line segments at a single orientation suffer from the aperture problem (Horn and\nSchunck 1981; Lucas and Kanade 1981; Anandan 1989), i.e., it is only possible to align\nthe patches along the direction normal to the edge direction (Figure 4.4b). Patches with",
  "232": "210\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nxi\nxi+u\nu\ni\n(a)\n(b)\n(c)\nFigure 4.4\nAperture problems for different image patches: (a) stable (“corner-like”) ﬂow;\n(b) classic aperture problem (barber-pole illusion); (c) textureless region. The two images I0\n(yellow) and I1 (red) are overlaid. The red vector u indicates the displacement between the\npatch centers and the w(xi) weighting function (patch window) is shown as a dark circle.\ngradients in at least two (signiﬁcantly) different orientations are the easiest to localize, as\nshown schematically in Figure 4.4a.\nThese intuitions can be formalized by looking at the simplest possible matching criterion\nfor comparing two image patches, i.e., their (weighted) summed square difference,\nEWSSD(u) =\nX\ni\nw(xi)[I1(xi + u) −I0(xi)]2,\n(4.1)\nwhere I0 and I1 are the two images being compared, u = (u, v) is the displacement vector,\nw(x) is a spatially varying weighting (or window) function, and the summation i is over all\nthe pixels in the patch. Note that this is the same formulation we later use to estimate motion\nbetween complete images (Section 8.1).\nWhen performing feature detection, we do not know which other image locations the\nfeature will end up being matched against. Therefore, we can only compute how stable this\nmetric is with respect to small variations in position ∆u by comparing an image patch against\nitself, which is known as an auto-correlation function or surface\nEAC(∆u) =\nX\ni\nw(xi)[I0(xi + ∆u) −I0(xi)]2\n(4.2)\n(Figure 4.5).1 Note how the auto-correlation surface for the textured ﬂower bed (Figure 4.5b\nand the red cross in the lower right quadrant of Figure 4.5a) exhibits a strong minimum,\nindicating that it can be well localized. The correlation surface corresponding to the roof\nedge (Figure 4.5c) has a strong ambiguity along one direction, while the correlation surface\ncorresponding to the cloud region (Figure 4.5d) has no stable minimum.\n1 Strictly speaking, a correlation is the product of two patches (3.12); I’m using the term here in a more qualitative\nsense. The weighted sum of squared differences is often called an SSD surface (Section 8.1).",
  "233": "4.1 Points and patches\n211\n(a)\n(b)\n(c)\n(d)\nFigure 4.5 Three auto-correlation surfaces EAC(∆u) shown as both grayscale images and\nsurface plots: (a) The original image is marked with three red crosses to denote where the\nauto-correlation surfaces were computed; (b) this patch is from the ﬂower bed (good unique\nminimum); (c) this patch is from the roof edge (one-dimensional aperture problem); and (d)\nthis patch is from the cloud (no good peak). Each grid point in ﬁgures b–d is one value of\n∆u.",
  "234": "212\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nUsing a Taylor Series expansion of the image function I0(xi+∆u) ≈I0(xi)+∇I0(xi)·\n∆u (Lucas and Kanade 1981; Shi and Tomasi 1994), we can approximate the auto-correlation\nsurface as\nEAC(∆u)\n=\nX\ni\nw(xi)[I0(xi + ∆u) −I0(xi)]2\n(4.3)\n≈\nX\ni\nw(xi)[I0(xi) + ∇I0(xi) · ∆u −I0(xi)]2\n(4.4)\n=\nX\ni\nw(xi)[∇I0(xi) · ∆u]2\n(4.5)\n=\n∆uT A∆u,\n(4.6)\nwhere\n∇I0(xi) = (∂I0\n∂x , ∂I0\n∂y )(xi)\n(4.7)\nis the image gradient at xi. This gradient can be computed using a variety of techniques\n(Schmid, Mohr, and Bauckhage 2000). The classic “Harris” detector (Harris and Stephens\n1988) uses a [-2 -1 0 1 2] ﬁlter, but more modern variants (Schmid, Mohr, and Bauckhage\n2000; Triggs 2004) convolve the image with horizontal and vertical derivatives of a Gaussian\n(typically with σ = 1).\nThe auto-correlation matrix A can be written as\nA = w ∗\n\"\nI2\nx\nIxIy\nIxIy\nI2\ny\n#\n,\n(4.8)\nwhere we have replaced the weighted summations with discrete convolutions with the weight-\ning kernel w. This matrix can be interpreted as a tensor (multiband) image, where the outer\nproducts of the gradients ∇I are convolved with a weighting function w to provide a per-pixel\nestimate of the local (quadratic) shape of the auto-correlation function.\nAs ﬁrst shown by Anandan (1984; 1989) and further discussed in Section 8.1.3 and (8.44),\nthe inverse of the matrix A provides a lower bound on the uncertainty in the location of a\nmatching patch. It is therefore a useful indicator of which patches can be reliably matched.\nThe easiest way to visualize and reason about this uncertainty is to perform an eigenvalue\nanalysis of the auto-correlation matrix A, which produces two eigenvalues (λ0, λ1) and two\neigenvector directions (Figure 4.6). Since the larger uncertainty depends on the smaller eigen-\nvalue, i.e., λ−1/2\n0\n, it makes sense to ﬁnd maxima in the smaller eigenvalue to locate good\nfeatures to track (Shi and Tomasi 1994).\nF¨orstner–Harris.\nWhile Anandan and Lucas and Kanade (1981) were the ﬁrst to analyze\nthe uncertainty structure of the auto-correlation matrix, they did so in the context of asso-\nciating certainties with optic ﬂow measurements. F¨orstner (1986) and Harris and Stephens",
  "235": "4.1 Points and patches\n213\nFigure 4.6\nUncertainty ellipse corresponding to an eigenvalue analysis of the auto-\ncorrelation matrix A.\n(1988) were the ﬁrst to propose using local maxima in rotationally invariant scalar measures\nderived from the auto-correlation matrix to locate keypoints for the purpose of sparse feature\nmatching. (Schmid, Mohr, and Bauckhage (2000); Triggs (2004) give more detailed histori-\ncal reviews of feature detection algorithms.) Both of these techniques also proposed using a\nGaussian weighting window instead of the previously used square patches, which makes the\ndetector response insensitive to in-plane image rotations.\nThe minimum eigenvalue λ0 (Shi and Tomasi 1994) is not the only quantity that can be\nused to ﬁnd keypoints. A simpler quantity, proposed by Harris and Stephens (1988), is\ndet(A) −α trace(A)2 = λ0λ1 −α(λ0 + λ1)2\n(4.9)\nwith α = 0.06. Unlike eigenvalue analysis, this quantity does not require the use of square\nroots and yet is still rotationally invariant and also downweights edge-like features where\nλ1 ≫λ0. Triggs (2004) suggests using the quantity\nλ0 −αλ1\n(4.10)\n(say, with α = 0.05), which also reduces the response at 1D edges, where aliasing errors\nsometimes inﬂate the smaller eigenvalue. He also shows how the basic 2 × 2 Hessian can be\nextended to parametric motions to detect points that are also accurately localizable in scale\nand rotation. Brown, Szeliski, and Winder (2005), on the other hand, use the harmonic mean,\ndet A\ntr A\n=\nλ0λ1\nλ0 + λ1\n,\n(4.11)\nwhich is a smoother function in the region where λ0 ≈λ1. Figure 4.7 shows isocontours\nof the various interest point operators, from which we can see how the two eigenvalues are\nblended to determine the ﬁnal interest value.",
  "236": "214\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.7\nIsocontours of popular keypoint detection functions (Brown, Szeliski, and\nWinder 2004).\nEach detector looks for points where the eigenvalues λ0, λ1 of A =\nw ∗∇I∇IT are both large.\n1. Compute the horizontal and vertical derivatives of the image Ix and Iy by con-\nvolving the original image with derivatives of Gaussians (Section 3.2.3).\n2. Compute the three images corresponding to the outer products of these gradients.\n(The matrix A is symmetric, so only three entries are needed.)\n3. Convolve each of these images with a larger Gaussian.\n4. Compute a scalar interest measure using one of the formulas discussed above.\n5. Find local maxima above a certain threshold and report them as detected feature\npoint locations.\nAlgorithm 4.1 Outline of a basic feature detection algorithm.",
  "237": "4.1 Points and patches\n215\n(a)\n(b)\n(c)\nFigure 4.8 Interest operator responses: (a) Sample image, (b) Harris response, and (c) DoG\nresponse. The circle sizes and colors indicate the scale at which each interest point was\ndetected. Notice how the two detectors tend to respond at complementary locations.\nThe steps in the basic auto-correlation-based keypoint detector are summarized in Algo-\nrithm 4.1. Figure 4.8 shows the resulting interest operator responses for the classic Harris\ndetector as well as the difference of Gaussian (DoG) detector discussed below.\nAdaptive non-maximal suppression (ANMS).\nWhile most feature detectors simply look\nfor local maxima in the interest function, this can lead to an uneven distribution of feature\npoints across the image, e.g., points will be denser in regions of higher contrast. To mitigate\nthis problem, Brown, Szeliski, and Winder (2005) only detect features that are both local\nmaxima and whose response value is signiﬁcantly (10%) greater than that of all of its neigh-\nbors within a radius r (Figure 4.9c–d). They devise an efﬁcient way to associate suppression\nradii with all local maxima by ﬁrst sorting them by their response strength and then creating\na second list sorted by decreasing suppression radius (Brown, Szeliski, and Winder 2005).\nFigure 4.9 shows a qualitative comparison of selecting the top n features and using ANMS.\nMeasuring repeatability.\nGiven the large number of feature detectors that have been de-\nveloped in computer vision, how can we decide which ones to use? Schmid, Mohr, and\nBauckhage (2000) were the ﬁrst to propose measuring the repeatability of feature detectors,\nwhich they deﬁne as the frequency with which keypoints detected in one image are found\nwithin ϵ (say, ϵ = 1.5) pixels of the corresponding location in a transformed image. In their\npaper, they transform their planar images by applying rotations, scale changes, illumination\nchanges, viewpoint changes, and adding noise. They also measure the information content\navailable at each detected feature point, which they deﬁne as the entropy of a set of rotation-\nally invariant local grayscale descriptors. Among the techniques they survey, they ﬁnd that\nthe improved (Gaussian derivative) version of the Harris operator with σd = 1 (scale of the\nderivative Gaussian) and σi = 2 (scale of the integration Gaussian) works best.",
  "238": "216\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a) Strongest 250\n(b) Strongest 500\n(c) ANMS 250, r = 24\n(d) ANMS 500, r = 16\nFigure 4.9\nAdaptive non-maximal suppression (ANMS) (Brown, Szeliski, and Winder\n2005) c⃝2005 IEEE: The upper two images show the strongest 250 and 500 interest points,\nwhile the lower two images show the interest points selected with adaptive non-maximal sup-\npression, along with the corresponding suppression radius r. Note how the latter features\nhave a much more uniform spatial distribution across the image.\nScale invariance\nIn many situations, detecting features at the ﬁnest stable scale possible may not be appro-\npriate. For example, when matching images with little high frequency detail (e.g., clouds),\nﬁne-scale features may not exist.\nOne solution to the problem is to extract features at a variety of scales, e.g., by performing\nthe same operations at multiple resolutions in a pyramid and then matching features at the\nsame level. This kind of approach is suitable when the images being matched do not undergo\nlarge scale changes, e.g., when matching successive aerial images taken from an airplane or\nstitching panoramas taken with a ﬁxed-focal-length camera. Figure 4.10 shows the output of\none such approach, the multi-scale, oriented patch detector of Brown, Szeliski, and Winder\n(2005), for which responses at ﬁve different scales are shown.\nHowever, for most object recognition applications, the scale of the object in the image",
  "239": "4.1 Points and patches\n217\nFigure 4.10\nMulti-scale oriented patches (MOPS) extracted at ﬁve pyramid levels (Brown,\nSzeliski, and Winder 2005) c⃝2005 IEEE. The boxes show the feature orientation and the\nregion from which the descriptor vectors are sampled.\nis unknown. Instead of extracting features at many different scales and then matching all of\nthem, it is more efﬁcient to extract features that are stable in both location and scale (Lowe\n2004; Mikolajczyk and Schmid 2004).\nEarly investigations into scale selection were performed by Lindeberg (1993; 1998b),\nwho ﬁrst proposed using extrema in the Laplacian of Gaussian (LoG) function as interest\npoint locations. Based on this work, Lowe (2004) proposed computing a set of sub-octave\nDifference of Gaussian ﬁlters (Figure 4.11a), looking for 3D (space+scale) maxima in the re-\nsulting structure (Figure 4.11b), and then computing a sub-pixel space+scale location using a\nquadratic ﬁt (Brown and Lowe 2002). The number of sub-octave levels was determined, after\ncareful empirical investigation, to be three, which corresponds to a quarter-octave pyramid,\nwhich is the same as used by Triggs (2004).\nAs with the Harris operator, pixels where there is strong asymmetry in the local curvature\nof the indicator function (in this case, the DoG) are rejected. This is implemented by ﬁrst\ncomputing the local Hessian of the difference image D,\nH =\n\"\nDxx\nDxy\nDxy\nDyy\n#\n,\n(4.12)\nand then rejecting keypoints for which\nTr(H)2\nDet(H) > 10.\n(4.13)",
  "240": "218\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n Scale\n (first\n octave)\nScale\n(next\noctave)\nGaussian\nDifference of\nGaussian (DOG)\n. . .\nScale\n(a)\n(b)\nFigure 4.11 Scale-space feature detection using a sub-octave Difference of Gaussian pyra-\nmid (Lowe 2004) c⃝2004 Springer: (a) Adjacent levels of a sub-octave Gaussian pyramid\nare subtracted to produce Difference of Gaussian images; (b) extrema (maxima and minima)\nin the resulting 3D volume are detected by comparing a pixel to its 26 neighbors.\nWhile Lowe’s Scale Invariant Feature Transform (SIFT) performs well in practice, it is not\nbased on the same theoretical foundation of maximum spatial stability as the auto-correlation-\nbased detectors. (In fact, its detection locations are often complementary to those produced\nby such techniques and can therefore be used in conjunction with these other approaches.)\nIn order to add a scale selection mechanism to the Harris corner detector, Mikolajczyk and\nSchmid (2004) evaluate the Laplacian of Gaussian function at each detected Harris point (in\na multi-scale pyramid) and keep only those points for which the Laplacian is extremal (larger\nor smaller than both its coarser and ﬁner-level values). An optional iterative reﬁnement for\nboth scale and position is also proposed and evaluated. Additional examples of scale invariant\nregion detectors are discussed by Mikolajczyk, Tuytelaars, Schmid et al. (2005); Tuytelaars\nand Mikolajczyk (2007).\nRotational invariance and orientation estimation\nIn addition to dealing with scale changes, most image matching and object recognition algo-\nrithms need to deal with (at least) in-plane image rotation. One way to deal with this problem\nis to design descriptors that are rotationally invariant (Schmid and Mohr 1997), but such\ndescriptors have poor discriminability, i.e. they map different looking patches to the same\ndescriptor.",
  "241": "4.1 Points and patches\n219\nFigure 4.12\nA dominant orientation estimate can be computed by creating a histogram of\nall the gradient orientations (weighted by their magnitudes or after thresholding out small\ngradients) and then ﬁnding the signiﬁcant peaks in this distribution (Lowe 2004) c⃝2004\nSpringer.\nA better method is to estimate a dominant orientation at each detected keypoint. Once\nthe local orientation and scale of a keypoint have been estimated, a scaled and oriented patch\naround the detected point can be extracted and used to form a feature descriptor (Figures 4.10\nand 4.17).\nThe simplest possible orientation estimate is the average gradient within a region around\nthe keypoint. If a Gaussian weighting function is used (Brown, Szeliski, and Winder 2005),\nthis average gradient is equivalent to a ﬁrst-order steerable ﬁlter (Section 3.2.3), i.e., it can be\ncomputed using an image convolution with the horizontal and vertical derivatives of Gaus-\nsian ﬁlter (Freeman and Adelson 1991). In order to make this estimate more reliable, it is\nusually preferable to use a larger aggregation window (Gaussian kernel size) than detection\nwindow (Brown, Szeliski, and Winder 2005). The orientations of the square boxes shown in\nFigure 4.10 were computed using this technique.\nSometimes, however, the averaged (signed) gradient in a region can be small and therefore\nan unreliable indicator of orientation. A more reliable technique is to look at the histogram\nof orientations computed around the keypoint. Lowe (2004) computes a 36-bin histogram\nof edge orientations weighted by both gradient magnitude and Gaussian distance to the cen-\nter, ﬁnds all peaks within 80% of the global maximum, and then computes a more accurate\norientation estimate using a three-bin parabolic ﬁt (Figure 4.12).\nAfﬁne invariance\nWhile scale and rotation invariance are highly desirable, for many applications such as wide\nbaseline stereo matching (Pritchett and Zisserman 1998; Schaffalitzky and Zisserman 2002)\nor location recognition (Chum, Philbin, Sivic et al. 2007), full afﬁne invariance is preferred.",
  "242": "220\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.13\nAfﬁne region detectors used to match two images taken from dramatically\ndifferent viewpoints (Mikolajczyk and Schmid 2004) c⃝2004 Springer.\nx0 →\nA−1/2\n0\nx′\n0\nx′\n0 →\nRx′\n1\nA−1/2\n1\nx′\n1\n←x1\nFigure 4.14\nAfﬁne normalization using the second moment matrices, as described by Miko-\nlajczyk, Tuytelaars, Schmid et al. (2005) c⃝2005 Springer. After image coordinates are trans-\nformed using the matrices A−1/2\n0\nand A−1/2\n1\n, they are related by a pure rotation R, which\ncan be estimated using a dominant orientation technique.\nAfﬁne-invariant detectors not only respond at consistent locations after scale and orientation\nchanges, they also respond consistently across afﬁne deformations such as (local) perspective\nforeshortening (Figure 4.13). In fact, for a small enough patch, any continuous image warping\ncan be well approximated by an afﬁne deformation.\nTo introduce afﬁne invariance, several authors have proposed ﬁtting an ellipse to the auto-\ncorrelation or Hessian matrix (using eigenvalue analysis) and then using the principal axes\nand ratios of this ﬁt as the afﬁne coordinate frame (Lindeberg and Garding 1997; Baumberg\n2000; Mikolajczyk and Schmid 2004; Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuyte-\nlaars and Mikolajczyk 2007). Figure 4.14 shows how the square root of the moment matrix\ncan be used to transform local patches into a frame which is similar up to rotation.\nAnother important afﬁne invariant region detector is the maximally stable extremal region\n(MSER) detector developed by Matas, Chum, Urban et al. (2004). To detect MSERs, binary\nregions are computed by thresholding the image at all possible gray levels (the technique\ntherefore only works for grayscale images). This operation can be performed efﬁciently by\nﬁrst sorting all pixels by gray value and then incrementally adding pixels to each connected\ncomponent as the threshold is changed (Nist´er and Stew´enius 2008). As the threshold is\nchanged, the area of each component (region) is monitored; regions whose rate of change of\narea with respect to the threshold is minimal are deﬁned as maximally stable. Such regions",
  "243": "4.1 Points and patches\n221\nFigure 4.15\nMaximally stable extremal regions (MSERs) extracted and matched from a\nnumber of images (Matas, Chum, Urban et al. 2004) c⃝2004 Elsevier.\nFigure 4.16\nFeature matching: how can we extract local descriptors that are invariant\nto inter-image variations and yet still discriminative enough to establish correct correspon-\ndences?\nare therefore invariant to both afﬁne geometric and photometric (linear bias-gain or smooth\nmonotonic) transformations (Figure 4.15). If desired, an afﬁne coordinate frame can be ﬁt to\neach detected region using its moment matrix.\nThe area of feature point detectors continues to be very active, with papers appearing ev-\nery year at major computer vision conferences (Xiao and Shah 2003; Koethe 2003; Carneiro\nand Jepson 2005; Kenney, Zuliani, and Manjunath 2005; Bay, Tuytelaars, and Van Gool 2006;\nPlatel, Balmachnova, Florack et al. 2006; Rosten and Drummond 2006). Mikolajczyk, Tuyte-\nlaars, Schmid et al. (2005) survey a number of popular afﬁne region detectors and provide\nexperimental comparisons of their invariance to common image transformations such as scal-\ning, rotations, noise, and blur. These experimental results, code, and pointers to the surveyed\npapers can be found on their Web site at http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/.\nOf course, keypoints are not the only features that can be used for registering images.\nZoghlami, Faugeras, and Deriche (1997) use line segments as well as point-like features to\nestimate homographies between pairs of images, whereas Bartoli, Coquerelle, and Sturm\n(2004) use line segments with local correspondences along the edges to extract 3D structure\nand motion. Tuytelaars and Van Gool (2004) use afﬁne invariant regions to detect corre-\nspondences for wide baseline stereo matching, whereas Kadir, Zisserman, and Brady (2004)\ndetect salient regions where patch entropy and its rate of change with scale are locally max-\nimal. Corso and Hager (2005) use a related technique to ﬁt 2D oriented Gaussian kernels\nto homogeneous regions. More details on techniques for ﬁnding and matching curves, lines,\nand regions can be found later in this chapter.",
  "244": "222\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.17\nMOPS descriptors are formed using an 8 × 8 sampling of bias and gain nor-\nmalized intensity values, with a sample spacing of ﬁve pixels relative to the detection scale\n(Brown, Szeliski, and Winder 2005) c⃝2005 IEEE. This low frequency sampling gives the\nfeatures some robustness to interest point location error and is achieved by sampling at a\nhigher pyramid level than the detection scale.\n4.1.2 Feature descriptors\nAfter detecting features (keypoints), we must match them, i.e., we must determine which\nfeatures come from corresponding locations in different images. In some situations, e.g., for\nvideo sequences (Shi and Tomasi 1994) or for stereo pairs that have been rectiﬁed (Zhang,\nDeriche, Faugeras et al. 1995; Loop and Zhang 1999; Scharstein and Szeliski 2002), the lo-\ncal motion around each feature point may be mostly translational. In this case, simple error\nmetrics, such as the sum of squared differences or normalized cross-correlation, described\nin Section 8.1 can be used to directly compare the intensities in small patches around each\nfeature point. (The comparative study by Mikolajczyk and Schmid (2005), discussed below,\nuses cross-correlation.) Because feature points may not be exactly located, a more accurate\nmatching score can be computed by performing incremental motion reﬁnement as described\nin Section 8.1.3 but this can be time consuming and can sometimes even decrease perfor-\nmance (Brown, Szeliski, and Winder 2005).\nIn most cases, however, the local appearance of features will change in orientation and\nscale, and sometimes even undergo afﬁne deformations. Extracting a local scale, orientation,\nor afﬁne frame estimate and then using this to resample the patch before forming the feature\ndescriptor is thus usually preferable (Figure 4.17).\nEven after compensating for these changes, the local appearance of image patches will\nusually still vary from image to image. How can we make image descriptors more invariant to\nsuch changes, while still preserving discriminability between different (non-corresponding)\npatches (Figure 4.16)? Mikolajczyk and Schmid (2005) review some recently developed\nview-invariant local image descriptors and experimentally compare their performance. Be-\nlow, we describe a few of these descriptors in more detail.",
  "245": "4.1 Points and patches\n223\nBias and gain normalization (MOPS).\nFor tasks that do not exhibit large amounts of fore-\nshortening, such as image stitching, simple normalized intensity patches perform reasonably\nwell and are simple to implement (Brown, Szeliski, and Winder 2005) (Figure 4.17). In or-\nder to compensate for slight inaccuracies in the feature point detector (location, orientation,\nand scale), these multi-scale oriented patches (MOPS) are sampled at a spacing of ﬁve pixels\nrelative to the detection scale, using a coarser level of the image pyramid to avoid aliasing.\nTo compensate for afﬁne photometric variations (linear exposure changes or bias and gain,\n(3.3)), patch intensities are re-scaled so that their mean is zero and their variance is one.\nScale invariant feature transform (SIFT).\nSIFT features are formed by computing the\ngradient at each pixel in a 16×16 window around the detected keypoint, using the appropriate\nlevel of the Gaussian pyramid at which the keypoint was detected. The gradient magnitudes\nare downweighted by a Gaussian fall-off function (shown as a blue circle in (Figure 4.18a) in\norder to reduce the inﬂuence of gradients far from the center, as these are more affected by\nsmall misregistrations.\nIn each 4 × 4 quadrant, a gradient orientation histogram is formed by (conceptually)\nadding the weighted gradient value to one of eight orientation histogram bins. To reduce the\neffects of location and dominant orientation misestimation, each of the original 256 weighted\ngradient magnitudes is softly added to 2 × 2 × 2 histogram bins using trilinear interpolation.\nSoftly distributing values to adjacent histogram bins is generally a good idea in any appli-\ncation where histograms are being computed, e.g., for Hough transforms (Section 4.3.2) or\nlocal histogram equalization (Section 3.1.4).\nThe resulting 128 non-negative values form a raw version of the SIFT descriptor vector.\nTo reduce the effects of contrast or gain (additive variations are already removed by the gra-\ndient), the 128-D vector is normalized to unit length. To further make the descriptor robust to\nother photometric variations, values are clipped to 0.2 and the resulting vector is once again\nrenormalized to unit length.\nPCA-SIFT.\nKe and Sukthankar (2004) propose a simpler way to compute descriptors in-\nspired by SIFT; it computes the x and y (gradient) derivatives over a 39 × 39 patch and\nthen reduces the resulting 3042-dimensional vector to 36 using principal component analysis\n(PCA) (Section 14.2.1 and Appendix A.1.2). Another popular variant of SIFT is SURF (Bay,\nTuytelaars, and Van Gool 2006), which uses box ﬁlters to approximate the derivatives and\nintegrals used in SIFT.\nGradient location-orientation histogram (GLOH).\nThis descriptor, developed by Miko-\nlajczyk and Schmid (2005), is a variant on SIFT that uses a log-polar binning structure instead\nof the four quadrants used by Lowe (2004) (Figure 4.19). The spatial bins are of radius 6,",
  "246": "224\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.18 A schematic representation of Lowe’s (2004) scale invariant feature transform\n(SIFT): (a) Gradient orientations and magnitudes are computed at each pixel and weighted\nby a Gaussian fall-off function (blue circle). (b) A weighted gradient orientation histogram\nis then computed in each subregion, using trilinear interpolation. While this ﬁgure shows an\n8 × 8 pixel patch and a 2 × 2 descriptor array, Lowe’s actual implementation uses 16 × 16\npatches and a 4 × 4 array of eight-bin histograms.\n11, and 15, with eight angular bins (except for the central region), for a total of 17 spa-\ntial bins and 16 orientation bins. The 272-dimensional histogram is then projected onto\na 128-dimensional descriptor using PCA trained on a large database. In their evaluation,\nMikolajczyk and Schmid (2005) found that GLOH, which has the best performance overall,\noutperforms SIFT by a small margin.\nSteerable ﬁlters.\nSteerable ﬁlters (Section 3.2.3) are combinations of derivative of Gaus-\nsian ﬁlters that permit the rapid computation of even and odd (symmetric and anti-symmetric)\nedge-like and corner-like features at all possible orientations (Freeman and Adelson 1991).\nBecause they use reasonably broad Gaussians, they too are somewhat insensitive to localiza-\ntion and orientation errors.\nPerformance of local descriptors.\nAmong the local descriptors that Mikolajczyk and Schmid\n(2005) compared, they found that GLOH performed best, followed closely by SIFT (see Fig-\nure 4.25). They also present results for many other descriptors not covered in this book.\nThe ﬁeld of feature descriptors continues to evolve rapidly, with some of the newer tech-\nniques looking at local color information (van de Weijer and Schmid 2006; Abdel-Hakim\nand Farag 2006). Winder and Brown (2007) develop a multi-stage framework for feature\ndescriptor computation that subsumes both SIFT and GLOH (Figure 4.20a) and also allows\nthem to learn optimal parameters for newer descriptors that outperform previous hand-tuned",
  "247": "4.1 Points and patches\n225\n(a) image gradients\n(b) keypoint descriptor\nFigure 4.19 The gradient location-orientation histogram (GLOH) descriptor uses log-polar\nbins instead of square bins to compute orientation histograms (Mikolajczyk and Schmid\n2005).\ndescriptors. Hua, Brown, and Winder (2007) extend this work by learning lower-dimensional\nprojections of higher-dimensional descriptors that have the best discriminative power. Both\nof these papers use a database of real-world image patches (Figure 4.20b) obtained by sam-\npling images at locations that were reliably matched using a robust structure-from-motion\nalgorithm applied to Internet photo collections (Snavely, Seitz, and Szeliski 2006; Goesele,\nSnavely, Curless et al. 2007). In concurrent work, Tola, Lepetit, and Fua (2010) developed a\nsimilar DAISY descriptor for dense stereo matching and optimized its parameters based on\nground truth stereo data.\nWhile these techniques construct feature detectors that optimize for repeatability across\nall object classes, it is also possible to develop class- or instance-speciﬁc feature detectors that\nmaximize discriminability from other classes (Ferencz, Learned-Miller, and Malik 2008).\n4.1.3 Feature matching\nOnce we have extracted features and their descriptors from two or more images, the next step\nis to establish some preliminary feature matches between these images. In this section, we\ndivide this problem into two separate components. The ﬁrst is to select a matching strategy,\nwhich determines which correspondences are passed on to the next stage for further process-\ning. The second is to devise efﬁcient data structures and algorithms to perform this matching\nas quickly as possible. (See the discussion of related techniques in Section 14.3.2.)",
  "248": "226\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 4.20 Spatial summation blocks for SIFT, GLOH, and some newly developed feature\ndescriptors (Winder and Brown 2007) c⃝2007 IEEE: (a) The parameters for the new features,\ne.g., their Gaussian weights, are learned from a training database of (b) matched real-world\nimage patches obtained from robust structure from motion applied to Internet photo collec-\ntions (Hua, Brown, and Winder 2007).\nMatching strategy and error rates\nDetermining which feature matches are reasonable to process further depends on the context\nin which the matching is being performed. Say we are given two images that overlap to a fair\namount (e.g., for image stitching, as in Figure 4.16, or for tracking objects in a video). We\nknow that most features in one image are likely to match the other image, although some may\nnot match because they are occluded or their appearance has changed too much.\nOn the other hand, if we are trying to recognize how many known objects appear in a clut-\ntered scene (Figure 4.21), most of the features may not match. Furthermore, a large number\nof potentially matching objects must be searched, which requires more efﬁcient strategies, as\ndescribed below.\nTo begin with, we assume that the feature descriptors have been designed so that Eu-\nclidean (vector magnitude) distances in feature space can be directly used for ranking poten-\ntial matches. If it turns out that certain parameters (axes) in a descriptor are more reliable\nthan others, it is usually preferable to re-scale these axes ahead of time, e.g., by determin-\ning how much they vary when compared against other known good matches (Hua, Brown,\nand Winder 2007). A more general process, which involves transforming feature vectors\ninto a new scaled basis, is called whitening and is discussed in more detail in the context of\neigenface-based face recognition (Section 14.2.1).\nGiven a Euclidean distance metric, the simplest matching strategy is to set a threshold\n(maximum distance) and to return all matches from other images within this threshold. Set-\nting the threshold too high results in too many false positives, i.e., incorrect matches being\nreturned. Setting the threshold too low results in too many false negatives, i.e., too many\ncorrect matches being missed (Figure 4.22).\nWe can quantify the performance of a matching algorithm at a particular threshold by",
  "249": "4.1 Points and patches\n227\nFigure 4.21 Recognizing objects in a cluttered scene (Lowe 2004) c⃝2004 Springer. Two of\nthe training images in the database are shown on the left. These are matched to the cluttered\nscene in the middle using SIFT features, shown as small squares in the right image. The afﬁne\nwarp of each recognized database image onto the scene is shown as a larger parallelogram in\nthe right image.\n1\n1\n2\n1\n3\n4\nFigure 4.22\nFalse positives and negatives: The black digits 1 and 2 are features being\nmatched against a database of features in other images. At the current threshold setting (the\nsolid circles), the green 1 is a true positive (good match), the blue 1 is a false negative (failure\nto match), and the red 3 is a false positive (incorrect match). If we set the threshold higher\n(the dashed circles), the blue 1 becomes a true positive but the brown 4 becomes an additional\nfalse positive.",
  "250": "228\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPredicted matches\nTP = 18\nFP = 4\nP' = 22\nPPV = 0.82\nPredicted non-matches\nFN = 2\nTN = 76\nN' = 78\nP = 20\nN = 80\nTotal = 100\nTPR = 0.90\nFPR = 0.05\nACC = 0.94\nTrue matches\nTrue non-matches\nTable 4.1 The number of matches correctly and incorrectly estimated by a feature matching\nalgorithm, showing the number of true positives (TP), false positives (FP), false negatives\n(FN) and true negatives (TN). The columns sum up to the actual number of positives (P) and\nnegatives (N), while the rows sum up to the predicted number of positives (P’) and negatives\n(N’). The formulas for the true positive rate (TPR), the false positive rate (FPR), the positive\npredictive value (PPV), and the accuracy (ACC) are given in the text.\nﬁrst counting the number of true and false matches and match failures, using the following\ndeﬁnitions (Fawcett 2006):\n• TP: true positives, i.e., number of correct matches;\n• FN: false negatives, matches that were not correctly detected;\n• FP: false positives, proposed matches that are incorrect;\n• TN: true negatives, non-matches that were correctly rejected.\nTable 4.1 shows a sample confusion matrix (contingency table) containing such numbers.\nWe can convert these numbers into unit rates by deﬁning the following quantities (Fawcett\n2006):\n• true positive rate (TPR),\nTPR =\nTP\nTP+FN = TP\nP ;\n(4.14)\n• false positive rate (FPR),\nFPR =\nFP\nFP+TN = FP\nN ;\n(4.15)\n• positive predictive value (PPV),\nPPV =\nTP\nTP+FP = TP\nP’ ;\n(4.16)\n• accuracy (ACC),\nACC = TP+TN\nP+N .\n(4.17)",
  "251": "4.1 Points and patches\n229\nfalse positive rate\ntrue positive rate\n0.1\n0.8\n0\n1\n1\nequal error\n \nrate\nrandom chance\nTP\nFP FN\nTN\nθ\nd\n#\n(a)\n(b)\nFigure 4.23 ROC curve and its related rates: (a) The ROC curve plots the true positive rate\nagainst the false positive rate for a particular combination of feature extraction and match-\ning algorithms. Ideally, the true positive rate should be close to 1, while the false positive\nrate is close to 0. The area under the ROC curve (AUC) is often used as a single (scalar)\nmeasure of algorithm performance. Alternatively, the equal error rate is sometimes used. (b)\nThe distribution of positives (matches) and negatives (non-matches) as a function of inter-\nfeature distance d. As the threshold θ is increased, the number of true positives (TP) and false\npositives (FP) increases.\nIn the information retrieval (or document retrieval) literature (Baeza-Yates and Ribeiro-\nNeto 1999; Manning, Raghavan, and Sch¨utze 2008), the term precision (how many returned\ndocuments are relevant) is used instead of PPV and recall (what fraction of relevant docu-\nments was found) is used instead of TPR.\nAny particular matching strategy (at a particular threshold or parameter setting) can be\nrated by the TPR and FPR numbers; ideally, the true positive rate will be close to 1 and the\nfalse positive rate close to 0. As we vary the matching threshold, we obtain a family of such\npoints, which are collectively known as the receiver operating characteristic (ROC curve)\n(Fawcett 2006) (Figure 4.23a). The closer this curve lies to the upper left corner, i.e., the\nlarger the area under the curve (AUC), the better its performance. Figure 4.23b shows how\nwe can plot the number of matches and non-matches as a function of inter-feature distance d.\nThese curves can then be used to plot an ROC curve (Exercise 4.3). The ROC curve can also\nbe used to calculate the mean average precision, which is the average precision (PPV) as you\nvary the threshold to select the best results, then the two top results, etc.\nThe problem with using a ﬁxed threshold is that it is difﬁcult to set; the useful range",
  "252": "230\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nDB\nDA\nd1\nDD\nDC\nd2\nDE\nd1\n \n’\nd2\n \n’\nFigure 4.24 Fixed threshold, nearest neighbor, and nearest neighbor distance ratio matching.\nAt a ﬁxed distance threshold (dashed circles), descriptor DA fails to match DB and DD\nincorrectly matches DC and DE. If we pick the nearest neighbor, DA correctly matches DB\nbut DD incorrectly matches DC. Using nearest neighbor distance ratio (NNDR) matching,\nthe small NNDR d1/d2 correctly matches DA with DB, and the large NNDR d′\n1/d′\n2 correctly\nrejects matches for DD.\nof thresholds can vary a lot as we move to different parts of the feature space (Lowe 2004;\nMikolajczyk and Schmid 2005). A better strategy in such cases is to simply match the nearest\nneighbor in feature space. Since some features may have no matches (e.g., they may be part\nof background clutter in object recognition or they may be occluded in the other image), a\nthreshold is still used to reduce the number of false positives.\nIdeally, this threshold itself will adapt to different regions of the feature space. If sufﬁcient\ntraining data is available (Hua, Brown, and Winder 2007), it is sometimes possible to learn\ndifferent thresholds for different features. Often, however, we are simply given a collection\nof images to match, e.g., when stitching images or constructing 3D models from unordered\nphoto collections (Brown and Lowe 2007, 2003; Snavely, Seitz, and Szeliski 2006). In this\ncase, a useful heuristic can be to compare the nearest neighbor distance to that of the second\nnearest neighbor, preferably taken from an image that is known not to match the target (e.g.,\na different object in the database) (Brown and Lowe 2002; Lowe 2004). We can deﬁne this\nnearest neighbor distance ratio (Mikolajczyk and Schmid 2005) as\nNNDR = d1\nd2\n= ∥DA −DB|\n∥DA −DC|,\n(4.18)\nwhere d1 and d2 are the nearest and second nearest neighbor distances, DA is the target\ndescriptor, and DB and DC are its closest two neighbors (Figure 4.24).\nThe effects of using these three different matching strategies for the feature descriptors\nevaluated by Mikolajczyk and Schmid (2005) are shown in Figure 4.25. As you can see, the\nnearest neighbor and NNDR strategies produce improved ROC curves.",
  "253": "4.1 Points and patches\n231\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 3708\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1−precision\n#correct / 926\ngradient moments\ncross correlation\nsteerable filters\ncomplex filters\ndifferential invariants\ngloh\nsift\npca −sift\nshape context\nspin\nhes−lap gloh\n(b)\n(c)\nFigure 4.25\nPerformance of the feature descriptors evaluated by Mikolajczyk and Schmid\n(2005) c⃝2005 IEEE, shown for three matching strategies: (a) ﬁxed threshold; (b) nearest\nneighbor; (c) nearest neighbor distance ratio (NNDR). Note how the ordering of the algo-\nrithms does not change that much, but the overall performance varies signiﬁcantly between\nthe different matching strategies.",
  "254": "232\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.26 The three Haar wavelet coefﬁcients used for hashing the MOPS descriptor de-\nvised by Brown, Szeliski, and Winder (2005) are computed by summing each 8×8 normalized\npatch over the light and dark gray regions and taking their difference.\nEfﬁcient matching\nOnce we have decided on a matching strategy, we still need to search efﬁciently for poten-\ntial candidates. The simplest way to ﬁnd all corresponding feature points is to compare all\nfeatures against all other features in each pair of potentially matching images. Unfortunately,\nthis is quadratic in the number of extracted features, which makes it impractical for most\napplications.\nA better approach is to devise an indexing structure, such as a multi-dimensional search\ntree or a hash table, to rapidly search for features near a given feature. Such indexing struc-\ntures can either be built for each image independently (which is useful if we want to only\nconsider certain potential matches, e.g., searching for a particular object) or globally for all\nthe images in a given database, which can potentially be faster, since it removes the need to it-\nerate over each image. For extremely large databases (millions of images or more), even more\nefﬁcient structures based on ideas from document retrieval (e.g., vocabulary trees, (Nist´er and\nStew´enius 2006)) can be used (Section 14.3.2).\nOne of the simpler techniques to implement is multi-dimensional hashing, which maps\ndescriptors into ﬁxed size buckets based on some function applied to each descriptor vector.\nAt matching time, each new feature is hashed into a bucket, and a search of nearby buckets\nis used to return potential candidates, which can then be sorted or graded to determine which\nare valid matches.\nA simple example of hashing is the Haar wavelets used by Brown, Szeliski, and Winder\n(2005) in their MOPS paper. During the matching structure construction, each 8 × 8 scaled,\noriented, and normalized MOPS patch is converted into a three-element index by perform-\ning sums over different quadrants of the patch (Figure 4.26). The resulting three values are\nnormalized by their expected standard deviations and then mapped to the two (of b = 10)\nnearest 1D bins. The three-dimensional indices formed by concatenating the three quantized\nvalues are used to index the 23 = 8 bins where the feature is stored (added). At query time,\nonly the primary (closest) indices are used, so only a single three-dimensional bin needs to",
  "255": "4.1 Points and patches\n233\n(a)\n(b)\nFigure 4.27 K-d tree and best bin ﬁrst (BBF) search (Beis and Lowe 1999) c⃝1999 IEEE:\n(a) The spatial arrangement of the axis-aligned cutting planes is shown using dashed lines.\nIndividual data points are shown as small diamonds. (b) The same subdivision can be repre-\nsented as a tree, where each interior node represents an axis-aligned cutting plane (e.g., the\ntop node cuts along dimension d1 at value .34) and each leaf node is a data point. During a\nBBF search, a query point (denoted by “+”) ﬁrst looks in its containing bin (D) and then in\nits nearest adjacent bin (B), rather than its closest neighbor in the tree (C).\nbe examined. The coefﬁcients in the bin can then be used to select k approximate nearest\nneighbors for further processing (such as computing the NNDR).\nA more complex, but more widely applicable, version of hashing is called locality sen-\nsitive hashing, which uses unions of independently computed hashing functions to index\nthe features (Gionis, Indyk, and Motwani 1999; Shakhnarovich, Darrell, and Indyk 2006).\nShakhnarovich, Viola, and Darrell (2003) extend this technique to be more sensitive to the\ndistribution of points in parameter space, which they call parameter-sensitive hashing. Even\nmore recent work converts high-dimensional descriptor vectors into binary codes that can be\ncompared using Hamming distances (Torralba, Weiss, and Fergus 2008; Weiss, Torralba, and\nFergus 2008) or that can accommodate arbitrary kernel functions (Kulis and Grauman 2009;\nRaginsky and Lazebnik 2009).\nAnother widely used class of indexing structures are multi-dimensional search trees. The\nbest known of these are k-d trees, also often written as kd-trees, which divide the multi-\ndimensional feature space along alternating axis-aligned hyperplanes, choosing the threshold\nalong each axis so as to maximize some criterion, such as the search tree balance (Samet\n1989). Figure 4.27 shows an example of a two-dimensional k-d tree. Here, eight different data\npoints A–H are shown as small diamonds arranged on a two-dimensional plane. The k-d tree",
  "256": "234\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nrecursively splits this plane along axis-aligned (horizontal or vertical) cutting planes. Each\nsplit can be denoted using the dimension number and split value (Figure 4.27b). The splits are\narranged so as to try to balance the tree, i.e., to keep its maximum depth as small as possible.\nAt query time, a classic k-d tree search ﬁrst locates the query point (+) in its appropriate\nbin (D), and then searches nearby leaves in the tree (C, B, . . .) until it can guarantee that\nthe nearest neighbor has been found. The best bin ﬁrst (BBF) search (Beis and Lowe 1999)\nsearches bins in order of their spatial proximity to the query point and is therefore usually\nmore efﬁcient.\nMany additional data structures have been developed over the years for solving nearest\nneighbor problems (Arya, Mount, Netanyahu et al. 1998; Liang, Liu, Xu et al. 2001; Hjalta-\nson and Samet 2003). For example, Nene and Nayar (1997) developed a technique they call\nslicing that uses a series of 1D binary searches on the point list sorted along different dimen-\nsions to efﬁciently cull down a list of candidate points that lie within a hypercube of the query\npoint. Grauman and Darrell (2005) reweight the matches at different levels of an indexing\ntree, which allows their technique to be less sensitive to discretization errors in the tree con-\nstruction. Nist´er and Stew´enius (2006) use a metric tree, which compares feature descriptors\nto a small number of prototypes at each level in a hierarchy. The resulting quantized visual\nwords can then be used with classical information retrieval (document relevance) techniques\nto quickly winnow down a set of potential candidates from a database of millions of images\n(Section 14.3.2). Muja and Lowe (2009) compare a number of these approaches, introduce a\nnew one of their own (priority search on hierarchical k-means trees), and conclude that mul-\ntiple randomized k-d trees often provide the best performance. Despite all of this promising\nwork, the rapid computation of image feature correspondences remains a challenging open\nresearch problem.\nFeature match veriﬁcation and densiﬁcation\nOnce we have some hypothetical (putative) matches, we can often use geometric alignment\n(Section 6.1) to verify which matches are inliers and which ones are outliers. For example,\nif we expect the whole image to be translated or rotated in the matching view, we can ﬁt a\nglobal geometric transform and keep only those feature matches that are sufﬁciently close to\nthis estimated transformation. The process of selecting a small set of seed matches and then\nverifying a larger set is often called random sampling or RANSAC (Section 6.1.4). Once an\ninitial set of correspondences has been established, some systems look for additional matches,\ne.g., by looking for additional correspondences along epipolar lines (Section 11.1) or in the\nvicinity of estimated locations based on the global transform. These topics are discussed\nfurther in Sections 6.1, 11.2, and 14.3.1.",
  "257": "4.1 Points and patches\n235\n4.1.4 Feature tracking\nAn alternative to independently ﬁnding features in all candidate images and then matching\nthem is to ﬁnd a set of likely feature locations in a ﬁrst image and to then search for their\ncorresponding locations in subsequent images. This kind of detect then track approach is\nmore widely used for video tracking applications, where the expected amount of motion and\nappearance deformation between adjacent frames is expected to be small.\nThe process of selecting good features to track is closely related to selecting good features\nfor more general recognition applications. In practice, regions containing high gradients in\nboth directions, i.e., which have high eigenvalues in the auto-correlation matrix (4.8), provide\nstable locations at which to ﬁnd correspondences (Shi and Tomasi 1994).\nIn subsequent frames, searching for locations where the corresponding patch has low\nsquared difference (4.1) often works well enough. However, if the images are undergo-\ning brightness change, explicitly compensating for such variations (8.9) or using normalized\ncross-correlation (8.11) may be preferable. If the search range is large, it is also often more\nefﬁcient to use a hierarchical search strategy, which uses matches in lower-resolution images\nto provide better initial guesses and hence speed up the search (Section 8.1.1). Alternatives\nto this strategy involve learning what the appearance of the patch being tracked should be and\nthen searching for it in the vicinity of its predicted position (Avidan 2001; Jurie and Dhome\n2002; Williams, Blake, and Cipolla 2003). These topics are all covered in more detail in\nSection 8.1.3.\nIf features are being tracked over longer image sequences, their appearance can undergo\nlarger changes. You then have to decide whether to continue matching against the originally\ndetected patch (feature) or to re-sample each subsequent frame at the matching location. The\nformer strategy is prone to failure as the original patch can undergo appearance changes such\nas foreshortening. The latter runs the risk of the feature drifting from its original location\nto some other location in the image (Shi and Tomasi 1994). (Mathematically, small mis-\nregistration errors compound to create a Markov Random Walk, which leads to larger drift\nover time.)\nA preferable solution is to compare the original patch to later image locations using an\nafﬁne motion model (Section 8.2). Shi and Tomasi (1994) ﬁrst compare patches in neigh-\nboring frames using a translational model and then use the location estimates produced by\nthis step to initialize an afﬁne registration between the patch in the current frame and the\nbase frame where a feature was ﬁrst detected (Figure 4.28). In their system, features are only\ndetected infrequently, i.e., only in regions where tracking has failed. In the usual case, an\narea around the current predicted location of the feature is searched with an incremental reg-\nistration algorithm (Section 8.1.3). The resulting tracker is often called the Kanade–Lucas–\nTomasi (KLT) tracker.",
  "258": "236\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 4.28 Feature tracking using an afﬁne motion model (Shi and Tomasi 1994) c⃝1994\nIEEE, Top row: image patch around the tracked feature location. Bottom row: image patch\nafter warping back toward the ﬁrst frame using an afﬁne deformation. Even though the speed\nsign gets larger from frame to frame, the afﬁne transformation maintains a good resemblance\nbetween the original and subsequent tracked frames.\nSince their original work on feature tracking, Shi and Tomasi’s approach has generated a\nstring of interesting follow-on papers and applications. Beardsley, Torr, and Zisserman (1996)\nuse extended feature tracking combined with structure from motion (Chapter 7) to incremen-\ntally build up sparse 3D models from video sequences. Kang, Szeliski, and Shum (1997)\ntie together the corners of adjacent (regularly gridded) patches to provide some additional\nstability to the tracking, at the cost of poorer handling of occlusions. Tommasini, Fusiello,\nTrucco et al. (1998) provide a better spurious match rejection criterion for the basic Shi and\nTomasi algorithm, Collins and Liu (2003) provide improved mechanisms for feature selec-\ntion and dealing with larger appearance changes over time, and Shaﬁque and Shah (2005)\ndevelop algorithms for feature matching (data association) for videos with large numbers of\nmoving objects or points. Yilmaz, Javed, and Shah (2006) and Lepetit and Fua (2005) survey\nthe larger ﬁeld of object tracking, which includes not only feature-based techniques but also\nalternative techniques based on contour and region (Section 5.1).\nOne of the newest developments in feature tracking is the use of learning algorithms to\nbuild special-purpose recognizers to rapidly search for matching features anywhere in an\nimage (Lepetit, Pilet, and Fua 2006; Hinterstoisser, Benhimane, Navab et al. 2008; Rogez,\nRihan, Ramalingam et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010).2 By taking the time\nto train classiﬁers on sample patches and their afﬁne deformations, extremely fast and reliable\nfeature detectors can be constructed, which enables much faster motions to be supported\n(Figure 4.29). Coupling such features to deformable models (Pilet, Lepetit, and Fua 2008) or\nstructure-from-motion algorithms (Klein and Murray 2008) can result in even higher stability.\n2 See also my previous comment on earlier work in learning-based tracking (Avidan 2001; Jurie and Dhome\n2002; Williams, Blake, and Cipolla 2003).",
  "259": "4.1 Points and patches\n237\nFigure 4.29 Real-time head tracking using the fast trained classiﬁers of Lepetit, Pilet, and\nFua (2004) c⃝2004 IEEE.\n4.1.5 Application: Performance-driven animation\nOne of the most compelling applications of fast feature tracking is performance-driven an-\nimation, i.e., the interactive deformation of a 3D graphics model based on tracking a user’s\nmotions (Williams 1990; Litwinowicz and Williams 1994; Lepetit, Pilet, and Fua 2004).\nBuck, Finkelstein, Jacobs et al. (2000) present a system that tracks a user’s facial expres-\nsions and head motions and then uses them to morph among a series of hand-drawn sketches.\nAn animator ﬁrst extracts the eye and mouth regions of each sketch and draws control lines\nover each image (Figure 4.30a). At run time, a face-tracking system (Toyama 1998) deter-\nmines the current location of these features (Figure 4.30b). The animation system decides\nwhich input images to morph based on nearest neighbor feature appearance matching and\ntriangular barycentric interpolation. It also computes the global location and orientation of\nthe head from the tracked features. The resulting morphed eye and mouth regions are then\ncomposited back into the overall head model to yield a frame of hand-drawn animation (Fig-\nure 4.30d).\nIn more recent work, Barnes, Jacobs, Sanders et al. (2008) watch users animate paper\ncutouts on a desk and then turn the resulting motions and drawings into seamless 2D anima-\ntions.",
  "260": "238\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 4.30\nPerformance-driven, hand-drawn animation (Buck, Finkelstein, Jacobs et al.\n2000) c⃝2000 ACM: (a) eye and mouth portions of hand-drawn sketch with their overlaid\ncontrol lines; (b) an input video frame with the tracked features overlaid; (c) a different input\nvideo frame along with its (d) corresponding hand-drawn animation.\n4.2 Edges\nWhile interest points are useful for ﬁnding image locations that can be accurately matched\nin 2D, edge points are far more plentiful and often carry important semantic associations.\nFor example, the boundaries of objects, which also correspond to occlusion events in 3D, are\nusually delineated by visible contours. Other kinds of edges correspond to shadow boundaries\nor crease edges, where surface orientation changes rapidly. Isolated edge points can also be\ngrouped into longer curves or contours, as well as straight line segments (Section 4.3). It\nis interesting that even young children have no difﬁculty in recognizing familiar objects or\nanimals from such simple line drawings.\n4.2.1 Edge detection\nGiven an image, how can we ﬁnd the salient edges? Consider the color images in Figure 4.31.\nIf someone asked you to point out the most “salient” or “strongest” edges or the object bound-\naries (Martin, Fowlkes, and Malik 2004; Arbel´aez, Maire, Fowlkes et al. 2010), which ones\nwould you trace? How closely do your perceptions match the edge images shown in Fig-\nure 4.31?\nQualitatively, edges occur at boundaries between regions of different color, intensity, or\ntexture. Unfortunately, segmenting an image into coherent regions is a difﬁcult task, which\nwe address in Chapter 5. Often, it is preferable to detect edges using only purely local infor-\nmation.\nUnder such conditions, a reasonable approach is to deﬁne an edge as a location of rapid",
  "261": "4.2 Edges\n239\nFigure 4.31 Human boundary detection (Martin, Fowlkes, and Malik 2004) c⃝2004 IEEE.\nThe darkness of the edges corresponds to how many human subjects marked an object bound-\nary at that location.\nintensity variation.3 Think of an image as a height ﬁeld. On such a surface, edges occur\nat locations of steep slopes, or equivalently, in regions of closely packed contour lines (on a\ntopographic map).\nA mathematical way to deﬁne the slope and direction of a surface is through its gradient,\nJ(x) = ∇I(x) = (∂I\n∂x, ∂I\n∂y )(x).\n(4.19)\nThe local gradient vector J points in the direction of steepest ascent in the intensity function.\nIts magnitude is an indication of the slope or strength of the variation, while its orientation\npoints in a direction perpendicular to the local contour.\nUnfortunately, taking image derivatives accentuates high frequencies and hence ampliﬁes\nnoise, since the proportion of noise to signal is larger at high frequencies. It is therefore\nprudent to smooth the image with a low-pass ﬁlter prior to computing the gradient. Because\nwe would like the response of our edge detector to be independent of orientation, a circularly\nsymmetric smoothing ﬁlter is desirable. As we saw in Section 3.2, the Gaussian is the only\nseparable circularly symmetric ﬁlter and so it is used in most edge detection algorithms.\nCanny (1986) discusses alternative ﬁlters and a number of researcher review alternative edge\ndetection algorithms and compare their performance (Davis 1975; Nalwa and Binford 1986;\nNalwa 1987; Deriche 1987; Freeman and Adelson 1991; Nalwa 1993; Heath, Sarkar, Sanocki\net al. 1998; Crane 1997; Ritter and Wilson 2000; Bowyer, Kranenburg, and Dougherty 2001;\nArbel´aez, Maire, Fowlkes et al. 2010).\nBecause differentiation is a linear operation, it commutes with other linear ﬁltering oper-\n3 We defer the topic of edge detection in color images.",
  "262": "240\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nations. The gradient of the smoothed image can therefore be written as\nJσ(x) = ∇[Gσ(x) ∗I(x)] = [∇Gσ](x) ∗I(x),\n(4.20)\ni.e., we can convolve the image with the horizontal and vertical derivatives of the Gaussian\nkernel function,\n∇Gσ(x) = (∂Gσ\n∂x , ∂Gσ\n∂y )(x) = [−x −y] 1\nσ3 exp\n\u0012\n−x2 + y2\n2σ2\n\u0013\n(4.21)\n(The parameter σ indicates the width of the Gaussian.) This is the same computation that\nis performed by Freeman and Adelson’s (1991) ﬁrst-order steerable ﬁlter, which we already\ncovered in Section 3.2.3.\nFor many applications, however, we wish to thin such a continuous gradient image to\nonly return isolated edges, i.e., as single pixels at discrete locations along the edge contours.\nThis can be achieved by looking for maxima in the edge strength (gradient magnitude) in a\ndirection perpendicular to the edge orientation, i.e., along the gradient direction.\nFinding this maximum corresponds to taking a directional derivative of the strength ﬁeld\nin the direction of the gradient and then looking for zero crossings. The desired directional\nderivative is equivalent to the dot product between a second gradient operator and the results\nof the ﬁrst,\nSσ(x) = ∇· Jσ(x) = [∇2Gσ](x) ∗I(x)].\n(4.22)\nThe gradient operator dot product with the gradient is called the Laplacian. The convolution\nkernel\n∇2Gσ(x) = 1\nσ3\n\u0012\n2 −x2 + y2\n2σ2\n\u0013\nexp\n\u0012\n−x2 + y2\n2σ2\n\u0013\n(4.23)\nis therefore called the Laplacian of Gaussian (LoG) kernel (Marr and Hildreth 1980). This\nkernel can be split into two separable parts,\n∇2Gσ(x) = 1\nσ3\n\u0012\n1 −x2\n2σ2\n\u0013\nGσ(x)Gσ(y) + 1\nσ3\n\u0012\n1 −y2\n2σ2\n\u0013\nGσ(y)Gσ(x)\n(4.24)\n(Wiejak, Buxton, and Buxton 1985), which allows for a much more efﬁcient implementation\nusing separable ﬁltering (Section 3.2.1).\nIn practice, it is quite common to replace the Laplacian of Gaussian convolution with a\nDifference of Gaussian (DoG) computation, since the kernel shapes are qualitatively similar\n(Figure 3.35). This is especially convenient if a “Laplacian pyramid” (Section 3.5) has already\nbeen computed.4\n4 Recall that Burt and Adelson’s (1983a) “Laplacian pyramid” actually computed differences of Gaussian-ﬁltered\nlevels.",
  "263": "4.2 Edges\n241\nIn fact, it is not strictly necessary to take differences between adjacent levels when com-\nputing the edge ﬁeld. Think about what a zero crossing in a “generalized” difference of\nGaussians image represents. The ﬁner (smaller kernel) Gaussian is a noise-reduced version\nof the original image. The coarser (larger kernel) Gaussian is an estimate of the average in-\ntensity over a larger region. Thus, whenever the DoG image changes sign, this corresponds\nto the (slightly blurred) image going from relatively darker to relatively lighter, as compared\nto the average intensity in that neighborhood.\nOnce we have computed the sign function S(x), we must ﬁnd its zero crossings and\nconvert these into edge elements (edgels). An easy way to detect and represent zero crossings\nis to look for adjacent pixel locations xi and xj where the sign changes value, i.e., [S(xi) >\n0] ̸= [S(xj) > 0].\nThe sub-pixel location of this crossing can be obtained by computing the “x-intercept” of\nthe “line” connecting S(xi) and S(xj),\nxz = xiS(xj) −xjS(xi)\nS(xj) −S(xi)\n.\n(4.25)\nThe orientation and strength of such edgels can be obtained by linearly interpolating the\ngradient values computed on the original pixel grid.\nAn alternative edgel representation can be obtained by linking adjacent edgels on the\ndual grid to form edgels that live inside each square formed by four adjacent pixels in the\noriginal pixel grid.5 The (potential) advantage of this representation is that the edgels now\nlive on a grid offset by half a pixel from the original pixel grid and are thus easier to store\nand access.\nAs before, the orientations and strengths of the edges can be computed by\ninterpolating the gradient ﬁeld or estimating these values from the difference of Gaussian\nimage (see Exercise 4.7).\nIn applications where the accuracy of the edge orientation is more important, higher-order\nsteerable ﬁlters can be used (Freeman and Adelson 1991) (see Section 3.2.3). Such ﬁlters are\nmore selective for more elongated edges and also have the possibility of better modeling curve\nintersections because they can represent multiple orientations at the same pixel (Figure 3.16).\nTheir disadvantage is that they are more expensive to compute and the directional derivative\nof the edge strength does not have a simple closed form solution.6",
  "264": "242\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 4.32\nScale selection for edge detection (Elder and Zucker 1998) c⃝1998 IEEE:\n(a) original image; (b–c) Canny/Deriche edge detector tuned to the ﬁner (mannequin) and\ncoarser (shadow) scales; (d) minimum reliable scale for gradient estimation; (e) minimum\nreliable scale for second derivative estimation; (f) ﬁnal detected edges.\nScale selection and blur estimation\nAs we mentioned before, the derivative, Laplacian, and Difference of Gaussian ﬁlters (4.20–\n4.23) all require the selection of a spatial scale parameter σ. If we are only interested in\ndetecting sharp edges, the width of the ﬁlter can be determined from image noise characteris-\ntics (Canny 1986; Elder and Zucker 1998). However, if we want to detect edges that occur at\ndifferent resolutions (Figures 4.32b–c), a scale-space approach that detects and then selects\nedges at different scales may be necessary (Witkin 1983; Lindeberg 1994, 1998a; Nielsen,\nFlorack, and Deriche 1997).\nElder and Zucker (1998) present a principled approach to solving this problem. Given\na known image noise level, their technique computes, for every pixel, the minimum scale\nat which an edge can be reliably detected (Figure 4.32d). Their approach ﬁrst computes\n5 This algorithm is a 2D version of the 3D marching cubes isosurface extraction algorithm (Lorensen and Cline\n1987).\n6 In fact, the edge orientation can have a 180◦ambiguity for “bar edges”, which makes the computation of zero\ncrossings in the derivative more tricky.",
  "265": "4.2 Edges\n243\ngradients densely over an image by selecting among gradient estimates computed at different\nscales, based on their gradient magnitudes. It then performs a similar estimate of minimum\nscale for directed second derivatives and uses zero crossings of this latter quantity to robustly\nselect edges (Figures 4.32e–f). As an optional ﬁnal step, the blur width of each edge can\nbe computed from the distance between extrema in the second derivative response minus the\nwidth of the Gaussian ﬁlter.\nColor edge detection\nWhile most edge detection techniques have been developed for grayscale images, color im-\nages can provide additional information. For example, noticeable edges between iso-luminant\ncolors (colors that have the same luminance) are useful cues but fail to be detected by grayscale\nedge operators.\nOne simple approach is to combine the outputs of grayscale detectors run on each color\nband separately.7 However, some care must be taken. For example, if we simply sum up\nthe gradients in each of the color bands, the signed gradients may actually cancel each other!\n(Consider, for example a pure red-to-green edge.) We could also detect edges independently\nin each band and then take the union of these, but this might lead to thickened or doubled\nedges that are hard to link.\nA better approach is to compute the oriented energy in each band (Morrone and Burr\n1988; Perona and Malik 1990a), e.g., using a second-order steerable ﬁlter (Section 3.2.3)\n(Freeman and Adelson 1991), and then sum up the orientation-weighted energies and ﬁnd\ntheir joint best orientation. Unfortunately, the directional derivative of this energy may not\nhave a closed form solution (as in the case of signed ﬁrst-order steerable ﬁlters), so a simple\nzero crossing-based strategy cannot be used. However, the technique described by Elder and\nZucker (1998) can be used to compute these zero crossings numerically instead.\nAn alternative approach is to estimate local color statistics in regions around each pixel\n(Ruzon and Tomasi 2001; Martin, Fowlkes, and Malik 2004). This has the advantage that\nmore sophisticated techniques (e.g., 3D color histograms) can be used to compare regional\nstatistics and that additional measures, such as texture, can also be considered. Figure 4.33\nshows the output of such detectors.\nOf course, many other approaches have been developed for detecting color edges, dating\nback to early work by Nevatia (1977). Ruzon and Tomasi (2001) and Gevers, van de Weijer,\nand Stokman (2006) provide good reviews of these approaches, which include ideas such as\nfusing outputs from multiple channels, using multidimensional gradients, and vector-based\n7 Instead of using the raw RGB space, a more perceptually uniform color space such as L*a*b* (see Section 2.3.2)\ncan be used instead. When trying to match human performance (Martin, Fowlkes, and Malik 2004), this makes sense.\nHowever, in terms of the physics of the underlying image formation and sensing, it may be a questionable strategy.",
  "266": "244\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmethods.\nCombining edge feature cues\nIf the goal of edge detection is to match human boundary detection performance (Bowyer,\nKranenburg, and Dougherty 2001; Martin, Fowlkes, and Malik 2004; Arbel´aez, Maire, Fowlkes\net al. 2010), as opposed to simply ﬁnding stable features for matching, even better detectors\ncan be constructed by combining multiple low-level cues such as brightness, color, and tex-\nture.\nMartin, Fowlkes, and Malik (2004) describe a system that combines brightness, color, and\ntexture edges to produce state-of-the-art performance on a database of hand-segmented natu-\nral color images (Martin, Fowlkes, Tal et al. 2001). First, they construct and train8 separate\noriented half-disc detectors for measuring signiﬁcant differences in brightness (luminance),\ncolor (a* and b* channels, summed responses), and texture (un-normalized ﬁlter bank re-\nsponses from the work of Malik, Belongie, Leung et al. (2001)). Some of the responses\nare then sharpened using a soft non-maximal suppression technique. Finally, the outputs of\nthe three detectors are combined using a variety of machine-learning techniques, from which\nlogistic regression is found to have the best tradeoff between speed, space and accuracy .\nThe resulting system (see Figure 4.33 for some examples) is shown to outperform previously\ndeveloped techniques. Maire, Arbelaez, Fowlkes et al. (2008) improve on these results by\ncombining the detector based on local appearance with a spectral (segmentation-based) de-\ntector (Belongie and Malik 1998). In more recent work, Arbel´aez, Maire, Fowlkes et al.\n(2010) build a hierarchical segmentation on top of this edge detector using a variant of the\nwatershed algorithm.\n4.2.2 Edge linking\nWhile isolated edges can be useful for a variety of applications, such as line detection (Sec-\ntion 4.3) and sparse stereo matching (Section 11.2), they become even more useful when\nlinked into continuous contours.\nIf the edges have been detected using zero crossings of some function, linking them up\nis straightforward, since adjacent edgels share common endpoints. Linking the edgels into\nchains involves picking up an unlinked edgel and following its neighbors in both directions.\nEither a sorted list of edgels (sorted ﬁrst by x coordinates and then by y coordinates, for\nexample) or a 2D array can be used to accelerate the neighbor ﬁnding. If edges were not\ndetected using zero crossings, ﬁnding the continuation of an edgel can be tricky. In this\ncase, comparing the orientation (and, optionally, phase) of adjacent edgels can be used for\n8 The training uses 200 labeled images and testing is performed on a different set of 100 images.",
  "267": "4.2 Edges\n245\nFigure 4.33\nCombined brightness, color, texture boundary detector (Martin, Fowlkes, and\nMalik 2004) c⃝2004 IEEE. Successive rows show the outputs of the brightness gradient\n(BG), color gradient (CG), texture gradient (TG), and combined (BG+CG+TG) detectors.\nThe ﬁnal row shows human-labeled boundaries derived from a database of hand-segmented\nimages (Martin, Fowlkes, Tal et al. 2001).",
  "268": "246\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nN   0\nN   0\nW\n6\nNE  \n     1\n      7\nNW\nSW  \n     5\nFigure 4.34\nChain code representation of a grid-aligned linked edge chain. The code is\nrepresented as a series of direction codes, e.g, 0 1 0 7 6 5, which can further be compressed\nusing predictive and run-length coding.\ndisambiguation. Ideas from connected component computation can also sometimes be used\nto make the edge linking process even faster (see Exercise 4.8).\nOnce the edgels have been linked into chains, we can apply an optional thresholding\nwith hysteresis to remove low-strength contour segments (Canny 1986). The basic idea of\nhysteresis is to set two different thresholds and allow a curve being tracked above the higher\nthreshold to dip in strength down to the lower threshold.\nLinked edgel lists can be encoded more compactly using a variety of alternative repre-\nsentations. A chain code encodes a list of connected points lying on an N8 grid using a\nthree-bit code corresponding to the eight cardinal directions (N, NE, E, SE, S, SW, W, NW)\nbetween a point and its successor (Figure 4.34). While this representation is more compact\nthan the original edgel list (especially if predictive variable-length coding is used), it is not\nvery suitable for further processing.\nA more useful representation is the arc length parameterization of a contour, x(s), where\ns denotes the arc length along a curve. Consider the linked set of edgels shown in Fig-\nure 4.35a. We start at one point (the dot at (1.0, 0.5) in Figure 4.35a) and plot it at coordinate\ns = 0 (Figure 4.35b). The next point at (2.0, 0.5) gets plotted at s = 1, and the next point\nat (2.5, 1.0) gets plotted at s = 1.7071, i.e., we increment s by the length of each edge seg-\nment. The resulting plot can be resampled on a regular (say, integral) s grid before further\nprocessing.\nThe advantage of the arc-length parameterization is that it makes matching and processing\n(e.g., smoothing) operations much easier. Consider the two curves describing similar shapes\nshown in Figure 4.36. To compare the curves, we ﬁrst subtract the average values x0 =\nR\ns x(s) from each descriptor. Next, we rescale each descriptor so that s goes from 0 to 1\ninstead of 0 to S, i.e., we divide x(s) by S. Finally, we take the Fourier transform of each",
  "269": "4.2 Edges\n247\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\nx\ny\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11\ns\n.\nx\ny\n(a)\n(b)\nFigure 4.35 Arc-length parameterization of a contour: (a) discrete points along the contour\nare ﬁrst transcribed as (b) (x, y) pairs along the arc length s. This curve can then be regularly\nre-sampled or converted into alternative (e.g., Fourier) representations.\nt\nx(s)\nκ\ns=0=1\nx0\ns=0=1\nx0\nFigure 4.36 Matching two contours using their arc-length parameterization. If both curves\nare normalized to unit length, s ∈[0, 1] and centered around their centroid x0, they will\nhave the same descriptor up to an overall “temporal” shift (due to different starting points for\ns = 0) and a phase (x-y) shift (due to rotation).",
  "270": "248\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 4.37 Curve smoothing with a Gaussian kernel (Lowe 1988) c⃝1998 IEEE: (a) with-\nout a shrinkage correction term; (b) with a shrinkage correction term.\nFigure 4.38 Changing the character of a curve without affecting its sweep (Finkelstein and\nSalesin 1994) c⃝1994 ACM: higher frequency wavelets can be replaced with exemplars from\na style library to effect different local appearances.\nnormalized descriptor, treating each x = (x, y) value as a complex number. If the original\ncurves are the same (up to an unknown scale and rotation), the resulting Fourier transforms\nshould differ only by a scale change in magnitude plus a constant complex phase shift, due\nto rotation, and a linear phase shift in the domain, due to different starting points for s (see\nExercise 4.9).\nArc-length parameterization can also be used to smooth curves in order to remove digiti-\nzation noise. However, if we just apply a regular smoothing ﬁlter, the curve tends to shrink\non itself (Figure 4.37a). Lowe (1989) and Taubin (1995) describe techniques that compensate\nfor this shrinkage by adding an offset term based on second derivative estimates or a larger\nsmoothing kernel (Figure 4.37b). An alternative approach, based on selectively modifying\ndifferent frequencies in a wavelet decomposition, is presented by Finkelstein and Salesin\n(1994). In addition to controlling shrinkage without affecting its “sweep”, wavelets allow the\n“character” of a curve to be interactively modiﬁed, as shown in Figure 4.38.\nThe evolution of curves as they are smoothed and simpliﬁed is related to “grassﬁre” (dis-",
  "271": "4.2 Edges\n249\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 4.39 Image editing in the contour domain (Elder and Goldberg 2001) c⃝2001 IEEE:\n(a) and (d) original images; (b) and (e) extracted edges (edges to be deleted are marked in\nwhite); (c) and (f) reconstructed edited images.\ntance) transforms and region skeletons (Section 3.3.3) (Tek and Kimia 2003), and can be used\nto recognize objects based on their contour shape (Sebastian and Kimia 2005). More local de-\nscriptors of curve shape such as shape contexts (Belongie, Malik, and Puzicha 2002) can also\nbe used for recognition and are potentially more robust to missing parts due to occlusions.\nThe ﬁeld of contour detection and linking continues to evolve rapidly and now includes\ntechniques for global contour grouping, boundary completion, and junction detection (Maire,\nArbelaez, Fowlkes et al. 2008), as well as grouping contours into likely regions (Arbel´aez,\nMaire, Fowlkes et al. 2010) and wide-baseline correspondence (Meltzer and Soatto 2008).\n4.2.3 Application: Edge editing and enhancement\nWhile edges can serve as components for object recognition or features for matching, they\ncan also be used directly for image editing.\nIn fact, if the edge magnitude and blur estimate are kept along with each edge, a visually\nsimilar image can be reconstructed from this information (Elder 1999). Based on this princi-\nple, Elder and Goldberg (2001) propose a system for “image editing in the contour domain”.\nTheir system allows users to selectively remove edges corresponding to unwanted features\nsuch as specularities, shadows, or distracting visual elements. After reconstructing the image\nfrom the remaining edges, the undesirable visual features have been removed (Figure 4.39).",
  "272": "250\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 4.40 Approximating a curve (shown in black) as a polyline or B-spline: (a) original\ncurve and a polyline approximation shown in red; (b) successive approximation by recursively\nﬁnding points furthest away from the current approximation; (c) smooth interpolating spline,\nshown in dark blue, ﬁt to the polyline vertices.\nAnother potential application is to enhance perceptually salient edges while simplifying\nthe underlying image to produce a cartoon-like or “pen-and-ink” stylized image (DeCarlo and\nSantella 2002). This application is discussed in more detail in Section 10.5.2.\n4.3 Lines\nWhile edges and general curves are suitable for describing the contours of natural objects,\nthe man-made world is full of straight lines. Detecting and matching these lines can be\nuseful in a variety of applications, including architectural modeling, pose estimation in urban\nenvironments, and the analysis of printed document layouts.\nIn this section, we present some techniques for extracting piecewise linear descriptions\nfrom the curves computed in the previous section. We begin with some algorithms for approx-\nimating a curve as a piecewise-linear polyline. We then describe the Hough transform, which\ncan be used to group edgels into line segments even across gaps and occlusions. Finally, we\ndescribe how 3D lines with common vanishing points can be grouped together. These van-\nishing points can be used to calibrate a camera and to determine its orientation relative to a\nrectahedral scene, as described in Section 6.3.2.\n4.3.1 Successive approximation\nAs we saw in Section 4.2.2, describing a curve as a series of 2D locations xi = x(si) provides\na general representation suitable for matching and further processing. In many applications,\nhowever, it is preferable to approximate such a curve with a simpler representation, e.g., as a\npiecewise-linear polyline or as a B-spline curve (Farin 1996), as shown in Figure 4.40.\nMany techniques have been developed over the years to perform this approximation,\nwhich is also known as line simpliﬁcation. One of the oldest, and simplest, is the one proposed",
  "273": "4.3 Lines\n251\nθi\nri\nθ\n(xi,yi)\n0\n360\n0\nrmax\nr\n-rmax\nx\ny\n(a)\n(b)\nFigure 4.41 Original Hough transform: (a) each point votes for a complete family of poten-\ntial lines ri(θ) = xi cos θ + yi sin θ; (b) each pencil of lines sweeps out a sinusoid in (r, θ);\ntheir intersection provides the desired line equation.\nby Ramer (1972) and Douglas and Peucker (1973), who recursively subdivide the curve at\nthe point furthest away from the line joining the two endpoints (or the current coarse polyline\napproximation), as shown in Figure 4.40. Hershberger and Snoeyink (1992) provide a more\nefﬁcient implementation and also cite some of the other related work in this area.\nOnce the line simpliﬁcation has been computed, it can be used to approximate the orig-\ninal curve. If a smoother representation or visualization is desired, either approximating or\ninterpolating splines or curves can be used (Sections 3.5.1 and 5.1.1) (Szeliski and Ito 1986;\nBartels, Beatty, and Barsky 1987; Farin 1996), as shown in Figure 4.40c.\n4.3.2 Hough transforms\nWhile curve approximation with polylines can often lead to successful line extraction, lines\nin the real world are sometimes broken up into disconnected components or made up of many\ncollinear line segments. In many cases, it is desirable to group such collinear segments into\nextended lines. At a further processing stage (described in Section 4.3.3), we can then group\nsuch lines into collections with common vanishing points.\nThe Hough transform, named after its original inventor (Hough 1962), is a well-known\ntechnique for having edges “vote” for plausible line locations (Duda and Hart 1972; Ballard\n1981; Illingworth and Kittler 1988). In its original formulation (Figure 4.41), each edge point\nvotes for all possible lines passing through it, and lines corresponding to high accumulator or\nbin values are examined for potential line ﬁts.9 Unless the points on a line are truly punctate,\na better approach (in my experience) is to use the local orientation information at each edgel\nto vote for a single accumulator cell (Figure 4.42), as described below. A hybrid strategy,\n9 The Hough transform can also be generalized to look for other geometric features such as circles (Ballard\n1981), but we do not cover such extensions in this book.",
  "274": "252\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nθi\nri\nθ\n(xi,yi)\n0\n360\n0\nrmax\nr\n-rmax\nx\ny\n(a)\n(b)\nFigure 4.42 Oriented Hough transform: (a) an edgel re-parameterized in polar (r, θ) coor-\ndinates, with ˆni = (cos θi, sin θi) and ri = ˆni · xi; (b) (r, θ) accumulator array, showing the\nvotes for the three edgels marked in red, green, and blue.\ny\nx\nd\nθ\nn\nl\n^\nFigure 4.43 2D line equation expressed in terms of the normal ˆn and distance to the origin\nd.\nwhere each edgel votes for a number of possible orientation or location pairs centered around\nthe estimate orientation, may be desirable in some cases.\nBefore we can vote for line hypotheses, we must ﬁrst choose a suitable representation.\nFigure 4.43 (copied from Figure 2.2a) shows the normal-distance (ˆn, d) parameterization for\na line. Since lines are made up of edge segments, we adopt the convention that the line normal\nˆn points in the same direction (i.e., has the same sign) as the image gradient J(x) = ∇I(x)\n(4.19). To obtain a minimal two-parameter representation for lines, we convert the normal\nvector into an angle\nθ = tan−1 ny/nx,\n(4.26)\nas shown in Figure 4.43. The range of possible (θ, d) values is [−180◦, 180◦] × [−\n√\n2,\n√\n2],\nassuming that we are using normalized pixel coordinates (2.61) that lie in [−1, 1]. The number\nof bins to use along each axis depends on the accuracy of the position and orientation estimate\navailable at each edgel and the expected line density, and is best set experimentally with some\ntest runs on sample imagery.\nGiven the line parameterization, the Hough transform proceeds as shown in Algorithm 4.2.",
  "275": "4.3 Lines\n253\nprocedure Hough({(x, y, θ)}):\n1. Clear the accumulator array.\n2. For each detected edgel at location (x, y) and orientation θ = tan−1 ny/nx,\ncompute the value of\nd = x nx + y ny\nand increment the accumulator corresponding to (θ, d).\n3. Find the peaks in the accumulator corresponding to lines.\n4. Optionally re-ﬁt the lines to the constituent edgels.\nAlgorithm 4.2 Outline of a Hough transform algorithm based on oriented edge segments.\nNote that the original formulation of the Hough transform, which assumed no knowledge of\nthe edgel orientation θ, has an additional loop inside Step 2 that iterates over all possible\nvalues of θ and increments a whole series of accumulators.\nThere are a lot of details in getting the Hough transform to work well, but these are\nbest worked out by writing an implementation and testing it out on sample data. Exercise\n4.12 describes some of these steps in more detail, including using edge segment lengths or\nstrengths during the voting process, keeping a list of constituent edgels in the accumulator\narray for easier post-processing, and optionally combining edges of different “polarity” into\nthe same line segments.\nAn alternative to the 2D polar (θ, d) representation for lines is to use the full 3D m =\n(ˆn, d) line equation, projected onto the unit sphere. While the sphere can be parameterized\nusing spherical coordinates (2.8),\nˆm = (cos θ cos φ, sin θ cos φ, sin φ),\n(4.27)\nthis does not uniformly sample the sphere and still requires the use of trigonometry.\nAn alternative representation can be obtained by using a cube map, i.e., projecting m onto\nthe face of a unit cube (Figure 4.44a). To compute the cube map coordinate of a 3D vector\nm, ﬁrst ﬁnd the largest (absolute value) component of m, i.e., m = ± max(|nx|, |ny|, |d|),\nand use this to select one of the six cube faces. Divide the remaining two coordinates by m\nand use these as indices into the cube face. While this avoids the use of trigonometry, it does\nrequire some decision logic.\nOne advantage of using the cube map, ﬁrst pointed out by Tuytelaars, Van Gool, and\nProesmans (1997), is that all of the lines passing through a point correspond to line segments",
  "276": "254\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n⇒\n(a)\n(b)\nFigure 4.44 Cube map representation for line equations and vanishing points: (a) a cube map\nsurrounding the unit sphere; (b) projecting the half-cube onto three subspaces (Tuytelaars,\nVan Gool, and Proesmans 1997) c⃝1997 IEEE.\non the cube faces, which is useful if the original (full voting) variant of the Hough transform\nis being used. In their work, they represent the line equation as ax + b + y = 0, which\ndoes not treat the x and y axes symmetrically. Note that if we restrict d ≥0 by ignoring the\npolarity of the edge orientation (gradient sign), we can use a half-cube instead, which can be\nrepresented using only three cube faces, as shown in Figure 4.44b (Tuytelaars, Van Gool, and\nProesmans 1997).\nRANSAC-based line detection.\nAnother alternative to the Hough transform is the RAN-\ndom SAmple Consensus (RANSAC) algorithm described in more detail in Section 6.1.4. In\nbrief, RANSAC randomly chooses pairs of edgels to form a line hypothesis and then tests\nhow many other edgels fall onto this line. (If the edge orientations are accurate enough, a\nsingle edgel can produce this hypothesis.) Lines with sufﬁciently large numbers of inliers\n(matching edgels) are then selected as the desired line segments.\nAn advantage of RANSAC is that no accumulator array is needed and so the algorithm can\nbe more space efﬁcient and potentially less prone to the choice of bin size. The disadvantage\nis that many more hypotheses may need to be generated and tested than those obtained by\nﬁnding peaks in the accumulator array.\nIn general, there is no clear consensus on which line estimation technique performs best.\nIt is therefore a good idea to think carefully about the problem at hand and to implement\nseveral approaches (successive approximation, Hough, and RANSAC) to determine the one\nthat works best for your application.\n4.3.3 Vanishing points\nIn many scenes, structurally important lines have the same vanishing point because they are\nparallel in 3D. Examples of such lines are horizontal and vertical building edges, zebra cross-\nings, railway tracks, the edges of furniture such as tables and dressers, and of course, the\nubiquitous calibration pattern (Figure 4.45). Finding the vanishing points common to such",
  "277": "4.3 Lines\n255\n(a)\n(b)\n(c)\nFigure 4.45\nReal-world vanishing points: (a) architecture (Sinha, Steedly, Szeliski et al.\n2008), (b) furniture (Miˇcuˇs`ık, Wildenauer, and Koˇseck´a 2008) c⃝2008 IEEE, and (c) cali-\nbration patterns (Zhang 2000).\nline sets can help reﬁne their position in the image and, in certain cases, help determine the\nintrinsic and extrinsic orientation of the camera (Section 6.3.2).\nOver the years, a large number of techniques have been developed for ﬁnding vanishing\npoints, including (Quan and Mohr 1989; Collins and Weiss 1990; Brillaut-O’Mahoney 1991;\nMcLean and Kotturi 1995; Becker and Bove 1995; Shufelt 1999; Tuytelaars, Van Gool, and\nProesmans 1997; Schaffalitzky and Zisserman 2000; Antone and Teller 2002; Rother 2002;\nKoˇseck´a and Zhang 2005; Pﬂugfelder 2008; Tardif 2009)—see some of the more recent pa-\npers for additional references.\nIn this section, we present a simple Hough technique based\non having line pairs vote for potential vanishing point locations, followed by a robust least\nsquares ﬁtting stage. For alternative approaches, please see some of the more recent papers\nlisted above.\nThe ﬁrst stage in my vanishing point detection algorithm uses a Hough transform to accu-\nmulate votes for likely vanishing point candidates. As with line ﬁtting, one possible approach\nis to have each line vote for all possible vanishing point directions, either using a cube map\n(Tuytelaars, Van Gool, and Proesmans 1997; Antone and Teller 2002) or a Gaussian sphere\n(Collins and Weiss 1990), optionally using knowledge about the uncertainty in the vanish-\ning point location to perform a weighted vote (Collins and Weiss 1990; Brillaut-O’Mahoney\n1991; Shufelt 1999). My preferred approach is to use pairs of detected line segments to form\ncandidate vanishing point locations. Let ˆ\nmi and ˆ\nmj be the (unit norm) line equations for a\npair of line segments and li and lj be their corresponding segment lengths. The location of\nthe corresponding vanishing point hypothesis can be computed as\nvij = ˆ\nmi × ˆ\nmj\n(4.28)\nand the corresponding weight set to\nwij = ∥vij∥lilj.\n(4.29)",
  "278": "256\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\npi1\npi0\nv\nmi\n^\nd1\nA\nFigure 4.46\nTriple product of the line segments endpoints pi0 and pi1 and the vanishing\npoint v. The area A is proportional to the perpendicular distance d1 and the distance between\nthe other endpoint pi0 and the vanishing point.\nThis has the desirable effect of downweighting (near-)collinear line segments and short line\nsegments. The Hough space itself can either be represented using spherical coordinates (4.27)\nor as a cube map (Figure 4.44a).\nOnce the Hough accumulator space has been populated, peaks can be detected in a manner\nsimilar to that previously discussed for line detection. Given a set of candidate line segments\nthat voted for a vanishing point, which can optionally be kept as a list at each Hough accu-\nmulator cell, I then use a robust least squares ﬁt to estimate a more accurate location for each\nvanishing point.\nConsider the relationship between the two line segment endpoints {pi0, pi1} and the van-\nishing point v, as shown in Figure 4.46. The area A of the triangle given by these three points,\nwhich is the magnitude of their triple product\nAi = |(pi0 × pi1) · v|,\n(4.30)\nis proportional to the perpendicular distance d1 between each endpoint and the line through\nv and the other endpoint, as well as the distance between pi0 and v. Assuming that the\naccuracy of a ﬁtted line segment is proportional to its endpoint accuracy (Exercise 4.13), this\ntherefore serves as an optimal metric for how well a vanishing point ﬁts a set of extracted\nlines (Leibowitz (2001, Section 3.6.1) and Pﬂugfelder (2008, Section 2.1.1.3)). A robustiﬁed\nleast squares estimate (Appendix B.3) for the vanishing point can therefore be written as\nE =\nX\ni\nρ(Ai) = vT\n X\ni\nwi(Ai)mimT\ni\n!\nv = vT Mv,\n(4.31)\nwhere mi = pi0 × pi1 is the segment line equation weighted by its length li, and wi =\nρ′(Ai)/Ai is the inﬂuence of each robustiﬁed (reweighted) measurement on the ﬁnal error\n(Appendix B.3). Notice how this metric is closely related to the original formula for the pair-\nwise weighted Hough transform accumulation step. The ﬁnal desired value for v is computed\nas the least eigenvector of M.",
  "279": "4.4 Additional reading\n257\nWhile the technique described above proceeds in two discrete stages, better results may\nbe obtained by alternating between assigning lines to vanishing points and reﬁtting the van-\nishing point locations (Antone and Teller 2002; Koˇseck´a and Zhang 2005; Pﬂugfelder 2008).\nThe results of detecting individual vanishing points can also be made more robust by simulta-\nneously searching for pairs or triplets of mutually orthogonal vanishing points (Shufelt 1999;\nAntone and Teller 2002; Rother 2002; Sinha, Steedly, Szeliski et al. 2008). Some results of\nsuch vanishing point detection algorithms can be seen in Figure 4.45.\n4.3.4 Application: Rectangle detection\nOnce sets of mutually orthogonal vanishing points have been detected, it now becomes pos-\nsible to search for 3D rectangular structures in the image (Figure 4.47). Over the last decade,\na variety of techniques have been developed to ﬁnd such rectangles, primarily focused on\narchitectural scenes (Koˇseck´a and Zhang 2005; Han and Zhu 2005; Shaw and Barnes 2006;\nMiˇcuˇs`ık, Wildenauer, and Koˇseck´a 2008; Schindler, Krishnamurthy, Lublinerman et al. 2008).\nAfter detecting orthogonal vanishing directions, Koˇseck´a and Zhang (2005) reﬁne the\nﬁtted line equations, search for corners near line intersections, and then verify rectangle hy-\npotheses by rectifying the corresponding patches and looking for a preponderance of hori-\nzontal and vertical edges (Figures 4.47a–b). In follow-on work, Miˇcuˇs`ık, Wildenauer, and\nKoˇseck´a (2008) use a Markov random ﬁeld (MRF) to disambiguate between potentially over-\nlapping rectangle hypotheses. They also use a plane sweep algorithm to match rectangles\nbetween different views (Figures 4.47d–f).\nA different approach is proposed by Han and Zhu (2005), who use a grammar of potential\nrectangle shapes and nesting structures (between rectangles and vanishing points) to infer the\nmost likely assignment of line segments to rectangles (Figure 4.47c).\n4.4 Additional reading\nOne of the seminal papers on feature detection, description, and matching is by Lowe (2004).\nComprehensive surveys and evaluations of such techniques have been made by Schmid,\nMohr, and Bauckhage (2000); Mikolajczyk and Schmid (2005); Mikolajczyk, Tuytelaars,\nSchmid et al. (2005); Tuytelaars and Mikolajczyk (2007) while Shi and Tomasi (1994) and\nTriggs (2004) also provide nice reviews.\nIn the area of feature detectors (Mikolajczyk, Tuytelaars, Schmid et al. 2005), in addition\nto such classic approaches as F¨orstner–Harris (F¨orstner 1986; Harris and Stephens 1988) and\ndifference of Gaussians (Lindeberg 1993, 1998b; Lowe 2004), maximally stable extremal re-\ngions (MSERs) are widely used for applications that require afﬁne invariance (Matas, Chum,",
  "280": "258\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 4.47 Rectangle detection: (a) indoor corridor and (b) building exterior with grouped\nfacades (Koˇseck´a and Zhang 2005) c⃝2005 Elsevier; (c) grammar-based recognition (Han\nand Zhu 2005) c⃝2005 IEEE; (d–f) rectangle matching using a plane sweep algorithm\n(Miˇcuˇs`ık, Wildenauer, and Koˇseck´a 2008) c⃝2008 IEEE.\nUrban et al. 2004; Nist´er and Stew´enius 2008). More recent interest point detectors are\ndiscussed by Xiao and Shah (2003); Koethe (2003); Carneiro and Jepson (2005); Kenney,\nZuliani, and Manjunath (2005); Bay, Tuytelaars, and Van Gool (2006); Platel, Balmachnova,\nFlorack et al. (2006); Rosten and Drummond (2006), as well as techniques based on line\nmatching (Zoghlami, Faugeras, and Deriche 1997; Bartoli, Coquerelle, and Sturm 2004) and\nregion detection (Kadir, Zisserman, and Brady 2004; Matas, Chum, Urban et al. 2004; Tuyte-\nlaars and Van Gool 2004; Corso and Hager 2005).\nA variety of local feature descriptors (and matching heuristics) are surveyed and com-\npared by Mikolajczyk and Schmid (2005). More recent publications in this area include\nthose by van de Weijer and Schmid (2006); Abdel-Hakim and Farag (2006); Winder and\nBrown (2007); Hua, Brown, and Winder (2007). Techniques for efﬁciently matching features\ninclude k-d trees (Beis and Lowe 1999; Lowe 2004; Muja and Lowe 2009), pyramid match-\ning kernels (Grauman and Darrell 2005), metric (vocabulary) trees (Nist´er and Stew´enius\n2006), and a variety of multi-dimensional hashing techniques (Shakhnarovich, Viola, and\nDarrell 2003; Torralba, Weiss, and Fergus 2008; Weiss, Torralba, and Fergus 2008; Kulis and",
  "281": "4.5 Exercises\n259\nGrauman 2009; Raginsky and Lazebnik 2009).\nThe classic reference on feature detection and tracking is (Shi and Tomasi 1994). More\nrecent work in this ﬁeld has focused on learning better matching functions for speciﬁc features\n(Avidan 2001; Jurie and Dhome 2002; Williams, Blake, and Cipolla 2003; Lepetit and Fua\n2005; Lepetit, Pilet, and Fua 2006; Hinterstoisser, Benhimane, Navab et al. 2008; Rogez,\nRihan, Ramalingam et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010).\nA highly cited and widely used edge detector is the one developed by Canny (1986).\nAlternative edge detectors as well as experimental comparisons can be found in publica-\ntions by Nalwa and Binford (1986); Nalwa (1987); Deriche (1987); Freeman and Adelson\n(1991); Nalwa (1993); Heath, Sarkar, Sanocki et al. (1998); Crane (1997); Ritter and Wilson\n(2000); Bowyer, Kranenburg, and Dougherty (2001); Arbel´aez, Maire, Fowlkes et al. (2010).\nThe topic of scale selection in edge detection is nicely treated by Elder and Zucker (1998),\nwhile approaches to color and texture edge detection can be found in (Ruzon and Tomasi\n2001; Martin, Fowlkes, and Malik 2004; Gevers, van de Weijer, and Stokman 2006). Edge\ndetectors have also recently been combined with region segmentation techniques to further\nimprove the detection of semantically salient boundaries (Maire, Arbelaez, Fowlkes et al.\n2008; Arbel´aez, Maire, Fowlkes et al. 2010). Edges linked into contours can be smoothed\nand manipulated for artistic effect (Lowe 1989; Finkelstein and Salesin 1994; Taubin 1995)\nand used for recognition (Belongie, Malik, and Puzicha 2002; Tek and Kimia 2003; Sebastian\nand Kimia 2005).\nAn early, well-regarded paper on straight line extraction in images was written by Burns,\nHanson, and Riseman (1986). More recent techniques often combine line detection with van-\nishing point detection (Quan and Mohr 1989; Collins and Weiss 1990; Brillaut-O’Mahoney\n1991; McLean and Kotturi 1995; Becker and Bove 1995; Shufelt 1999; Tuytelaars, Van Gool,\nand Proesmans 1997; Schaffalitzky and Zisserman 2000; Antone and Teller 2002; Rother\n2002; Koˇseck´a and Zhang 2005; Pﬂugfelder 2008; Sinha, Steedly, Szeliski et al. 2008; Tardif\n2009).\n4.5 Exercises\nEx 4.1: Interest point detector\nImplement one or more keypoint detectors and compare\ntheir performance (with your own or with a classmate’s detector).\nPossible detectors:\n• Laplacian or Difference of Gaussian;\n• F¨orstner–Harris Hessian (try different formula variants given in (4.9–4.11));",
  "282": "260\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n• oriented/steerable ﬁlter, looking for either second-order high second response or two\nedges in a window (Koethe 2003), as discussed in Section 4.1.1.\nOther detectors are described by Mikolajczyk, Tuytelaars, Schmid et al. (2005); Tuytelaars\nand Mikolajczyk (2007). Additional optional steps could include:\n1. Compute the detections on a sub-octave pyramid and ﬁnd 3D maxima.\n2. Find local orientation estimates using steerable ﬁlter responses or a gradient histogram-\nming method.\n3. Implement non-maximal suppression, such as the adaptive technique of Brown, Szeliski,\nand Winder (2005).\n4. Vary the window shape and size (pre-ﬁlter and aggregation).\nTo test for repeatability, download the code from http://www.robots.ox.ac.uk/∼vgg/research/\nafﬁne/ (Mikolajczyk, Tuytelaars, Schmid et al. 2005; Tuytelaars and Mikolajczyk 2007) or\nsimply rotate or shear your own test images. (Pick a domain you may want to use later, e.g.,\nfor outdoor stitching.)\nBe sure to measure and report the stability of your scale and orientation estimates.\nEx 4.2: Interest point descriptor\nImplement one or more descriptors (steered to local scale\nand orientation) and compare their performance (with your own or with a classmate’s detec-\ntor).\nSome possible descriptors include\n• contrast-normalized patches (Brown, Szeliski, and Winder 2005);\n• SIFT (Lowe 2004);\n• GLOH (Mikolajczyk and Schmid 2005);\n• DAISY (Winder and Brown 2007; Tola, Lepetit, and Fua 2010).\nOther detectors are described by Mikolajczyk and Schmid (2005).\nEx 4.3: ROC curve computation\nGiven a pair of curves (histograms) plotting the number\nof matching and non-matching features as a function of Euclidean distance d as shown in\nFigure 4.23b, derive an algorithm for plotting a ROC curve (Figure 4.23a). In particular, let\nt(d) be the distribution of true matches and f(d) be the distribution of (false) non-matches.\nWrite down the equations for the ROC, i.e., TPR(FPR), and the AUC.\n(Hint: Plot the cumulative distributions T(d) =\nR\nt(d) and F(d) =\nR\nf(d) and see if\nthese help you derive the TPR and FPR at a given threshold θ.)",
  "283": "4.5 Exercises\n261\nEx 4.4: Feature matcher\nAfter extracting features from a collection of overlapping or dis-\ntorted images,10 match them up by their descriptors either using nearest neighbor matching\nor a more efﬁcient matching strategy such as a k-d tree.\nSee whether you can improve the accuracy of your matches using techniques such as the\nnearest neighbor distance ratio.\nEx 4.5: Feature tracker\nInstead of ﬁnding feature points independently in multiple images\nand then matching them, ﬁnd features in the ﬁrst image of a video or image sequence and\nthen re-locate the corresponding points in the next frames using either search and gradient\ndescent (Shi and Tomasi 1994) or learned feature detectors (Lepetit, Pilet, and Fua 2006;\nFossati, Dimitrijevic, Lepetit et al. 2007). When the number of tracked points drops below a\nthreshold or new regions in the image become visible, ﬁnd additional points to track.\n(Optional) Winnow out incorrect matches by estimating a homography (6.19–6.23) or\nfundamental matrix (Section 7.2.1).\n(Optional) Reﬁne the accuracy of your matches using the iterative registration algorithm\ndescribed in Section 8.2 and Exercise 8.2.\nEx 4.6: Facial feature tracker\nApply your feature tracker to tracking points on a person’s\nface, either manually initialized to interesting locations such as eye corners or automatically\ninitialized at interest points.\n(Optional) Match features between two people and use these features to perform image\nmorphing (Exercise 3.25).\nEx 4.7: Edge detector\nImplement an edge detector of your choice. Compare its perfor-\nmance to that of your classmates’ detectors or code downloaded from the Internet.\nA simple but well-performing sub-pixel edge detector can be created as follows:\n1. Blur the input image a little,\nBσ(x) = Gσ(x) ∗I(x).\n2. Construct a Gaussian pyramid (Exercise 3.19),\nP = Pyramid{Bσ(x)}\n3. Subtract an interpolated coarser-level pyramid image from the original resolution blurred\nimage,\nS(x) = Bσ(x) −P.InterpolatedLevel(L).\n10 http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/.",
  "284": "262\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nstruct SEdgel {\nfloat e[2][2];\n// edgel endpoints (zero crossing)\nfloat x, y;\n// sub-pixel edge position (midpoint)\nfloat n_x, n_y;\n// orientation, as normal vector\nfloat theta;\n// orientation, as angle (degrees)\nfloat length;\n// length of edgel\nfloat strength;\n// strength of edgel (gradient magnitude)\n};\nstruct SLine : public SEdgel {\nfloat line_length;\n// length of line (est. from ellipsoid)\nfloat sigma;\n// estimated std. dev. of edgel noise\nfloat r;\n// line equation: x * n_y - y * n_x = r\n};\nFigure 4.48 A potential C++ structure for edgel and line elements.\n4. For each quad of pixels, {(i, j), (i + 1, j), (i, j + 1), (i + 1, j + 1)}, count the number\nof zero crossings along the four edges.\n5. When there are exactly two zero crossings, compute their locations using (4.25) and\nstore these edgel endpoints along with the midpoint in the edgel structure (Figure 4.48).\n6. For each edgel, compute the local gradient by taking the horizontal and vertical differ-\nences between the values of S along the zero crossing edges.\n7. Store the magnitude of this gradient as the edge strength and either its orientation or\nthat of the segment joining the edgel endpoints as the edge orientation.\n8. Add the edgel to a list of edgels or store it in a 2D array of edgels (addressed by pixel\ncoordinates).\nFigure 4.48 shows a possible representation for each computed edgel.\nEx 4.8: Edge linking and thresholding\nLink up the edges computed in the previous exer-\ncise into chains and optionally perform thresholding with hysteresis.\nThe steps may include:\n1. Store the edgels either in a 2D array (say, an integer image with indices into the edgel\nlist) or pre-sort the edgel list ﬁrst by (integer) x coordinates and then y coordinates, for\nfaster neighbor ﬁnding.",
  "285": "4.5 Exercises\n263\n2. Pick up an edgel from the list of unlinked edgels and ﬁnd its neighbors in both direc-\ntions until no neighbor is found or a closed contour is obtained. Flag edgels as linked\nas you visit them and push them onto your list of linked edgels.\n3. Alternatively, generalize a previously developed connected component algorithm (Ex-\nercise 3.14) to perform the linking in just two raster passes.\n4. (Optional) Perform hysteresis-based thresholding (Canny 1986). Use two thresholds\n”hi” and ”lo” for the edge strength. A candidate edgel is considered an edge if either\nits strength is above the ”hi” threshold or its strength is above the ”lo” threshold and it\nis (recursively) connected to a previously detected edge.\n5. (Optional) Link together contours that have small gaps but whose endpoints have sim-\nilar orientations.\n6. (Optional) Find junctions between adjacent contours, e.g., using some of the ideas (or\nreferences) from Maire, Arbelaez, Fowlkes et al. (2008).\nEx 4.9: Contour matching\nConvert a closed contour (linked edgel list) into its arc-length\nparameterization and use this to match object outlines.\nThe steps may include:\n1. Walk along the contour and create a list of (xi, yi, si) triplets, using the arc-length\nformula\nsi+1 = si + ∥xi+1 −xi∥.\n(4.32)\n2. Resample this list onto a regular set of (xj, yj, j) samples using linear interpolation of\neach segment.\n3. Compute the average values of x and y, i.e., x and y and subtract them from your\nsampled curve points.\n4. Resample the original (xi, yi, si) piecewise-linear function onto a length-independent\nset of samples, say j ∈[0, 1023]. (Using a length which is a power of two makes\nsubsequent Fourier transforms more convenient.)\n5. Compute the Fourier transform of the curve, treating each (x, y) pair as a complex\nnumber.\n6. To compare two curves, ﬁt a linear equation to the phase difference between the two\ncurves. (Careful: phase wraps around at 360◦. Also, you may wish to weight samples\nby their Fourier spectrum magnitude—see Section 8.1.2.)",
  "286": "264\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7. (Optional) Prove that the constant phase component corresponds to the temporal shift\nin s, while the linear component corresponds to rotation.\nOf course, feel free to try any other curve descriptor and matching technique from the com-\nputer vision literature (Tek and Kimia 2003; Sebastian and Kimia 2005).\nEx 4.10: Jigsaw puzzle solver—challenging\nWrite a program to automatically solve a jig-\nsaw puzzle from a set of scanned puzzle pieces. Your software may include the following\ncomponents:\n1. Scan the pieces (either face up or face down) on a ﬂatbed scanner with a distinctively\ncolored background.\n2. (Optional) Scan in the box top to use as a low-resolution reference image.\n3. Use color-based thresholding to isolate the pieces.\n4. Extract the contour of each piece using edge ﬁnding and linking.\n5. (Optional) Re-represent each contour using an arc-length or some other re-parameterization.\nBreak up the contours into meaningful matchable pieces. (Is this hard?)\n6. (Optional) Associate color values with each contour to help in the matching.\n7. (Optional) Match pieces to the reference image using some rotationally invariant fea-\nture descriptors.\n8. Solve a global optimization or (backtracking) search problem to snap pieces together\nand place them in the correct location relative to the reference image.\n9. Test your algorithm on a succession of more difﬁcult puzzles and compare your results\nwith those of others.\nEx 4.11: Successive approximation line detector\nImplement a line simpliﬁcation algorithm\n(Section 4.3.1) (Ramer 1972; Douglas and Peucker 1973) to convert a hand-drawn curve (or\nlinked edge image) into a small set of polylines.\n(Optional) Re-render this curve using either an approximating or interpolating spline or\nBezier curve (Szeliski and Ito 1986; Bartels, Beatty, and Barsky 1987; Farin 1996).\nEx 4.12: Hough transform line detector\nImplement a Hough transform for ﬁnding lines\nin images:",
  "287": "4.5 Exercises\n265\n1. Create an accumulator array of the appropriate user-speciﬁed size and clear it. The user\ncan specify the spacing in degrees between orientation bins and in pixels between dis-\ntance bins. The array can be allocated as integer (for simple counts), ﬂoating point (for\nweighted counts), or as an array of vectors for keeping back pointers to the constituent\nedges.\n2. For each detected edgel at location (x, y) and orientation θ = tan−1 ny/nx, compute\nthe value of\nd = xnx + yny\n(4.33)\nand increment the accumulator corresponding to (θ, d).\n(Optional) Weight the vote of each edge by its length (see Exercise 4.7) or the strength\nof its gradient.\n3. (Optional) Smooth the scalar accumulator array by adding in values from its immediate\nneighbors. This can help counteract the discretization effect of voting for only a single\nbin—see Exercise 3.7.\n4. Find the largest peaks (local maxima) in the accumulator corresponding to lines.\n5. (Optional) For each peak, re-ﬁt the lines to the constituent edgels, using total least\nsquares (Appendix A.2). Use the original edgel lengths or strength weights to weight\nthe least squares ﬁt, as well as the agreement between the hypothesized line orienta-\ntion and the edgel orientation. Determine whether these heuristics help increase the\naccuracy of the ﬁt.\n6. After ﬁtting each peak, zero-out or eliminate that peak and its adjacent bins in the array,\nand move on to the next largest peak.\nTest out your Hough transform on a variety of images taken indoors and outdoors, as well\nas checkerboard calibration patterns.\nFor checkerboard patterns, you can modify your Hough transform by collapsing antipodal\nbins (θ ± 180◦, −d) with (θ, d) to ﬁnd lines that do not care about polarity changes. Can you\nthink of examples in real-world images where this might be desirable as well?\nEx 4.13: Line ﬁtting uncertainty\nEstimate the uncertainty (covariance) in your line ﬁt us-\ning uncertainty analysis.\n1. After determining which edgels belong to the line segment (using either successive\napproximation or Hough transform), re-ﬁt the line segment using total least squares\n(Van Huffel and Vandewalle 1991; Van Huffel and Lemmerling 2002), i.e., ﬁnd the\nmean or centroid of the edgels and then use eigenvalue analysis to ﬁnd the dominant\norientation.",
  "288": "266\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n2. Compute the perpendicular errors (deviations) to the line and robustly estimate the\nvariance of the ﬁtting noise using an estimator such as MAD (Appendix B.3).\n3. (Optional) re-ﬁt the line parameters by throwing away outliers or using a robust norm\nor inﬂuence function.\n4. Estimate the error in the perpendicular location of the line segment and its orientation.\nEx 4.14: Vanishing points\nCompute the vanishing points in an image using one of the tech-\nniques described in Section 4.3.3 and optionally reﬁne the original line equations associated\nwith each vanishing point. Your results can be used later to track a target (Exercise 6.5) or\nreconstruct architecture (Section 12.6.1).\nEx 4.15: Vanishing point uncertainty\nPerform an uncertainty analysis on your estimated\nvanishing points. You will need to decide how to represent your vanishing point, e.g., homo-\ngeneous coordinates on a sphere, to handle vanishing points near inﬁnity.\nSee the discussion of Bingham distributions by Collins and Weiss (1990) for some ideas.",
  "289": "Chapter 5\nSegmentation\n5.1\nActive contours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n5.1.1\nSnakes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n5.1.2\nDynamic snakes and CONDENSATION . . . . . . . . . . . . . . . . 276\n5.1.3\nScissors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\n5.1.4\nLevel Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n5.1.5\nApplication: Contour tracking and rotoscoping . . . . . . . . . . . . 282\n5.2\nSplit and merge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n5.2.1\nWatershed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n5.2.2\nRegion splitting (divisive clustering) . . . . . . . . . . . . . . . . . . 286\n5.2.3\nRegion merging (agglomerative clustering) . . . . . . . . . . . . . . 286\n5.2.4\nGraph-based segmentation . . . . . . . . . . . . . . . . . . . . . . . 286\n5.2.5\nProbabilistic aggregation . . . . . . . . . . . . . . . . . . . . . . . . 288\n5.3\nMean shift and mode ﬁnding . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n5.3.1\nK-means and mixtures of Gaussians . . . . . . . . . . . . . . . . . . 289\n5.3.2\nMean shift\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292\n5.4\nNormalized cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\n5.5\nGraph cuts and energy-based methods . . . . . . . . . . . . . . . . . . . . . 300\n5.5.1\nApplication: Medical image segmentation . . . . . . . . . . . . . . . 304\n5.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\n5.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306",
  "290": "268\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 5.1\nSome popular image segmentation techniques: (a) active contours (Isard and\nBlake 1998) c⃝1998 Springer; (b) level sets (Cremers, Rousson, and Deriche 2007) c⃝\n2007 Springer; (c) graph-based merging (Felzenszwalb and Huttenlocher 2004b) c⃝2004\nSpringer; (d) mean shift (Comaniciu and Meer 2002) c⃝2002 IEEE; (e) texture and interven-\ning contour-based normalized cuts (Malik, Belongie, Leung et al. 2001) c⃝2001 Springer;\n(f) binary MRF solved using graph cuts (Boykov and Funka-Lea 2006) c⃝2006 Springer.",
  "291": "5 Segmentation\n269\nImage segmentation is the task of ﬁnding groups of pixels that “go together”. In statistics, this\nproblem is known as cluster analysis and is a widely studied area with hundreds of different\nalgorithms (Jain and Dubes 1988; Kaufman and Rousseeuw 1990; Jain, Duin, and Mao 2000;\nJain, Topchy, Law et al. 2004).\nIn computer vision, image segmentation is one of the oldest and most widely studied prob-\nlems (Brice and Fennema 1970; Pavlidis 1977; Riseman and Arbib 1977; Ohlander, Price,\nand Reddy 1978; Rosenfeld and Davis 1979; Haralick and Shapiro 1985). Early techniques\ntend to use region splitting or merging (Brice and Fennema 1970; Horowitz and Pavlidis 1976;\nOhlander, Price, and Reddy 1978; Pavlidis and Liow 1990), which correspond to divisive and\nagglomerative algorithms in the clustering literature (Jain, Topchy, Law et al. 2004). More\nrecent algorithms often optimize some global criterion, such as intra-region consistency and\ninter-region boundary lengths or dissimilarity (Leclerc 1989; Mumford and Shah 1989; Shi\nand Malik 2000; Comaniciu and Meer 2002; Felzenszwalb and Huttenlocher 2004b; Cremers,\nRousson, and Deriche 2007).\nWe have already seen examples of image segmentation in Sections 3.3.2 and 3.7.2. In\nthis chapter, we review some additional techniques that have been developed for image seg-\nmentation. These include algorithms based on active contours (Section 5.1) and level sets\n(Section 5.1.4), region splitting and merging (Section 5.2), mean shift (mode ﬁnding) (Sec-\ntion 5.3), normalized cuts (splitting based on pixel similarity metrics) (Section 5.4), and bi-\nnary Markov random ﬁelds solved using graph cuts (Section 5.5). Figure 5.1 shows some\nexamples of these techniques applied to different images.\nSince the literature on image segmentation is so vast, a good way to get a handle on some\nof the better performing algorithms is to look at experimental comparisons on human-labeled\ndatabases (Arbel´aez, Maire, Fowlkes et al. 2010). The best known of these is the Berkeley\nSegmentation Dataset and Benchmark1 (Martin, Fowlkes, Tal et al. 2001), which consists\nof 1000 images from a Corel image dataset that were hand-labeled by 30 human subjects.\nMany of the more recent image segmentation algorithms report comparative results on this\ndatabase. For example, Unnikrishnan, Pantofaru, and Hebert (2007) propose new metrics\nfor comparing such algorithms. Estrada and Jepson (2009) compare four well-known seg-\nmentation algorithms on the Berkeley data set and conclude that while their own SE-MinCut\nalgorithm (Estrada, Jepson, and Chennubhotla 2004) algorithm outperforms the others by a\nsmall margin, there still exists a wide gap between automated and human segmentation per-\nformance.2 A new database of foreground and background segmentations, used by Alpert,\nGalun, Basri et al. (2007), is also available.3\n1 http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/\n2 An interesting observation about their ROC plots is that automated techniques cluster tightly along similar\ncurves, but human performance is all over the map.\n3 http://www.wisdom.weizmann.ac.il/∼vision/Seg Evaluation DB/index.html",
  "292": "270\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n5.1 Active contours\nWhile lines, vanishing points, and rectangles are commonplace in the man-made world,\ncurves corresponding to object boundaries are even more common, especially in the natural\nenvironment. In this section, we describe three related approaches to locating such boundary\ncurves in images.\nThe ﬁrst, originally called snakes by its inventors (Kass, Witkin, and Terzopoulos 1988)\n(Section 5.1.1), is an energy-minimizing, two-dimensional spline curve that evolves (moves)\ntowards image features such as strong edges. The second, intelligent scissors (Mortensen\nand Barrett 1995) (Section 5.1.3), allow the user to sketch in real time a curve that clings to\nobject boundaries. Finally, level set techniques (Section 5.1.4) evolve the curve as the zero-\nset of a characteristic function, which allows them to easily change topology and incorporate\nregion-based statistics.\nAll three of these are examples of active contours (Blake and Isard 1998; Mortensen\n1999), since these boundary detectors iteratively move towards their ﬁnal solution under the\ncombination of image and optional user-guidance forces.\n5.1.1 Snakes\nSnakes are a two-dimensional generalization of the 1D energy-minimizing splines ﬁrst intro-\nduced in Section 3.7.1,\nEint =\nZ\nα(s)∥f s(s)∥2 + β(s)∥f ss(s)∥2 ds,\n(5.1)\nwhere s is the arc-length along the curve f(s) = (x(s), y(s)) and α(s) and β(s) are ﬁrst-\nand second-order continuity weighting functions analogous to the s(x, y) and c(x, y) terms\nintroduced in (3.100–3.101). We can discretize this energy by sampling the initial curve\nposition evenly along its length (Figure 4.35) to obtain\nEint\n=\nX\ni\nα(i)∥f(i + 1) −f(i)∥2/h2\n(5.2)\n+ β(i)∥f(i + 1) −2f(i) + f(i −1)∥2/h4,\nwhere h is the step size, which can be neglected if we resample the curve along its arc-length\nafter each iteration.\nIn addition to this internal spline energy, a snake simultaneously minimizes external\nimage-based and constraint-based potentials. The image-based potentials are the sum of sev-\neral terms\nEimage = wlineEline + wedgeEedge + wtermEterm,\n(5.3)",
  "293": "5.1 Active contours\n271\n(a)\n(b)\nFigure 5.2 Snakes (Kass, Witkin, and Terzopoulos 1988) c⃝1988 Springer: (a) the “snake\npit” for interactively controlling shape; (b) lip tracking.\nwhere the line term attracts the snake to dark ridges, the edge term attracts it to strong gradi-\nents (edges), and the term term attracts it to line terminations. In practice, most systems only\nuse the edge term, which can either be directly proportional to the image gradients,\nEedge =\nX\ni\n−∥∇I(f(i))∥2,\n(5.4)\nor to a smoothed version of the image Laplacian,\nEedge =\nX\ni\n−|(Gσ ∗∇2I)(f(i))|2.\n(5.5)\nPeople also sometimes extract edges and then use a distance map to the edges as an alternative\nto these two originally proposed potentials.\nIn interactive applications, a variety of user-placed constraints can also be added, e.g.,\nattractive (spring) forces towards anchor points d(i),\nEspring = ki∥f(i) −d(i)∥2,\n(5.6)\nas well as repulsive 1/r (“volcano”) forces (Figure 5.2a). As the snakes evolve by minimiz-\ning their energy, they often “wiggle” and “slither”, which accounts for their popular name.\nFigure 5.2b shows snakes being used to track a person’s lips.\nBecause regular snakes have a tendency to shrink (Exercise 5.1), it is usually better to\ninitialize them by drawing the snake outside the object of interest to be tracked. Alterna-\ntively, an expansion ballooning force can be added to the dynamics (Cohen and Cohen 1993),\nessentially moving each point outwards along its normal.\nTo efﬁciently solve the sparse linear system arising from snake energy minimization, a\nsparse direct solver (Appendix A.4) can be used, since the linear system is essentially penta-\ndiagonal.4 Snake evolution is usually implemented as an alternation between this linear sys-\n4 A closed snake has a Toeplitz matrix form, which can still be factored and solved in O(N) time.",
  "294": "272\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 5.3 Elastic net: The open squares indicate the cities and the closed squares linked by\nstraight line segments are the tour points. The blue circles indicate the approximate extent of\nthe attraction force of each city, which is reduced over time. Under the Bayesian interpretation\nof the elastic net, the blue circles correspond to one standard deviation of the circular Gaussian\nthat generates each city from some unknown tour point.\ntem solution and the linearization of non-linear constraints such as edge energy. A more direct\nway to ﬁnd a global energy minimum is to use dynamic programming (Amini, Weymouth,\nand Jain 1990; Williams and Shah 1992), but this is not often used in practice, since it has\nbeen superseded by even more efﬁcient or interactive algorithms such as intelligent scissors\n(Section 5.1.3) and GrabCut (Section 5.5).\nElastic nets and slippery springs\nAn interesting variant on snakes, ﬁrst proposed by Durbin and Willshaw (1987) and later\nre-formulated in an energy-minimizing framework by Durbin, Szeliski, and Yuille (1989), is\nthe elastic net formulation of the Traveling Salesman Problem (TSP). Recall that in a TSP,\nthe salesman must visit each city once while minimizing the total distance traversed. A snake\nthat is constrained to pass through each city could solve this problem (without any optimality\nguarantees) but it is impossible to tell ahead of time which snake control point should be\nassociated with each city.\nInstead of having a ﬁxed constraint between snake nodes and cities, as in (5.6), a city is\nassumed to pass near some point along the tour (Figure 5.3). In a probabilistic interpretation,\neach city is generated as a mixture of Gaussians centered at each tour point,\np(d(j)) =\nX\ni\npij with pij = e−d2\nij/(2σ2)\n(5.7)\nwhere σ is the standard deviation of the Gaussian and\ndij = ∥f(i) −d(j)∥\n(5.8)",
  "295": "5.1 Active contours\n273\nis the Euclidean distance between a tour point f(i) and a city location d(j). The correspond-\ning data ﬁtting energy (negative log likelihood) is\nEslippery = −\nX\nj\nlog p(d(j)) = −\nX\nj\nlog\nhX\ne−∥f (i)−d(j)∥2/2σ2i\n.\n(5.9)\nThis energy derives its name from the fact that, unlike a regular spring, which couples a\ngiven snake point to a given constraint (5.6), this alternative energy deﬁnes a slippery spring\nthat allows the association between constraints (cities) and curve (tour) points to evolve over\ntime (Szeliski 1989). Note that this is a soft variant of the popular iterated closest point\ndata constraint that is often used in ﬁtting or aligning surfaces to data points or to each other\n(Section 12.2.1) (Besl and McKay 1992; Zhang 1994).\nTo compute a good solution to the TSP, the slippery spring data association energy is\ncombined with a regular ﬁrst-order internal smoothness energy (5.3) to deﬁne the cost of a\ntour. The tour f(s) is initialized as a small circle around the mean of the city points and σ is\nprogressively lowered (Figure 5.3). For large σ values, the tour tries to stay near the centroid\nof the points but as σ decreases each city pulls more and more strongly on its closest tour\npoints (Durbin, Szeliski, and Yuille 1989). In the limit as σ →0, each city is guaranteed to\ncapture at least one tour point and the tours between subsequent cites become straight lines.\nSplines and shape priors\nWhile snakes can be very good at capturing the ﬁne and irregular detail in many real-world\ncontours, they sometimes exhibit too many degrees of freedom, making it more likely that\nthey can get trapped in local minima during their evolution.\nOne solution to this problem is to control the snake with fewer degrees of freedom through\nthe use of B-spline approximations (Menet, Saint-Marc, and Medioni 1990b,a; Cipolla and\nBlake 1990). The resulting B-snake can be written as\nf(s) =\nX\nk\nBk(s)xk\n(5.10)\nor in discrete form as\nF = BX\n(5.11)\nwith\nF =\n\n\nf T (0)\n...\nf T (N)\n\n, B =\n\n\nB0(s0)\n. . .\nBK(s0)\n...\n...\n...\nB0(sN)\n. . .\nBK(sN)\n\n, and X =\n\n\nxT (0)\n...\nxT (K)\n\n.\n(5.12)",
  "296": "274\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 5.4\nPoint distribution model for a set of resistors (Cootes, Cooper, Taylor et al.\n1995) c⃝1995 Elsevier: (a) set of input resistor shapes; (b) assignment of control points\nto the boundary; (c) distribution (scatter plot) of point locations; (d) ﬁrst (largest) mode of\nvariation in the ensemble shapes.\nIf the object being tracked or recognized has large variations in location, scale, or ori-\nentation, these can be modeled as an additional transformation on the control points, e.g.,\nx′\nk = sRxk + t (2.18), which can be estimated at the same time as the values of the control\npoints. Alternatively, separate detection and alignment stages can be run to ﬁrst localize and\norient the objects of interest (Cootes, Cooper, Taylor et al. 1995).\nIn a B-snake, because the snake is controlled by fewer degrees of freedom, there is less\nneed for the internal smoothness forces used with the original snakes, although these can still\nbe derived and implemented using ﬁnite element analysis, i.e., taking derivatives and integrals\nof the B-spline basis functions (Terzopoulos 1983; Bathe 2007).\nIn practice, it is more common to estimate a set of shape priors on the typical distribution\nof the control points {xk} (Cootes, Cooper, Taylor et al. 1995). Consider the set of resistor\nshapes shown in Figure 5.4a. If we describe each contour with the set of control points\nshown in Figure 5.4b, we can plot the distribution of each point in a scatter plot, as shown in\nFigure 5.4c.\nOne potential way of describing this distribution would be by the location ¯xk and 2D\ncovariance Ck of each individual point xk. These could then be turned into a quadratic\npenalty (prior energy) on the point location,\nEloc(xk) = 1\n2(xk −¯xk)T C−1\nk (xk −¯xk).\n(5.13)\nIn practice, however, the variation in point locations is usually highly correlated.\nA preferable approach is to estimate the joint covariance of all the points simultaneously.\nFirst, concatenate all of the point locations {xk} into a single vector x, e.g., by interleaving\nthe x and y locations of each point. The distribution of these vectors across all training",
  "297": "5.1 Active contours\n275\n(a)\n(b)\nFigure 5.5 Active Shape Model (ASM): (a) the effect of varying the ﬁrst four shape param-\neters for a set of faces (Cootes, Taylor, Lanitis et al. 1993) c⃝1993 IEEE; (b) searching for\nthe strongest gradient along the normal to each control point (Cootes, Cooper, Taylor et al.\n1995) c⃝1995 Elsevier.\nexamples (Figure 5.4a) can be described with a mean ¯x and a covariance\nC = 1\nP\nX\np\n(xp −¯x)(xp −¯x)T ,\n(5.14)\nwhere xp are the P training examples. Using eigenvalue analysis (Appendix A.1.2), which is\nalso known as Principal Component Analysis (PCA) (Appendix B.1.1), the covariance matrix\ncan be written as,\nC = Φ diag(λ0 . . . λK−1) ΦT .\n(5.15)\nIn most cases, the likely appearance of the points can be modeled using only a few eigen-\nvectors with the largest eigenvalues. The resulting point distribution model (Cootes, Taylor,\nLanitis et al. 1993; Cootes, Cooper, Taylor et al. 1995) can be written as\nx = ¯x + ˆΦ b,\n(5.16)\nwhere b is an M ≪K element shape parameter vector and ˆΦ are the ﬁrst m columns of Φ.\nTo constrain the shape parameters to reasonable values, we can use a quadratic penalty of the\nform\nEshape = 1\n2bT diag(λ0 . . . λM−1) b =\nX\nm\nb2\nm/2λm.\n(5.17)\nAlternatively, the range of allowable bm values can be limited to some range, e.g., |bm| ≤\n3√λm (Cootes, Cooper, Taylor et al. 1995). Alternative approaches for deriving a set of\nshape vectors are reviewed by Isard and Blake (1998).",
  "298": "276\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nVarying the individual shape parameters bm over the range −2√λm ≤2√λm can give\na good indication of the expected variation in appearance, as shown in Figure 5.4d. Another\nexample, this time related to face contours, is shown in Figure 5.5a.\nIn order to align a point distribution model with an image, each control point searches\nin a direction normal to the contour to ﬁnd the most likely corresponding image edge point\n(Figure 5.5b). These individual measurements can be combined with priors on the shape\nparameters (and, if desired, position, scale, and orientation parameters) to estimate a new set\nof parameters. The resulting Active Shape Model (ASM) can be iteratively minimized to ﬁt\nimages to non-rigidly deforming objects such as medical images or body parts such as hands\n(Cootes, Cooper, Taylor et al. 1995). The ASM can also be combined with a PCA analysis of\nthe underlying gray-level distribution to create an Active Appearance Model (AAM) (Cootes,\nEdwards, and Taylor 2001), which we discuss in more detail in Section 14.2.2.\n5.1.2 Dynamic snakes and CONDENSATION\nIn many applications of active contours, the object of interest is being tracked from frame\nto frame as it deforms and evolves. In this case, it makes sense to use estimates from the\nprevious frame to predict and constrain the new estimates.\nOne way to do this is to use Kalman ﬁltering, which results in a formulation called Kalman\nsnakes (Terzopoulos and Szeliski 1992; Blake, Curwen, and Zisserman 1993). The Kalman\nﬁlter is based on a linear dynamic model of shape parameter evolution,\nxt = Axt−1 + wt,\n(5.18)\nwhere xt and xt−1 are the current and previous state variables, A is the linear transition\nmatrix, and w is a noise (perturbation) vector, which is often modeled as a Gaussian (Gelb\n1974). The matrices A and the noise covariance can be learned ahead of time by observing\ntypical sequences of the object being tracked (Blake and Isard 1998).\nThe qualitative behavior of the Kalman ﬁlter can be seen in Figure 5.6a. The linear dy-\nnamic model causes a deterministic change (drift) in the previous estimate, while the process\nnoise (perturbation) causes a stochastic diffusion that increases the system entropy (lack of\ncertainty). New measurements from the current frame restore some of the certainty (peaked-\nness) in the updated estimate.\nIn many situations, however, such as when tracking in clutter, a better estimate for the\ncontour can be obtained if we remove the assumptions that the distribution are Gaussian,\nwhich is what the Kalman ﬁlter requires. In this case, a general multi-modal distribution is\npropagated, as shown in Figure 5.6b. In order to model such multi-modal distributions, Isard\nand Blake (1998) introduced the use of particle ﬁltering to the computer vision community.5\n5 Alternatives to modeling multi-modal distributions include mixtures of Gaussians (Bishop 2006) and multiple",
  "299": "5.1 Active contours\n277\n(a)\n(b)\nFigure 5.6\nProbability density propagation (Isard and Blake 1998) c⃝1998 Springer. At\nthe beginning of each estimation step, the probability density is updated according to the\nlinear dynamic model (deterministic drift) and its certainty is reduced due to process noise\n(stochastic diffusion). New measurements introduce additional information that helps reﬁne\nthe current estimate. (a) The Kalman ﬁlter models the distributions as uni-modal, i.e., using a\nmean and covariance. (b) Some applications require more general multi-modal distributions.",
  "300": "278\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 5.7 Factored sampling using particle ﬁlter in the CONDENSATION algorithm (Is-\nard and Blake 1998) c⃝1998 Springer: (a) each density distribution is represented using a\nsuperposition of weighted particles; (b) the drift-diffusion-measurement cycle implemented\nusing random sampling, perturbation, and re-weighting stages.",
  "301": "5.1 Active contours\n279\n(a)\n(b)\n(c)\nFigure 5.8\nHead tracking using CONDENSATION (Isard and Blake 1998)\nc⃝1998\nSpringer: (a) sample set representation of head estimate distribution; (b) multiple measure-\nments at each control vertex location; (c) multi-hypothesis tracking over time.\nParticle ﬁltering techniques represent a probability distribution using a collection of weighted\npoint samples (Figure 5.7a) (Andrieu, de Freitas, Doucet et al. 2003; Bishop 2006; Koller\nand Friedman 2009). To update the locations of the samples according to the linear dy-\nnamics (deterministic drift), the centers of the samples are updated according to (5.18) and\nmultiple samples are generated for each point (Figure 5.7b). These are then perturbed to\naccount for the stochastic diffusion, i.e., their locations are moved by random vectors taken\nfrom the distribution of w.6 Finally, the weights of these samples are multiplied by the mea-\nsurement probability density, i.e., we take each sample and measure its likelihood given the\ncurrent (new) measurements. Because the point samples represent and propagate conditional\nestimates of the multi-modal density, Isard and Blake (1998) dubbed their algorithm CONdi-\ntional DENSity propagATION or CONDENSATION.\nFigure 5.8a shows what a factored sample of a head tracker might look like, drawing\na red B-spline contour for each of (a subset of) the particles being tracked. Figure 5.8b\nshows why the measurement density itself is often multi-modal: the locations of the edges\nperpendicular to the spline curve can have multiple local maxima due to background clutter.\nFinally, Figure 5.8c shows the temporal evolution of the conditional density (x coordinate of\nthe head and shoulder tracker centroid) as it tracks several people over time.\nhypothesis tracking (Bar-Shalom and Fortmann 1988; Cham and Rehg 1999).\n6 Note that because of the structure of these steps, non-linear dynamics and non-Gaussian noise can be used.",
  "302": "280\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 5.9\nIntelligent scissors: (a) as the mouse traces the white path, the scissors follow\nthe orange path along the object boundary (the green curves show intermediate positions)\n(Mortensen and Barrett 1995) c⃝1995 ACM; (b) regular scissors can sometimes jump to a\nstronger (incorrect) boundary; (c) after training to the previous segment, similar edge proﬁles\nare preferred (Mortensen and Barrett 1998) c⃝1995 Elsevier.\n5.1.3 Scissors\nActive contours allow a user to roughly specify a boundary of interest and have the system\nevolve the contour towards a more accurate location as well as track it over time. The results\nof this curve evolution, however, may be unpredictable and may require additional user-based\nhints to achieve the desired result.\nAn alternative approach is to have the system optimize the contour in real time as the\nuser is drawing (Mortensen 1999). The intelligent scissors system developed by Mortensen\nand Barrett (1995) does just that. As the user draws a rough outline (the white curve in\nFigure 5.9a), the system computes and draws a better curve that clings to high-contrast edges\n(the orange curve).\nTo compute the optimal curve path (live-wire), the image is ﬁrst pre-processed to associate\nlow costs with edges (links between neighboring horizontal, vertical, and diagonal, i.e., N8\nneighbors) that are likely to be boundary elements. Their system uses a combination of zero-\ncrossing, gradient magnitudes, and gradient orientations to compute these costs.\nNext, as the user traces a rough curve, the system continuously recomputes the lowest-\ncost path between the starting seed point and the current mouse location using Dijkstra’s al-\ngorithm, a breadth-ﬁrst dynamic programming algorithm that terminates at the current target\nlocation.\nIn order to keep the system from jumping around unpredictably, the system will “freeze”\nthe curve to date (reset the seed point) after a period of inactivity. To prevent the live wire\nfrom jumping onto adjacent higher-contrast contours, the system also “learns” the intensity",
  "303": "5.1 Active contours\n281\n-1\n+1\nϕ = 0\nϕ∆\ng(I)\nFigure 5.10\nLevel set evolution for a geodesic active contour. The embedding function φ\nis updated based on the curvature of the underlying surface modulated by the edge/speed\nfunction g(I), as well as the gradient of g(I), thereby attracting it to strong edges.\nproﬁle under the current optimized curve, and uses this to preferentially keep the wire moving\nalong the same (or a similar looking) boundary (Figure 5.9b–c).\nSeveral extensions have been proposed to the basic algorithm, which works remarkably\nwell even in its original form. Mortensen and Barrett (1999) use tobogganing, which is a\nsimple form of watershed region segmentation, to pre-segment the image into regions whose\nboundaries become candidates for optimized curve paths. The resulting region boundaries\nare turned into a much smaller graph, where nodes are located wherever three or four regions\nmeet. The Dijkstra algorithm is then run on this reduced graph, resulting in much faster (and\noften more stable) performance. Another extension to intelligent scissors is to use a proba-\nbilistic framework that takes into account the current trajectory of the boundary, resulting in\na system called JetStream (P´erez, Blake, and Gangnet 2001).\nInstead of re-computing an optimal curve at each time instant, a simpler system can be\ndeveloped by simply “snapping” the current mouse position to the nearest likely boundary\npoint (Gleicher 1995). Applications of these boundary extraction techniques to image cutting\nand pasting are presented in Section 10.4.\n5.1.4 Level Sets\nA limitation of active contours based on parametric curves of the form f(s), e.g., snakes, B-\nsnakes, and CONDENSATION, is that it is challenging to change the topology of the curve\nas it evolves. (McInerney and Terzopoulos (1999, 2000) describe one approach to doing\nthis.) Furthermore, if the shape changes dramatically, curve reparameterization may also be\nrequired.\nAn alternative representation for such closed contours is to use a level set, where the zero-\ncrossing(s) of a characteristic (or signed distance (Section 3.3.3)) function deﬁne the curve.",
  "304": "282\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLevel sets evolve to ﬁt and track objects of interest by modifying the underlying embedding\nfunction (another name for this 2D function) φ(x, y) instead of the curve f(s) (Malladi,\nSethian, and Vemuri 1995; Sethian 1999; Sapiro 2001; Osher and Paragios 2003). To reduce\nthe amount of computation required, only a small strip (frontier) around the locations of the\ncurrent zero-crossing needs to updated at each step, which results in what are called fast\nmarching methods (Sethian 1999).\nAn example of an evolution equation is the geodesic active contour proposed by Caselles,\nKimmel, and Sapiro (1997) and Yezzi, Kichenassamy, Kumar et al. (1997),\ndφ\ndt\n=\n|∇φ|div\n\u0012\ng(I) ∇φ\n|∇φ|\n\u0013\n=\ng(I)|∇φ|div\n\u0012 ∇φ\n|∇φ|\n\u0013\n+ ∇g(I) · ∇φ,\n(5.19)\nwhere g(I) is a generalized version of the snake edge potential (5.5). To get an intuitive sense\nof the curve’s behavior, assume that the embedding function φ is a signed distance function\naway from the curve (Figure 5.10), in which case |φ| = 1. The ﬁrst term in Equation (5.19)\nmoves the curve in the direction of its curvature, i.e., it acts to straighten the curve, under\nthe inﬂuence of the modulation function g(I). The second term moves the curve down the\ngradient of g(I), encouraging the curve to migrate towards minima of g(I).\nWhile this level-set formulation can readily change topology, it is still susceptible to lo-\ncal minima, since it is based on local measurements such as image gradients. An alternative\napproach is to re-cast the problem in a segmentation framework, where the energy measures\nthe consistency of the image statistics (e.g., color, texture, motion) inside and outside the seg-\nmented regions (Cremers, Rousson, and Deriche 2007; Rousson and Paragios 2008; Houhou,\nThiran, and Bresson 2008). These approaches build on earlier energy-based segmentation\nframeworks introduced by Leclerc (1989), Mumford and Shah (1989), and Chan and Vese\n(1992), which are discussed in more detail in Section 5.5. Examples of such level-set seg-\nmentations are shown in Figure 5.11, which shows the evolution of the level sets from a series\nof distributed circles towards the ﬁnal binary segmentation.\nFor more information on level sets and their applications, please see the collection of\npapers edited by Osher and Paragios (2003) as well as the series of Workshops on Variational\nand Level Set Methods in Computer Vision (Paragios, Faugeras, Chan et al. 2005) and Special\nIssues on Scale Space and Variational Methods in Computer Vision (Paragios and Sgallari\n2009).\n5.1.5 Application: Contour tracking and rotoscoping\nActive contours can be used in a wide variety of object-tracking applications (Blake and Isard\n1998; Yilmaz, Javed, and Shah 2006). For example, they can be used to track facial features",
  "305": "5.1 Active contours\n283\n(a)\n(b)\nFigure 5.11\nLevel set segmentation (Cremers, Rousson, and Deriche 2007) c⃝2007\nSpringer: (a) grayscale image segmentation and (b) color image segmentation. Uni-variate\nand multi-variate Gaussians are used to model the foreground and background pixel dis-\ntributions. The initial circles evolve towards an accurate segmentation of foreground and\nbackground, adapting their topology as they evolve.\nfor performance-driven animation (Terzopoulos and Waters 1990; Lee, Terzopoulos, and Wa-\nters 1995; Parke and Waters 1996; Bregler, Covell, and Slaney 1997) (Figure 5.2b). They can\nalso be used to track heads and people, as shown in Figure 5.8, as well as moving vehicles\n(Paragios and Deriche 2000). Additional applications include medical image segmentation,\nwhere contours can be tracked from slice to slice in computerized tomography (3D medical\nimagery) (Cootes and Taylor 2001) or over time, as in ultrasound scans.\nAn interesting application that is closer to computer animation and visual effects is ro-\ntoscoping, which uses the tracked contours to deform a set of hand-drawn animations (or\nto modify or replace the original video frames).7 Agarwala, Hertzmann, Seitz et al. (2004)\npresent a system based on tracking hand-drawn B-spline contours drawn at selected keyframes,\nusing a combination of geometric and appearance-based criteria (Figure 5.12). They also pro-\nvide an excellent review of previous rotoscoping and image-based, contour-tracking systems.\n7 The term comes from a device (a rotoscope) that projected frames of a live-action ﬁlm underneath an acetate so\nthat artists could draw animations directly over the actors’ shapes.",
  "306": "284\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 5.12 Keyframe-based rotoscoping (Agarwala, Hertzmann, Seitz et al. 2004) c⃝2004\nACM: (a) original frames; (b) rotoscoped contours; (c) re-colored blouse; (d) rotoscoped\nhand-drawn animation.\nAdditional applications of rotoscoping (object contour detection and segmentation), such\nas cutting and pasting objects from one photograph into another, are presented in Section 10.4.\n5.2 Split and merge\nAs mentioned in the introduction to this chapter, the simplest possible technique for seg-\nmenting a grayscale image is to select a threshold and then compute connected components\n(Section 3.3.2). Unfortunately, a single threshold is rarely sufﬁcient for the whole image\nbecause of lighting and intra-object statistical variations.\nIn this section, we describe a number of algorithms that proceed either by recursively\nsplitting the whole image into pieces based on region statistics or, conversely, merging pixels\nand regions together in a hierarchical fashion. It is also possible to combine both splitting and\nmerging by starting with a medium-grain segmentation (in a quadtree representation) and\nthen allowing both merging and splitting operations (Horowitz and Pavlidis 1976; Pavlidis\nand Liow 1990).\n5.2.1 Watershed\nA technique related to thresholding, since it operates on a grayscale image, is watershed com-\nputation (Vincent and Soille 1991). This technique segments an image into several catchment\nbasins, which are the regions of an image (interpreted as a height ﬁeld or landscape) where",
  "307": "5.2 Split and merge\n285\n(a)\n(b)\n(c)\nFigure 5.13\nLocally constrained watershed segmentation (Beare 2006) c⃝2006 IEEE: (a)\noriginal confocal microscopy image with marked seeds (line segments); (b) standard water-\nshed segmentation; (c) locally constrained watershed segmentation.\nrain would ﬂow into the same lake. An efﬁcient way to compute such regions is to start ﬂood-\ning the landscape at all of the local minima and to label ridges wherever differently evolving\ncomponents meet. The whole algorithm can be implemented using a priority queue of pixels\nand breadth-ﬁrst search (Vincent and Soille 1991).8\nSince images rarely have dark regions separated by lighter ridges, watershed segmen-\ntation is usually applied to a smoothed version of the gradient magnitude image, which also\nmakes it usable with color images. As an alternative, the maximum oriented energy in a steer-\nable ﬁlter (3.28–3.29) (Freeman and Adelson 1991) can be used as the basis of the oriented\nwatershed transform developed by Arbel´aez, Maire, Fowlkes et al. (2010). Such techniques\nend up ﬁnding smooth regions separated by visible (higher gradient) boundaries. Since such\nboundaries are what active contours usually follow, active contour algorithms (Mortensen and\nBarrett 1999; Li, Sun, Tang et al. 2004) often precompute such a segmentation using either\nthe watershed or the related tobogganing technique (Section 5.1.3).\nUnfortunately, watershed segmentation associates a unique region with each local mini-\nmum, which can lead to over-segmentation. Watershed segmentation is therefore often used\nas part of an interactive system, where the user ﬁrst marks seed locations (with a click or\na short stroke) that correspond to the centers of different desired components. Figure 5.13\nshows the results of running the watershed algorithm with some manually placed markers on\na confocal microscopy image. It also shows the result for an improved version of watershed\nthat uses local morphology to smooth out and optimize the boundaries separating the regions\n(Beare 2006).\n8 A related algorithm can be used to compute maximally stable extremal regions (MSERs) efﬁciently (Sec-\ntion 4.1.1) (Nist´er and Stew´enius 2008).",
  "308": "286\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n5.2.2 Region splitting (divisive clustering)\nSplitting the image into successively ﬁner regions is one of the oldest techniques in computer\nvision. Ohlander, Price, and Reddy (1978) present such a technique, which ﬁrst computes a\nhistogram for the whole image and then ﬁnds a threshold that best separates the large peaks\nin the histogram. This process is repeated until regions are either fairly uniform or below a\ncertain size.\nMore recent splitting algorithms often optimize some metric of intra-region similarity and\ninter-region dissimilarity. These are covered in Sections 5.4 and 5.5.\n5.2.3 Region merging (agglomerative clustering)\nRegion merging techniques also date back to the beginnings of computer vision. Brice and\nFennema (1970) use a dual grid for representing boundaries between pixels and merge re-\ngions based on their relative boundary lengths and the strength of the visible edges at these\nboundaries.\nIn data clustering, algorithms can link clusters together based on the distance between\ntheir closest points (single-link clustering), their farthest points (complete-link clustering), or\nsomething in between (Jain, Topchy, Law et al. 2004). Kamvar, Klein, and Manning (2002)\nprovide a probabilistic interpretation of these algorithms and show how additional models\ncan be incorporated within this framework.\nA very simple version of pixel-based merging combines adjacent regions whose average\ncolor difference is below a threshold or whose regions are too small. Segmenting the image\ninto such superpixels (Mori, Ren, Efros et al. 2004), which are not semantically meaningful,\ncan be a useful pre-processing stage to make higher-level algorithms such as stereo matching\n(Zitnick, Kang, Uyttendaele et al. 2004; Taguchi, Wilburn, and Zitnick 2008), optic ﬂow\n(Zitnick, Jojic, and Kang 2005; Brox, Bregler, and Malik 2009), and recognition (Mori, Ren,\nEfros et al. 2004; Mori 2005; Gu, Lim, Arbelaez et al. 2009; Lim, Arbel´aez, Gu et al. 2009)\nboth faster and more robust.\n5.2.4 Graph-based segmentation\nWhile many merging algorithms simply apply a ﬁxed rule that groups pixels and regions\ntogether, Felzenszwalb and Huttenlocher (2004b) present a merging algorithm that uses rel-\native dissimilarities between regions to determine which ones should be merged; it produces\nan algorithm that provably optimizes a global grouping metric. They start with a pixel-to-\npixel dissimilarity measure w(e) that measures, for example, intensity differences between\nN8 neighbors. (Alternatively, they can use the joint feature space distances (5.42) introduced\nby Comaniciu and Meer (2002), which we discuss in Section 5.3.2.)",
  "309": "5.2 Split and merge\n287\n(a)\n(b)\n(c)\nFigure 5.14\nGraph-based merging segmentation (Felzenszwalb and Huttenlocher 2004b)\nc⃝2004 Springer: (a) input grayscale image that is successfully segmented into three regions\neven though the variation inside the smaller rectangle is larger than the variation across the\nmiddle edge; (b) input grayscale image; (c) resulting segmentation using an N8 pixel neigh-\nborhood.\nFor any region R, its internal difference is deﬁned as the largest edge weight in the re-\ngion’s minimum spanning tree,\nInt(R) =\nmin\ne∈MST (R) w(e).\n(5.20)\nFor any two adjacent regions with at least one edge connecting their vertices, the difference\nbetween these regions is deﬁned as the minimum weight edge connecting the two regions,\nDif (R1, R2) =\nmin\ne=(v1,v2)|v1∈R1,v2∈R2 w(e).\n(5.21)\nTheir algorithm merges any two adjacent regions whose difference is smaller than the mini-\nmum internal difference of these two regions,\nMInt(R1, R2) = min(Int(R1) + τ(R1), Int(R2) + τ(R2)),\n(5.22)\nwhere τ(R) is a heuristic region penalty that Felzenszwalb and Huttenlocher (2004b) set to\nk/|R|, but which can be set to any application-speciﬁc measure of region goodness.\nBy merging regions in decreasing order of the edges separating them (which can be efﬁ-\nciently evaluated using a variant of Kruskal’s minimum spanning tree algorithm), they prov-\nably produce segmentations that are neither too ﬁne (there exist regions that could have been\nmerged) nor too coarse (there are regions that could be split without being mergeable). For\nﬁxed-size pixel neighborhoods, the running time for this algorithm is O(N log N), where N\nis the number of image pixels, which makes it one of the fastest segmentation algorithms\n(Paris and Durand 2007). Figure 5.14 shows two examples of images segmented using their\ntechnique.",
  "310": "288\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 5.15\nCoarse to ﬁne node aggregation in segmentation by weighted aggregation\n(SWA) (Sharon, Galun, Sharon et al. 2006) c⃝2006 Macmillan Publishers Ltd [Nature]: (a)\noriginal gray-level pixel grid; (b) inter-pixel couplings, where thicker lines indicate stronger\ncouplings; (c) after one level of coarsening, where each original pixel is strongly coupled to\none of the coarse-level nodes; (d) after two levels of coarsening.\n5.2.5 Probabilistic aggregation\nAlpert, Galun, Basri et al. (2007) develop a probabilistic merging algorithm based on two\ncues, namely gray-level similarity and texture similarity. The gray-level similarity between\nregions Ri and Rj is based on the minimal external difference from other neighboring regions,\nσ+\nlocal = min(∆+\ni , ∆+\nj ),\n(5.23)\nwhere ∆+\ni = mink |∆ik| and ∆ik is the difference in average intensities between regions Ri\nand Rk. This is compared to the average intensity difference,\nσ−\nlocal =\n∆−\ni + ∆−\nj\n2\n,\n(5.24)\nwhere ∆−\ni = P\nk(τik∆ik)/ P\nk(τik) and τik is the boundary length between regions Ri and\nRk. The texture similarity is deﬁned using relative differences between histogram bins of\nsimple oriented Sobel ﬁlter responses. The pairwise statistics σ+\nlocal and σ−\nlocal are used to\ncompute the likelihoods pij that two regions should be merged. (See the paper by Alpert,\nGalun, Basri et al. (2007) for more details.)\nMerging proceeds in a hierarchical fashion inspired by algebraic multigrid techniques\n(Brandt 1986; Briggs, Henson, and McCormick 2000) and previously used by Alpert, Galun,\nBasri et al. (2007) in their segmentation by weighted aggregation (SWA) algorithm (Sharon,\nGalun, Sharon et al. 2006), which we discuss in Section 5.4. A subset of the nodes C ⊂V\nthat are (collectively) strongly coupled to all of the original nodes (regions) are used to deﬁne\nthe problem at a coarser scale (Figure 5.15), where strong coupling is deﬁned as\nP\nj∈C pij\nP\nj∈V pij\n> φ,\n(5.25)",
  "311": "5.3 Mean shift and mode ﬁnding\n289\nwith φ usually set to 0.2. The intensity and texture similarity statistics for the coarser nodes\nare recursively computed using weighted averaging, where the relative strengths (couplings)\nbetween coarse- and ﬁne-level nodes are based on their merge probabilities pij. This allows\nthe algorithm to run in essentially O(N) time, using the same kind of hierarchical aggrega-\ntion operations that are used in pyramid-based ﬁltering or preconditioning algorithms. After\na segmentation has been identiﬁed at a coarser level, the exact memberships of each pixel are\ncomputed by propagating coarse-level assignments to their ﬁner-level “children” (Sharon,\nGalun, Sharon et al. 2006; Alpert, Galun, Basri et al. 2007). Figure 5.22 shows the segmen-\ntations produced by this algorithm compared to other popular segmentation algorithms.\n5.3 Mean shift and mode ﬁnding\nMean-shift and mode ﬁnding techniques, such as k-means and mixtures of Gaussians, model\nthe feature vectors associated with each pixel (e.g., color and position) as samples from an\nunknown probability density function and then try to ﬁnd clusters (modes) in this distribution.\nConsider the color image shown in Figure 5.16a. How would you segment this image\nbased on color alone? Figure 5.16b shows the distribution of pixels in L*u*v* space, which\nis equivalent to what a vision algorithm that ignores spatial location would see. To make the\nvisualization simpler, let us only consider the L*u* coordinates, as shown in Figure 5.16c.\nHow many obvious (elongated) clusters do you see? How would you go about ﬁnding these\nclusters?\nThe k-means and mixtures of Gaussians techniques use a parametric model of the den-\nsity function to answer this question, i.e., they assume the density is the superposition of a\nsmall number of simpler distributions (e.g., Gaussians) whose locations (centers) and shape\n(covariance) can be estimated. Mean shift, on the other hand, smoothes the distribution and\nﬁnds its peaks as well as the regions of feature space that correspond to each peak. Since\na complete density is being modeled, this approach is called non-parametric (Bishop 2006).\nLet us look at these techniques in more detail.\n5.3.1 K-means and mixtures of Gaussians\nWhile k-means implicitly models the probability density as a superposition of spherically\nsymmetric distributions, it does not require any probabilistic reasoning or modeling (Bishop\n2006). Instead, the algorithm is given the number of clusters k it is supposed to ﬁnd; it\nthen iteratively updates the cluster center location based on the samples that are closest to\neach center. The algorithm can be initialized by randomly sampling k centers from the input\nfeature vectors. Techniques have also been developed for splitting or merging cluster centers",
  "312": "290\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 5.16\nMean-shift image segmentation (Comaniciu and Meer 2002) c⃝2002 IEEE:\n(a) input color image; (b) pixels plotted in L*u*v* space; (c) L*u* space distribution; (d)\nclustered results after 159 mean-shift procedures; (e) corresponding trajectories with peaks\nmarked as red dots.",
  "313": "5.3 Mean shift and mode ﬁnding\n291\nbased on their statistics, and for accelerating the process of ﬁnding the nearest mean center\n(Bishop 2006).\nIn mixtures of Gaussians, each cluster center is augmented by a covariance matrix whose\nvalues are re-estimated from the corresponding samples. Instead of using nearest neighbors\nto associate input samples with cluster centers, a Mahalanobis distance (Appendix B.1.1) is\nused:\nd(xi, µk; Σk) = ∥xi −µk∥Σ\n−1\nk\n= (xi −µk)T Σ−1\nk (xi −µk)\n(5.26)\nwhere xi are the input samples, µk are the cluster centers, and Σk are their covariance es-\ntimates. Samples can be associated with the nearest cluster center (a hard assignment of\nmembership) or can be softly assigned to several nearby clusters.\nThis latter, more commonly used, approach corresponds to iteratively re-estimating the\nparameters for a mixture of Gaussians density function,\np(x|{πk, µk, Σk}) =\nX\nk\nπk N(x|µk, Σk),\n(5.27)\nwhere πk are the mixing coefﬁcients, µk and Σk are the Gaussian means and covariances,\nand\nN(x|µk, Σk) =\n1\n|Σk|e−d(x,µk;Σk)\n(5.28)\nis the normal (Gaussian) distribution (Bishop 2006).\nTo iteratively compute (a local) maximum likely estimate for the unknown mixture pa-\nrameters {πk, µk, Σk}, the expectation maximization (EM) algorithm (Dempster, Laird, and\nRubin 1977) proceeds in two alternating stages:\n1. The expectation stage (E step) estimates the responsibilities\nzik = 1\nZi\nπk N(x|µk, Σk)\nwith\nX\nk\nzik = 1,\n(5.29)\nwhich are the estimates of how likely a sample xi was generated from the kth Gaussian\ncluster.\n2. The maximization stage (M step) updates the parameter values\nµk\n=\n1\nNk\nX\ni\nzikxi,\n(5.30)\nΣk\n=\n1\nNk\nX\ni\nzik(xi −µk)(xi −µk)T ,\n(5.31)\nπk\n=\nNk\nN ,\n(5.32)",
  "314": "292\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhere\nNk =\nX\ni\nzik.\n(5.33)\nis an estimate of the number of sample points assigned to each cluster.\nBishop (2006) has a wonderful exposition of both mixture of Gaussians estimation and the\nmore general topic of expectation maximization.\nIn the context of image segmentation, Ma, Derksen, Hong et al. (2007) present a nice\nreview of segmentation using mixtures of Gaussians and develop their own extension based\non Minimum Description Length (MDL) coding, which they show produces good results on\nthe Berkeley segmentation database.\n5.3.2 Mean shift\nWhile k-means and mixtures of Gaussians use a parametric form to model the probability den-\nsity function being segmented, mean shift implicitly models this distribution using a smooth\ncontinuous non-parametric model. The key to mean shift is a technique for efﬁciently ﬁnd-\ning peaks in this high-dimensional data distribution without ever computing the complete\nfunction explicitly (Fukunaga and Hostetler 1975; Cheng 1995; Comaniciu and Meer 2002).\nConsider once again the data points shown in Figure 5.16c, which can be thought of as\nhaving been drawn from some probability density function. If we could compute this density\nfunction, as visualized in Figure 5.16e, we could ﬁnd its major peaks (modes) and identify\nregions of the input space that climb to the same peak as being part of the same region. This\nis the inverse of the watershed algorithm described in Section 5.2.1, which climbs downhill\nto ﬁnd basins of attraction.\nThe ﬁrst question, then, is how to estimate the density function given a sparse set of\nsamples. One of the simplest approaches is to just smooth the data, e.g., by convolving it\nwith a ﬁxed kernel of width h,\nf(x) =\nX\ni\nK(x −xi) =\nX\ni\nk\n\u0012∥x −xi∥2\nh2\n\u0013\n,\n(5.34)\nwhere xi are the input samples and k(r) is the kernel function (or Parzen window).9 This\napproach is known as kernel density estimation or the Parzen window technique (Duda, Hart,\nand Stork 2001, Section 4.3; Bishop 2006, Section 2.5.1). Once we have computed f(x), as\nshown in Figures 5.16e and 5.17, we can ﬁnd its local maxima using gradient ascent or some\nother optimization technique.\n9 In this simpliﬁed formula, a Euclidean metric is used. We discuss a little later (5.42) how to generalize this\nto non-uniform (scaled or oriented) metrics. Note also that this distribution may not be proper, i.e., integrate to 1.\nSince we are looking for maxima in the density, this does not matter.",
  "315": "5.3 Mean shift and mode ﬁnding\n293\nx\nf (x)\nxi\nK(x)\nG(x)\nf '(xk)\nxk\nm(xk)\nFigure 5.17 One-dimensional visualization of the kernel density estimate, its derivative, and\na mean shift. The kernel density estimate f(x) is obtained by convolving the sparse set of\ninput samples xi with the kernel function K(x). The derivative of this function, f ′(x), can\nbe obtained by convolving the inputs with the derivative kernel G(x). Estimating the local\ndisplacement vectors around a current estimate xk results in the mean-shift vector m(xk),\nwhich, in a multi-dimensional setting, point in the same direction as the function gradient\n∇f(xk). The red dots indicate local maxima in f(x) to which the mean shifts converge.\nThe problem with this “brute force” approach is that, for higher dimensions, it becomes\ncomputationally prohibitive to evaluate f(x) over the complete search space.10 Instead, mean\nshift uses a variant of what is known in the optimization literature as multiple restart gradient\ndescent. Starting at some guess for a local maximum, yk, which can be a random input data\npoint xi, mean shift computes the gradient of the density estimate f(x) at yk and takes an\nuphill step in that direction (Figure 5.17). The gradient of f(x) is given by\n∇f(x) =\nX\ni\n(xi −x)G(x −xi) =\nX\ni\n(xi −x)g\n\u0012∥x −xi∥2\nh2\n\u0013\n,\n(5.35)\nwhere\ng(r) = −k′(r),\n(5.36)\nand k′(r) is the ﬁrst derivative of k(r). We can re-write the gradient of the density function\nas\n∇f(x) =\n\"X\ni\nG(x −xi)\n#\nm(x),\n(5.37)\nwhere the vector\nm(x) =\nP\ni xiG(x −xi)\nP\ni G(x −xi)\n−x\n(5.38)\nis called the mean shift, since it is the difference between the weighted mean of the neighbors\nxi around x and the current value of x.\n10 Even for one dimension, if the space is extremely sparse, it may be inefﬁcient.",
  "316": "294\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn the mean-shift procedure, the current estimate of the mode yk at iteration k is replaced\nby its locally weighted mean,\nyk+1 = yk + m(yk) =\nP\ni xiG(yk −xi)\nP\ni G(yk −xi) .\n(5.39)\nComaniciu and Meer (2002) prove that this algorithm converges to a local maximum of f(x)\nunder reasonably weak conditions on the kernel k(r), i.e., that it is monotonically decreasing.\nThis convergence is not guaranteed for regular gradient descent unless appropriate step size\ncontrol is used.\nThe two kernels that Comaniciu and Meer (2002) studied are the Epanechnikov kernel,\nkE(r) = max(0, 1 −r),\n(5.40)\nwhich is a radial generalization of a bilinear kernel, and the Gaussian (normal) kernel,\nkN(r) = exp\n\u0012\n−1\n2r\n\u0013\n.\n(5.41)\nThe corresponding derivative kernels g(r) are a unit ball and another Gaussian, respectively.\nUsing the Epanechnikov kernel converges in a ﬁnite number of steps, while the Gaussian\nkernel has a smoother trajectory (and produces better results), but converges very slowly near\na mode (Exercise 5.5).\nThe simplest way to apply mean shift is to start a separate mean-shift mode estimate\ny at every input point xi and to iterate for a ﬁxed number of steps or until the mean-shift\nmagnitude is below a threshold. A faster approach is to randomly subsample the input points\nxi and to keep track of each point’s temporal evolution. The remaining points can then be\nclassiﬁed based on the nearest evolution path (Comaniciu and Meer 2002). Paris and Durand\n(2007) review a number of other more efﬁcient implementations of mean shift, including their\nown approach, which is based on using an efﬁcient low-resolution estimate of the complete\nmulti-dimensional space of f(x) along with its stationary points.\nThe color-based segmentation shown in Figure 5.16 only looks at pixel colors when deter-\nmining the best clustering. It may therefore cluster together small isolated pixels that happen\nto have the same color, which may not correspond to a semantically meaningful segmentation\nof the image.\nBetter results can usually be obtained by clustering in the joint domain of color and lo-\ncation. In this approach, the spatial coordinates of the image xs = (x, y), which are called\nthe spatial domain, are concatenated with the color values xr, which are known as the range\ndomain, and mean-shift clustering is applied in this ﬁve-dimensional space xj. Since location\nand color may have different scales, the kernels are adjusted accordingly, i.e., we use a kernel\nof the form\nK(xj) = k\n\u0012∥xr∥2\nh2r\n\u0013\nk\n\u0012∥xs∥2\nh2s\n\u0013\n,\n(5.42)",
  "317": "5.3 Mean shift and mode ﬁnding\n295\nFigure 5.18\nMean-shift color image segmentation with parameters (hs, hr, M)\n=\n(16, 19, 40) (Comaniciu and Meer 2002) c⃝2002 IEEE.\nwhere separate parameters hs and hr are used to control the spatial and range bandwidths of\nthe ﬁlter kernels. Figure 5.18 shows an example of mean-shift clustering in the joint domain,\nwith parameters (hs, hr, M) = (16, 19, 40), where spatial regions containing less than M\npixels are eliminated.\nThe form of the joint domain ﬁlter kernel (5.42) is reminiscent of the bilateral ﬁlter kernel\n(3.34–3.37) discussed in Section 3.3.1. The difference between mean shift and bilateral ﬁl-\ntering, however, is that in mean shift the spatial coordinates of each pixel are adjusted along\nwith its color values, so that the pixel migrates more quickly towards other pixels with similar\ncolors, and can therefore later be used for clustering and segmentation.\nDetermining the best bandwidth parameters h to use with mean shift remains something\nof an art, although a number of approaches have been explored. These include optimizing\nthe bias–variance tradeoff, looking for parameter ranges where the number of clusters varies\nslowly, optimizing some external clustering criterion, or using top-down (application domain)\nknowledge (Comaniciu and Meer 2003). It is also possible to change the orientation of the\nkernel in joint parameter space for applications such as spatio-temporal (video) segmentations\n(Wang, Thiesson, Xu et al. 2004).\nMean shift has been applied to a number of different problems in computer vision, includ-\ning face tracking, 2D shape extraction, and texture segmentation (Comaniciu and Meer 2002),\nand more recently in stereo matching (Chapter 11) (Wei and Quan 2004), non-photorealistic\nrendering (Section 10.5.2) (DeCarlo and Santella 2002), and video editing (Section 10.4.5)\n(Wang, Bhat, Colburn et al. 2005). Paris and Durand (2007) provide a nice review of such\napplications, as well as techniques for more efﬁciently solving the mean-shift equations and\nproducing hierarchical segmentations.",
  "318": "296\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nA\nA\nA\nA\nB\nB\nB\nA\nB\nsum\nA\nassoc(A, A)\ncut(A, B)\nassoc(A, V )\nB\ncut(B, A)\nassoc(B, B)\nassoc(B, V )\nsum\nassoc(A, V )\nassoc(B, v)\n(a)\n(b)\nFigure 5.19\nSample weighted graph and its normalized cut: (a) a small sample graph and\nits smallest normalized cut; (b) tabular form of the associations and cuts for this graph. The\nassoc and cut entries are computed as area sums of the associated weight matrix W (Fig-\nure 5.20). Normalizing the table entries by the row or column sums produces normalized\nassociations and cuts Nassoc and Ncut.\n5.4 Normalized cuts\nWhile bottom-up merging techniques aggregate regions into coherent wholes and mean-shift\ntechniques try to ﬁnd clusters of similar pixels using mode ﬁnding, the normalized cuts\ntechnique introduced by Shi and Malik (2000) examines the afﬁnities (similarities) between\nnearby pixels and tries to separate groups that are connected by weak afﬁnities.\nConsider the simple graph shown in Figure 5.19a. The pixels in group A are all strongly\nconnected with high afﬁnities, shown as thick red lines, as are the pixels in group B. The\nconnections between these two groups, shown as thinner blue lines, are much weaker. A\nnormalized cut between the two groups, shown as a dashed line, separates them into two\nclusters.\nThe cut between two groups A and B is deﬁned as the sum of all the weights being cut,\ncut(A, B) =\nX\ni∈A,j∈B\nwij,\n(5.43)\nwhere the weights between two pixels (or regions) i and j measure their similarity. Using\na minimum cut as a segmentation criterion, however, does not result in reasonable clusters,\nsince the smallest cuts usually involve isolating a single pixel.\nA better measure of segmentation is the normalized cut, which is deﬁned as\nNcut(A, B) =\ncut(A, B)\nassoc(A, V ) +\ncut(A, B)\nassoc(B, V ),\n(5.44)\nwhere assoc(A, A) = P\ni∈A,j∈A wij is the association (sum of all the weights) within a\ncluster and assoc(A, V ) = assoc(A, A) + cut(A, B) is the sum of all the weights associated",
  "319": "5.4 Normalized cuts\n297\nwith nodes in A. Figure 5.19b shows how the cuts and associations can be thought of as area\nsums in the weight matrix W = [wij], where the entries of the matrix have been arranged so\nthat the nodes in A come ﬁrst and the nodes in B come second. Figure 5.20 shows an actual\nweight matrix for which these area sums can be computed. Dividing each of these areas by\nthe corresponding row sum (the rightmost column of Figure 5.19b) results in the normalized\ncut and association values. These normalized values better reﬂect the ﬁtness of a particular\nsegmentation, since they look for collections of edges that are weak relative to all of the edges\nboth inside and emanating from a particular region.\nUnfortunately, computing the optimal normalized cut is NP-complete. Instead, Shi and\nMalik (2000) suggest computing a real-valued assignment of nodes to groups. Let x be the\nindicator vector where xi = +1 iff i ∈A and xi = −1 iff i ∈B. Let d = W 1 be the row\nsums of the symmetric matrix W and D = diag(d) be the corresponding diagonal matrix.\nShi and Malik (2000) show that minimizing the normalized cut over all possible indicator\nvectors x is equivalent to minimizing\nmin\ny\nyT (D −W )y\nyT Dy\n,\n(5.45)\nwhere y = ((1+x)−b(1−x))/2 is a vector consisting of all 1s and −bs such that y·d = 0.\nMinimizing this Rayleigh quotient is equivalent to solving the generalized eigenvalue system\n(D −W )y = λDy,\n(5.46)\nwhich can be turned into a regular eigenvalue problem\n(I −N)z = λz,\n(5.47)\nwhere N = D−1/2W D−1/2 is the normalized afﬁnity matrix (Weiss 1999) and z =\nD1/2y. Because these eigenvectors can be interpreted as the large modes of vibration in\na spring-mass system, normalized cuts is an example of a spectral method for image segmen-\ntation.\nExtending an idea originally proposed by Scott and Longuet-Higgins (1990), Weiss (1999)\nsuggests normalizing the afﬁnity matrix and then using the top k eigenvectors to reconstitute a\nQ matrix. Other papers have extended the basic normalized cuts framework by modifying the\nafﬁnity matrix in different ways, ﬁnding better discrete solutions to the minimization prob-\nlem, or applying multi-scale techniques (Meil˘a and Shi 2000, 2001; Ng, Jordan, and Weiss\n2001; Yu and Shi 2003; Cour, B´en´ezit, and Shi 2005; Tolliver and Miller 2006).\nFigure 5.20b shows the second smallest (real-valued) eigenvector corresponding to the\nweight matrix shown in Figure 5.20a. (Here, the rows have been permuted to separate the\ntwo groups of variables that belong to the different components of this eigenvector.) Af-\nter this real-valued vector is computed, the variables corresponding to positive and negative",
  "320": "298\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 5.20 Sample weight table and its second smallest eigenvector (Shi and Malik 2000)\nc⃝2000 IEEE: (a) sample 32 × 32 weight matrix W ; (b) eigenvector corresponding to the\nsecond smallest eigenvalue of the generalized eigenvalue problem (D −W )y = λDy.\neigenvector values are associated with the two cut components. This process can be further\nrepeated to hierarchically subdivide an image, as shown in Figure 5.21.\nThe original algorithm proposed by Shi and Malik (2000) used spatial position and image\nfeature differences to compute the pixel-wise afﬁnities,\nwij = exp\n\u0012\n−∥F i −F j∥2\nσ2\nF\n−∥xi −xj∥2\nσ2s\n\u0013\n,\n(5.48)\nfor pixels within a radius ∥xi −xj∥< r, where F is a feature vector that consists of intensi-\nties, colors, or oriented ﬁlter histograms. (Note how (5.48) is the negative exponential of the\njoint feature space distance (5.42).)\nIn subsequent work, Malik, Belongie, Leung et al. (2001) look for intervening contours\nbetween pixels i and j and deﬁne an intervening contour weight\nwIC\nij = 1 −max\nx∈lij pcon(x),\n(5.49)\nwhere lij is the image line joining pixels i and j and pcon(x) is the probability of an inter-\nvening contour perpendicular to this line, which is deﬁned as the negative exponential of the\noriented energy in the perpendicular direction. They multiply these weights with a texton-\nbased texture similarity metric and use an initial over-segmentation based purely on local\npixel-wise features to re-estimate intervening contours and texture statistics in a region-based\nmanner. Figure 5.22 shows the results of running this improved algorithm on a number of\ntest images.\nBecause it requires the solution of large sparse eigenvalue problems, normalized cuts can\nbe quite slow. Sharon, Galun, Sharon et al. (2006) present a way to accelerate the com-\nputation of the normalized cuts using an approach inspired by algebraic multigrid (Brandt",
  "321": "5.4 Normalized cuts\n299\nFigure 5.21 Normalized cuts segmentation (Shi and Malik 2000) c⃝2000 IEEE: The input\nimage and the components returned by the normalized cuts algorithm.\nFigure 5.22\nComparative segmentation results (Alpert, Galun, Basri et al. 2007) c⃝2007\nIEEE. “Our method” refers to the probabilistic bottom-up merging algorithm developed by\nAlpert et al.",
  "322": "300\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1986; Briggs, Henson, and McCormick 2000). To coarsen the original problem, they select\na smaller number of variables such that the remaining ﬁne-level variables are strongly cou-\npled to at least one coarse-level variable. Figure 5.15 shows this process schematically, while\n(5.25) gives the deﬁnition for strong coupling except that, in this case, the original weights\nwij in the normalized cut are used instead of merge probabilities pij.\nOnce a set of coarse variables has been selected, an inter-level interpolation matrix with\nelements similar to the left hand side of (5.25) is used to deﬁne a reduced version of the nor-\nmalized cuts problem. In addition to computing the weight matrix using interpolation-based\ncoarsening, additional region statistics are used to modulate the weights. After a normalized\ncut has been computed at the coarsest level of analysis, the membership values of ﬁner-level\nnodes are computed by interpolating parent values and mapping values within ϵ = 0.1 of 0\nand 1 to pure Boolean values.\nAn example of the segmentation produced by weighted aggregation (SWA) is shown in\nFigure 5.22, along with the most recent probabilistic bottom-up merging algorithm by Alpert,\nGalun, Basri et al. (2007), which was described in Section 5.2. In even more recent work,\nWang and Oliensis (2010) show how to estimate statistics over segmentations (e.g., mean\nregion size) directly from the afﬁnity graph. They use this to produce segmentations that are\nmore central with respect to other possible segmentations.\n5.5 Graph cuts and energy-based methods\nA common theme in image segmentation algorithms is the desire to group pixels that have\nsimilar appearance (statistics) and to have the boundaries between pixels in different regions\nbe of short length and across visible discontinuities. If we restrict the boundary measurements\nto be between immediate neighbors and compute region membership statistics by summing\nover pixels, we can formulate this as a classic pixel-based energy function using either a\nvariational formulation (regularization, see Section 3.7.1) or as a binary Markov random\nﬁeld (Section 3.7.2).\nExamples of the continuous approach include (Mumford and Shah 1989; Chan and Vese\n1992; Zhu and Yuille 1996; Tabb and Ahuja 1997) along with the level set approaches dis-\ncussed in Section 5.1.4.\nAn early example of a discrete labeling problem that combines\nboth region-based and boundary-based energy terms is the work of Leclerc (1989), who used\nminimum description length (MDL) coding to derive the energy function being minimized.\nBoykov and Funka-Lea (2006) present a wonderful survey of various energy-based tech-\nniques for binary object segmentation, some of which we discuss below.\nAs we saw in Section 3.7.2, the energy corresponding to a segmentation problem can be",
  "323": "5.5 Graph cuts and energy-based methods\n301\nwritten (c.f. Equations (3.100) and (3.108–3.113)) as\nE(f) =\nX\ni,j\nEr(i, j) + Eb(i, j),\n(5.50)\nwhere the region term\nEr(i, j) = ES(I(i, j); R(f(i, j)))\n(5.51)\nis the negative log likelihood that pixel intensity (or color) I(i, j) is consistent with the statis-\ntics of region R(f(i, j)) and the boundary term\nEb(i, j) = sx(i, j)δ(f(i, j) −f(i + 1, j)) + sy(i, j)δ(f(i, j) −f(i, j + 1))\n(5.52)\nmeasures the inconsistency between N4 neighbors modulated by local horizontal and vertical\nsmoothness terms sx(i, j) and sy(i, j).\nRegion statistics can be something as simple as the mean gray level or color (Leclerc\n1989), in which case\nES(I; µk) = ∥I −µk∥2.\n(5.53)\nAlternatively, they can be more complex, such as region intensity histograms (Boykov and\nJolly 2001) or color Gaussian mixture models (Rother, Kolmogorov, and Blake 2004). For\nsmoothness (boundary) terms, it is common to make the strength of the smoothness sx(i, j)\ninversely proportional to the local edge strength (Boykov, Veksler, and Zabih 2001).\nOriginally, energy-based segmentation problems were optimized using iterative gradient\ndescent techniques, which were slow and prone to getting trapped in local minima. Boykov\nand Jolly (2001) were the ﬁrst to apply the binary MRF optimization algorithm developed by\nGreig, Porteous, and Seheult (1989) to binary object segmentation.\nIn this approach, the user ﬁrst delineates pixels in the background and foreground regions\nusing a few strokes of an image brush (Figure 3.61). These pixels then become the seeds that\ntie nodes in the S–T graph to the source and sink labels S and T (Figure 5.23a). Seed pixels\ncan also be used to estimate foreground and background region statistics (intensity or color\nhistograms).\nThe capacities of the other edges in the graph are derived from the region and boundary\nenergy terms, i.e., pixels that are more compatible with the foreground or background region\nget stronger connections to the respective source or sink; adjacent pixels with greater smooth-\nness also get stronger links. Once the minimum-cut/maximum-ﬂow problem has been solved\nusing a polynomial time algorithm (Goldberg and Tarjan 1988; Boykov and Kolmogorov\n2004), pixels on either side of the computed cut are labeled according to the source or sink to\nwhich they remain connected (Figure 5.23b). While graph cuts is just one of several known\ntechniques for MRF energy minimization (Appendix B.5.4), it is still the one most commonly\nused for solving binary MRF problems.",
  "324": "302\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n  Object\nterminal\n  terminal\nBackground \np\nq\nr\nw\nv\nS\nT\nBackground \n  Object\n  terminal\nterminal\np\nq\nr\nw\nv\nS\nT\ncut\n(a)\n(b)\nFigure 5.23 Graph cuts for region segmentation (Boykov and Jolly 2001) c⃝2001 IEEE: (a)\nthe energy function is encoded as a maximum ﬂow problem; (b) the minimum cut determines\nthe region boundary.\n(a)\n(b)\n(c)\nFigure 5.24\nGrabCut image segmentation (Rother, Kolmogorov, and Blake 2004) c⃝2004\nACM: (a) the user draws a bounding box in red; (b) the algorithm guesses color distributions\nfor the object and background and performs a binary segmentation; (c) the process is repeated\nwith better region statistics.\nThe basic binary segmentation algorithm of Boykov and Jolly (2001) has been extended\nin a number of directions. The GrabCut system of Rother, Kolmogorov, and Blake (2004)\niteratively re-estimates the region statistics, which are modeled as a mixtures of Gaussians in\ncolor space. This allows their system to operate given minimal user input, such as a single\nbounding box (Figure 5.24a)—the background color model is initialized from a strip of pixels\naround the box outline. (The foreground color model is initialized from the interior pixels,\nbut quickly converges to a better estimate of the object.) The user can also place additional\nstrokes to reﬁne the segmentation as the solution progresses. In more recent work, Cui, Yang,\nWen et al. (2008) use color and edge models derived from previous segmentations of similar\nobjects to improve the local models used in GrabCut.\nAnother major extension to the original binary segmentation formulation is the addition of",
  "325": "5.5 Graph cuts and energy-based methods\n303\nFigure 5.25\nSegmentation with a directed graph cut (Boykov and Funka-Lea 2006) c⃝2006\nSpringer: (a) directed graph; (b) image with seed points; (c) the undirected graph incorrectly\ncontinues the boundary along the bright object; (d) the directed graph correctly segments the\nlight gray region from its darker surround.\ndirected edges, which allows boundary regions to be oriented, e.g., to prefer light to dark tran-\nsitions or vice versa (Kolmogorov and Boykov 2005). Figure 5.25 shows an example where\nthe directed graph cut correctly segments the light gray liver from its dark gray surround. The\nsame approach can be used to measure the ﬂux exiting a region, i.e., the signed gradient pro-\njected normal to the region boundary. Combining oriented graphs with larger neighborhoods\nenables approximating continuous problems such as those traditionally solved using level sets\nin the globally optimal graph cut framework (Boykov and Kolmogorov 2003; Kolmogorov\nand Boykov 2005).\nEven more recent developments in graph cut-based segmentation techniques include the\naddition of connectivity priors to force the foreground to be in a single piece (Vicente, Kol-\nmogorov, and Rother 2008) and shape priors to use knowledge about an object’s shape during\nthe segmentation process (Lempitsky and Boykov 2007; Lempitsky, Blake, and Rother 2008).\nWhile optimizing the binary MRF energy (5.50) requires the use of combinatorial op-\ntimization techniques, such as maximum ﬂow, an approximate solution can be obtained by\nconverting the binary energy terms into quadratic energy terms deﬁned over a continuous\n[0, 1] random ﬁeld, which then becomes a classical membrane-based regularization problem\n(3.100–3.102). The resulting quadratic energy function can then be solved using standard\nlinear system solvers (3.102–3.103), although if speed is an issue, you should use multigrid\nor one of its variants (Appendix A.5).\nOnce the continuous solution has been computed, it\ncan be thresholded at 0.5 to yield a binary segmentation.\nThe [0, 1] continuous optimization problem can also be interpreted as computing the prob-",
  "326": "304\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nability at each pixel that a random walker starting at that pixel ends up at one of the labeled\nseed pixels, which is also equivalent to computing the potential in a resistive grid where the\nresistors are equal to the edge weights (Grady 2006; Sinop and Grady 2007). K-way seg-\nmentations can also be computed by iterating through the seed labels, using a binary problem\nwith one label set to 1 and all the others set to 0 to compute the relative membership proba-\nbilities for each pixel. In follow-on work, Grady and Ali (2008) use a precomputation of the\neigenvectors of the linear system to make the solution with a novel set of seeds faster, which\nis related to the Laplacian matting problem presented in Section 10.4.3 (Levin, Acha, and\nLischinski 2008). Couprie, Grady, Najman et al. (2009) relate the random walker to water-\nsheds and other segmentation techniques. Singaraju, Grady, and Vidal (2008) add directed-\nedge constraints in order to support ﬂux, which makes the energy piecewise quadratic and\nhence not solvable as a single linear system. The random walker algorithm can also be used\nto solve the Mumford–Shah segmentation problem (Grady and Alvino 2008) and to com-\npute fast multigrid solutions (Grady 2008). A nice review of these techniques is given by\nSingaraju, Grady, Sinop et al. (2010).\nAn even faster way to compute a continuous [0, 1] approximate segmentation is to com-\npute weighted geodesic distances between the 0 and 1 seed regions (Bai and Sapiro 2009),\nwhich can also be used to estimate soft alpha mattes (Section 10.4.3). A related approach by\nCriminisi, Sharp, and Blake (2008) can be used to ﬁnd fast approximate solutions to general\nbinary Markov random ﬁeld optimization problems.\n5.5.1 Application: Medical image segmentation\nOne of the most promising applications of image segmentation is in the medical imaging\ndomain, where it can be used to segment anatomical tissues for later quantitative analysis.\nFigure 5.25 shows a binary graph cut with directed edges being used to segment the liver tis-\nsue (light gray) from its surrounding bone (white) and muscle (dark gray) tissue. Figure 5.26\nshows the segmentation of bones in a 256 × 256 × 119 computed X-ray tomography (CT)\nvolume. Without the powerful optimization techniques available in today’s image segmen-\ntation algorithms, such processing used to require much more laborious manual tracing of\nindividual X-ray slices.\nThe ﬁelds of medical image segmentation (McInerney and Terzopoulos 1996) and med-\nical image registration (Kybic and Unser 2003) (Section 8.3.1) are rich research ﬁelds with\ntheir own specialized conferences, such as Medical Imaging Computing and Computer As-\nsisted Intervention (MICCAI),11 and journals, such as Medical Image Analysis and IEEE\nTransactions on Medical Imaging. These can be great sources of references and ideas for\nresearch in this area.\n11http://www.miccai.org/.",
  "327": "5.6 Additional reading\n305\n(a)\n(b)\nFigure 5.26\n3D volumetric medical image segmentation using graph cuts (Boykov and\nFunka-Lea 2006) c⃝2006 Springer: (a) computed tomography (CT) slice with some seeds;\n(b) recovered 3D volumetric bone model (on a 256 × 256 × 119 voxel grid).\n5.6 Additional reading\nThe topic of image segmentation is closely related to clustering techniques, which are treated\nin a number of monographs and review articles (Jain and Dubes 1988; Kaufman and Rousseeuw\n1990; Jain, Duin, and Mao 2000; Jain, Topchy, Law et al. 2004). Some early segmentation\ntechniques include those describerd by Brice and Fennema (1970); Pavlidis (1977); Riseman\nand Arbib (1977); Ohlander, Price, and Reddy (1978); Rosenfeld and Davis (1979); Haralick\nand Shapiro (1985), while examples of newer techniques are developed by Leclerc (1989);\nMumford and Shah (1989); Shi and Malik (2000); Felzenszwalb and Huttenlocher (2004b).\nArbel´aez, Maire, Fowlkes et al. (2010) provide a good review of automatic segmentation\ntechniques and also compare their performance on the Berkeley Segmentation Dataset and\nBenchmark (Martin, Fowlkes, Tal et al. 2001).12 Additional comparison papers and databases\ninclude those by Unnikrishnan, Pantofaru, and Hebert (2007); Alpert, Galun, Basri et al.\n(2007); Estrada and Jepson (2009).\nThe topic of active contours has a long history, beginning with the seminal work on\nsnakes and other energy-minimizing variational methods (Kass, Witkin, and Terzopoulos\n1988; Cootes, Cooper, Taylor et al. 1995; Blake and Isard 1998), continuing through tech-\nniques such as intelligent scissors (Mortensen and Barrett 1995, 1999; P´erez, Blake, and\nGangnet 2001), and culminating in level sets (Malladi, Sethian, and Vemuri 1995; Caselles,\nKimmel, and Sapiro 1997; Sethian 1999; Paragios and Deriche 2000; Sapiro 2001; Osher and\nParagios 2003; Paragios, Faugeras, Chan et al. 2005; Cremers, Rousson, and Deriche 2007;\nRousson and Paragios 2008; Paragios and Sgallari 2009), which are currently the most widely\n12 http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/.",
  "328": "306\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nused active contour methods.\nTechniques for segmenting images based on local pixel similarities combined with ag-\ngregation or splitting methods include watersheds (Vincent and Soille 1991; Beare 2006;\nArbel´aez, Maire, Fowlkes et al. 2010), region splitting (Ohlander, Price, and Reddy 1978),\nregion merging (Brice and Fennema 1970; Pavlidis and Liow 1990; Jain, Topchy, Law et al.\n2004), as well as graph-based and probabilistic multi-scale approaches (Felzenszwalb and\nHuttenlocher 2004b; Alpert, Galun, Basri et al. 2007).\nMean-shift algorithms, which ﬁnd modes (peaks) in a density function representation of\nthe pixels, are presented by Comaniciu and Meer (2002); Paris and Durand (2007). Parametric\nmixtures of Gaussians can also be used to represent and segment such pixel densities (Bishop\n2006; Ma, Derksen, Hong et al. 2007).\nThe seminal work on spectral (eigenvalue) methods for image segmentation is the nor-\nmalized cut algorithm of Shi and Malik (2000). Related work includes that by Weiss (1999);\nMeil˘a and Shi (2000, 2001); Malik, Belongie, Leung et al. (2001); Ng, Jordan, and Weiss\n(2001); Yu and Shi (2003); Cour, B´en´ezit, and Shi (2005); Sharon, Galun, Sharon et al.\n(2006); Tolliver and Miller (2006); Wang and Oliensis (2010).\nContinuous-energy-based (variational) approaches to interactive segmentation include Leclerc\n(1989); Mumford and Shah (1989); Chan and Vese (1992); Zhu and Yuille (1996); Tabb and\nAhuja (1997). Discrete variants of such problems are usually optimized using binary graph\ncuts or other combinatorial energy minimization methods (Boykov and Jolly 2001; Boykov\nand Kolmogorov 2003; Rother, Kolmogorov, and Blake 2004; Kolmogorov and Boykov 2005;\nCui, Yang, Wen et al. 2008; Vicente, Kolmogorov, and Rother 2008; Lempitsky and Boykov\n2007; Lempitsky, Blake, and Rother 2008), although continuous optimization techniques fol-\nlowed by thresholding can also be used (Grady 2006; Grady and Ali 2008; Singaraju, Grady,\nand Vidal 2008; Criminisi, Sharp, and Blake 2008; Grady 2008; Bai and Sapiro 2009; Cou-\nprie, Grady, Najman et al. 2009). Boykov and Funka-Lea (2006) present a good survey of\nvarious energy-based techniques for binary object segmentation.\n5.7 Exercises\nEx 5.1: Snake evolution\nProve that, in the absence of external forces, a snake will always\nshrink to a small circle and eventually a single point, regardless of whether ﬁrst- or second-\norder smoothness (or some combination) is used.\n(Hint: If you can show that the evolution of the x(s) and y(s) components are indepen-\ndent, you can analyze the 1D case more easily.)\nEx 5.2: Snake tracker\nImplement a snake-based contour tracker:",
  "329": "5.7 Exercises\n307\n1. Decide whether to use a large number of contour points or a smaller number interpo-\nlated with a B-spline.\n2. Deﬁne your internal smoothness energy function and decide what image-based attrac-\ntive forces to use.\n3. At each iteration, set up the banded linear system of equations (quadratic energy func-\ntion) and solve it using banded Cholesky factorization (Appendix A.4).\nEx 5.3: Intelligent scissors\nImplement the intelligent scissors (live-wire) interactive seg-\nmentation algorithm (Mortensen and Barrett 1995) and design a graphical user interface\n(GUI) to let you draw such curves over an image and use them for segmentation.\nEx 5.4: Region segmentation\nImplement one of the region segmentation algorithms de-\nscribed in this chapter. Some popular segmentation algorithms include:\n• k-means (Section 5.3.1);\n• mixtures of Gaussians (Section 5.3.1);\n• mean shift (Section 5.3.2) and Exercise 5.5;\n• normalized cuts (Section 5.4);\n• similarity graph-based segmentation (Section 5.2.4);\n• binary Markov random ﬁelds solved using graph cuts (Section 5.5).\nApply your region segmentation to a video sequence and use it to track moving regions\nfrom frame to frame.\nAlternatively, test out your segmentation algorithm on the Berkeley segmentation database\n(Martin, Fowlkes, Tal et al. 2001).\nEx 5.5: Mean shift\nDevelop a mean-shift segmentation algorithm for color images (Co-\nmaniciu and Meer 2002).\n1. Convert your image to L*a*b* space, or keep the original RGB colors, and augment\nthem with the pixel (x, y) locations.\n2. For every pixel (L, a, b, x, y), compute the weighted mean of its neighbors using either\na unit ball (Epanechnikov kernel) or ﬁnite-radius Gaussian, or some other kernel of\nyour choosing. Weight the color and spatial scales differently, e.g., using values of\n(hs, hr, M) = (16, 19, 40) as shown in Figure 5.18.",
  "330": "308\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Replace the current value with this weighted mean and iterate until either the motion is\nbelow a threshold or a ﬁnite number of steps has been taken.\n4. Cluster all ﬁnal values (modes) that are within a threshold, i.e., ﬁnd the connected\ncomponents. Since each pixel is associated with a ﬁnal mean-shift (mode) value, this\nresults in an image segmentation, i.e., each pixel is labeled with its ﬁnal component.\n5. (Optional) Use a random subset of the pixels as starting points and ﬁnd which com-\nponent each unlabeled pixel belongs to, either by ﬁnding its nearest neighbor or by\niterating the mean shift until it ﬁnds a neighboring track of mean-shift values. Describe\nthe data structures you use to make this efﬁcient.\n6. (Optional) Mean shift divides the kernel density function estimate by the local weight-\ning to obtain a step size that is guaranteed to converge but may be slow. Use an alter-\nnative step size estimation algorithm from the optimization literature to see if you can\nmake the algorithm converge faster.",
  "331": "Chapter 6\nFeature-based alignment\n6.1\n2D and 3D feature-based alignment\n. . . . . . . . . . . . . . . . . . . . . . 311\n6.1.1\n2D alignment using least squares . . . . . . . . . . . . . . . . . . . . 312\n6.1.2\nApplication: Panography . . . . . . . . . . . . . . . . . . . . . . . . 314\n6.1.3\nIterative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\n6.1.4\nRobust least squares and RANSAC\n. . . . . . . . . . . . . . . . . . 318\n6.1.5\n3D alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320\n6.2\nPose estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\n6.2.1\nLinear algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\n6.2.2\nIterative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n6.2.3\nApplication: Augmented reality . . . . . . . . . . . . . . . . . . . . 326\n6.3\nGeometric intrinsic calibration . . . . . . . . . . . . . . . . . . . . . . . . . 327\n6.3.1\nCalibration patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n6.3.2\nVanishing points\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\n6.3.3\nApplication: Single view metrology . . . . . . . . . . . . . . . . . . 331\n6.3.4\nRotational motion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 332\n6.3.5\nRadial distortion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n6.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n6.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336",
  "332": "310\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nx1\nx0\nx2\n(c)\n(d)\nFigure 6.1 Geometric alignment and calibration: (a) geometric alignment of 2D images for\nstitching (Szeliski and Shum 1997) c⃝1997 ACM; (b) a two-dimensional calibration target\n(Zhang 2000) c⃝2000 IEEE; (c) calibration from vanishing points; (d) scene with easy-to-\nﬁnd lines and vanishing directions (Criminisi, Reid, and Zisserman 2000) c⃝2000 Springer.",
  "333": "6.1 2D and 3D feature-based alignment\n311\ny\nx\nsimilarity\nEuclidean\naffine\nprojective\ntranslation\nFigure 6.2 Basic set of 2D planar transformations\nOnce we have extracted features from images, the next stage in many vision algorithms is\nto match these features across different images (Section 4.1.3). An important component of\nthis matching is to verify whether the set of matching features is geometrically consistent,\ne.g., whether the feature displacements can be described by a simple 2D or 3D geometric\ntransformation. The computed motions can then be used in other applications such as image\nstitching (Chapter 9) or augmented reality (Section 6.2.3).\nIn this chapter, we look at the topic of geometric image registration, i.e., the computation\nof 2D and 3D transformations that map features in one image to another (Section 6.1). One\nspecial case of this problem is pose estimation, which is determining a camera’s position\nrelative to a known 3D object or scene (Section 6.2). Another case is the computation of a\ncamera’s intrinsic calibration, which consists of the internal parameters such as focal length\nand radial distortion (Section 6.3). In Chapter 7, we look at the related problems of how\nto estimate 3D point structure from 2D matches (triangulation) and how to simultaneously\nestimate 3D geometry and camera motion (structure from motion).\n6.1 2D and 3D feature-based alignment\nFeature-based alignment is the problem of estimating the motion between two or more sets\nof matched 2D or 3D points. In this section, we restrict ourselves to global parametric trans-\nformations, such as those described in Section 2.1.2 and shown in Table 2.1 and Figure 6.2,\nor higher order transformation for curved surfaces (Shashua and Toelg 1997; Can, Stewart,\nRoysam et al. 2002). Applications to non-rigid or elastic deformations (Bookstein 1989;\nSzeliski and Lavall´ee 1996; Torresani, Hertzmann, and Bregler 2008) are examined in Sec-\ntions 8.3 and 12.6.4.",
  "334": "312\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTransform\nMatrix\nParameters p\nJacobian J\ntranslation\n\"\n1\n0\ntx\n0\n1\nty\n#\n(tx, ty)\n\"\n1\n0\n0\n1\n#\nEuclidean\n\"\ncθ\n−sθ\ntx\nsθ\ncθ\nty\n#\n(tx, ty, θ)\n\"\n1\n0\n−sθx −cθy\n0\n1\ncθx −sθy\n#\nsimilarity\n\"\n1 + a\n−b\ntx\nb\n1 + a\nty\n#\n(tx, ty, a, b)\n\"\n1\n0\nx\n−y\n0\n1\ny\nx\n#\nafﬁne\n\"\n1 + a00\na01\ntx\na10\n1 + a11\nty\n#\n(tx, ty, a00, a01, a10, a11)\n\"\n1\n0\nx\ny\n0\n0\n0\n1\n0\n0\nx\ny\n#\nprojective\n\n\n1 + h00\nh01\nh02\nh10\n1 + h11\nh12\nh20\nh21\n1\n\n\n(h00, h01, . . . , h21)\n(see Section 6.1.3)\nTable 6.1 Jacobians of the 2D coordinate transformations x′ = f(x; p) shown in Table 2.1,\nwhere we have re-parameterized the motions so that they are identity for p = 0.\n6.1.1 2D alignment using least squares\nGiven a set of matched feature points {(xi, x′\ni)} and a planar parametric transformation1 of\nthe form\nx′ = f(x; p),\n(6.1)\nhow can we produce the best estimate of the motion parameters p? The usual way to do this\nis to use least squares, i.e., to minimize the sum of squared residuals\nELS =\nX\ni\n∥ri∥2 =\nX\ni\n∥f(xi; p) −x′\ni∥2,\n(6.2)\nwhere\nri = f(xi; p) −x′\ni = ˆx′\ni −˜x′\ni\n(6.3)\nis the residual between the measured location ˆx′\ni and its corresponding current predicted\nlocation ˜x′\ni = f(xi; p). (See Appendix A.2 for more on least squares and Appendix B.2 for\na statistical justiﬁcation.)\n1 For examples of non-planar parametric models, such as quadrics, see the work of Shashua and Toelg (1997);\nShashua and Wexler (2001).",
  "335": "6.1 2D and 3D feature-based alignment\n313\nMany of the motion models presented in Section 2.1.2 and Table 2.1, i.e., translation,\nsimilarity, and afﬁne, have a linear relationship between the amount of motion ∆x = x′ −x\nand the unknown parameters p,\n∆x = x′ −x = J(x)p,\n(6.4)\nwhere J = ∂f/∂p is the Jacobian of the transformation f with respect to the motion param-\neters p (see Table 6.1). In this case, a simple linear regression (linear least squares problem)\ncan be formulated as\nELLS\n=\nX\ni\n∥J(xi)p −∆xi∥2\n(6.5)\n=\npT\n\"X\ni\nJT (xi)J(xi)\n#\np −2pT\n\"X\ni\nJT (xi)∆xi\n#\n+\nX\ni\n∥∆xi∥2 (6.6)\n=\npT Ap −2pT b + c.\n(6.7)\nThe minimum can be found by solving the symmetric positive deﬁnite (SPD) system of nor-\nmal equations2\nAp = b,\n(6.8)\nwhere\nA =\nX\ni\nJT (xi)J(xi)\n(6.9)\nis called the Hessian and b = P\ni JT (xi)∆xi. For the case of pure translation, the result-\ning equations have a particularly simple form, i.e., the translation is the average translation\nbetween corresponding points or, equivalently, the translation of the point centroids.\nUncertainty weighting.\nThe above least squares formulation assumes that all feature points\nare matched with the same accuracy. This is often not the case, since certain points may fall\ninto more textured regions than others. If we associate a scalar variance estimate σ2\ni with\neach correspondence, we can minimize the weighted least squares problem instead,3\nEWLS =\nX\ni\nσ−2\ni\n∥ri∥2.\n(6.10)\nAs shown in Section 8.1.3, a covariance estimate for patch-based matching can be obtained\nby multiplying the inverse of the patch Hessian Ai (8.55) with the per-pixel noise covariance\n2 For poorly conditioned problems, it is better to use QR decomposition on the set of linear equations J(xi)p =\n∆xi instead of the normal equations (Bj¨orck 1996; Golub and Van Loan 1996). However, such conditions rarely\narise in image registration.\n3 Problems where each measurement can have a different variance or certainty are called heteroscedastic models.",
  "336": "314\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 6.3\nA simple panograph consisting of three images automatically aligned with a\ntranslational model and then averaged together.\nσ2\nn (8.44). Weighting each squared residual by its inverse covariance Σ−1\ni\n= σ−2\nn Ai (which\nis called the information matrix), we obtain\nECWLS =\nX\ni\n∥ri∥2\nΣ−1\ni\n=\nX\ni\nrT\ni Σ−1\ni ri =\nX\ni\nσ−2\nn rT\ni Airi.\n(6.11)\n6.1.2 Application: Panography\nOne of the simplest (and most fun) applications of image alignment is a special form of image\nstitching called panography. In a panograph, images are translated and optionally rotated and\nscaled before being blended with simple averaging (Figure 6.3). This process mimics the\nphotographic collages created by artist David Hockney, although his compositions use an\nopaque overlay model, being created out of regular photographs.\nIn most of the examples seen on the Web, the images are aligned by hand for best artistic\neffect.4 However, it is also possible to use feature matching and alignment techniques to\nperform the registration automatically (Nomura, Zhang, and Nayar 2007; Zelnik-Manor and\nPerona 2007).\nConsider a simple translational model. We want all the corresponding features in different\nimages to line up as best as possible. Let tj be the location of the jth image coordinate frame\nin the global composite frame and xij be the location of the ith matched feature in the jth\nimage. In order to align the images, we wish to minimize the least squares error\nEPLS =\nX\nij\n∥(tj + xij) −xi∥2,\n(6.12)\n4 http://www.ﬂickr.com/groups/panography/.",
  "337": "6.1 2D and 3D feature-based alignment\n315\nwhere xi is the consensus (average) position of feature i in the global coordinate frame.\n(An alternative approach is to register each pair of overlapping images separately and then\ncompute a consensus location for each frame—see Exercise 6.2.)\nThe above least squares problem is indeterminate (you can add a constant offset to all the\nframe and point locations tj and xi). To ﬁx this, either pick one frame as being at the origin\nor add a constraint to make the average frame offsets be 0.\nThe formulas for adding rotation and scale transformations are straightforward and are\nleft as an exercise (Exercise 6.2). See if you can create some collages that you would be\nhappy to share with others on the Web.\n6.1.3 Iterative algorithms\nWhile linear least squares is the simplest method for estimating parameters, most problems in\ncomputer vision do not have a simple linear relationship between the measurements and the\nunknowns. In this case, the resulting problem is called non-linear least squares or non-linear\nregression.\nConsider, for example, the problem of estimating a rigid Euclidean 2D transformation\n(translation plus rotation) between two sets of points. If we parameterize this transformation\nby the translation amount (tx, ty) and the rotation angle θ, as in Table 2.1, the Jacobian of\nthis transformation, given in Table 6.1, depends on the current value of θ. Notice how in\nTable 6.1, we have re-parameterized the motion matrices so that they are always the identity\nat the origin p = 0, which makes it easier to initialize the motion parameters.\nTo minimize the non-linear least squares problem, we iteratively ﬁnd an update ∆p to the\ncurrent parameter estimate p by minimizing\nENLS(∆p)\n=\nX\ni\n∥f(xi; p + ∆p) −x′\ni∥2\n(6.13)\n≈\nX\ni\n∥J(xi; p)∆p −ri∥2\n(6.14)\n=\n∆pT\n\"X\ni\nJT J\n#\n∆p −2∆pT\n\"X\ni\nJT ri\n#\n+\nX\ni\n∥ri∥2\n(6.15)\n=\n∆pT A∆p −2∆pT b + c,\n(6.16)\nwhere the “Hessian”5 A is the same as Equation (6.9) and the right hand side vector\nb =\nX\ni\nJT (xi)ri\n(6.17)\n5 The “Hessian” A is not the true Hessian (second derivative) of the non-linear least squares problem (6.13).\nInstead, it is the approximate Hessian, which neglects second (and higher) order derivatives of f(xi; p + ∆p).",
  "338": "316\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nis now a Jacobian-weighted sum of residual vectors. This makes intuitive sense, as the pa-\nrameters are pulled in the direction of the prediction error with a strength proportional to the\nJacobian.\nOnce A and b have been computed, we solve for ∆p using\n(A + λdiag(A))∆p = b,\n(6.18)\nand update the parameter vector p ←p + ∆p accordingly. The parameter λ is an addi-\ntional damping parameter used to ensure that the system takes a “downhill” step in energy\n(squared error) and is an essential component of the Levenberg–Marquardt algorithm (de-\nscribed in more detail in Appendix A.3). In many applications, it can be set to 0 if the system\nis successfully converging.\nFor the case of our 2D translation+rotation, we end up with a 3×3 set of normal equations\nin the unknowns (δtx, δty, δθ). An initial guess for (tx, ty, θ) can be obtained by ﬁtting a\nfour-parameter similarity transform in (tx, ty, c, s) and then setting θ = tan−1(s/c). An\nalternative approach is to estimate the translation parameters using the centroids of the 2D\npoints and to then estimate the rotation angle using polar coordinates (Exercise 6.3).\nFor the other 2D motion models, the derivatives in Table 6.1 are all fairly straightforward,\nexcept for the projective 2D motion (homography), which arises in image-stitching applica-\ntions (Chapter 9). These equations can be re-written from (2.21) in their new parametric form\nas\nx′ = (1 + h00)x + h01y + h02\nh20x + h21y + 1\nand y′ = h10x + (1 + h11)y + h12\nh20x + h21y + 1\n.\n(6.19)\nThe Jacobian is therefore\nJ = ∂f\n∂p = 1\nD\n\"\nx\ny\n1\n0\n0\n0\n−x′x\n−x′y\n0\n0\n0\nx\ny\n1\n−y′x\n−y′y\n#\n,\n(6.20)\nwhere D = h20x + h21y + 1 is the denominator in (6.19), which depends on the current\nparameter settings (as do x′ and y′).\nAn initial guess for the eight unknowns {h00, h01, . . . , h21} can be obtained by multiply-\ning both sides of the equations in (6.19) through by the denominator, which yields the linear\nset of equations,\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\nx\ny\n1\n0\n0\n0\n−ˆx′x\n−ˆx′y\n0\n0\n0\nx\ny\n1\n−ˆy′x\n−ˆy′y\n#\n\n\nh00\n...\nh21\n\n.\n(6.21)\nHowever, this is not optimal from a statistical point of view, since the denominator D, which\nwas used to multiply each equation, can vary quite a bit from point to point.6\n6 Hartley and Zisserman (2004) call this strategy of forming linear equations from rational equations the direct",
  "339": "6.1 2D and 3D feature-based alignment\n317\nOne way to compensate for this is to reweight each equation by the inverse of the current\nestimate of the denominator, D,\n1\nD\n\"\nˆx′ −x\nˆy′ −y\n#\n= 1\nD\n\"\nx\ny\n1\n0\n0\n0\n−ˆx′x\n−ˆx′y\n0\n0\n0\nx\ny\n1\n−ˆy′x\n−ˆy′y\n#\n\n\nh00\n...\nh21\n\n.\n(6.22)\nWhile this may at ﬁrst seem to be the exact same set of equations as (6.21), because least\nsquares is being used to solve the over-determined set of equations, the weightings do matter\nand produce a different set of normal equations that performs better in practice.\nThe most principled way to do the estimation, however, is to directly minimize the squared\nresidual equations (6.13) using the Gauss–Newton approximation, i.e., performing a ﬁrst-\norder Taylor series expansion in p, as shown in (6.14), which yields the set of equations\n\"\nˆx′ −˜x′\nˆy′ −˜y′\n#\n= 1\nD\n\"\nx\ny\n1\n0\n0\n0\n−˜x′x\n−˜x′y\n0\n0\n0\nx\ny\n1\n−˜y′x\n−˜y′y\n#\n\n\n∆h00\n...\n∆h21\n\n.\n(6.23)\nWhile these look similar to (6.22), they differ in two important respects. First, the left hand\nside consists of unweighted prediction errors rather than point displacements and the solution\nvector is a perturbation to the parameter vector p. Second, the quantities inside J involve\npredicted feature locations (˜x′, ˜y′) instead of sensed feature locations (ˆx′, ˆy′). Both of these\ndifferences are subtle and yet they lead to an algorithm that, when combined with proper\nchecking for downhill steps (as in the Levenberg–Marquardt algorithm), will converge to a\nlocal minimum. Note that iterating Equations (6.22) is not guaranteed to converge, since it is\nnot minimizing a well-deﬁned energy function.\nEquation (6.23) is analogous to the additive algorithm for direct intensity-based regis-\ntration (Section 8.2), since the change to the full transformation is being computed. If we\nprepend an incremental homography to the current homography instead, i.e., we use a com-\npositional algorithm (described in Section 8.2), we get D = 1 (since p = 0) and the above\nformula simpliﬁes to\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\nx\ny\n1\n0\n0\n0\n−x2\n−xy\n0\n0\n0\nx\ny\n1\n−xy\n−y2\n#\n\n\n∆h00\n...\n∆h21\n\n,\n(6.24)\nwhere we have replaced (˜x′, ˜y′) with (x, y) for conciseness. (Notice how this results in the\nsame Jacobian as (8.63).)\nlinear transform, but that term is more commonly associated with pose estimation (Section 6.2). Note also that our\ndeﬁnition of the hij parameters differs from that used in their book, since we deﬁne hii to be the difference from\nunity and we do not leave h22 as a free parameter, which means that we cannot handle certain extreme homographies.",
  "340": "318\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n6.1.4 Robust least squares and RANSAC\nWhile regular least squares is the method of choice for measurements where the noise follows\na normal (Gaussian) distribution, more robust versions of least squares are required when\nthere are outliers among the correspondences (as there almost always are). In this case, it is\npreferable to use an M-estimator (Huber 1981; Hampel, Ronchetti, Rousseeuw et al. 1986;\nBlack and Rangarajan 1996; Stewart 1999), which involves applying a robust penalty function\nρ(r) to the residuals\nERLS(∆p) =\nX\ni\nρ(∥ri∥)\n(6.25)\ninstead of squaring them.\nWe can take the derivative of this function with respect to p and set it to 0,\nX\ni\nψ(∥ri∥)∂∥ri∥\n∂p\n=\nX\ni\nψ(∥ri∥)\n∥ri∥\nrT\ni\n∂ri\n∂p = 0,\n(6.26)\nwhere ψ(r) = ρ′(r) is the derivative of ρ and is called the inﬂuence function. If we introduce\na weight function, w(r) = Ψ(r)/r, we observe that ﬁnding the stationary point of (6.25) using\n(6.26) is equivalent to minimizing the iteratively reweighted least squares (IRLS) problem\nEIRLS =\nX\ni\nw(∥ri∥)∥ri∥2,\n(6.27)\nwhere the w(∥ri∥) play the same local weighting role as σ−2\ni\nin (6.10). The IRLS algo-\nrithm alternates between computing the inﬂuence functions w(∥ri∥) and solving the result-\ning weighted least squares problem (with ﬁxed w values).\nOther incremental robust least\nsquares algorithms can be found in the work of Sawhney and Ayer (1996); Black and Anan-\ndan (1996); Black and Rangarajan (1996); Baker, Gross, Ishikawa et al. (2003) and textbooks\nand tutorials on robust statistics (Huber 1981; Hampel, Ronchetti, Rousseeuw et al. 1986;\nRousseeuw and Leroy 1987; Stewart 1999).\nWhile M-estimators can deﬁnitely help reduce the inﬂuence of outliers, in some cases,\nstarting with too many outliers will prevent IRLS (or other gradient descent algorithms) from\nconverging to the global optimum. A better approach is often to ﬁnd a starting set of inlier\ncorrespondences, i.e., points that are consistent with a dominant motion estimate.7\nTwo widely used approaches to this problem are called RANdom SAmple Consensus, or\nRANSAC for short (Fischler and Bolles 1981), and least median of squares (LMS) (Rousseeuw\n1984). Both techniques start by selecting (at random) a subset of k correspondences, which is\n7 For pixel-based alignment methods (Section 8.1.1), hierarchical (coarse-to-ﬁne) techniques are often used to\nlock onto the dominant motion in a scene.",
  "341": "6.1 2D and 3D feature-based alignment\n319\nthen used to compute an initial estimate for p. The residuals of the full set of correspondences\nare then computed as\nri = ˜x′\ni(xi; p) −ˆx′\ni,\n(6.28)\nwhere ˜x′\ni are the estimated (mapped) locations and ˆx′\ni are the sensed (detected) feature point\nlocations.\nThe RANSAC technique then counts the number of inliers that are within ϵ of their pre-\ndicted location, i.e., whose ∥ri∥≤ϵ. (The ϵ value is application dependent but is often\naround 1–3 pixels.) Least median of squares ﬁnds the median value of the ∥ri∥2 values. The\nrandom selection process is repeated S times and the sample set with the largest number of\ninliers (or with the smallest median residual) is kept as the ﬁnal solution. Either the initial\nparameter guess p or the full set of computed inliers is then passed on to the next data ﬁtting\nstage.\nWhen the number of measurements is quite large, it may be preferable to only score a\nsubset of the measurements in an initial round that selects the most plausible hypotheses for\nadditional scoring and selection. This modiﬁcation of RANSAC, which can signiﬁcantly\nspeed up its performance, is called Preemptive RANSAC (Nist´er 2003). In another variant\non RANSAC called PROSAC (PROgressive SAmple Consensus), random samples are ini-\ntially added from the most “conﬁdent” matches, thereby speeding up the process of ﬁnding a\n(statistically) likely good set of inliers (Chum and Matas 2005).\nTo ensure that the random sampling has a good chance of ﬁnding a true set of inliers, a\nsufﬁcient number of trials S must be tried. Let p be the probability that any given correspon-\ndence is valid and P be the total probability of success after S trials. The likelihood in one\ntrial that all k random samples are inliers is pk. Therefore, the likelihood that S such trials\nwill all fail is\n1 −P = (1 −pk)S\n(6.29)\nand the required minimum number of trials is\nS = log(1 −P)\nlog(1 −pk).\n(6.30)\nStewart (1999) gives examples of the required number of trials S to attain a 99% proba-\nbility of success. As you can see from Table 6.2, the number of trials grows quickly with the\nnumber of sample points used. This provides a strong incentive to use the minimum number\nof sample points k possible for any given trial, which is how RANSAC is normally used in\npractice.\nUncertainty modeling\nIn addition to robustly computing a good alignment, some applications require the compu-\ntation of uncertainty (see Appendix B.6). For linear problems, this estimate can be obtained",
  "342": "320\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nk\np\nS\n3\n0.5\n35\n6\n0.6\n97\n6\n0.5\n293\nTable 6.2 Number of trials S to attain a 99% probability of success (Stewart 1999).\nby inverting the Hessian matrix (6.9) and multiplying it by the feature position noise (if these\nhave not already been used to weight the individual measurements, as in Equations (6.10)\nand 6.11)). In statistics, the Hessian, which is the inverse covariance, is sometimes called the\n(Fisher) information matrix (Appendix B.1.1).\nWhen the problem involves non-linear least squares, the inverse of the Hessian matrix\nprovides the Cramer–Rao lower bound on the covariance matrix, i.e., it provides the minimum\namount of covariance in a given solution, which can actually have a wider spread (“longer\ntails”) if the energy ﬂattens out away from the local minimum where the optimal solution is\nfound.\n6.1.5 3D alignment\nInstead of aligning 2D sets of image features, many computer vision applications require the\nalignment of 3D points. In the case where the 3D transformations are linear in the motion\nparameters, e.g., for translation, similarity, and afﬁne, regular least squares (6.5) can be used.\nThe case of rigid (Euclidean) motion,\nER3D =\nX\ni\n∥x′\ni −Rxi −t∥2,\n(6.31)\nwhich arises more frequently and is often called the absolute orientation problem (Horn\n1987), requires slightly different techniques. If only scalar weightings are being used (as\nopposed to full 3D per-point anisotropic covariance estimates), the weighted centroids of the\ntwo point clouds c and c′ can be used to estimate the translation t = c′ −Rc.8 We are then\nleft with the problem of estimating the rotation between two sets of points {ˆxi = xi −c}\nand {ˆx′\ni = x′\ni −c′} that are both centered at the origin.\nOne commonly used technique is called the orthogonal Procrustes algorithm (Golub and\nVan Loan 1996, p. 601) and involves computing the singular value decomposition (SVD) of\n8 When full covariances are used, they are transformed by the rotation and so a closed-form solution for transla-\ntion is not possible.",
  "343": "6.2 Pose estimation\n321\nthe 3 × 3 correlation matrix\nC =\nX\ni\nˆx′ˆxT = UΣV T .\n(6.32)\nThe rotation matrix is then obtained as R = UV T . (Verify this for yourself when ˆx′ = Rˆx.)\nAnother technique is the absolute orientation algorithm (Horn 1987) for estimating the\nunit quaternion corresponding to the rotation matrix R, which involves forming a 4×4 matrix\nfrom the entries in C and then ﬁnding the eigenvector associated with its largest positive\neigenvalue.\nLorusso, Eggert, and Fisher (1995) experimentally compare these two techniques to two\nadditional techniques proposed in the literature, but ﬁnd that the difference in accuracy is\nnegligible (well below the effects of measurement noise).\nIn situations where these closed-form algorithms are not applicable, e.g., when full 3D\ncovariances are being used or when the 3D alignment is part of some larger optimization, the\nincremental rotation update introduced in Section 2.1.4 (2.35–2.36), which is parameterized\nby an instantaneous rotation vector ω, can be used (See Section 9.1.3 for an application to\nimage stitching.)\nIn some situations, e.g., when merging range data maps, the correspondence between\ndata points is not known a priori. In this case, iterative algorithms that start by matching\nnearby points and then update the most likely correspondence can be used (Besl and McKay\n1992; Zhang 1994; Szeliski and Lavall´ee 1996; Gold, Rangarajan, Lu et al. 1998; David,\nDeMenthon, Duraiswami et al. 2004; Li and Hartley 2007; Enqvist, Josephson, and Kahl\n2009). These techniques are discussed in more detail in Section 12.2.1.\n6.2 Pose estimation\nA particular instance of feature-based alignment, which occurs very often, is estimating an\nobject’s 3D pose from a set of 2D point projections. This pose estimation problem is also\nknown as extrinsic calibration, as opposed to the intrinsic calibration of internal camera pa-\nrameters such as focal length, which we discuss in Section 6.3. The problem of recovering\npose from three correspondences, which is the minimal amount of information necessary,\nis known as the perspective-3-point-problem (P3P), with extensions to larger numbers of\npoints collectively known as PnP (Haralick, Lee, Ottenberg et al. 1994; Quan and Lan 1999;\nMoreno-Noguer, Lepetit, and Fua 2007).\nIn this section, we look at some of the techniques that have been developed to solve such\nproblems, starting with the direct linear transform (DLT), which recovers a 3×4 camera ma-\ntrix, followed by other “linear” algorithms, and then looking at statistically optimal iterative\nalgorithms.",
  "344": "322\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n6.2.1 Linear algorithms\nThe simplest way to recover the pose of the camera is to form a set of linear equations analo-\ngous to those used for 2D motion estimation (6.19) from the camera matrix form of perspec-\ntive projection (2.55–2.56),\nxi\n=\np00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(6.33)\nyi\n=\np10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n,\n(6.34)\nwhere (xi, yi) are the measured 2D feature locations and (Xi, Yi, Zi) are the known 3D\nfeature locations (Figure 6.4). As with (6.21), this system of equations can be solved in a\nlinear fashion for the unknowns in the camera matrix P by multiplying the denominator on\nboth sides of the equation.9 The resulting algorithm is called the direct linear transform\n(DLT) and is commonly attributed to Sutherland (1974). (For a more in-depth discussion,\nrefer to the work of Hartley and Zisserman (2004).) In order to compute the 12 (or 11)\nunknowns in P , at least six correspondences between 3D and 2D locations must be known.\nAs with the case of estimating homographies (6.21–6.23), more accurate results for the\nentries in P can be obtained by directly minimizing the set of Equations (6.33–6.34) using\nnon-linear least squares with a small number of iterations.\nOnce the entries in P have been recovered, it is possible to recover both the intrinsic\ncalibration matrix K and the rigid transformation (R, t) by observing from Equation (2.56)\nthat\nP = K[R|t].\n(6.35)\nSince K is by convention upper-triangular (see the discussion in Section 2.1.5), both K and\nR can be obtained from the front 3 × 3 sub-matrix of P using RQ factorization (Golub and\nVan Loan 1996).10\nIn most applications, however, we have some prior knowledge about the intrinsic cali-\nbration matrix K, e.g., that the pixels are square, the skew is very small, and the optical\ncenter is near the center of the image (2.57–2.59). Such constraints can be incorporated into\na non-linear minimization of the parameters in K and (R, t), as described in Section 6.2.2.\nIn the case where the camera is already calibrated, i.e., the matrix K is known (Sec-\ntion 6.3), we can perform pose estimation using as few as three points (Fischler and Bolles\n1981; Haralick, Lee, Ottenberg et al. 1994; Quan and Lan 1999). The basic observation that\nthese linear PnP (perspective n-point) algorithms employ is that the visual angle between any\n9 Because P is unknown up to a scale, we can either ﬁx one of the entries, e.g., p23 = 1, or ﬁnd the smallest\nsingular vector of the set of linear equations.\n10 Note the unfortunate clash of terminologies: In matrix algebra textbooks, R represents an upper-triangular\nmatrix; in computer vision, R is an orthogonal rotation.",
  "345": "6.2 Pose estimation\n323\npi = (Xi,Yi,Zi,Wi)\nxi\npj\ndij\ndi\ndj\nxj\nθij\nc\nFigure 6.4\nPose estimation by the direct linear transform and by measuring visual angles\nand distances between pairs of points.\npair of 2D points ˆxi and ˆxj must be the same as the angle between their corresponding 3D\npoints pi and pj (Figure 6.4).\nGiven a set of corresponding 2D and 3D points {(ˆxi, pi)}, where the ˆxi are unit directions\nobtained by transforming 2D pixel measurements xi to unit norm 3D directions ˆxi through\nthe inverse calibration matrix K,\nˆxi = N(K−1xi) = K−1xi/∥K−1xi∥,\n(6.36)\nthe unknowns are the distances di from the camera origin c to the 3D points pi, where\npi = diˆxi + c\n(6.37)\n(Figure 6.4). The cosine law for triangle ∆(c, pi, pj) gives us\nfij(di, dj) = d2\ni + d2\nj −2didjcij −d2\nij = 0,\n(6.38)\nwhere\ncij = cos θij = ˆxi · ˆxj\n(6.39)\nand\nd2\nij = ∥pi −pj∥2.\n(6.40)\nWe can take any triplet of constraints (fij, fik, fjk) and eliminate the dj and dk using\nSylvester resultants (Cox, Little, and O’Shea 2007) to obtain a quartic equation in d2\ni ,\ngijk(d2\ni ) = a4d8\ni + a3d6\ni + a2d4\ni + a1d2\ni + a0 = 0.\n(6.41)\nGiven ﬁve or more correspondences, we can generate (n−1)(n−2)\n2\ntriplets to obtain a linear\nestimate (using SVD) for the values of (d8\ni , d6\ni , d4\ni , d2\ni ) (Quan and Lan 1999). Estimates for",
  "346": "324\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nd2\ni can computed as ratios of successive d2n+2\ni\n/d2n\ni\nestimates and these can be averaged to\nobtain a ﬁnal estimate of d2\ni (and hence di).\nOnce the individual estimates of the di distances have been computed, we can generate\na 3D structure consisting of the scaled point directions diˆxi, which can then be aligned with\nthe 3D point cloud {pi} using absolute orientation (Section 6.1.5) to obtained the desired\npose estimate. Quan and Lan (1999) give accuracy results for this and other techniques,\nwhich use fewer points but require more complicated algebraic manipulations. The paper by\nMoreno-Noguer, Lepetit, and Fua (2007) reviews more recent alternatives and also gives a\nlower complexity algorithm that typically produces more accurate results.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 7.4.3), it is often preferable to use\nthe linear six-point algorithm to guess an initial pose and then optimize this estimate using\nthe iterative technique described in Section 6.2.2.\nAn alternative pose estimation algorithm involves starting with a scaled orthographic pro-\njection model and then iteratively reﬁning this initial estimate using a more accurate perspec-\ntive projection model (DeMenthon and Davis 1995). The attraction of this model, as stated\nin the paper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\n6.2.2 Iterative algorithms\nThe most accurate (and ﬂexible) way to estimate pose is to directly minimize the squared (or\nrobust) reprojection error for the 2D points as a function of the unknown pose parameters in\n(R, t) and optionally K using non-linear least squares (Tsai 1987; Bogart 1991; Gleicher\nand Witkin 1992). We can write the projection equations as\nxi = f(pi; R, t, K)\n(6.42)\nand iteratively minimize the robustiﬁed linearized reprojection errors\nENLP =\nX\ni\nρ\n\u0012 ∂f\n∂R∆R + ∂f\n∂t ∆t + ∂f\n∂K ∆K −ri\n\u0013\n,\n(6.43)\nwhere ri = ˜xi −ˆxi is the current residual vector (2D error in predicted position) and the\npartial derivatives are with respect to the unknown pose parameters (rotation, translation, and\noptionally calibration). Note that if full 2D covariance estimates are available for the 2D\nfeature locations, the above squared norm can be weighted by the inverse point covariance\nmatrix, as in Equation (6.11).\nAn easier to understand (and implement) version of the above non-linear regression prob-\nlem can be constructed by re-writing the projection equations as a concatenation of simpler\nsteps, each of which transforms a 4D homogeneous coordinate pi by a simple transformation",
  "347": "6.2 Pose estimation\n325\nfC(x) = Kx\nk\nfP(x) = p/z\nfR(x) = Rx\nqj\nfT(x) = x-c\ncj\npi\nxi\ny(1)\ny(2)\ny(3)\nFigure 6.5 A set of chained transforms for projecting a 3D point pi to a 2D measurement xi\nthrough a series of transformations f (k), each of which is controlled by its own set of param-\neters. The dashed lines indicate the ﬂow of information as partial derivatives are computed\nduring a backward pass.\nsuch as translation, rotation, or perspective division (Figure 6.5). The resulting projection\nequations can be written as\ny(1)\n=\nf T(pi; cj) = pi −cj,\n(6.44)\ny(2)\n=\nf R(y(1); qj) = R(qj) y(1),\n(6.45)\ny(3)\n=\nf P(y(2)) = y(2)\nz(2) ,\n(6.46)\nxi\n=\nf C(y(3); k) = K(k) y(3).\n(6.47)\nNote that in these equations, we have indexed the camera centers cj and camera rotation\nquaternions qj by an index j, in case more than one pose of the calibration object is being\nused (see also Section 7.4.) We are also using the camera center cj instead of the world\ntranslation tj, since this is a more natural parameter to estimate.\nThe advantage of this chained set of transformations is that each one has a simple partial\nderivative with respect both to its parameters and to its input. Thus, once the predicted value\nof ˜xi has been computed based on the 3D point location pi and the current values of the pose\nparameters (cj, qj, k), we can obtain all of the required partial derivatives using the chain\nrule\n∂ri\n∂p(k) =\n∂ri\n∂y(k)\n∂y(k)\n∂p(k) ,\n(6.48)\nwhere p(k) indicates one of the parameter vectors that is being optimized. (This same “trick”\nis used in neural networks as part of the backpropagation algorithm (Bishop 2006).)\nThe one special case in this formulation that can be considerably simpliﬁed is the compu-\ntation of the rotation update. Instead of directly computing the derivatives of the 3×3 rotation\nmatrix R(q) as a function of the unit quaternion entries, you can prepend the incremental ro-\ntation matrix ∆R(ω) given in Equation (2.35) to the current rotation matrix and compute the",
  "348": "326\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 6.6 The VideoMouse can sense six degrees of freedom relative to a specially printed\nmouse pad using its embedded camera (Hinckley, Sinclair, Hanson et al. 1999) c⃝1999\nACM: (a) top view of the mouse; (b) view of the mouse showing the curved base for rocking;\n(c) moving the mouse pad with the other hand extends the interaction capabilities; (d) the\nresulting movement seen on the screen.\npartial derivative of the transform with respect to these parameters, which results in a simple\ncross product of the backward chaining partial derivative and the outgoing 3D vector (2.36).\n6.2.3 Application: Augmented reality\nA widely used application of pose estimation is augmented reality, where virtual 3D images\nor annotations are superimposed on top of a live video feed, either through the use of see-\nthrough glasses (a head-mounted display) or on a regular computer or mobile device screen\n(Azuma, Baillot, Behringer et al. 2001; Haller, Billinghurst, and Thomas 2007). In some\napplications, a special pattern printed on cards or in a book is tracked to perform the aug-\nmentation (Kato, Billinghurst, Poupyrev et al. 2000; Billinghurst, Kato, and Poupyrev 2001).\nFor a desktop application, a grid of dots printed on a mouse pad can be tracked by a camera\nembedded in an augmented mouse to give the user control of a full six degrees of freedom\nover their position and orientation in a 3D space (Hinckley, Sinclair, Hanson et al. 1999), as\nshown in Figure 6.6.\nSometimes, the scene itself provides a convenient object to track, such as the rectangle\ndeﬁning a desktop used in through-the-lens camera control (Gleicher and Witkin 1992). In\noutdoor locations, such as ﬁlm sets, it is more common to place special markers such as\nbrightly colored balls in the scene to make it easier to ﬁnd and track them (Bogart 1991). In\nolder applications, surveying techniques were used to determine the locations of these balls\nbefore ﬁlming. Today, it is more common to apply structure-from-motion directly to the ﬁlm\nfootage itself (Section 7.4.2).\nRapid pose estimation is also central to tracking the position and orientation of the hand-\nheld remote controls used in Nintendo’s Wii game systems. A high-speed camera embedded\nin the remote control is used to track the locations of the infrared (IR) LEDs in the bar that",
  "349": "6.3 Geometric intrinsic calibration\n327\nis mounted on the TV monitor. Pose estimation is then used to infer the remote control’s\nlocation and orientation at very high frame rates. The Wii system can be extended to a variety\nof other user interaction applications by mounting the bar on a hand-held device, as described\nby Johnny Lee.11\nExercises 6.4 and 6.5 have you implement two different tracking and pose estimation sys-\ntems for augmented-reality applications. The ﬁrst system tracks the outline of a rectangular\nobject, such as a book cover or magazine page, and the second has you track the pose of a\nhand-held Rubik’s cube.\n6.3 Geometric intrinsic calibration\nAs described above in Equations (6.42–6.43), the computation of the internal (intrinsic) cam-\nera calibration parameters can occur simultaneously with the estimation of the (extrinsic)\npose of the camera with respect to a known calibration target. This, indeed, is the “classic”\napproach to camera calibration used in both the photogrammetry (Slama 1980) and the com-\nputer vision (Tsai 1987) communities. In this section, we look at alternative formulations\n(which may not involve the full solution of a non-linear regression problem), the use of alter-\nnative calibration targets, and the estimation of the non-linear part of camera optics such as\nradial distortion.12\n6.3.1 Calibration patterns\nThe use of a calibration pattern or set of markers is one of the more reliable ways to estimate\na camera’s intrinsic parameters. In photogrammetry, it is common to set up a camera in a\nlarge ﬁeld looking at distant calibration targets whose exact location has been precomputed\nusing surveying equipment (Slama 1980; Atkinson 1996; Kraus 1997). In this case, the trans-\nlational component of the pose becomes irrelevant and only the camera rotation and intrinsic\nparameters need to be recovered.\nIf a smaller calibration rig needs to be used, e.g., for indoor robotics applications or for\nmobile robots that carry their own calibration target, it is best if the calibration object can span\nas much of the workspace as possible (Figure 6.8a), as planar targets often fail to accurately\npredict the components of the pose that lie far away from the plane. A good way to determine\nif the calibration has been successfully performed is to estimate the covariance in the param-\neters (Section 6.1.4) and then project 3D points from various points in the workspace into the\nimage in order to estimate their 2D positional uncertainty.\n11 http://johnnylee.net/projects/wii/.\n12 In some applications, you can use the EXIF tags associated with a JPEG image to obtain a rough estimate of a\ncamera’s focal length but this technique should be used with caution as the results are often inaccurate.",
  "350": "328\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 6.7\nCalibrating a lens by drawing straight lines on cardboard (Debevec, Wenger,\nTchou et al. 2002) c⃝2002 ACM: (a) an image taken by the video camera showing a hand\nholding a metal ruler whose right edge appears vertical in the image; (b) the set of lines drawn\non the cardboard converging on the front nodal point (center of projection) of the lens and\nindicating the horizontal ﬁeld of view.\nAn alternative method for estimating the focal length and center of projection of a lens\nis to place the camera on a large ﬂat piece of cardboard and use a long metal ruler to draw\nlines on the cardboard that appear vertical in the image, as shown in Figure 6.7a (Debevec,\nWenger, Tchou et al. 2002). Such lines lie on planes that are parallel to the vertical axis of\nthe camera sensor and also pass through the lens’ front nodal point. The location of the nodal\npoint (projected vertically onto the cardboard plane) and the horizontal ﬁeld of view (deter-\nmined from lines that graze the left and right edges of the visible image) can be recovered by\nintersecting these lines and measuring their angular extent (Figure 6.7b).\nIf no calibration pattern is available, it is also possible to perform calibration simulta-\nneously with structure and pose recovery (Sections 6.3.4 and 7.4), which is known as self-\ncalibration (Faugeras, Luong, and Maybank 1992; Hartley and Zisserman 2004; Moons, Van\nGool, and Vergauwen 2010). However, such an approach requires a large amount of imagery\nto be accurate.\nPlanar calibration patterns\nWhen a ﬁnite workspace is being used and accurate machining and motion control platforms\nare available, a good way to perform calibration is to move a planar calibration target in a\ncontrolled fashion through the workspace volume. This approach is sometimes called the N-\nplanes calibration approach (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee,\nSzeliski et al. 1992; Grossberg and Nayar 2001) and has the advantage that each camera pixel\ncan be mapped to a unique 3D ray in space, which takes care of both linear effects modeled",
  "351": "6.3 Geometric intrinsic calibration\n329\n(a)\n(b)\nFigure 6.8 Calibration patterns: (a) a three-dimensional target (Quan and Lan 1999) c⃝1999\nIEEE; (b) a two-dimensional target (Zhang 2000) c⃝2000 IEEE. Note that radial distortion\nneeds to be removed from such images before the feature points can be used for calibration.\nby the calibration matrix K and non-linear effects such as radial distortion (Section 6.3.5).\nA less cumbersome but also less accurate calibration can be obtained by waving a pla-\nnar calibration pattern in front of a camera (Figure 6.8b). In this case, the pattern’s pose\nhas (in principle) to be recovered in conjunction with the intrinsics. In this technique, each\ninput image is used to compute a separate homography (6.19–6.23) ˜\nH mapping the plane’s\ncalibration points (Xi, Yi, 0) into image coordinates (xi, yi),\nxi =\n\n\nxi\nyi\n1\n\n∼K\nh\nr0\nr1\nt\ni\n\n\nXi\nYi\n1\n\n∼˜\nHpi,\n(6.49)\nwhere the ri are the ﬁrst two columns of R and ∼indicates equality up to scale. From\nthese, Zhang (2000) shows how to form linear constraints on the nine entries in the B =\nK−T K−1 matrix, from which the calibration matrix K can be recovered using a matrix\nsquare root and inversion. (The matrix B is known as the image of the absolute conic (IAC)\nin projective geometry and is commonly used for camera calibration (Hartley and Zisserman\n2004, Section 7.5).) If only the focal length is being recovered, the even simpler approach of\nusing vanishing points can be used instead.\n6.3.2 Vanishing points\nA common case for calibration that occurs often in practice is when the camera is looking at\na man-made scene with strong extended rectahedral objects such as boxes or room walls. In\nthis case, we can intersect the 2D lines corresponding to 3D parallel lines to compute their\nvanishing points, as described in Section 4.3.3, and use these to determine the intrinsic and\nextrinsic calibration parameters (Caprile and Torre 1990; Becker and Bove 1995; Liebowitz",
  "352": "330\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx1\nx0\nx2\nx1\nx0\nx2\nc\n(a)\n(b)\nFigure 6.9 Calibration from vanishing points: (a) any pair of ﬁnite vanishing points (ˆxi, ˆxj)\ncan be used to estimate the focal length; (b) the orthocenter of the vanishing point triangle\ngives the optical center of the image c.\nand Zisserman 1998; Cipolla, Drummond, and Robertson 1999; Antone and Teller 2002;\nCriminisi, Reid, and Zisserman 2000; Hartley and Zisserman 2004; Pﬂugfelder 2008).\nLet us assume that we have detected two or more orthogonal vanishing points, all of which\nare ﬁnite, i.e., they are not obtained from lines that appear to be parallel in the image plane\n(Figure 6.9a). Let us also assume a simpliﬁed form for the calibration matrix K where only\nthe focal length is unknown (2.59). (It is often safe for rough 3D modeling to assume that\nthe optical center is at the center of the image, that the aspect ratio is 1, and that there is no\nskew.) In this case, the projection equation for the vanishing points can be written as\nˆxi =\n\n\nxi −cx\nyi −cy\nf\n\n∼Rpi = ri,\n(6.50)\nwhere pi corresponds to one of the cardinal directions (1, 0, 0), (0, 1, 0), or (0, 0, 1), and ri\nis the ith column of the rotation matrix R.\nFrom the orthogonality between columns of the rotation matrix, we have\nri · rj ∼(xi −cx)(xj −cy) + (yi −cy)(yj −cy) + f 2 = 0\n(6.51)\nfrom which we can obtain an estimate for f 2. Note that the accuracy of this estimate increases\nas the vanishing points move closer to the center of the image. In other words, it is best to tilt\nthe calibration pattern a decent amount around the 45◦axis, as in Figure 6.9a. Once the focal\nlength f has been determined, the individual columns of R can be estimated by normalizing\nthe left hand side of (6.50) and taking cross products. Alternatively, an SVD of the initial R\nestimate, which is a variant on orthogonal Procrustes (6.32), can be used.\nIf all three vanishing points are visible and ﬁnite in the same image, it is also possible to\nestimate the optical center as the orthocenter of the triangle formed by the three vanishing\npoints (Caprile and Torre 1990; Hartley and Zisserman 2004, Section 7.6) (Figure 6.9b).",
  "353": "6.3 Geometric intrinsic calibration\n331\n(a)\n(b)\nFigure 6.10\nSingle view metrology (Criminisi, Reid, and Zisserman 2000)\nc⃝2000\nSpringer: (a) input image showing the three coordinate axes computed from the two hori-\nzontal vanishing points (which can be determined from the sidings on the shed); (b) a new\nview of the 3D reconstruction.\nIn practice, however, it is more accurate to re-estimate any unknown intrinsic calibration\nparameters using non-linear least squares (6.42).\n6.3.3 Application: Single view metrology\nA fun application of vanishing point estimation and camera calibration is the single view\nmetrology system developed by Criminisi, Reid, and Zisserman (2000). Their system allows\npeople to interactively measure heights and other dimensions as well as to build piecewise-\nplanar 3D models, as shown in Figure 6.10.\nThe ﬁrst step in their system is to identify two orthogonal vanishing points on the ground\nplane and the vanishing point for the vertical direction, which can be done by drawing some\nparallel sets of lines in the image. (Alternatively, automated techniques such as those dis-\ncussed in Section 4.3.3 or by Schaffalitzky and Zisserman (2000) could be used.) The user\nthen marks a few dimensions in the image, such as the height of a reference object, and\nthe system can automatically compute the height of another object. Walls and other planar\nimpostors (geometry) can also be sketched and reconstructed.\nIn the formulation originally developed by Criminisi, Reid, and Zisserman (2000), the\nsystem produces an afﬁne reconstruction, i.e., one that is only known up to a set of indepen-\ndent scaling factors along each axis. A potentially more useful system can be constructed by\nassuming that the camera is calibrated up to an unknown focal length, which can be recov-\nered from orthogonal (ﬁnite) vanishing directions, as we just described in Section 6.3.2. Once\nthis is done, the user can indicate an origin on the ground plane and another point a known\ndistance away. From this, points on the ground plane can be directly projected into 3D and",
  "354": "332\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 6.11\nFour images taken with a hand-held camera registered using a 3D rotation\nmotion model, which can be used to estimate the focal length of the camera (Szeliski and\nShum 1997) c⃝2000 ACM.\npoints above the ground plane, when paired with their ground plane projections, can also be\nrecovered. A fully metric reconstruction of the scene then becomes possible.\nExercise 6.9 has you implement such a system and then use it to model some simple\n3D scenes. Section 12.6.1 describes other, potentially multi-view, approaches to architectural\nreconstruction, including an interactive piecewise-planar modeling system that uses vanishing\npoints to establish 3D line directions and plane normals (Sinha, Steedly, Szeliski et al. 2008).\n6.3.4 Rotational motion\nWhen no calibration targets or known structures are available but you can rotate the camera\naround its front nodal point (or, equivalently, work in a large open environment where all ob-\njects are distant), the camera can be calibrated from a set of overlapping images by assuming\nthat it is undergoing pure rotational motion, as shown in Figure 6.11 (Stein 1995; Hartley\n1997b; Hartley, Hayman, de Agapito et al. 2000; de Agapito, Hayman, and Reid 2001; Kang\nand Weiss 1999; Shum and Szeliski 2000; Frahm and Koch 2003). When a full 360◦mo-\ntion is used to perform this calibration, a very accurate estimate of the focal length f can be\nobtained, as the accuracy in this estimate is proportional to the total number of pixels in the\nresulting cylindrical panorama (Section 9.1.6) (Stein 1995; Shum and Szeliski 2000).\nTo use this technique, we ﬁrst compute the homographies ˜\nHij between all overlapping\npairs of images, as explained in Equations (6.19–6.23). Then, we use the observation, ﬁrst\nmade in Equation (2.72) and explored in more detail in Section 9.1.3 (9.5), that each homog-\nraphy is related to the inter-camera rotation Rij through the (unknown) calibration matrices",
  "355": "6.3 Geometric intrinsic calibration\n333\nKi and Kj,\n˜\nHij = KiRiR−1\nj K−1\nj\n= KiRijK−1\nj .\n(6.52)\nThe simplest way to obtain the calibration is to use the simpliﬁed form of the calibra-\ntion matrix (2.59), where we assume that the pixels are square and the optical center lies at\nthe center of the image, i.e., Kk = diag(fk, fk, 1). (We number the pixel coordinates ac-\ncordingly, i.e., place pixel (x, y) = (0, 0) at the center of the image.) We can then rewrite\nEquation (6.52) as\nR10 ∼K−1\n1\n˜\nH10K0 ∼\n\n\nh00\nh01\nf −1\n0 h02\nh10\nh11\nf −1\n0 h12\nf1h20\nf1h21\nf −1\n0 f1h22\n\n,\n(6.53)\nwhere hij are the elements of ˜\nH10.\nUsing the orthonormality properties of the rotation matrix R10 and the fact that the right\nhand side of (6.53) is known only up to a scale, we obtain\nh2\n00 + h2\n01 + f −2\n0 h2\n02 = h2\n10 + h2\n11 + f −2\n0 h2\n12\n(6.54)\nand\nh00h10 + h01h11 + f −2\n0 h02h12 = 0.\n(6.55)\nFrom this, we can compute estimates for f0 of\nf 2\n0 =\nh2\n12 −h2\n02\nh2\n00 + h2\n01 −h2\n10 −h2\n11\nif h2\n00 + h2\n01 ̸= h2\n10 + h2\n11\n(6.56)\nor\nf 2\n0 = −\nh02h12\nh00h10 + h01h11\nif h00h10 ̸= −h01h11.\n(6.57)\n(Note that the equations originally given by Szeliski and Shum (1997) are erroneous; the\ncorrect equations are given by Shum and Szeliski (2000).) If neither of these conditions\nholds, we can also take the dot products between the ﬁrst (or second) row and the third one.\nSimilar results can be obtained for f1 as well, by analyzing the columns of ˜\nH10. If the focal\nlength is the same for both images, we can take the geometric mean of f0 and f1 as the\nestimated focal length f = √f1f0. When multiple estimates of f are available, e.g., from\ndifferent homographies, the median value can be used as the ﬁnal estimate.\nA more general (upper-triangular) estimate of K can be obtained in the case of a ﬁxed-\nparameter camera Ki = K using the technique of Hartley (1997b). Observe from (6.52)\nthat Rij ∼K−1 ˜\nHijK and R−T\nij\n∼KT ˜\nH\n−T\nij K−T . Equating Rij = R−T\nij\nwe obtain\nK−1 ˜\nHijK ∼KT ˜\nH\n−T\nij K−T , from which we get\n˜\nHij(KKT ) ∼(KKT ) ˜\nH\n−T\nij .\n(6.58)",
  "356": "334\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis provides us with some homogeneous linear constraints on the entries in A = KKT ,\nwhich is known as the dual of the image of the absolute conic (Hartley 1997b; Hartley and\nZisserman 2004). (Recall that when we estimate a homography, we can only recover it up to\nan unknown scale.) Given a sufﬁcient number of independent homography estimates ˜\nHij,\nwe can recover A (up to a scale) using either SVD or eigenvalue analysis and then recover\nK through Cholesky decomposition (Appendix A.1.4). Extensions to the cases of temporally\nvarying calibration parameters and non-stationary cameras are discussed by Hartley, Hayman,\nde Agapito et al. (2000) and de Agapito, Hayman, and Reid (2001).\nThe quality of the intrinsic camera parameters can be greatly increased by constructing a\nfull 360◦panorama, since mis-estimating the focal length will result in a gap (or excessive\noverlap) when the ﬁrst image in the sequence is stitched to itself (Figure 9.5). The resulting\nmis-alignment can be used to improve the estimate of the focal length and to re-adjust the\nrotation estimates, as described in Section 9.1.4. Rotating the camera by 90◦around its optic\naxis and re-shooting the panorama is a good way to check for aspect ratio and skew pixel\nproblems, as is generating a full hemi-spherical panorama when there is sufﬁcient texture.\nUltimately, however, the most accurate estimate of the calibration parameters (including\nradial distortion) can be obtained using a full simultaneous non-linear minimization of the\nintrinsic and extrinsic (rotation) parameters, as described in Section 9.2.\n6.3.5 Radial distortion\nWhen images are taken with wide-angle lenses, it is often necessary to model lens distor-\ntions such as radial distortion. As discussed in Section 2.1.6, the radial distortion model\nsays that coordinates in the observed images are displaced away from (barrel distortion) or\ntowards (pincushion distortion) the image center by an amount proportional to their radial\ndistance (Figure 2.13a–b). The simplest radial distortion models use low-order polynomials\n(c.f. Equation (2.78)),\nˆx\n=\nx(1 + κ1r2 + κ2r4)\nˆy\n=\ny(1 + κ1r2 + κ2r4),\n(6.59)\nwhere r2 = x2 + y2 and κ1 and κ2 are called the radial distortion parameters (Brown 1971;\nSlama 1980).13\nA variety of techniques can be used to estimate the radial distortion parameters for a\ngiven lens.14 One of the simplest and most useful is to take an image of a scene with a lot\n13 Sometimes the relationship between x and ˆx is expressed the other way around, i.e., using primed (ﬁnal)\ncoordinates on the right-hand side, x = ˆx(1 + κ1ˆr2 + κ2ˆr4). This is convenient if we map image pixels into\n(warped) rays and then undistort the rays to obtain 3D rays in space, i.e., if we are using inverse warping.\n14 Some of today’s digital cameras are starting to remove radial distortion using software in the camera itself.",
  "357": "6.4 Additional reading\n335\nof straight lines, especially lines aligned with and near the edges of the image. The radial\ndistortion parameters can then be adjusted until all of the lines in the image are straight,\nwhich is commonly called the plumb-line method (Brown 1971; Kang 2001; El-Melegy and\nFarag 2003). Exercise 6.10 gives some more details on how to implement such a technique.\nAnother approach is to use several overlapping images and to combine the estimation\nof the radial distortion parameters with the image alignment process, i.e., by extending the\npipeline used for stitching in Section 9.2.1. Sawhney and Kumar (1999) use a hierarchy\nof motion models (translation, afﬁne, projective) in a coarse-to-ﬁne strategy coupled with\na quadratic radial distortion correction term. They use direct (intensity-based) minimiza-\ntion to compute the alignment. Stein (1997) uses a feature-based approach combined with\na general 3D motion model (and quadratic radial distortion), which requires more matches\nthan a parallax-free rotational panorama but is potentially more general. More recent ap-\nproaches sometimes simultaneously compute both the unknown intrinsic parameters and the\nradial distortion coefﬁcients, which may include higher-order terms or more complex rational\nor non-parametric forms (Claus and Fitzgibbon 2005; Sturm 2005; Thirthala and Pollefeys\n2005; Barreto and Daniilidis 2005; Hartley and Kang 2005; Steele and Jaynes 2006; Tardif,\nSturm, Trudeau et al. 2009).\nWhen a known calibration target is being used (Figure 6.8), the radial distortion estima-\ntion can be folded into the estimation of the other intrinsic and extrinsic parameters (Zhang\n2000; Hartley and Kang 2007; Tardif, Sturm, Trudeau et al. 2009). This can be viewed as\nadding another stage to the general non-linear minimization pipeline shown in Figure 6.5\nbetween the intrinsic parameter multiplication box f C and the perspective division box f P.\n(See Exercise 6.11 on more details for the case of a planar calibration target.)\nOf course, as discussed in Section 2.1.6, more general models of lens distortion, such as\nﬁsheye and non-central projection, may sometimes be required. While the parameterization\nof such lenses may be more complicated (Section 2.1.6), the general approach of either us-\ning calibration rigs with known 3D positions or self-calibration through the use of multiple\noverlapping images of a scene can both be used (Hartley and Kang 2007; Tardif, Sturm, and\nRoy 2007). The same techniques used to calibrate for radial distortion can also be used to\nreduce the amount of chromatic aberration by separately calibrating each color channel and\nthen warping the channels to put them back into alignment (Exercise 6.12).\n6.4 Additional reading\nHartley and Zisserman (2004) provide a wonderful introduction to the topics of feature-based\nalignment and optimal motion estimation, as well as an in-depth discussion of camera cali-\nbration and pose estimation techniques.",
  "358": "336\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTechniques for robust estimation are discussed in more detail in Appendix B.3 and in\nmonographs and review articles on this topic (Huber 1981; Hampel, Ronchetti, Rousseeuw et\nal. 1986; Rousseeuw and Leroy 1987; Black and Rangarajan 1996; Stewart 1999). The most\ncommonly used robust initialization technique in computer vision is RANdom SAmple Con-\nsensus (RANSAC) (Fischler and Bolles 1981), which has spawned a series of more efﬁcient\nvariants (Nist´er 2003; Chum and Matas 2005).\nThe topic of registering 3D point data sets is called absolute orientation (Horn 1987) and\n3D pose estimation (Lorusso, Eggert, and Fisher 1995). A variety of techniques has been\ndeveloped for simultaneously computing 3D point correspondences and their corresponding\nrigid transformations (Besl and McKay 1992; Zhang 1994; Szeliski and Lavall´ee 1996; Gold,\nRangarajan, Lu et al. 1998; David, DeMenthon, Duraiswami et al. 2004; Li and Hartley 2007;\nEnqvist, Josephson, and Kahl 2009).\nCamera calibration was ﬁrst studied in photogrammetry (Brown 1971; Slama 1980; Atkin-\nson 1996; Kraus 1997) but it has also been widely studied in computer vision (Tsai 1987;\nGremban, Thorpe, and Kanade 1988; Champleboux, Lavall´ee, Szeliski et al. 1992; Zhang\n2000; Grossberg and Nayar 2001). Vanishing points observed either from rectahedral cali-\nbration objects or man-made architecture are often used to perform rudimentary calibration\n(Caprile and Torre 1990; Becker and Bove 1995; Liebowitz and Zisserman 1998; Cipolla,\nDrummond, and Robertson 1999; Antone and Teller 2002; Criminisi, Reid, and Zisserman\n2000; Hartley and Zisserman 2004; Pﬂugfelder 2008). Performing camera calibration without\nusing known targets is known as self-calibration and is discussed in textbooks and surveys on\nstructure from motion (Faugeras, Luong, and Maybank 1992; Hartley and Zisserman 2004;\nMoons, Van Gool, and Vergauwen 2010). One popular subset of such techniques uses pure\nrotational motion (Stein 1995; Hartley 1997b; Hartley, Hayman, de Agapito et al. 2000; de\nAgapito, Hayman, and Reid 2001; Kang and Weiss 1999; Shum and Szeliski 2000; Frahm\nand Koch 2003).\n6.5 Exercises\nEx 6.1: Feature-based image alignment for ﬂip-book animations\nTake a set of photos of\nan action scene or portrait (preferably in motor-drive—continuous shooting—mode) and\nalign them to make a composite or ﬂip-book animation.\n1. Extract features and feature descriptors using some of the techniques described in Sec-\ntions 4.1.1–4.1.2.\n2. Match your features using nearest neighbor matching with a nearest neighbor distance\nratio test (4.18).",
  "359": "6.5 Exercises\n337\n3. Compute an optimal 2D translation and rotation between the ﬁrst image and all subse-\nquent images, using least squares (Section 6.1.1) with optional RANSAC for robustness\n(Section 6.1.4).\n4. Resample all of the images onto the ﬁrst image’s coordinate frame (Section 3.6.1) using\neither bilinear or bicubic resampling and optionally crop them to their common area.\n5. Convert the resulting images into an animated GIF (using software available from the\nWeb) or optionally implement cross-dissolves to turn them into a “slo-mo” video.\n6. (Optional) Combine this technique with feature-based (Exercise 3.25) morphing.\nEx 6.2: Panography\nCreate the kind of panograph discussed in Section 6.1.2 and com-\nmonly found on the Web.\n1. Take a series of interesting overlapping photos.\n2. Use the feature detector, descriptor, and matcher developed in Exercises 4.1–4.4 (or\nexisting software) to match features among the images.\n3. Turn each connected component of matching features into a track, i.e., assign a unique\nindex i to each track, discarding any tracks that are inconsistent (contain two different\nfeatures in the same image).\n4. Compute a global translation for each image using Equation (6.12).\n5. Since your matches probably contain errors, turn the above least square metric into a\nrobust metric (6.25) and re-solve your system using iteratively reweighted least squares.\n6. Compute the size of the resulting composite canvas and resample each image into its\nﬁnal position on the canvas. (Keeping track of bounding boxes will make this more\nefﬁcient.)\n7. Average all of the images, or choose some kind of ordering and implement translucent\nover compositing (3.8).\n8. (Optional) Extend your parametric motion model to include rotations and scale, i.e.,\nthe similarity transform given in Table 6.1. Discuss how you could handle the case of\ntranslations and rotations only (no scale).\n9. (Optional) Write a simple tool to let the user adjust the ordering and opacity, and add\nor remove images.",
  "360": "338\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n10. (Optional) Write down a different least squares problem that involves pairwise match-\ning of images. Discuss why this might be better or worse than the global matching\nformula given in (6.12).\nEx 6.3: 2D rigid/Euclidean matching\nSeveral alternative approaches are given in Section 6.1.3\nfor estimating a 2D rigid (Euclidean) alignment.\n1. Implement the various alternatives and compare their accuracy on synthetic data, i.e.,\nrandom 2D point clouds with noisy feature positions.\n2. One approach is to estimate the translations from the centroids and then estimate ro-\ntation in polar coordinates. Do you need to weight the angles obtained from a polar\ndecomposition in some way to get the statistically correct estimate?\n3. How can you modify your techniques to take into account either scalar (6.10) or full\ntwo-dimensional point covariance weightings (6.11)? Do all of the previously devel-\noped “shortcuts” still work or does full weighting require iterative optimization?\nEx 6.4: 2D match move/augmented reality\nReplace a picture in a magazine or a book\nwith a different image or video.\n1. With a webcam, take a picture of a magazine or book page.\n2. Outline a ﬁgure or picture on the page with a rectangle, i.e., draw over the four sides as\nthey appear in the image.\n3. Match features in this area with each new image frame.\n4. Replace the original image with an “advertising” insert, warping the new image with\nthe appropriate homography.\n5. Try your approach on a clip from a sporting event (e.g., indoor or outdoor soccer) to\nimplement a billboard replacement.\nEx 6.5: 3D joystick\nTrack a Rubik’s cube to implement a 3D joystick/mouse control.\n1. Get out an old Rubik’s cube (or get one from your parents).\n2. Write a program to detect the center of each colored square.\n3. Group these centers into lines and then ﬁnd the vanishing points for each face.\n4. Estimate the rotation angle and focal length from the vanishing points.",
  "361": "6.5 Exercises\n339\n5. Estimate the full 3D pose (including translation) by ﬁnding one or more 3×3 grids and\nrecovering the plane’s full equation from this known homography using the technique\ndeveloped by Zhang (2000).\n6. Alternatively, since you already know the rotation, simply estimate the unknown trans-\nlation from the known 3D corner points on the cube and their measured 2D locations\nusing either linear or non-linear least squares.\n7. Use the 3D rotation and position to control a VRML or 3D game viewer.\nEx 6.6: Rotation-based calibration\nTake an outdoor or indoor sequence from a rotating\ncamera with very little parallax and use it to calibrate the focal length of your camera using\nthe techniques described in Section 6.3.4 or Sections 9.1.3–9.2.1.\n1. Take out any radial distortion in the images using one of the techniques from Exer-\ncises 6.10–6.11 or using parameters supplied for a given camera by your instructor.\n2. Detect and match feature points across neighboring frames and chain them into feature\ntracks.\n3. Compute homographies between overlapping frames and use Equations (6.56–6.57) to\nget an estimate of the focal length.\n4. Compute a full 360◦panorama and update your focal length estimate to close the gap\n(Section 9.1.4).\n5. (Optional) Perform a complete bundle adjustment in the rotation matrices and focal\nlength to obtain the highest quality estimate (Section 9.2.1).\nEx 6.7: Target-based calibration\nUse a three-dimensional target to calibrate your camera.\n1. Construct a three-dimensional calibration pattern with known 3D locations. It is not\neasy to get high accuracy unless you use a machine shop, but you can get close using\nheavy plywood and printed patterns.\n2. Find the corners, e.g, using a line ﬁnder and intersecting the lines.\n3. Implement one of the iterative calibration and pose estimation algorithms described\nin Tsai (1987); Bogart (1991); Gleicher and Witkin (1992) or the system described in\nSection 6.2.2.\n4. Take many pictures at different distances and orientations relative to the calibration\ntarget and report on both your re-projection errors and accuracy. (To do the latter, you\nmay need to use simulated data.)",
  "362": "340\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 6.8: Calibration accuracy\nCompare the three calibration techniques (plane-based, rotation-\nbased, and 3D-target-based).\nOne approach is to have a different student implement each one and to compare the results.\nAnother approach is to use synthetic data, potentially re-using the software you developed\nfor Exercise 2.3. The advantage of using synthetic data is that you know the ground truth\nfor the calibration and pose parameters, you can easily run lots of experiments, and you can\nsynthetically vary the noise in your measurements.\nHere are some possible guidelines for constructing your test sets:\n1. Assume a medium-wide focal length (say, 50◦ﬁeld of view).\n2. For the plane-based technique, generate a 2D grid target and project it at different\ninclinations.\n3. For a 3D target, create an inner cube corner and position it so that it ﬁlls most of ﬁeld\nof view.\n4. For the rotation technique, scatter points uniformly on a sphere until you get a similar\nnumber of points as for other techniques.\nBefore comparing your techniques, predict which one will be the most accurate (normalize\nyour results by the square root of the number of points used).\nAdd varying amounts of noise to your measurements and describe the noise sensitivity of\nyour various techniques.\nEx 6.9: Single view metrology\nImplement a system to measure dimensions and reconstruct\na 3D model from a single image of a man-made scene using visible vanishing directions (Sec-\ntion 6.3.3) (Criminisi, Reid, and Zisserman 2000).\n1. Find the three orthogonal vanishing points from parallel lines and use them to establish\nthe three coordinate axes (rotation matrix R of the camera relative to the scene). If\ntwo of the vanishing points are ﬁnite (not at inﬁnity), use them to compute the focal\nlength, assuming a known optical center. Otherwise, ﬁnd some other way to calibrate\nyour camera; you could use some of the techniques described by Schaffalitzky and\nZisserman (2000).\n2. Click on a ground plane point to establish your origin and click on a point a known\ndistance away to establish the scene scale. This lets you compute the translation t\nbetween the camera and the scene. As an alternative, click on a pair of points, one\non the ground plane and one above it, and use the known height to establish the scene\nscale.",
  "363": "6.5 Exercises\n341\n3. Write a user interface that lets you click on ground plane points to recover their 3D\nlocations. (Hint: you already know the camera matrix, so knowledge of a point’s z\nvalue is sufﬁcient to recover its 3D location.) Click on pairs of points (one on the\nground plane, one above it) to measure vertical heights.\n4. Extend your system to let you draw quadrilaterals in the scene that correspond to axis-\naligned rectangles in the world, using some of the techniques described by Sinha,\nSteedly, Szeliski et al. (2008). Export your 3D rectangles to a VRML or PLY15 ﬁle.\n5. (Optional) Warp the pixels enclosed by the quadrilateral using the correct homography\nto produce a texture map for each planar polygon.\nEx 6.10: Radial distortion with plumb lines\nImplement a plumb-line algorithm to deter-\nmine the radial distortion parameters.\n1. Take some images of scenes with lots of straight lines, e.g., hallways in your home or\nofﬁce, and try to get some of the lines as close to the edges of the image as possible.\n2. Extract the edges and link them into curves, as described in Section 4.2.2 and Exer-\ncise 4.8.\n3. Fit quadratic or elliptic curves to the linked edges using a generalization of the suc-\ncessive line approximation algorithm described in Section 4.3.1 and Exercise 4.11 and\nkeep the curves that ﬁt this form well.\n4. For each curved segment, ﬁt a straight line and minimize the perpendicular distance\nbetween the curve and the line while adjusting the radial distortion parameters.\n5. Alternate between re-ﬁtting the straight line and adjusting the radial distortion param-\neters until convergence.\nEx 6.11: Radial distortion with a calibration target\nUse a grid calibration target to de-\ntermine the radial distortion parameters.\n1. Print out a planar calibration target, mount it on a stiff board, and get it to ﬁll your ﬁeld\nof view.\n2. Detect the squares, lines, or dots in your calibration target.\n3. Estimate the homography mapping the target to the camera from the central portion of\nthe image that does not have any radial distortion.\n15 http://meshlab.sf.net.",
  "364": "342\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n4. Predict the positions of the remaining targets and use the differences between the ob-\nserved and predicted positions to estimate the radial distortion.\n5. (Optional) Fit a general spline model (for severe distortion) instead of the quartic dis-\ntortion model.\n6. (Optional) Extend your technique to calibrate a ﬁsheye lens.\nEx 6.12: Chromatic aberration\nUse the radial distortion estimates for each color channel\ncomputed in the previous exercise to clean up wide-angle lens images by warping all of the\nchannels into alignment. (Optional) Straighten out the images at the same time.\nCan you think of any reasons why this warping strategy may not always work?",
  "365": "Chapter 7\nStructure from motion\n7.1\nTriangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\n7.2\nTwo-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . . 347\n7.2.1\nProjective (uncalibrated) reconstruction . . . . . . . . . . . . . . . . 353\n7.2.2\nSelf-calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n7.2.3\nApplication: View morphing . . . . . . . . . . . . . . . . . . . . . . 357\n7.3\nFactorization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\n7.3.1\nPerspective and projective factorization . . . . . . . . . . . . . . . . 360\n7.3.2\nApplication: Sparse 3D model extraction\n. . . . . . . . . . . . . . . 362\n7.4\nBundle adjustment\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\n7.4.1\nExploiting sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n7.4.2\nApplication: Match move and augmented reality\n. . . . . . . . . . . 368\n7.4.3\nUncertainty and ambiguities . . . . . . . . . . . . . . . . . . . . . . 370\n7.4.4\nApplication: Reconstruction from Internet photos . . . . . . . . . . . 371\n7.5\nConstrained structure and motion . . . . . . . . . . . . . . . . . . . . . . . . 374\n7.5.1\nLine-based techniques . . . . . . . . . . . . . . . . . . . . . . . . . 374\n7.5.2\nPlane-based techniques . . . . . . . . . . . . . . . . . . . . . . . . . 376\n7.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n7.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377",
  "366": "344\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n(l)\n(m)\n(n)\nFigure 7.1\nStructure from motion systems: (a–d) orthographic factorization (Tomasi and\nKanade 1992) c⃝1992 Springer; (e–f) line matching (Schmid and Zisserman 1997) c⃝1997\nIEEE; (g–k) incremental structure from motion (Snavely, Seitz, and Szeliski 2006); (l) 3D\nreconstruction of Trafalgar Square (Snavely, Seitz, and Szeliski 2006); (m) 3D reconstruction\nof the Great Wall of China (Snavely, Seitz, and Szeliski 2006); (n) 3D reconstruction of the\nOld Town Square, Prague (Snavely, Seitz, and Szeliski 2006) c⃝2006 ACM.",
  "367": "7.1 Triangulation\n345\nIn the previous chapter, we saw how 2D and 3D point sets could be aligned and how such\nalignments could be used to estimate both a camera’s pose and its internal calibration parame-\nters. In this chapter, we look at the converse problem of estimating the locations of 3D points\nfrom multiple images given only a sparse set of correspondences between image features.\nWhile this process often involves simultaneously estimating both 3D geometry (structure)\nand camera pose (motion), it is commonly known as structure from motion (Ullman 1979).\nThe topics of projective geometry and structure from motion are extremely rich and\nsome excellent textbooks and surveys have been written on them (Faugeras and Luong 2001;\nHartley and Zisserman 2004; Moons, Van Gool, and Vergauwen 2010). This chapter skips\nover a lot of the richer material available in these books, such as the trifocal tensor and al-\ngebraic techniques for full self-calibration, and concentrates instead on the basics that we\nhave found useful in large-scale, image-based reconstruction problems (Snavely, Seitz, and\nSzeliski 2006).\nWe begin with a brief discussion of triangulation (Section 7.1), which is the problem of\nestimating a point’s 3D location when it is seen from multiple cameras. Next, we look at the\ntwo-frame structure from motion problem (Section 7.2), which involves the determination of\nthe epipolar geometry between two cameras and which can also be used to recover certain\ninformation about the camera intrinsics using self-calibration (Section 7.2.2). Section 7.3\nlooks at factorization approaches to simultaneously estimating structure and motion from\nlarge numbers of point tracks using orthographic approximations to the projection model.\nWe then develop a more general and useful approach to structure from motion, namely the\nsimultaneous bundle adjustment of all the camera and 3D structure parameters (Section 7.4).\nWe also look at special cases that arise when there are higher-level structures, such as lines\nand planes, in the scene (Section 7.5).\n7.1 Triangulation\nThe problem of determining a point’s 3D position from a set of corresponding image locations\nand known camera positions is known as triangulation. This problem is the converse of the\npose estimation problem we studied in Section 6.2.\nOne of the simplest ways to solve this problem is to ﬁnd the 3D point p that lies closest to\nall of the 3D rays corresponding to the 2D matching feature locations {xj} observed by cam-\neras {P j = Kj[Rj|tj]}, where tj = −Rjcj and cj is the jth camera center (2.55–2.56).\nAs you can see in Figure 7.2, these rays originate at cj in a direction ˆvj = N(R−1\nj K−1\nj xj).\nThe nearest point to p on this ray, which we denote as qj, minimizes the distance\n∥cj + djˆvj −p∥2,\n(7.1)",
  "368": "346\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np\nx1\nx0\nR0\nc0\nc1\nR1\nv0\nv1\nd0\nd1\nq0\n^\n^\nq1\nFigure 7.2\n3D point triangulation by ﬁnding the point p that lies nearest to all of the optical\nrays cj + djˆvj.\nwhich has a minimum at dj = ˆvj · (p −cj). Hence,\nqj = cj + (ˆvjˆvT\nj )(p −cj) = cj + (p −cj)∥,\n(7.2)\nin the notation of Equation (2.29), and the squared distance between p and qj is\nr2\nj = ∥(I −ˆvjˆvT\nj )(p −cj)∥2 = ∥(p −cj)⊥∥2.\n(7.3)\nThe optimal value for p, which lies closest to all of the rays, can be computed as a regular\nleast squares problem by summing over all the r2\nj and ﬁnding the optimal value of p,\np =\n\nX\nj\n(I −ˆvjˆvT\nj )\n\n\n−1 \nX\nj\n(I −ˆvjˆvT\nj )cj\n\n.\n(7.4)\nAn alternative formulation, which is more statistically optimal and which can produce\nsigniﬁcantly better estimates if some of the cameras are closer to the 3D point than others, is\nto minimize the residual in the measurement equations\nxj\n=\np(j)\n00 X + p(j)\n01 Y + p(j)\n02 Z + p(j)\n03 W\np(j)\n20 X + p(j)\n21 Y + p(j)\n22 Z + p(j)\n23 W\n(7.5)\nyj\n=\np(j)\n10 X + p(j)\n11 Y + p(j)\n12 Z + p(j)\n13 W\np(j)\n20 X + p(j)\n21 Y + p(j)\n22 Z + p(j)\n23 W\n,\n(7.6)\nwhere (xj, yj) are the measured 2D feature locations and {p(j)\n00 . . . p(j)\n23 } are the known entries\nin camera matrix P j (Sutherland 1974).\nAs with Equations (6.21, 6.33, and 6.34), this set of non-linear equations can be converted\ninto a linear least squares problem by multiplying both sides of the denominator. Note that if",
  "369": "7.2 Two-frame structure from motion\n347\nwe use homogeneous coordinates p = (X, Y, Z, W), the resulting set of equations is homo-\ngeneous and is best solved as a singular value decomposition (SVD) or eigenvalue problem\n(looking for the smallest singular vector or eigenvector). If we set W = 1, we can use regular\nlinear least squares, but the resulting system may be singular or poorly conditioned, i.e., if all\nof the viewing rays are parallel, as occurs for points far away from the camera.\nFor this reason, it is generally preferable to parameterize 3D points using homogeneous\ncoordinates, especially if we know that there are likely to be points at greatly varying dis-\ntances from the cameras. Of course, minimizing the set of observations (7.5–7.6) using non-\nlinear least squares, as described in (6.14 and 6.23), is preferable to using linear least squares,\nregardless of the representation chosen.\nFor the case of two observations, it turns out that the location of the point p that exactly\nminimizes the true reprojection error (7.5–7.6) can be computed using the solution of degree\nsix equations (Hartley and Sturm 1997). Another problem to watch out for with triangulation\nis the issue of chirality, i.e., ensuring that the reconstructed points lie in front of all the\ncameras (Hartley 1998). While this cannot always be guaranteed, a useful heuristic is to take\nthe points that lie behind the cameras because their rays are diverging (imagine Figure 7.2\nwhere the rays were pointing away from each other) and to place them on the plane at inﬁnity\nby setting their W values to 0.\n7.2 Two-frame structure from motion\nSo far in our study of 3D reconstruction, we have always assumed that either the 3D point\npositions or the 3D camera poses are known in advance. In this section, we take our ﬁrst look\nat structure from motion, which is the simultaneous recovery of 3D structure and pose from\nimage correspondences.\nConsider Figure 7.3, which shows a 3D point p being viewed from two cameras whose\nrelative position can be encoded by a rotation R and a translation t. Since we do not know\nanything about the camera positions, without loss of generality, we can set the ﬁrst camera at\nthe origin c0 = 0 and at a canonical orientation R0 = I.\nNow notice that the observed location of point p in the ﬁrst image, p0 = d0ˆx0 is mapped\ninto the second image by the transformation\nd1ˆx1 = p1 = Rp0 + t = R(d0ˆx0) + t,\n(7.7)\nwhere ˆxj = K−1\nj xj are the (local) ray direction vectors. Taking the cross product of both\nsides with t in order to annihilate it on the right hand side yields1\nd1[t]×ˆx1 = d0[t]×Rˆx0.\n(7.8)\n1 The cross-product operator [ ]× was introduced in (2.32).",
  "370": "348\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n       epipolar plane\np∞\np\n(R,t)\nc0\nc1\nepipolar\nlines\nx\n0\ne0\ne1\nx1\nl1\nl0\nFigure 7.3 Epipolar geometry: The vectors t = c1 −c0, p −c0 and p −c1 are co-planar\nand deﬁne the basic epipolar constraint expressed in terms of the pixel measurements x0 and\nx1.\nTaking the dot product of both sides with ˆx1 yields\nd0ˆxT\n1 ([t]×R)ˆx0 = d1ˆxT\n1 [t]×ˆx1 = 0,\n(7.9)\nsince the right hand side is a triple product with two identical entries. (Another way to say\nthis is that the cross product matrix [t]× is skew symmetric and returns 0 when pre- and\npost-multiplied by the same vector.)\nWe therefore arrive at the basic epipolar constraint\nˆxT\n1 E ˆx0 = 0,\n(7.10)\nwhere\nE = [t]×R\n(7.11)\nis called the essential matrix (Longuet-Higgins 1981).\nAn alternative way to derive the epipolar constraint is to notice that in order for the cam-\neras to be oriented so that the rays ˆx0 and ˆx1 intersect in 3D at point p, the vectors connecting\nthe two camera centers c1 −c0 = −R−1\n1 t and the rays corresponding to pixels x0 and x1,\nnamely R−1\nj\nˆxj, must be co-planar. This requires that the triple product\n(ˆx0, R−1ˆx1, −R−1t) = (Rˆx0, ˆx1, −t) = ˆx1 · (t × Rˆx0) = ˆxT\n1 ([t]×R)ˆx0 = 0.\n(7.12)\nNotice that the essential matrix E maps a point ˆx0 in image 0 into a line l1 = Eˆx0\nin image 1, since ˆxT\n1 l1 = 0 (Figure 7.3). All such lines must pass through the second\nepipole e1, which is therefore deﬁned as the left singular vector of E with a 0 singular value,\nor, equivalently, the projection of the vector t into image 1. The dual (transpose) of these",
  "371": "7.2 Two-frame structure from motion\n349\nrelationships gives us the epipolar line in the ﬁrst image as l0 = ET ˆx1 and e0 as the zero-\nvalue right singular vector of E.\nGiven this fundamental relationship (7.10), how can we use it to recover the camera\nmotion encoded in the essential matrix E?\nIf we have N corresponding measurements\n{(xi0, xi1)}, we can form N homogeneous equations in the nine elements of E = {e00 . . . e22},\nxi0xi1e00\n+\nyi0xi1e01\n+\nxi1e02\n+\nxi0yi1e00\n+\nyi0yi1e11\n+\nyi1e12\n+\nxi0e20\n+\nyi0e21\n+\ne22\n=\n0\n(7.13)\nwhere xij = (xij, yij, 1). This can be written more compactly as\n[xi1 xT\ni0] ⊗E = Zi ⊗E = zi · f = 0,\n(7.14)\nwhere ⊗indicates an element-wise multiplication and summation of matrix elements, and zi\nand f are the rasterized (vector) forms of the Zi = ˆxi1ˆxT\ni0 and E matrices.2 Given N ≥8\nsuch equations, we can compute an estimate (up to scale) for the entries in E using an SVD.\nIn the presence of noisy measurements, how close is this estimate to being statistically\noptimal? If you look at the entries in (7.13), you can see that some entries are the products\nof image measurements such as xi0yi1 and others are direct image measurements (or even\nthe identity). If the measurements have comparable noise, the terms that are products of\nmeasurements have their noise ampliﬁed by the other element in the product, which can lead\nto very poor scaling, e.g., an inordinately large inﬂuence of points with large coordinates (far\naway from the image center).\nIn order to counteract this trend, Hartley (1997a) suggests that the point coordinates\nshould be translated and scaled so that their centroid lies at the origin and their variance\nis unity, i.e.,\n˜xi\n=\ns(xi −µx)\n(7.15)\n˜yi\n=\ns(xi −µy)\n(7.16)\nsuch that P\ni ˜xi = P\ni ˜yi = 0 and P\ni ˜x2\ni + P\ni ˜y2\ni = 2n, where n is the number of points.3\nOnce the essential matrix ˜E has been computed from the transformed coordinates\n{(˜xi0, ˜xi1)}, where ˜xij = T j ˆxij, the original essential matrix E can be recovered as\nE = T 1 ˜ET 0.\n(7.17)\n2 We use f instead of e to denote the rasterized form of E to avoid confusion with the epipoles ej.\n3 More precisely, Hartley (1997a) suggests scaling the points “so that the average distance from the origin is equal\nto\n√\n2” but the heuristic of unit variance is faster to compute (does not require per-point square roots) and should\nyield comparable improvements.",
  "372": "350\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn his paper, Hartley (1997a) compares the improvement due to his re-normalization strategy\nto alternative distance measures proposed by others such as Zhang (1998a,b) and concludes\nthat his simple re-normalization in most cases is as effective as (or better than) alternative\ntechniques. Torr and Fitzgibbon (2004) recommend a variant on this algorithm where the\nnorm of the upper 2 × 2 sub-matrix of E is set to 1 and show that it has even better stability\nwith respect to 2D coordinate transformations.\nOnce an estimate for the essential matrix E has been recovered, the direction of the trans-\nlation vector t can be estimated. Note that the absolute distance between the two cameras can\nnever be recovered from pure image measurements alone, regardless of how many cameras\nor points are used. Knowledge about absolute camera and point positions or distances, of-\nten called ground control points in photogrammetry, is always required to establish the ﬁnal\nscale, position, and orientation.\nTo estimate this direction ˆt, observe that under ideal noise-free conditions, the essential\nmatrix E is singular, i.e., ˆt\nT E = 0. This singularity shows up as a singular value of 0 when\nan SVD of E is performed,\nE = [ˆt]×R = UΣV T =\nh\nu0\nu1\nˆt\ni\n\n\n1\n1\n0\n\n\n\n\nvT\n0\nvT\n1\nvT\n2\n\n\n(7.18)\nWhen E is computed from noisy measurements, the singular vector associated with the small-\nest singular value gives us ˆt. (The other two singular values should be similar but are not, in\ngeneral, equal to 1 because E is only computed up to an unknown scale.)\nBecause E is rank-deﬁcient, it turns out that we actually only need seven correspondences\nof the form of Equation (7.14) instead of eight to estimate this matrix (Hartley 1994a; Torr and\nMurray 1997; Hartley and Zisserman 2004). (The advantage of using fewer correspondences\ninside a RANSAC robust ﬁtting stage is that fewer random samples need to be generated.)\nFrom this set of seven homogeneous equations (which we can stack into a 7 × 9 matrix for\nSVD analysis), we can ﬁnd two independent vectors, say f 0 and f 1 such that zi · f j = 0.\nThese two vectors can be converted back into 3 × 3 matrices E0 and E1, which span the\nsolution space for\nE = αE0 + (1 −α)E1.\n(7.19)\nTo ﬁnd the correct value of α, we observe that E has a zero determinant, since it is rank\ndeﬁcient, and hence\ndet |αE0 + (1 −α)E1| = 0.\n(7.20)\nThis gives us a cubic equation in α, which has either one or three solutions (roots). Substitut-\ning these values into (7.19) to obtain E, we can test this essential matrix against other unused\nfeature correspondences to select the correct one.",
  "373": "7.2 Two-frame structure from motion\n351\nOnce ˆt has been recovered, how can we estimate the corresponding rotation matrix R?\nRecall that the cross-product operator [ˆt]× (2.32) projects a vector onto a set of orthogonal\nbasis vectors that include ˆt, zeros out the ˆt component, and rotates the other two by 90◦,\n[ˆt]× = SZR90◦ST =\nh\ns0\ns1\nˆt\ni\n\n\n1\n1\n0\n\n\n\n\n0\n−1\n1\n0\n1\n\n\n\n\nsT\n0\nsT\n1\nˆt\nT\n\n,\n(7.21)\nwhere ˆt = s0 × s1. From Equations (7.18 and 7.21), we get\nE = [ˆt]×R = SZR90◦ST R = UΣV T ,\n(7.22)\nfrom which we can conclude that S = U. Recall that for a noise-free essential matrix,\n(Σ = Z), and hence\nR90◦U T R = V T\n(7.23)\nand\nR = URT\n90◦V T .\n(7.24)\nUnfortunately, we only know both E and ˆt up to a sign. Furthermore, the matrices U and V\nare not guaranteed to be rotations (you can ﬂip both their signs and still get a valid SVD). For\nthis reason, we have to generate all four possible rotation matrices\nR = ±URT\n±90◦V T\n(7.25)\nand keep the two whose determinant |R| = 1. To disambiguate between the remaining pair\nof potential rotations, which form a twisted pair (Hartley and Zisserman 2004, p. 240), we\nneed to pair them with both possible signs of the translation direction ±ˆt and select the\ncombination for which the largest number of points is seen in front of both cameras.4\nThe property that points must lie in front of the camera, i.e., at a positive distance along\nthe viewing rays emanating from the camera, is known as chirality (Hartley 1998). In addition\nto determining the signs of the rotation and translation, as described above, the chirality (sign\nof the distances) of the points in a reconstruction can be used inside a RANSAC procedure\n(along with the reprojection errors) to distinguish between likely and unlikely conﬁgurations.5\nChirality can also be used to transform projective reconstructions (Sections 7.2.1 and 7.2.2)\ninto quasi-afﬁne reconstructions (Hartley 1998).\nThe normalized “eight-point algorithm” (Hartley 1997a) described above is not the only\nway to estimate the camera motion from correspondences. Variants include using seven points\n4 In the noise-free case, a single point sufﬁces. It is safer, however, to test all or a sufﬁcient subset of points,\ndownweighting the ones that lie close to the plane at inﬁnity, for which it is easy to get depth reversals.\n5 Note that as points get further away from a camera, i.e., closer toward the plane at inﬁnity, errors in chirality\nbecome more likely.",
  "374": "352\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ne\nxi0\nxi1\nFigure 7.4\nPure translational camera motion results in visual motion where all the points\nmove towards (or away from) a common focus of expansion (FOE) e. They therefore satisfy\nthe triple product condition (x0, x1, e) = e · (x0 × x1) = 0.\nwhile enforcing the rank two constraint in E (7.19–7.20) and a ﬁve-point algorithm that\nrequires ﬁnding the roots of a 10th degree polynomial (Nist´er 2004). Since such algorithms\nuse fewer points to compute their estimates, they are less sensitive to outliers when used as\npart of a random sampling (RANSAC) strategy.\nPure translation (known rotation)\nIn the case where we know the rotation, we can pre-rotate the points in the second image to\nmatch the viewing direction of the ﬁrst. The resulting set of 3D points all move towards (or\naway from) the focus of expansion (FOE), as shown in Figure 7.4.6 The resulting essential\nmatrix E is (in the noise-free case) skew symmetric and so can be estimated more directly by\nsetting eij = −eji and eii = 0 in (7.13). Two points with non-zero parallax now sufﬁce to\nestimate the FOE.\nA more direct derivation of the FOE estimate can be obtained by minimizing the triple\nproduct\nX\ni\n(xi0, xi1, e)2 =\nX\ni\n((xi0 × xi1) · e)2,\n(7.26)\nwhich is equivalent to ﬁnding the null space for the set of equations\n(yi0 −yi1)e0 + (xi1 −xi0)e1 + (xi0yi1 −yi0xi1)e2 = 0.\n(7.27)\nNote that, as in the eight-point algorithm, it is advisable to normalize the 2D points to have\nunit variance before computing this estimate.\nIn situations where a large number of points at inﬁnity are available, e.g., when shooting\noutdoor scenes or when the camera motion is small compared to distant objects, this suggests\nan alternative RANSAC strategy for estimating the camera motion. First, pick a pair of\npoints to estimate a rotation, hoping that both of the points lie at inﬁnity (very far from the\n6 Fans of Star Trek and Star Wars will recognize this as the “jump to hyperdrive” visual effect.",
  "375": "7.2 Two-frame structure from motion\n353\ncamera). Then, compute the FOE and check whether the residual error is small (indicating\nagreement with this rotation hypothesis) and whether the motions towards or away from the\nepipole (FOE) are all in the same direction (ignoring very small motions, which may be\nnoise-contaminated).\nPure rotation\nThe case of pure rotation results in a degenerate estimate of the essential matrix E and of\nthe translation direction ˆt.\nConsider ﬁrst the case of the rotation matrix being known. The\nestimates for the FOE will be degenerate, since xi0 ≈xi1, and hence (7.27), is degenerate.\nA similar argument shows that the equations for the essential matrix (7.13) are also rank-\ndeﬁcient.\nThis suggests that it might be prudent before computing a full essential matrix to ﬁrst\ncompute a rotation estimate R using (6.32), potentially with just a small number of points,\nand then compute the residuals after rotating the points before proceeding with a full E\ncomputation.\n7.2.1 Projective (uncalibrated) reconstruction\nIn many cases, such as when trying to build a 3D model from Internet or legacy photos taken\nby unknown cameras without any EXIF tags, we do not know ahead of time the intrinsic\ncalibration parameters associated with the input images. In such situations, we can still esti-\nmate a two-frame reconstruction, although the true metric structure may not be available, e.g.,\northogonal lines or planes in the world may not end up being reconstructed as orthogonal.\nConsider the derivations we used to estimate the essential matrix E (7.10–7.12). In the\nuncalibrated case, we do not know the calibration matrices Kj, so we cannot use the normal-\nized ray directions ˆxj = K−1\nj xj. Instead, we have access only to the image coordinates xj,\nand so the essential matrix (7.10) becomes\nˆxT\n1 Eˆx1 = xT\n1 K−T\n1\nEK−1\n0 x0 = xT\n1 F x0 = 0,\n(7.28)\nwhere\nF = K−T\n1\nEK−1\n0\n= [e]× ˜\nH\n(7.29)\nis called the fundamental matrix (Faugeras 1992; Hartley, Gupta, and Chang 1992; Hartley\nand Zisserman 2004).\nLike the essential matrix, the fundamental matrix is (in principle) rank two,\nF = [e]× ˜\nH = UΣV T =\nh\nu0\nu1\ne1\ni\n\n\nσ0\nσ1\n0\n\n\n\n\nvT\n0\nvT\n1\neT\n0\n\n.\n(7.30)",
  "376": "354\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIts smallest left singular vector indicates the epipole e1 in the image 1 and its smallest right\nsingular vector is e0 (Figure 7.3). The homography ˜\nH in (7.29), which in principle should\nequal\n˜\nH = K−T\n1\nRK−1\n0 ,\n(7.31)\ncannot be uniquely recovered from F , since any homography of the form ˜\nH\n′ = ˜\nH + evT\nresults in the same F matrix. (Note that [e]× annihilates any multiple of e.)\nAny one of these valid homographies ˜\nH maps some plane in the scene from one image\nto the other. It is not possible to tell in advance which one it is without either selecting four\nor more co-planar correspondences to compute ˜\nH as part of the F estimation process (in a\nmanner analogous to guessing a rotation for E) or mapping all points in one image through\n˜\nH and seeing which ones line up with their corresponding locations in the other.7\nIn order to create a projective reconstruction of the scene, we can pick any valid homog-\nraphy ˜\nH that satisﬁes Equation (7.29). For example, following a technique analogous to\nEquations (7.18–7.24), we get\nF = [e]× ˜\nH = SZR90◦ST ˜\nH = UΣV T\n(7.32)\nand hence\n˜\nH = URT\n90◦ˆΣV T ,\n(7.33)\nwhere ˆΣ is the singular value matrix with the smallest value replaced by a reasonable alter-\nnative (say, the middle value).8 We can then form a pair of camera matrices\nP 0 = [I|0]\nand\nP 0 = [ ˜\nH|e],\n(7.34)\nfrom which a projective reconstruction of the scene can be computed using triangulation\n(Section 7.1).\nWhile the projective reconstruction may not be useful in practice, it can often be upgraded\nto an afﬁne or metric reconstruction, as detailed below. Even without this step, however,\nthe fundamental matrix F can be very useful in ﬁnding additional correspondences, as they\nmust all lie on corresponding epipolar lines, i.e., any feature x0 in image 0 must have its\ncorrespondence lying on the associated epipolar line l1 = F x0 in image 1, assuming that the\npoint motions are due to a rigid transformation.\n7 This process is sometimes referred to as plane plus parallax (Section 2.1.5) (Kumar, Anandan, and Hanna 1994;\nSawhney 1994).\n8 Hartley and Zisserman (2004, p. 237) recommend using ˜\nH = [e]×F (Luong and Vi´eville 1996), which places\nthe camera on the plane at inﬁnity.",
  "377": "7.2 Two-frame structure from motion\n355\n7.2.2 Self-calibration\nThe results of structure from motion computation are much more useful (and intelligible) if\na metric reconstruction is obtained, i.e., one in which parallel lines are parallel, orthogonal\nwalls are at right angles, and the reconstructed model is a scaled version of reality. Over\nthe years, a large number of self-calibration (or auto-calibration) techniques have been de-\nveloped for converting a projective reconstruction into a metric one, which is equivalent to\nrecovering the unknown calibration matrices Kj associated with each image (Hartley and\nZisserman 2004; Moons, Van Gool, and Vergauwen 2010).\nIn situations where certain additional information is known about the scene, different\nmethods may be employed. For example, if there are parallel lines in the scene (usually,\nhaving several lines converge on the same vanishing point is good evidence), three or more\nvanishing points, which are the images of points at inﬁnity, can be used to establish the ho-\nmography for the plane at inﬁnity, from which focal lengths and rotations can be recovered.\nIf two or more ﬁnite orthogonal vanishing points have been observed, the single-image cali-\nbration method based on vanishing points (Section 6.3.2) can be used instead.\nIn the absence of such external information, it is not possible to recover a fully parameter-\nized independent calibration matrix Kj for each image from correspondences alone. To see\nthis, consider the set of all camera matrices P j = Kj[Rj|tj] projecting world coordinates\npi = (Xi, Yi, Zi, Wi) into screen coordinates xij ∼P jpi. Now consider transforming the\n3D scene {pi} through an arbitrary 4 × 4 projective transformation ˜\nH, yielding a new model\nconsisting of points p′\ni = ˜\nHpi. Post-multiplying each P j matrix by ˜\nH\n−1 still produces the\nsame screen coordinates and a new set calibration matrices can be computed by applying RQ\ndecomposition to the new camera matrix P ′\nj = P j ˜\nH\n−1.\nFor this reason, all self-calibration methods assume some restricted form of the calibration\nmatrix, either by setting or equating some of their elements or by assuming that they do not\nvary over time. While most of the techniques discussed by Hartley and Zisserman (2004);\nMoons, Van Gool, and Vergauwen (2010) require three or more frames, in this section we\npresent a simple technique that can recover the focal lengths (f0, f1) of both images from the\nfundamental matrix F in a two-frame reconstruction (Hartley and Zisserman 2004, p. 456).\nTo accomplish this, we assume that the camera has zero skew, a known aspect ratio (usu-\nally set to 1), and a known optical center, as in Equation (2.59). How reasonable is this\nassumption in practice? The answer, as with many questions, is “it depends”.\nIf absolute metric accuracy is required, as in photogrammetry applications, it is imperative\nto pre-calibrate the cameras using one of the techniques from Section 6.3 and to use ground\ncontrol points to pin down the reconstruction. If instead, we simply wish to reconstruct the\nworld for visualization or image-based rendering applications, as in the Photo Tourism system\nof Snavely, Seitz, and Szeliski (2006), this assumption is quite reasonable in practice.",
  "378": "356\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMost cameras today have square pixels and an optical center near the middle of the image,\nand are much more likely to deviate from a simple camera model due to radial distortion\n(Section 6.3.5), which should be compensated for whenever possible. The biggest problems\noccur when images have been cropped off-center, in which case the optical center will no\nlonger be in the middle, or when perspective pictures have been taken of a different picture,\nin which case a general camera matrix becomes necessary.9\nGiven these caveats, the two-frame focal length estimation algorithm based on the Kruppa\nequations developed by Hartley and Zisserman (2004, p. 456) proceeds as follows. Take the\nleft and right singular vectors {u0, u1, v0, v1} of the fundamental matrix F (7.30) and their\nassociated singular values {σ0, σ1) and form the following set of equations:\nuT\n1 D0u1\nσ2\n0vT\n0 D1v0\n= −\nuT\n0 D0u1\nσ0σ1vT\n0 D1v1\n=\nuT\n0 D0u0\nσ2\n1vT\n1 D1v1\n,\n(7.35)\nwhere the two matrices\nDj = KjKT\nj = diag(f 2\nj , f 2\nj , 1) =\n\n\nf 2\nj\nf 2\nj\n1\n\n\n(7.36)\nencode the unknown focal lengths. For simplicity, let us rewrite each of the numerators and\ndenominators in (7.35) as\neij0(f 2\n0 )\n=\nuT\ni D0uj = aij + bijf 2\n0 ,\n(7.37)\neij1(f 2\n1 )\n=\nσiσjvT\ni D1vj = cij + dijf 2\n1 .\n(7.38)\nNotice that each of these is afﬁne (linear plus constant) in either f 2\n0 or f 2\n1 .\nHence, we\ncan cross-multiply these equations to obtain quadratic equations in f 2\nj , which can readily\nbe solved. (See also the work by Bougnoux (1998) for some alternative formulations.)\nAn alternative solution technique is to observe that we have a set of three equations related\nby an unknown scalar λ, i.e.,\neij0(f 2\n0 ) = λeij1(f 2\n1 )\n(7.39)\n(Richard Hartley, personal communication, July 2009). These can readily be solved to yield\n(f 2\n0 , λf 2\n1 , λ) and hence (f0, f1).\nHow well does this approach work in practice? There are certain degenerate conﬁgura-\ntions, such as when there is no rotation or when the optical axes intersect, when it does not\nwork at all. (In such a situation, you can vary the focal lengths of the cameras and obtain\n9 In Photo Tourism, our system registered photographs of an information sign outside Notre Dame with real\npictures of the cathedral.",
  "379": "7.3 Factorization\n357\na deeper or shallower reconstruction, which is an example of a bas-relief ambiguity (Sec-\ntion 7.4.3).) Hartley and Zisserman (2004) recommend using techniques based on three or\nmore frames. However, if you ﬁnd two images for which the estimates of (f 2\n0 , λf 2\n1 , λ) are\nwell conditioned, they can be used to initialize a more complete bundle adjustment of all\nthe parameters (Section 7.4). An alternative, which is often used in systems such as Photo\nTourism, is to use camera EXIF tags or generic default values to initialize focal length esti-\nmates and reﬁne them as part of bundle adjustment.\n7.2.3 Application: View morphing\nAn interesting application of basic two-frame structure from motion is view morphing (also\nknown as view interpolation, see Section 13.1), which can be used to generate a smooth 3D\nanimation from one view of a 3D scene to another (Chen and Williams 1993; Seitz and Dyer\n1996).\nTo create such a transition, you must ﬁrst smoothly interpolate the camera matrices, i.e.,\nthe camera positions, orientations, and focal lengths. While simple linear interpolation can be\nused (representing rotations as quaternions (Section 2.1.4)), a more pleasing effect is obtained\nby easing in and easing out the camera parameters, e.g., using a raised cosine, as well as\nmoving the camera along a more circular trajectory (Snavely, Seitz, and Szeliski 2006).\nTo generate in-between frames, either a full set of 3D correspondences needs to be es-\ntablished (Section 11.3) or 3D models (proxies) must be created for each reference view.\nSection 13.1 describes several widely used approaches to this problem. One of the simplest\nis to just triangulate the set of matched feature points in each image, e.g., using Delaunay\ntriangulation. As the 3D points are re-projected into their intermediate views, pixels can be\nmapped from their original source images to their new views using afﬁne or projective map-\nping (Szeliski and Shum 1997). The ﬁnal image is then composited using a linear blend of\nthe two reference images, as with usual morphing (Section 3.6.3).\n7.3 Factorization\nWhen processing video sequences, we often get extended feature tracks (Section 4.1.4) from\nwhich it is possible to recover the structure and motion using a process called factorization.\nConsider the tracks generated by a rotating ping pong ball, which has been marked with\ndots to make its shape and motion more discernable (Figure 7.5). We can readily see from\nthe shape of the tracks that the moving object must be a sphere, but how can we infer this\nmathematically?\nIt turns out that, under orthography or related models we discuss below, the shape and\nmotion can be recovered simultaneously using a singular value decomposition (Tomasi and",
  "380": "358\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 7.5\n3D reconstruction of a rotating ping pong ball using factorization (Tomasi and\nKanade 1992) c⃝1992 Springer: (a) sample image with tracked features overlaid; (b) sub-\nsampled feature motion stream; (c) two views of the reconstructed 3D model.\nKanade 1992). Consider the orthographic and weak perspective projection models introduced\nin Equations (2.47–2.49). Since the last row is always [0 0 0 1], there is no perspective division\nand we can write\nxji = ˜\nP j ¯pi,\n(7.40)\nwhere xji is the location of the ith point in the jth frame, ˜\nP j is the upper 2 × 4 portion of\nthe projection matrix P j, and ¯pi = (Xi, Yi, Zi, 1) is the augmented 3D point position.10\nLet us assume (for now) that every point i is visible in every frame j. We can take the\ncentroid (average) of the projected point locations xji in frame j,\n¯xj = 1\nN\nX\ni\nxji = ˜\nP j\n1\nN\nX\ni\n¯pi = ˜\nP j¯c,\n(7.41)\nwhere ¯c = ( ¯X, ¯Y , ¯Z, 1) is the augmented 3D centroid of the point cloud.\nSince world coordinate frames in structure from motion are always arbitrary, i.e., we\ncannot recover true 3D locations without ground control points (known measurements), we\ncan place the origin of the world at the centroid of the points, i.e, ¯X = ¯Y = ¯Z = 0, so that\n¯c = (0, 0, 0, 1). We see from this that the centroid of the 2D points in each frame ¯xj directly\ngives us the last element of ˜\nP j.\nLet ˜xji = xji −¯xj be the 2D point locations after their image centroid has been sub-\ntracted. We can now write\n˜xji = M jpi,\n(7.42)\n10 In this section, we index the 2D point positions as xji instead of xij, since this is the convention adopted by\nfactorization papers (Tomasi and Kanade 1992) and is consistent with the factorization given in (7.43).",
  "381": "7.3 Factorization\n359\nwhere M j is the upper 2 × 3 portion of the projection matrix P j and pi = (Xi, Yi, Zi). We\ncan concatenate all of these measurement equations into one large matrix\nˆ\nX =\n\n\n˜x11 · · · ˜x1i · · · ˜x1N\n...\n...\n...\n˜xj1 · · · ˜xji · · · ˜xjN\n...\n...\n...\n˜xM1 · · · ˜xMi · · · ˜xMN\n\n\n=\n\n\nM 1\n...\nM j\n...\nM M\n\n\nh\np1 · · · pi · · · pN\ni\n= ˆ\nM ˆS.\n(7.43)\nˆ\nX is called the measurement matrix and ˆ\nM and ( ˆS are the motion) and structure matrices,\nrespectively (Tomasi and Kanade 1992).\nBecause the motion matrix ˆ\nM is 2M × 3 and the structure matrix ˆS is 3 × N, an SVD\napplied to ˆ\nX has only three non-zero singular values. In the case where the measurements in\nˆ\nX are noisy, SVD returns the rank-three factorization of ˆ\nX that is the closest to ˆ\nX in a least\nsquares sense (Tomasi and Kanade 1992; Golub and Van Loan 1996; Hartley and Zisserman\n2004).\nIt would be nice if the SVD of ˆ\nX = UΣV T directly returned the matrices ˆ\nM and ˆS,\nbut it does not. Instead, we can write the relationship\nˆ\nX = UΣV T = [UQ][Q−1ΣV T ]\n(7.44)\nand set ˆ\nM = UQ and ˆS = Q−1ΣV T .11\nHow can we recover the values of the 3×3 matrix Q? This depends on the motion model\nbeing used. In the case of orthographic projection (2.47), the entries in M j are the ﬁrst two\nrows of rotation matrices Rj, so we have\nmj0 · mj0 =\nu2jQQT uT\n2j\n= 1,\nmj0 · mj1 =\nu2jQQT uT\n2j+1\n= 0,\nmj1 · mj1 =\nu2j+1QQT uT\n2j+1\n= 1,\n(7.45)\nwhere uk are the 3 × 1 rows of the matrix U. This gives us a large set of equations for the\nentries in the matrix QQT , from which the matrix Q can be recovered using a matrix square\nroot (Appendix A.1.4). If we have scaled orthography (2.48), i.e., M j = sjRj, the ﬁrst and\nthird equations are equal to sj and can be set equal to each other.\nNote that even once Q has been recovered, there still exists a bas-relief ambiguity, i.e.,\nwe can never be sure if the object is rotating left to right or if its depth reversed version is\nmoving the other way. (This can be seen in the classic rotating Necker Cube visual illusion.)\n11 Tomasi and Kanade (1992) ﬁrst take the square root of Σ and distribute this to U and V , but there is no\nparticular reason to do this.",
  "382": "360\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAdditional cues, such as the appearance and disappearance of points, or perspective effects,\nboth of which are discussed below, can be used to remove this ambiguity.\nFor motion models other than pure orthography, e.g., for scaled orthography or para-\nperspective, the approach above must be extended in the appropriate manner. Such tech-\nniques are relatively straightforward to derive from ﬁrst principles; more details can be found\nin papers that extend the basic factorization approach to these more ﬂexible models (Poel-\nman and Kanade 1997). Additional extensions of the original factorization algorithm include\nmulti-body rigid motion (Costeira and Kanade 1995), sequential updates to the factorization\n(Morita and Kanade 1997), the addition of lines and planes (Morris and Kanade 1998), and\nre-scaling the measurements to incorporate individual location uncertainties (Anandan and\nIrani 2002).\nA disadvantage of factorization approaches is that they require a complete set of tracks,\ni.e., each point must be visible in each frame, in order for the factorization approach to work.\nTomasi and Kanade (1992) deal with this problem by ﬁrst applying factorization to smaller\ndenser subsets and then using known camera (motion) or point (structure) estimates to hallu-\ncinate additional missing values, which allows them to incrementally incorporate more fea-\ntures and cameras. Huynh, Hartley, and Heyden (2003) extend this approach to view missing\ndata as special cases of outliers. Buchanan and Fitzgibbon (2005) develop fast iterative al-\ngorithms for performing large matrix factorizations with missing data. The general topic of\nprincipal component analysis (PCA) with missing data also appears in other computer vision\nproblems (Shum, Ikeuchi, and Reddy 1995; De la Torre and Black 2003; Gross, Matthews,\nand Baker 2006; Torresani, Hertzmann, and Bregler 2008; Vidal, Ma, and Sastry 2010).\n7.3.1 Perspective and projective factorization\nAnother disadvantage of regular factorization is that it cannot deal with perspective cameras.\nOne way to get around this problem is to perform an initial afﬁne (e.g., orthographic) recon-\nstruction and to then correct for the perspective effects in an iterative manner (Christy and\nHoraud 1996).\nObserve that the object-centered projection model (2.76)\nxji\n=\nsj\nrxj · pi + txj\n1 + ηjrzj · pi\n(7.46)\nyji\n=\nsj\nryj · pi + tyj\n1 + ηjrzj · pi\n(7.47)\ndiffers from the scaled orthographic projection model (7.40) by the inclusion of the denomi-\nnator terms (1 + ηjrzj · pi).12\n12 Assuming that the optical center (cx, cy) lies at (0, 0) and that pixels are square.",
  "383": "7.3 Factorization\n361\nIf we knew the correct values of ηj = t−1\nzj and the structure and motion parameters Rj and\npi, we could cross-multiply the left hand side (visible point measurements xji and yji) by the\ndenominator and get corrected values, for which the bilinear projection model (7.40) is exact.\nIn practice, after an initial reconstruction, the values of ηj can be estimated independently\nfor each frame by comparing reconstructed and sensed point positions. (The third row of the\nrotation matrix rzj is always available as the cross-product of the ﬁrst two rows.) Note that\nsince the ηj are determined from the image measurements, the cameras do not have to be\npre-calibrated, i.e., their focal lengths can be recovered from fj = sj/ηj.\nOnce the ηj have been estimated, the feature locations can then be corrected before apply-\ning another round of factorization. Note that because of the initial depth reversal ambiguity,\nboth reconstructions have to be tried while calculating ηj. (The incorrect reconstruction will\nresult in a negative ηj, which is not physically meaningful.) Christy and Horaud (1996) report\nthat their algorithm usually converges in three to ﬁve iterations, with the majority of the time\nspent in the SVD computation.\nAn alternative approach, which does not assume partially calibrated cameras (known op-\ntical center, square pixels, and zero skew) is to perform a fully projective factorization (Sturm\nand Triggs 1996; Triggs 1996). In this case, the inclusion of the third row of the camera\nmatrix in (7.40) is equivalent to multiplying each reconstructed measurement xji = M jpi\nby its inverse (projective) depth ηji = d−1\nji = 1/(P j2pi) or, equivalently, multiplying each\nmeasured position by its projective depth dji,\nˆ\nX =\n\n\nd11˜x11\n· · ·\nd1i˜x1i\n· · ·\nd1N ˜x1N\n...\n...\n...\ndj1˜xj1\n· · ·\ndji˜xji\n· · ·\ndjN ˜xjN\n...\n...\n...\ndM1˜xM1 · · · dMi˜xMi · · · dMN ˜xMN\n\n\n= ˆ\nM ˆS.\n(7.48)\nIn the original paper by Sturm and Triggs (1996), the projective depths dji are obtained from\ntwo-frame reconstructions, while in later work (Triggs 1996; Oliensis and Hartley 2007), they\nare initialized to dji = 1 and updated after each iteration. Oliensis and Hartley (2007) present\nan update formula that is guaranteed to converge to a ﬁxed point. None of these authors\nsuggest actually estimating the third row of P j as part of the projective depth computations.\nIn any case, it is unclear when a fully projective reconstruction would be preferable to a\npartially calibrated one, especially if they are being used to initialize a full bundle adjustment\nof all the parameters.\nOne of the attractions of factorization methods is that they provide a “closed form” (some-\ntimes called a “linear”) method to initialize iterative techniques such as bundle adjustment.\nAn alternative initialization technique is to estimate the homographies corresponding to some",
  "384": "362\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 7.6\n3D teacup model reconstructed from a 240-frame video sequence (Tomasi and\nKanade 1992) c⃝1992 Springer: (a) ﬁrst frame of video; (b) last frame of video; (c) side\nview of 3D model; (d) top view of 3D model.\ncommon plane seen by all the cameras (Rother and Carlsson 2002). In a calibrated camera\nsetting, this can correspond to estimating consistent rotations for all of the cameras, for ex-\nample, using matched vanishing points (Antone and Teller 2002). Once these have been\nrecovered, the camera positions can then be obtained by solving a linear system (Antone and\nTeller 2002; Rother and Carlsson 2002; Rother 2003).\n7.3.2 Application: Sparse 3D model extraction\nOnce a multi-view 3D reconstruction of the scene has been estimated, it then becomes possi-\nble to create a texture-mapped 3D model of the object and to look at it from new directions.\nThe ﬁrst step is to create a denser 3D model than the sparse point cloud that structure\nfrom motion produces. One alternative is to run dense multi-view stereo (Sections 11.3–\n11.6). Alternatively, a simpler technique such as 3D triangulation can be used, as shown in\nFigure 7.6, in which 207 reconstructed 3D points are triangulated to produce a surface mesh.\nIn order to create a more realistic model, a texture map can be extracted for each trian-\ngle face. The equations to map points on the surface of a 3D triangle to a 2D image are\nstraightforward: just pass the local 2D coordinates on the triangle through the 3 × 4 camera\nprojection matrix to obtain a 3 × 3 homography (planar perspective projection). When mul-\ntiple source images are available, as is usually the case in multi-view reconstruction, either\nthe closest and most fronto-parallel image can be used or multiple images can be blended in\nto deal with view-dependent foreshortening (Wang, Kang, Szeliski et al. 2001) or to obtain\nsuper-resolved results (Goldluecke and Cremers 2009) Another alternative is to create a sep-\narate texture map from each reference camera and to blend between them during rendering,\nwhich is known as view-dependent texture mapping (Section 13.1.1) (Debevec, Taylor, and\nMalik 1996; Debevec, Yu, and Borshukov 1998).",
  "385": "7.4 Bundle adjustment\n363\nfC(x)\n= Kx\nfj\nfP(x)\n= p/z\nfR(x)\n= Rjx\nqj\nfT(x)\n= x-cj\ncj\npi\ny(1)\ny(2)\ny(4)\nfRD(x)\n= ...\ny(3)\nκj\nρ(||x-xij||Σ)\neij\nΣij\nxij~\nxij\n^\n^\nFigure 7.7\nA set of chained transforms for projecting a 3D point pi into a 2D measure-\nment xij through a series of transformations f (k), each of which is controlled by its own\nset of parameters. The dashed lines indicate the ﬂow of information as partial derivatives\nare computed during a backward pass. The formula for the radial distortion function is\nf RD(x) = (1 + κ1r2 + κ2r4)x.\n7.4 Bundle adjustment\nAs we have mentioned several times before, the most accurate way to recover structure and\nmotion is to perform robust non-linear minimization of the measurement (re-projection) er-\nrors, which is commonly known in the photogrammetry (and now computer vision) commu-\nnities as bundle adjustment.13 Triggs, McLauchlan, Hartley et al. (1999) provide an excellent\noverview of this topic, including its historical development, pointers to the photogrammetry\nliterature (Slama 1980; Atkinson 1996; Kraus 1997), and subtle issues with gauge ambigu-\nities. The topic is also treated in depth in textbooks and surveys on multi-view geometry\n(Faugeras and Luong 2001; Hartley and Zisserman 2004; Moons, Van Gool, and Vergauwen\n2010).\nWe have already introduced the elements of bundle adjustment in our discussion on iter-\native pose estimation (Section 6.2.2), i.e., Equations (6.42–6.48) and Figure 6.5. The biggest\ndifference between these formulas and full bundle adjustment is that our feature location mea-\nsurements xij now depend not only on the point (track index) i but also on the camera pose\nindex j,\nxij = f(pi, Rj, cj, Kj),\n(7.49)\nand that the 3D point positions pi are also being simultaneously updated. In addition, it is\ncommon to add a stage for radial distortion parameter estimation (2.78),\nf RD(x) = (1 + κ1r2 + κ2r4)x,\n(7.50)\nif the cameras being used have not been pre-calibrated, as shown in Figure 7.7.\n13 The term ”bundle” refers to the bundles of rays connecting camera centers to 3D points and the term ”adjust-\nment” refers to the iterative minimization of re-projection error. Alternative terms for this in the vision community\ninclude optimal motion estimation (Weng, Ahuja, and Huang 1993) and non-linear least squares (Appendix A.3)\n(Taylor, Kriegman, and Anandan 1991; Szeliski and Kang 1994).",
  "386": "364\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhile most of the boxes (transforms) in Figure 7.7 have previously been explained (6.47),\nthe leftmost box has not. This box performs a robust comparison of the predicted and mea-\nsured 2D locations ˆxij and ˜xij after re-scaling by the measurement noise covariance Σij. In\nmore detail, this operation can be written as\nrij\n=\n˜xij −ˆxij,\n(7.51)\ns2\nij\n=\nrT\nijΣ−1\nij rij,\n(7.52)\neij\n=\nˆρ(s2\nij),\n(7.53)\nwhere ˆρ(r2) = ρ(r). The corresponding Jacobians (partial derivatives) can be written as\n∂eij\n∂s2\nij\n=\nˆρ′(s2\nij),\n(7.54)\n∂s2\nij\n∂˜xij\n=\nΣ−1\nij rij.\n(7.55)\nThe advantage of the chained representation introduced above is that it not only makes\nthe computations of the partial derivatives and Jacobians simpler but it can also be adapted\nto any camera conﬁguration. Consider for example a pair of cameras mounted on a robot\nthat is moving around in the world, as shown in Figure 7.8a. By replacing the rightmost\ntwo transformations in Figure 7.7 with the transformations shown in Figure 7.8b, we can\nsimultaneously recover the position of the robot at each time and the calibration of each\ncamera with respect to the rig, in addition to the 3D structure of the world.\n7.4.1 Exploiting sparsity\nLarge bundle adjustment problems, such as those involving reconstructing 3D scenes from\nthousands of Internet photographs (Snavely, Seitz, and Szeliski 2008b; Agarwal, Snavely,\nSimon et al. 2009; Agarwal, Furukawa, Snavely et al. 2010; Snavely, Simon, Goesele et al.\n2010), can require solving non-linear least squares problems with millions of measurements\n(feature matches) and tens of thousands of unknown parameters (3D point positions and cam-\nera poses). Unless some care is taken, these kinds of problem can become intractable, since\nthe (direct) solution of dense least squares problems is cubic in the number of unknowns.\nFortunately, structure from motion is a bipartite problem in structure and motion. Each\nfeature point xij in a given image depends on one 3D point position pi and one 3D camera\npose (Rj, cj). This is illustrated in Figure 7.9a, where each circle (1–9) indicates a 3D point,\neach square (A–D) indicates a camera, and lines (edges) indicate which points are visible in\nwhich cameras (2D features). If the values for all the points are known or ﬁxed, the equations\nfor all the cameras become independent, and vice versa.",
  "387": "7.4 Bundle adjustment\n365\npi\nw\npi\nr\n(Rt\nr,ct\nr)\n(Rj\nc,cj\nc)\nY\nX\n(a)\nfR(x)\n= Rj\ncx\nqj\nc\nfT(x)\n= x-cj\nc\ncj\nc\ny(1)\ny(2)\nfR(x)\n= Rt\nrx\nqt\nr\nfT(x)\n= x-ct\nr\nct\nr\npi\nw\ny(0)\npi\nr\n…\n(b)\nFigure 7.8\nA camera rig and its associated transform chain. (a) As the mobile rig (robot)\nmoves around in the world, its pose with respect to the world at time t is captured by (Rr\nt, cr\nt).\nEach camera’s pose with respect to the rig is captured by (Rc\nj, cc\nj). (b) A 3D point with world\ncoordinates pw\ni is ﬁrst transformed into rig coordinates pr\ni, and then through the rest of the\ncamera-speciﬁc chain, as shown in Figure 7.7.\nA\nB\nC\nD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\nA\nB\nC\nD\n1A\n1B\n2A\n2B\n3A\n3B\n4A\n4B\n4C\n5B\n5C\n6B\n6C\n7C\n7D\n8C\n8D\n9C\n9D\n1\n2\n3\n4\n5\n6\n7\n8\n9\nA\nB\nC\nD\n1\n2\n3\n4\n5\n6\n7\n8\n9\nA\nB\nC\nD\n(a)\n(b)\n(c)\nFigure 7.9\n(a) Bipartite graph for a toy structure from motion problem and (b) its associated\nJacobian J and (c) Hessian A. Numbers indicate 3D points and letters indicate cameras. The\ndashed arcs and light blue squares indicate the ﬁll-in that occurs when the structure (point)\nvariables are eliminated.",
  "388": "366\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIf we order the structure variables before the motion variables in the Hessian matrix A\n(and hence also the right hand side vector b), we obtain a structure for the Hessian shown in\nFigure 7.9c.14 When such a system is solved using sparse Cholesky factorization (see Ap-\npendix A.4) (Bj¨orck 1996; Golub and Van Loan 1996), the ﬁll-in occurs in the smaller motion\nHessian Acc (Szeliski and Kang 1994; Triggs, McLauchlan, Hartley et al. 1999; Hartley and\nZisserman 2004; Lourakis and Argyros 2009; Engels, Stew´enius, and Nist´er 2006). Some re-\ncent papers by (Byr¨od and øAstr¨om 2009), Jeong, Nist´er, Steedly et al. (2010) and (Agarwal,\nSnavely, Seitz et al. 2010) explore the use of iterative (conjugate gradient) techniques for the\nsolution of bundle adjustment problems.\nIn more detail, the reduced motion Hessian is computed using the Schur complement,\nA′\ncc = Acc −AT\npcA−1\npp Apc,\n(7.56)\nwhere App is the point (structure) Hessian (the top left block of Figure 7.9c), Apc is the\npoint-camera Hessian (the top right block), and Acc and A′\ncc are the motion Hessians before\nand after the point variable elimination (the bottom right block of Figure 7.9c). Notice that\nA′\ncc has a non-zero entry between two cameras if they see any 3D point in common. This is\nindicated with dashed arcs in Figure 7.9a and light blue squares in Figure 7.9c.\nWhenever there are global parameters present in the reconstruction algorithm, such as\ncamera intrinsics that are common to all of the cameras, or camera rig calibration parameters\nsuch as those shown in Figure 7.8, they should be ordered last (placed along the right and\nbottom edges of A) in order to reduce ﬁll-in.\nEngels, Stew´enius, and Nist´er (2006) provide a nice recipe for sparse bundle adjustment,\nincluding all the steps needed to initialize the iterations, as well as typical computation times\nfor a system that uses a ﬁxed number of backward-looking frames in a real-time setting. They\nalso recommend using homogeneous coordinates for the structure parameters pi, which is a\ngood idea, since it avoids numerical instabilities for points near inﬁnity.\nBundle adjustment is now the standard method of choice for most structure-from-motion\nproblems and is commonly applied to problems with hundreds of weakly calibrated images\nand tens of thousands of points, e.g., in systems such as Photosynth. (Much larger prob-\nlems are commonly solved in photogrammetry and aerial imagery, but these are usually care-\nfully calibrated and make use of surveyed ground control points.) However, as the problems\nbecome larger, it becomes impractical to re-solve full bundle adjustment problems at each\niteration.\nOne approach to dealing with this problem is to use an incremental algorithm, where new\ncameras are added over time. (This makes particular sense if the data is being acquired from\n14 This ordering is preferable when there are fewer cameras than 3D points, which is the usual case. The exception\nis when we are tracking a small number of points through many video frames, in which case this ordering should be\nreversed.",
  "389": "7.4 Bundle adjustment\n367\na video camera or moving vehicle (Nist´er, Naroditsky, and Bergen 2006; Pollefeys, Nist´er,\nFrahm et al. 2008).) A Kalman ﬁlter can be used to incrementally update estimates as new\ninformation is acquired. Unfortunately, such sequential updating is only statistically optimal\nfor linear least squares problems.\nFor non-linear problems such as structure from motion, an extended Kalman ﬁlter, which\nlinearizes measurement and update equations around the current estimate, needs to be used\n(Gelb 1974; Vi´eville and Faugeras 1990). To overcome this limitation, several passes can\nbe made through the data (Azarbayejani and Pentland 1995). Because points disappear from\nview (and old cameras become irrelevant), a variable state dimension ﬁlter (VSDF) can be\nused to adjust the set of state variables over time, for example, by keeping only cameras and\npoint tracks seen in the last k frames (McLauchlan 2000). A more ﬂexible approach to using\na ﬁxed number of frames is to propagate corrections backwards through points and cameras\nuntil the changes on parameters are below a threshold (Steedly and Essa 2001). Variants of\nthese techniques, including methods that use a ﬁxed window for bundle adjustment (Engels,\nStew´enius, and Nist´er 2006) or select keyframes for doing full bundle adjustment (Klein and\nMurray 2008) are now commonly used in real-time tracking and augmented-reality applica-\ntions, as discussed in Section 7.4.2.\nWhen maximum accuracy is required, it is still preferable to perform a full bundle ad-\njustment over all the frames. In order to control the resulting computational complexity, one\napproach is to lock together subsets of frames into locally rigid conﬁgurations and to optimize\nthe relative positions of these cluster (Steedly, Essa, and Dellaert 2003). A different approach\nis to select a smaller number of frames to form a skeletal set that still spans the whole dataset\nand produces reconstructions of comparable accuracy (Snavely, Seitz, and Szeliski 2008b).\nWe describe this latter technique in more detail in Section 7.4.4, where we discuss applica-\ntions of structure from motion to large image sets.\nWhile bundle adjustment and other robust non-linear least squares techniques are the\nmethods of choice for most structure-from-motion problems, they suffer from initialization\nproblems, i.e., they can get stuck in local energy minima if not started sufﬁciently close\nto the global optimum. Many systems try to mitigate this by being conservative in what\nreconstruction they perform early on and which cameras and points they add to the solution\n(Section 7.4.4). An alternative, however, is to re-formulate the problem using a norm that\nsupports the computation of global optima.\nKahl and Hartley (2008) describe techniques for using L∞norms in geometric recon-\nstruction problems. The advantage of such norms is that globally optimal solutions can be\nefﬁciently computed using second-order cone programming (SOCP). The disadvantage is that\nL∞norms are particularly sensitive to outliers and so must be combined with good outlier\nrejection techniques before they can be used.",
  "390": "368\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7.4.2 Application: Match move and augmented reality\nOne of the neatest applications of structure from motion is to estimate the 3D motion of a\nvideo or ﬁlm camera, along with the geometry of a 3D scene, in order to superimpose 3D\ngraphics or computer-generated images (CGI) on the scene. In the visual effects industry,\nthis is known as the match move problem (Roble 1999), since the motion of the synthetic 3D\ncamera used to render the graphics must be matched to that of the real-world camera. For\nvery small motions, or motions involving pure camera rotations, one or two tracked points can\nsufﬁce to compute the necessary visual motion. For planar surfaces moving in 3D, four points\nare needed to compute the homography, which can then be used to insert planar overlays, e.g.,\nto replace the contents of advertising billboards during sporting events.\nThe general version of this problem requires the estimation of the full 3D camera pose\nalong with the focal length (zoom) of the lens and potentially its radial distortion parameters\n(Roble 1999). When the 3D structure of the scene is known ahead of time, pose estima-\ntion techniques such as view correlation (Bogart 1991) or through-the-lens camera control\n(Gleicher and Witkin 1992) can be used, as described in Section 6.2.3.\nFor more complex scenes, it is usually preferable to recover the 3D structure simultane-\nously with the camera motion using structure-from-motion techniques. The trick with using\nsuch techniques is that in order to prevent any visible jitter between the synthetic graph-\nics and the actual scene, features must be tracked to very high accuracy and ample feature\ntracks must be available in the vicinity of the insertion location. Some of today’s best known\nmatch move software packages, such as the boujou package from 2d3,15 which won an Emmy\naward in 2002, originated in structure-from-motion research in the computer vision commu-\nnity (Fitzgibbon and Zisserman 1998).\nClosely related to the match move problem is robotics navigation, where a robot must es-\ntimate its location relative to its environment, while simultaneously avoiding any dangerous\nobstacles. This problem is often known as simultaneous localization and mapping (SLAM)\n(Thrun, Burgard, and Fox 2005) or visual odometry (Levin and Szeliski 2004; Nist´er, Nar-\noditsky, and Bergen 2006; Maimone, Cheng, and Matthies 2007). Early versions of such\nalgorithms used range-sensing techniques, such as ultrasound, laser range ﬁnders, or stereo\nmatching, to estimate local 3D geometry, which could then be fused into a 3D model. Newer\ntechniques can perform the same task based purely on visual feature tracking, sometimes not\neven requiring a stereo camera rig (Davison, Reid, Molton et al. 2007).\nAnother closely related application is augmented reality, where 3D objects are inserted\ninto a video feed in real time, often to annotate or help users understand a scene (Azuma,\nBaillot, Behringer et al. 2001). While traditional systems require prior knowledge about the\nscene or object being visually tracked (Rosten and Drummond 2005), newer systems can\n15 http://www.2d3.com/.",
  "391": "7.4 Bundle adjustment\n369\n(a)\n(b)\nFigure 7.10\n3D augmented reality: (a) Darth Vader and a horde of Ewoks battle it out\non a table-top recovered using real-time, keyframe-based structure from motion (Klein and\nMurray 2007) c⃝2007 IEEE; (b) a virtual teapot is ﬁxed to the top of a real-world coffee cup,\nwhose pose is re-recognized at each time frame (Gordon and Lowe 2006) c⃝2007 Springer.\nsimultaneously build up a model of the 3D environment and then track it, so that graphics can\nbe superimposed.\nKlein and Murray (2007) describe a parallel tracking and mapping (PTAM) system,\nwhich simultaneously applies full bundle adjustment to keyframes selected from a video\nstream, while performing robust real-time pose estimation on intermediate frames.\nFig-\nure 7.10a shows an example of their system in use. Once an initial 3D scene has been\nreconstructed, a dominant plane is estimated (in this case, the table-top) and 3D animated\ncharacters are virtually inserted. Klein and Murray (2008) extend their previous system to\nhandle even faster camera motion by adding edge features, which can still be detected even\nwhen interest points become too blurred. They also use a direct (intensity-based) rotation\nestimation algorithm for even faster motions.\nInstead of modeling the whole scene as one rigid reference frame, Gordon and Lowe\n(2006) ﬁrst build a 3D model of an individual object using feature matching and structure\nfrom motion. Once the system has been initialized, for every new frame, they ﬁnd the object\nand its pose using a 3D instance recognition algorithm, and then superimpose a graphical\nobject onto that model, as shown in Figure 7.10b.\nWhile reliably tracking such objects and environments is now a well-solved problem,\ndetermining which pixels should be occluded by foreground scene elements still remains an\nopen problem (Chuang, Agarwala, Curless et al. 2002; Wang and Cohen 2007a).",
  "392": "370\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n7.4.3 Uncertainty and ambiguities\nBecause structure from motion involves the estimation of so many highly coupled parameters,\noften with no known “ground truth” components, the estimates produced by structure from\nmotion algorithms can often exhibit large amounts of uncertainty (Szeliski and Kang 1997).\nAn example of this is the classic bas-relief ambiguity, which makes it hard to simultaneously\nestimate the 3D depth of a scene and the amount of camera motion (Oliensis 2005).16\nAs mentioned before, a unique coordinate frame and scale for a reconstructed scene can-\nnot be recovered from monocular visual measurements alone. (When a stereo rig is used,\nthe scale can be recovered if we know the distance (baseline) between the cameras.) This\nseven-degree-of-freedom gauge ambiguity makes it tricky to compute the covariance matrix\nassociated with a 3D reconstruction (Triggs, McLauchlan, Hartley et al. 1999; Kanatani and\nMorris 2001). A simple way to compute a covariance matrix that ignores the gauge freedom\n(indeterminacy) is to throw away the seven smallest eigenvalues of the information matrix (in-\nverse covariance), whose values are equivalent to the problem Hessian A up to noise scaling\n(see Section 6.1.4 and Appendix B.6). After we do this, the resulting matrix can be inverted\nto obtain an estimate of the parameter covariance.\nSzeliski and Kang (1997) use this approach to visualize the largest directions of variation\nin typical structure from motion problems. Not surprisingly, they ﬁnd that (ignoring the gauge\nfreedoms), the greatest uncertainties for problems such as observing an object from a small\nnumber of nearby viewpoints are in the depths of the 3D structure relative to the extent of the\ncamera motion.17\nIt is also possible to estimate local or marginal uncertainties for individual parameters,\nwhich corresponds simply to taking block sub-matrices from the full covariance matrix. Un-\nder certain conditions, such as when the camera poses are relatively certain compared to 3D\npoint locations, such uncertainty estimates can be meaningful. However, in many cases, indi-\nvidual uncertainty measures can mask the extent to which reconstruction errors are correlated,\nwhich is why looking at the ﬁrst few modes of greatest joint variation can be helpful.\nThe other way in which gauge ambiguities affect structure from motion and, in particular,\nbundle adjustment is that they make the system Hessian matrix A rank-deﬁcient and hence\nimpossible to invert. A number of techniques have been proposed to mitigate this problem\n(Triggs, McLauchlan, Hartley et al. 1999; Bartoli 2003). In practice, however, it appears that\nsimply adding a small amount of the Hessian diagonal λdiag(A) to the Hessian A itself, as is\ndone in the Levenberg–Marquardt non-linear least squares algorithm (Appendix A.3), usually\n16 Bas-relief refers to a kind of sculpture in which objects, often on ornamental friezes, are sculpted with less\ndepth than they actually occupy. When lit from above by sunlight, they appear to have true 3D depth because of the\nambiguity between relative depth and the angle of the illuminant (Section 12.1.1).\n17 A good way to minimize the amount of such ambiguities is to use wide ﬁeld of view cameras (Antone and\nTeller 2002; Levin and Szeliski 2006).",
  "393": "7.4 Bundle adjustment\n371\nworks well.\n7.4.4 Application: Reconstruction from Internet photos\nThe most widely used application of structure from motion is in the reconstruction of 3D\nobjects and scenes from video sequences and collections of images (Pollefeys and Van Gool\n2002). The last decade has seen an explosion of techniques for performing this task auto-\nmatically without the need for any manual correspondence or pre-surveyed ground control\npoints. A lot of these techniques assume that the scene is taken with the same camera and\nhence the images all have the same intrinsics (Fitzgibbon and Zisserman 1998; Koch, Polle-\nfeys, and Van Gool 2000; Schaffalitzky and Zisserman 2002; Tuytelaars and Van Gool 2004;\nPollefeys, Nist´er, Frahm et al. 2008; Moons, Van Gool, and Vergauwen 2010). Many of\nthese techniques take the results of the sparse feature matching and structure from motion\ncomputation and then compute dense 3D surface models using multi-view stereo techniques\n(Section 11.6) (Koch, Pollefeys, and Van Gool 2000; Pollefeys and Van Gool 2002; Pollefeys,\nNist´er, Frahm et al. 2008; Moons, Van Gool, and Vergauwen 2010).\nThe latest innovation in this space has been the application of structure from motion and\nmulti-view stereo techniques to thousands of images taken from the Internet, where very little\nis known about the cameras taking the photographs (Snavely, Seitz, and Szeliski 2008a). Be-\nfore the structure from motion computation can begin, it is ﬁrst necessary to establish sparse\ncorrespondences between different pairs of images and to then link such correspondences into\nfeature tracks, which associate individual 2D image features with global 3D points. Because\nthe O(N 2) comparison of all pairs of images can be very slow, a number of techniques have\nbeen developed in the recognition community to make this process faster (Section 14.3.2)\n(Nist´er and Stew´enius 2006; Philbin, Chum, Sivic et al. 2008; Li, Wu, Zach et al. 2008;\nChum, Philbin, and Zisserman 2008; Chum and Matas 2010).\nTo begin the reconstruction process, it is important to to select a good pair of images,\nwhere there are both a large number of consistent matches (to lower the likelihood of in-\ncorrect correspondences) and a signiﬁcant amount of out-of-plane parallax,18 to ensure that\na stable reconstruction can be obtained (Snavely, Seitz, and Szeliski 2006). The EXIF tags\nassociated with the photographs can be used to get good initial estimates for camera focal\nlengths, although this is not always strictly necessary, since these parameters are re-adjusted\nas part of the bundle adjustment process.\nOnce an initial pair has been reconstructed, the pose of cameras that see a sufﬁcient num-\nber of the resulting 3D points can be estimated (Section 6.2) and the complete set of cameras\nand feature correspondences can be used to perform another round of bundle adjustment. Fig-\n18 A simple way to compute this is to robustly ﬁt a homography to the correspondences and measure reprojection\nerrors.",
  "394": "372\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 7.11 Incremental structure from motion (Snavely, Seitz, and Szeliski 2006) c⃝2006\nACM: Starting with an initial two-frame reconstruction of Trevi Fountain, batches of images\nare added using pose estimation, and their positions (along with the 3D model) are reﬁned\nusing bundle adjustment.\nure 7.11 shows the progression of the incremental bundle adjustment algorithm, where sets of\ncameras are added after each successive round of bundle adjustment, while Figure 7.12 shows\nsome additional results. An alternative to this kind of seed and grow approach is to ﬁrst re-\nconstruct triplets of images and then hierarchically merge triplets into larger collections, as\ndescribed by Fitzgibbon and Zisserman (1998).\nUnfortunately, as the incremental structure from motion algorithm continues to add more\ncameras and points, it can become extremely slow. The direct solution of a dense system\nof O(N) equations for the camera pose updates can take O(N 3) time; while structure from\nmotion problems are rarely dense, scenes such as city squares have a high percentage of\ncameras that see points in common. Re-running the bundle adjustment algorithm after every\nfew camera additions results in a quartic scaling of the run time with the number of images\nin the dataset. One approach to solving this problem is to select a smaller number of images\nfor the original scene reconstruction and to fold in the remaining images at the very end.\nSnavely, Seitz, and Szeliski (2008b) develop an algorithm for computing such a skele-\ntal set of images, which is guaranteed to produce a reconstruction whose error is within a\nbounded factor of the optimal reconstruction accuracy. Their algorithm ﬁrst evaluates all\npairwise uncertainties (position covariances) between overlapping images and then chains\nthem together to estimate a lower bound for the relative uncertainty of any distant pair. The\nskeletal set is constructed so that the maximal uncertainty between any pair grows by no\nmore than a constant factor. Figure 7.13 shows an example of the skeletal set computed for\n784 images of the Pantheon in Rome. As you can see, even though the skeletal set contains\njust a fraction of the original images, the shapes of the skeletal set and full bundle adjusted\nreconstructions are virtually indistinguishable.\nThe ability to automatically reconstruct 3D models from large, unstructured image col-\nlections has opened a wide variety of additional applications, including the ability to automat-",
  "395": "7.4 Bundle adjustment\n373\n(a)\n(b)\n(c)\nFigure 7.12\n3D reconstructions produced by the incremental structure from motion algo-\nrithm developed by Snavely, Seitz, and Szeliski (2006) c⃝2006 ACM: (a) cameras and point\ncloud from Trafalgar Square; (b) cameras and points overlaid on an image from the Great Wall\nof China; (c) overhead view of a reconstruction of the Old Town Square in Prague registered\nto an aerial photograph.\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 7.13\nLarge scale structure from motion using skeletal sets (Snavely, Seitz, and\nSzeliski 2008b) c⃝2008 IEEE: (a) original match graph for 784 images; (b) skeletal set\ncontaining 101 images; (c) top-down view of scene (Pantheon) reconstructed from the skele-\ntal set; (d) reconstruction after adding in the remaining images using pose estimation; (e) ﬁnal\nbundle adjusted reconstruction, which is almost identical.",
  "396": "374\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nically ﬁnd and label locations and regions of interest (Simon, Snavely, and Seitz 2007; Simon\nand Seitz 2008; Gammeter, Bossard, Quack et al. 2009) and to cluster large image collections\nso that they can be automatically labeled (Li, Wu, Zach et al. 2008; Quack, Leibe, and Van\nGool 2008). Some of these application are discussed in more detail in Section 13.1.2.\n7.5 Constrained structure and motion\nThe most general algorithms for structure from motion make no prior assumptions about the\nobjects or scenes that they are reconstructing. In many cases, however, the scene contains\nhigher-level geometric primitives, such as lines and planes. These can provide information\ncomplementary to interest points and also serve as useful building blocks for 3D modeling\nand visualization. Furthermore, these primitives are often arranged in particular relationships,\ni.e., many lines and planes are either parallel or orthogonal to each other. This is particularly\ntrue of architectural scenes and models, which we study in more detail in Section 12.6.1.\nSometimes, instead of exploiting regularity in the scene structure, it is possible to take\nadvantage of a constrained motion model. For example, if the object of interest is rotating\non a turntable (Szeliski 1991b), i.e., around a ﬁxed but unknown axis, specialized techniques\ncan be used to recover this motion (Fitzgibbon, Cross, and Zisserman 1998). In other situa-\ntions, the camera itself may be moving in a ﬁxed arc around some center of rotation (Shum\nand He 1999). Specialized capture setups, such as mobile stereo camera rigs or moving ve-\nhicles equipped with multiple ﬁxed cameras, can also take advantage of the knowledge that\nindividual cameras are (mostly) ﬁxed with respect to the capture rig, as shown in Figure 7.8.19\n7.5.1 Line-based techniques\nIt is well known that pairwise epipolar geometry cannot be recovered from line matches\nalone, even if the cameras are calibrated. To see this, think of projecting the set of lines in\neach image into a set of 3D planes in space. You can move the two cameras around into any\nconﬁguration you like and still obtain a valid reconstruction for 3D lines.\nWhen lines are visible in three or more views, the trifocal tensor can be used to transfer\nlines from one pair of images to another (Hartley and Zisserman 2004). The trifocal tensor\ncan also be computed on the basis of line matches alone.\nSchmid and Zisserman (1997) describe a widely used technique for matching 2D lines\nbased on the average of 15 × 15 pixel correlation scores evaluated at all pixels along their\n19 Because of mechanical compliance and jitter, it may be prudent to allow for a small amount of individual camera\nrotation around a nominal position.",
  "397": "7.5 Constrained structure and motion\n375\nFigure 7.14 Two images of a toy house along with their matched 3D line segments (Schmid\nand Zisserman 1997) c⃝1997 Springer.\ncommon line segment intersection.20 In their system, the epipolar geometry is assumed to be\nknown, e.g., computed from point matches. For wide baselines, all possible homographies\ncorresponding to planes passing through the 3D line are used to warp pixels and the maximum\ncorrelation score is used. For triplets of images, the trifocal tensor is used to verify that\nthe lines are in geometric correspondence before evaluating the correlations between line\nsegments. Figure 7.14 shows the results of using their system.\nBartoli and Sturm (2003) describe a complete system for extending three view relations\n(trifocal tensors) computed from manual line correspondences to a full bundle adjustment of\nall the line and camera parameters. The key to their approach is to use the Pl¨ucker coor-\ndinates (2.12) to parameterize lines and to directly minimize reprojection errors. It is also\npossible to represent 3D line segments by their endpoints and to measure either the reprojec-\ntion error perpendicular to the detected 2D line segments in each image or the 2D errors using\nan elongated uncertainty ellipse aligned with the line segment direction (Szeliski and Kang\n1994).\nInstead of reconstructing 3D lines, Bay, Ferrari, and Van Gool (2005) use RANSAC to\ngroup lines into likely coplanar subsets. Four lines are chosen at random to compute a homog-\nraphy, which is then veriﬁed for these and other plausible line segment matches by evaluating\ncolor histogram-based correlation scores. The 2D intersection points of lines belonging to the\nsame plane are then used as virtual measurements to estimate the epipolar geometry, which\nis more accurate than using the homographies directly.\nAn alternative to grouping lines into coplanar subsets is to group lines by parallelism.\nWhenever three or more 2D lines share a common vanishing point, there is a good likelihood\nthat they are parallel in 3D. By ﬁnding multiple vanishing points in an image (Section 4.3.3)\nand establishing correspondences between such vanishing points in different images, the rel-\native rotations between the various images (and often the camera intrinsics) can be directly\nestimated (Section 6.3.2).\n20 Because lines often occur at depth or orientation discontinuities, it may be preferable to compute correlation\nscores (or to match color histograms (Bay, Ferrari, and Van Gool 2005)) separately on each side of the line.",
  "398": "376\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nShum, Han, and Szeliski (1998) describe a 3D modeling system which ﬁrst constructs\ncalibrated panoramas from multiple images (Section 7.4) and then has the user draw vertical\nand horizontal lines in the image to demarcate the boundaries of planar regions. The lines\nare initially used to establish an absolute rotation for each panorama and are later used (along\nwith the inferred vertices and planes) to infer a 3D structure, which can be recovered up to\nscale from one or more images (Figure 12.15).\nA fully automated approach to line-based structure from motion is presented vy Werner\nand Zisserman (2002). In their system, they ﬁrst ﬁnd lines and group them by common van-\nishing points in each image (Section 4.3.3). The vanishing points are then used to calibrate the\ncamera, i.e., to performa a “metric upgrade” (Section 6.3.2). Lines corresponding to common\nvanishing points are then matched using both appearance (Schmid and Zisserman 1997) and\ntrifocal tensors. The resulting set of 3D lines, color coded by common vanishing directions\n(3D orientations) is shown in Figure 12.16a. These lines are then used to infer planes and a\nblock-structured model for the scene, as described in more detail in Section 12.6.1.\n7.5.2 Plane-based techniques\nIn scenes that are rich in planar structures, e.g., in architecture and certain kinds of manu-\nfactured objects such as furniture, it is possible to directly estimate homographies between\ndifferent planes, using either feature-based or intensity-based methods. In principle, this in-\nformation can be used to simultaneously infer the camera poses and the plane equations, i.e.,\nto compute plane-based structure from motion.\nLuong and Faugeras (1996) show how a fundamental matrix can be directly computed\nfrom two or more homographies using algebraic manipulations and least squares. Unfortu-\nnately, this approach often performs poorly, since the algebraic errors do not correspond to\nmeaningful reprojection errors (Szeliski and Torr 1998).\nA better approach is to hallucinate virtual point correspondences within the areas from\nwhich each homography was computed and to feed them into a standard structure from mo-\ntion algorithm (Szeliski and Torr 1998). An even better approach is to use full bundle adjust-\nment with explicit plane equations, as well as additional constraints to force reconstructed\nco-planar features to lie exactly on their corresponding planes. (A principled way to do this\nis to establish a coordinate frame for each plane, e.g., at one of the feature points, and to use\n2D in-plane parameterizations for the other points.) The system developed by Shum, Han,\nand Szeliski (1998) shows an example of such an approach, where the directions of lines and\nnormals for planes in the scene are pre-speciﬁed by the user.",
  "399": "7.6 Additional reading\n377\n7.6 Additional reading\nThe topic of structure from motion is extensively covered in books and review articles on\nmulti-view geometry (Faugeras and Luong 2001; Hartley and Zisserman 2004; Moons, Van\nGool, and Vergauwen 2010). For two-frame reconstruction, Hartley (1997a) wrote a highly\ncited paper on the “eight-point algorithm” for computing an essential or fundamental ma-\ntrix with reasonable point normalization. When the cameras are calibrated, the ﬁve-point\nalgorithm of Nist´er (2004) can be used in conjunction with RANSAC to obtain initial recon-\nstructions from the minimum number of points. When the cameras are uncalibrated, various\nself-calibration techniques can be found in work by Hartley and Zisserman (2004); Moons,\nVan Gool, and Vergauwen (2010)—I only brieﬂy mention one of the simplest techniques, the\nKruppa equations (7.35).\nIn applications where points are being tracked from frame to frame, factorization tech-\nniques, based on either orthographic camera models (Tomasi and Kanade 1992; Poelman\nand Kanade 1997; Costeira and Kanade 1995; Morita and Kanade 1997; Morris and Kanade\n1998; Anandan and Irani 2002) or projective extensions (Christy and Horaud 1996; Sturm\nand Triggs 1996; Triggs 1996; Oliensis and Hartley 2007), can be used.\nTriggs, McLauchlan, Hartley et al. (1999) provide a good tutorial and survey on bundle\nadjustment, while Lourakis and Argyros (2009) and Engels, Stew´enius, and Nist´er (2006)\nprovide tips on implementation and effective practices. Bundle adjustment is also covered\nin textbooks and surveys on multi-view geometry (Faugeras and Luong 2001; Hartley and\nZisserman 2004; Moons, Van Gool, and Vergauwen 2010). Techniques for handling larger\nproblems are described by Snavely, Seitz, and Szeliski (2008b); Agarwal, Snavely, Simon\net al. (2009); Jeong, Nist´er, Steedly et al. (2010); Agarwal, Snavely, Seitz et al. (2010).\nWhile bundle adjustment is often called as an inner loop inside incremental reconstruction\nalgorithms (Snavely, Seitz, and Szeliski 2006), hierarchical (Fitzgibbon and Zisserman 1998;\nFarenzena, Fusiello, and Gherardi 2009) and global (Rother and Carlsson 2002; Martinec and\nPajdla 2007) approaches for initialization are also possible and perhaps even preferable.\nAs structure from motion starts being applied to dynamic scenes, the topic of non-rigid\nstructure from motion (Torresani, Hertzmann, and Bregler 2008), which we do not cover in\nthis book, will become more important.\n7.7 Exercises\nEx 7.1: Triangulation\nUse the calibration pattern you built and tested in Exercise 6.7 to\ntest your triangulation accuracy. As an alternative, generate synthetic 3D points and cameras\nand add noise to the 2D point measurements.",
  "400": "378\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Assume that you know the camera pose, i.e., the camera matrices. Use the 3D distance\nto rays (7.4) or linearized versions of Equations (7.5–7.6) to compute an initial set of\n3D locations. Compare these to your known ground truth locations.\n2. Use iterative non-linear minimization to improve your initial estimates and report on\nthe improvement in accuracy.\n3. (Optional) Use the technique described by Hartley and Sturm (1997) to perform two-\nframe triangulation.\n4. See if any of the failure modes reported by Hartley and Sturm (1997) or Hartley (1998)\noccur in practice.\nEx 7.2: Essential and fundamental matrix\nImplement the two-frame E and F matrix es-\ntimation techniques presented in Section 7.2, with suitable re-scaling for better noise immu-\nnity.\n1. Use the data from Exercise 7.1 to validate your algorithms and to report on their accu-\nracy.\n2. (Optional) Implement one of the improved F or E estimation algorithms, e.g., us-\ning renormalization (Zhang 1998b; Torr and Fitzgibbon 2004; Hartley and Zisserman\n2004), RANSAC (Torr and Murray 1997), least media squares (LMS), or the ﬁve-point\nalgorithm developed by Nist´er (2004).\nEx 7.3: View morphing and interpolation\nImplement automatic view morphing, i.e., com-\npute two-frame structure from motion and then use these results to generate a smooth anima-\ntion from one image to the next (Section 7.2.3).\n1. Decide how to represent your 3D scene, e.g., compute a Delaunay triangulation of the\nmatched point and decide what to do with the triangles near the border. (Hint: try ﬁtting\na plane to the scene, e.g., behind most of the points.)\n2. Compute your in-between camera positions and orientations.\n3. Warp each triangle to its new location, preferably using the correct perspective projec-\ntion (Szeliski and Shum 1997).\n4. (Optional) If you have a denser 3D model (e.g., from stereo), decide what to do at the\n“cracks”.\n5. (Optional) For a non-rigid scene, e.g., two pictures of a face with different expressions,\nnot all of your matched points will obey the epipolar geometry. Decide how to handle\nthem to achieve the best effect.",
  "401": "7.7 Exercises\n379\nEx 7.4: Factorization\nImplement the factorization algorithm described in Section 7.3 us-\ning point tracks you computed in Exercise 4.5.\n1. (Optional) Implement uncertainty rescaling (Anandan and Irani 2002) and comment on\nwhether this improves your results.\n2. (Optional) Implement one of the perspective improvements to factorization discussed\nin Section 7.3.1 (Christy and Horaud 1996; Sturm and Triggs 1996; Triggs 1996). Does\nthis produce signiﬁcantly lower reprojection errors? Can you upgrade this reconstruc-\ntion to a metric one?\nEx 7.5: Bundle adjuster\nImplement a full bundle adjuster. This may sound daunting, but\nit really is not.\n1. Devise the internal data structures and external ﬁle representations to hold your camera\nparameters (position, orientation, and focal length), 3D point locations (Euclidean or\nhomogeneous), and 2D point tracks (frame and point identiﬁer as well as 2D locations).\n2. Use some other technique, such as factorization, to initialize the 3D point and camera\nlocations from your 2D tracks (e.g., a subset of points that appears in all frames).\n3. Implement the code corresponding to the forward transformations in Figure 7.7, i.e.,\nfor each 2D point measurement, take the corresponding 3D point, map it through the\ncamera transformations (including perspective projection and focal length scaling), and\ncompare it to the 2D point measurement to get a residual error.\n4. Take the residual error and compute its derivatives with respect to all the unknown\nmotion and structure parameters, using backward chaining, as shown, e.g., in Figure 7.7\nand Equation (6.47). This gives you the sparse Jacobian J used in Equations (6.13–\n6.17) and Equation (6.43).\n5. Use a sparse least squares or linear system solver, e.g., MATLAB, SparseSuite, or\nSPARSKIT (see Appendix A.4 and A.5), to solve the corresponding linearized system,\nadding a small amount of diagonal preconditioning, as in Levenberg–Marquardt.\n6. Update your parameters, make sure your rotation matrices are still orthonormal (e.g.,\nby re-computing them from your quaternions), and continue iterating while monitoring\nyour residual error.\n7. (Optional) Use the “Schur complement trick” (7.56) to reduce the size of the system\nbeing solved (Triggs, McLauchlan, Hartley et al. 1999; Hartley and Zisserman 2004;\nLourakis and Argyros 2009; Engels, Stew´enius, and Nist´er 2006).",
  "402": "380\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n8. (Optional) Implement your own iterative sparse solver, e.g., conjugate gradient, and\ncompare its performance to a direct method.\n9. (Optional) Make your bundle adjuster robust to outliers, or try adding some of the other\nimprovements discussed in (Engels, Stew´enius, and Nist´er 2006). Can you think of any\nother ways to make your algorithm even faster or more robust?\nEx 7.6: Match move and augmented reality\nUse the results of the previous exercise to\nsuperimpose a rendered 3D model on top of video. See Section 7.4.2 for more details and\nideas. Check for how “locked down” the objects are.\nEx 7.7: Line-based reconstruction\nAugment the previously developed bundle adjuster to\ninclude lines, possibly with known 3D orientations.\nOptionally, use co-planar sets of points and lines to hypothesize planes and to enforce\nco-planarity (Schaffalitzky and Zisserman 2002; Robertson and Cipolla 2002)\nEx 7.8: Flexible bundle adjuster\nDesign a bundle adjuster that allows for arbitrary chains\nof transformations and prior knowledge about the unknowns, as suggested in Figures 7.7–7.8.\nEx 7.9: Unordered image matching\nCompute the camera pose and 3D structure of a scene\nfrom an arbitrary collection of photographs (Brown and Lowe 2003; Snavely, Seitz, and\nSzeliski 2006).",
  "403": "Chapter 8\nDense motion estimation\n8.1\nTranslational alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n8.1.1\nHierarchical motion estimation . . . . . . . . . . . . . . . . . . . . . 387\n8.1.2\nFourier-based alignment . . . . . . . . . . . . . . . . . . . . . . . . 388\n8.1.3\nIncremental reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . 392\n8.2\nParametric motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398\n8.2.1\nApplication: Video stabilization . . . . . . . . . . . . . . . . . . . . 401\n8.2.2\nLearned motion models . . . . . . . . . . . . . . . . . . . . . . . . . 403\n8.3\nSpline-based motion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\n8.3.1\nApplication: Medical image registration . . . . . . . . . . . . . . . . 408\n8.4\nOptical ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\n8.4.1\nMulti-frame motion estimation . . . . . . . . . . . . . . . . . . . . . 413\n8.4.2\nApplication: Video denoising\n. . . . . . . . . . . . . . . . . . . . . 414\n8.4.3\nApplication: De-interlacing\n. . . . . . . . . . . . . . . . . . . . . . 415\n8.5\nLayered motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n8.5.1\nApplication: Frame interpolation . . . . . . . . . . . . . . . . . . . . 418\n8.5.2\nTransparent layers and reﬂections . . . . . . . . . . . . . . . . . . . 419\n8.6\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\n8.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422",
  "404": "382\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nﬂow\ninitial layers ﬁnal layers\nlayers with pixel assignments and ﬂow\n(c)\n(d)\n(e)\n(f)\nFigure 8.1\nMotion estimation: (a–b) regularization-based optical ﬂow (Nagel and Enkel-\nmann 1986) c⃝1986 IEEE; (c–d) layered motion estimation (Wang and Adelson 1994) c⃝\n1994 IEEE; (e–f) sample image and ground truth ﬂow from evaluation database (Baker,\nBlack, Lewis et al. 2007) c⃝2007 IEEE.",
  "405": "8 Dense motion estimation\n383\nAlgorithms for aligning images and estimating motion in video sequences are among the most\nwidely used in computer vision. For example, frame-rate image alignment is widely used in\ncamcorders and digital cameras to implement their image stabilization (IS) feature.\nAn early example of a widely used image registration algorithm is the patch-based trans-\nlational alignment (optical ﬂow) technique developed by Lucas and Kanade (1981). Variants\nof this algorithm are used in almost all motion-compensated video compression schemes\nsuch as MPEG and H.263 (Le Gall 1991). Similar parametric motion estimation algorithms\nhave found a wide variety of applications, including video summarization (Teodosio and\nBender 1993; Irani and Anandan 1998), video stabilization (Hansen, Anandan, Dana et al.\n1994; Srinivasan, Chellappa, Veeraraghavan et al. 2005; Matsushita, Ofek, Ge et al. 2006),\nand video compression (Irani, Hsu, and Anandan 1995; Lee, ge Chen, lung Bruce Lin et\nal. 1997). More sophisticated image registration algorithms have also been developed for\nmedical imaging and remote sensing. Image registration techniques are surveyed by Brown\n(1992), Zitov’aa and Flusser (2003), Goshtasby (2005), and Szeliski (2006a).\nTo estimate the motion between two or more images, a suitable error metric must ﬁrst\nbe chosen to compare the images (Section 8.1). Once this has been established, a suitable\nsearch technique must be devised. The simplest technique is to exhaustively try all possible\nalignments, i.e., to do a full search. In practice, this may be too slow, so hierarchical coarse-\nto-ﬁne techniques (Section 8.1.1) based on image pyramids are normally used. Alternatively,\nFourier transforms (Section 8.1.2) can be used to speed up the computation.\nTo get sub-pixel precision in the alignment, incremental methods (Section 8.1.3) based\non a Taylor series expansion of the image function are often used. These can also be applied\nto parametric motion models (Section 8.2), which model global image transformations such\nas rotation or shearing. Motion estimation can be made more reliable by learning the typi-\ncal dynamics or motion statistics of the scenes or objects being tracked, e.g., the natural gait\nof walking people (Section 8.2.2). For more complex motions, piecewise parametric spline\nmotion models (Section 8.3) can be used. In the presence of multiple independent (and per-\nhaps non-rigid) motions, general-purpose optical ﬂow (or optic ﬂow) techniques need to be\nused (Section 8.4). For even more complex motions that include a lot of occlusions, layered\nmotion models (Section 8.5), which decompose the scene into coherently moving layers, can\nwork well.\nIn this chapter, we describe each of these techniques in more detail. Additional details\ncan be found in review and comparative evaluation papers on motion estimation (Barron,\nFleet, and Beauchemin 1994; Mitiche and Bouthemy 1996; Stiller and Konrad 1999; Szeliski\n2006a; Baker, Black, Lewis et al. 2007).",
  "406": "384\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n8.1 Translational alignment\nThe simplest way to establish an alignment between two images or image patches is to shift\none image relative to the other. Given a template image I0(x) sampled at discrete pixel\nlocations {xi = (xi, yi)}, we wish to ﬁnd where it is located in image I1(x). A least squares\nsolution to this problem is to ﬁnd the minimum of the sum of squared differences (SSD)\nfunction\nESSD(u) =\nX\ni\n[I1(xi + u) −I0(xi)]2 =\nX\ni\ne2\ni ,\n(8.1)\nwhere u = (u, v) is the displacement and ei = I1(xi + u) −I0(xi) is called the residual\nerror (or the displaced frame difference in the video coding literature).1 (We ignore for the\nmoment the possibility that parts of I0 may lie outside the boundaries of I1 or be otherwise\nnot visible.) The assumption that corresponding pixel values remain the same in the two\nimages is often called the brightness constancy constraint.2\nIn general, the displacement u can be fractional, so a suitable interpolation function must\nbe applied to image I1(x). In practice, a bilinear interpolant is often used but bicubic inter-\npolation can yield slightly better results (Szeliski and Scharstein 2004). Color images can be\nprocessed by summing differences across all three color channels, although it is also possible\nto ﬁrst transform the images into a different color space or to only use the luminance (which\nis often done in video encoders).\nRobust error metrics.\nWe can make the above error metric more robust to outliers by re-\nplacing the squared error terms with a robust function ρ(ei) (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Black and Anandan 1996; Stewart 1999) to obtain\nESRD(u) =\nX\ni\nρ(I1(xi + u) −I0(xi)) =\nX\ni\nρ(ei).\n(8.2)\nThe robust norm ρ(e) is a function that grows less quickly than the quadratic penalty associ-\nated with least squares. One such function, sometimes used in motion estimation for video\ncoding because of its speed, is the sum of absolute differences (SAD) metric3 or L1 norm,\ni.e.,\nESAD(u) =\nX\ni\n|I1(xi + u) −I0(xi)| =\nX\ni\n|ei|.\n(8.3)\n1 The usual justiﬁcation for using least squares is that it is the optimal estimate with respect to Gaussian noise.\nSee the discussion below on robust error metrics as well as Appendix B.3.\n2 Brightness constancy (Horn 1974) is the tendency for objects to maintain their perceived brightness under\nvarying illumination conditions.\n3 In video compression, e.g., the H.264 standard (http://www.itu.int/rec/T-REC-H.264), the sum of absolute trans-\nformed differences (SATD), which measures the differences in a frequency transform space, e.g., using a Hadamard\ntransform, is often used since it more accurately predicts quality (Richardson 2003).",
  "407": "8.1 Translational alignment\n385\nHowever, since this function is not differentiable at the origin, it is not well suited to gradient-\ndescent approaches such as the ones presented in Section 8.1.3.\nInstead, a smoothly varying function that is quadratic for small values but grows more\nslowly away from the origin is often used. Black and Rangarajan (1996) discuss a variety of\nsuch functions, including the Geman–McClure function,\nρGM(x) =\nx2\n1 + x2/a2 ,\n(8.4)\nwhere a is a constant that can be thought of as an outlier threshold. An appropriate value for\nthe threshold can itself be derived using robust statistics (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Rousseeuw and Leroy 1987), e.g., by computing the median absolute\ndeviation, MAD = medi|ei|, and multiplying it by 1.4 to obtain a robust estimate of the\nstandard deviation of the inlier noise process (Stewart 1999).\nSpatially varying weights.\nThe error metrics above ignore that fact that for a given align-\nment, some of the pixels being compared may lie outside the original image boundaries.\nFurthermore, we may want to partially or completely downweight the contributions of cer-\ntain pixels. For example, we may want to selectively “erase” some parts of an image from\nconsideration when stitching a mosaic where unwanted foreground objects have been cut out.\nFor applications such as background stabilization, we may want to downweight the middle\npart of the image, which often contains independently moving objects being tracked by the\ncamera.\nAll of these tasks can be accomplished by associating a spatially varying per-pixel weight\nvalue with each of the two images being matched.\nThe error metric then becomes the\nweighted (or windowed) SSD function,\nEWSSD(u) =\nX\ni\nw0(xi)w1(xi + u)[I1(xi + u) −I0(xi)]2,\n(8.5)\nwhere the weighting functions w0 and w1 are zero outside the image boundaries.\nIf a large range of potential motions is allowed, the above metric can have a bias towards\nsmaller overlap solutions. To counteract this bias, the windowed SSD score can be divided\nby the overlap area\nA =\nX\ni\nw0(xi)w1(xi + u)\n(8.6)\nto compute a per-pixel (or mean) squared pixel error EWSSD/A. The square root of this\nquantity is the root mean square intensity error\nRMS =\np\nEWSSD/A\n(8.7)\noften reported in comparative studies.",
  "408": "386\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBias and gain (exposure differences).\nOften, the two images being aligned were not taken\nwith the same exposure. A simple model of linear (afﬁne) intensity variation between the two\nimages is the bias and gain model,\nI1(x + u) = (1 + α)I0(x) + β,\n(8.8)\nwhere β is the bias and α is the gain (Lucas and Kanade 1981; Gennert 1988; Fuh and\nMaragos 1991; Baker, Gross, and Matthews 2003; Evangelidis and Psarakis 2008). The least\nsquares formulation then becomes\nEBG(u) =\nX\ni\n[I1(xi + u) −(1 + α)I0(xi) −β]2 =\nX\ni\n[αI0(xi) + β −ei]2.\n(8.9)\nRather than taking a simple squared difference between corresponding patches, it becomes\nnecessary to perform a linear regression (Appendix A.2), which is somewhat more costly.\nNote that for color images, it may be necessary to estimate a different bias and gain for each\ncolor channel to compensate for the automatic color correction performed by some digital\ncameras (Section 2.3.2). Bias and gain compensation is also used in video codecs, where it is\nknown as weighted prediction (Richardson 2003).\nA more general (spatially varying, non-parametric) model of intensity variation, which is\ncomputed as part of the registration process, is used in (Negahdaripour 1998; Jia and Tang\n2003; Seitz and Baker 2009). This can be useful for dealing with local variations such as\nthe vignetting caused by wide-angle lenses, wide apertures, or lens housings. It is also pos-\nsible to pre-process the images before comparing their values, e.g., using band-pass ﬁltered\nimages (Anandan 1989; Bergen, Anandan, Hanna et al. 1992), gradients (Scharstein 1994;\nPapenberg, Bruhn, Brox et al. 2006), or using other local transformations such as histograms\nor rank transforms (Cox, Roy, and Hingorani 1995; Zabih and Woodﬁll 1994), or to max-\nimize mutual information (Viola and Wells III 1997; Kim, Kolmogorov, and Zabih 2003).\nHirschm¨uller and Scharstein (2009) compare a number of these approaches and report on\ntheir relative performance in scenes with exposure differences.\nCorrelation.\nAn alternative to taking intensity differences is to perform correlation, i.e., to\nmaximize the product (or cross-correlation) of the two aligned images,\nECC(u) =\nX\ni\nI0(xi)I1(xi + u).\n(8.10)\nAt ﬁrst glance, this may appear to make bias and gain modeling unnecessary, since the images\nwill prefer to line up regardless of their relative scales and offsets. However, this is actually\nnot true. If a very bright patch exists in I1(x), the maximum product may actually lie in that\narea.",
  "409": "8.1 Translational alignment\n387\nFor this reason, normalized cross-correlation is more commonly used,\nENCC(u) =\nP\ni[I0(xi) −I0] [I1(xi + u) −I1]\nqP\ni[I0(xi) −I0]2\nqP\ni[I1(xi + u) −I1]2\n,\n(8.11)\nwhere\nI0\n=\n1\nN\nX\ni\nI0(xi)\nand\n(8.12)\nI1\n=\n1\nN\nX\ni\nI1(xi + u)\n(8.13)\nare the mean images of the corresponding patches and N is the number of pixels in the patch.\nThe normalized cross-correlation score is always guaranteed to be in the range [−1, 1], which\nmakes it easier to handle in some higher-level applications, such as deciding which patches\ntruly match. Normalized correlation works well when matching images taken with different\nexposures, e.g., when creating high dynamic range images (Section 10.2). Note, however,\nthat the NCC score is undeﬁned if either of the two patches has zero variance (and, in fact, its\nperformance degrades for noisy low-contrast regions).\nA variant on NCC, which is related to the bias–gain regression implicit in the matching\nscore (8.9), is the normalized SSD score\nENSSD(u) = 1\n2\nP\ni\n\u0002\n[I0(xi) −I0] −[I1(xi + u) −I1]\n\u00032\nqP\ni[I0(xi) −I0]2 + [I1(xi + u) −I1]2\n(8.14)\nrecently proposed by Criminisi, Shotton, Blake et al. (2007). In their experiments, they ﬁnd\nthat it produces comparable results to NCC, but is more efﬁcient when applied to a large\nnumber of overlapping patches using a moving average technique (Section 3.2.2).\n8.1.1 Hierarchical motion estimation\nNow that we have a well-deﬁned alignment cost function to optimize, how can we ﬁnd its\nminimum? The simplest solution is to do a full search over some range of shifts, using ei-\nther integer or sub-pixel steps. This is often the approach used for block matching in motion\ncompensated video compression, where a range of possible motions (say, ±16 pixels) is ex-\nplored.4\nTo accelerate this search process, hierarchical motion estimation is often used: an image\npyramid (Section 3.5) is constructed and a search over a smaller number of discrete pixels\n4 In stereo matching (Section 11.1.2), an explicit search over all possible disparities (i.e., a plane sweep) is almost\nalways performed, since the number of search hypotheses is much smaller due to the 1D nature of the potential\ndisplacements.",
  "410": "388\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(corresponding to the same range of motion) is ﬁrst performed at coarser levels (Quam 1984;\nAnandan 1989; Bergen, Anandan, Hanna et al. 1992). The motion estimate from one level\nof the pyramid is then used to initialize a smaller local search at the next ﬁner level. Al-\nternatively, several seeds (good solutions) from the coarse level can be used to initialize the\nﬁne-level search. While this is not guaranteed to produce the same result as a full search, it\nusually works almost as well and is much faster.\nMore formally, let\nI(l)\nk (xj) ←˜I(l−1)\nk\n(2xj)\n(8.15)\nbe the decimated image at level l obtained by subsampling (downsampling) a smoothed ver-\nsion of the image at level l−1. See Section 3.5 for how to perform the required downsampling\n(pyramid construction) without introducing too much aliasing.\nAt the coarsest level, we search for the best displacement u(l) that minimizes the dif-\nference between images I(l)\n0\nand I(l)\n1 . This is usually done using a full search over some\nrange of displacements u(l) ∈2−l[−S, S]2, where S is the desired search range at the ﬁnest\n(original) resolution level, optionally followed by the incremental reﬁnement step described\nin Section 8.1.3.\nOnce a suitable motion vector has been estimated, it is used to predict a likely displace-\nment\nˆu(l−1) ←2u(l)\n(8.16)\nfor the next ﬁner level.5 The search over displacements is then repeated at the ﬁner level over\na much narrower range of displacements, say ˆu(l−1) ± 1, again optionally combined with an\nincremental reﬁnement step (Anandan 1989). Alternatively, one of the images can be warped\n(resampled) by the current motion estimate, in which case only small incremental motions\nneed to be computed at the ﬁner level. A nice description of the whole process, extended to\nparametric motion estimation (Section 8.2), is provided by Bergen, Anandan, Hanna et al.\n(1992).\n8.1.2 Fourier-based alignment\nWhen the search range corresponds to a signiﬁcant fraction of the larger image (as is the case\nin image stitching, see Chapter 9), the hierarchical approach may not work that well, since\nit is often not possible to coarsen the representation too much before signiﬁcant features are\nblurred away. In this case, a Fourier-based approach may be preferable.\n5 This doubling of displacements is only necessary if displacements are deﬁned in integer pixel coordinates,\nwhich is the usual case in the literature (Bergen, Anandan, Hanna et al. 1992). If normalized device coordinates\n(Section 2.1.5) are used instead, the displacements (and search ranges) need not change from level to level, although\nthe step sizes will need to be adjusted, to keep search steps of roughly one pixel.",
  "411": "8.1 Translational alignment\n389\nFourier-based alignment relies on the fact that the Fourier transform of a shifted signal\nhas the same magnitude as the original signal but a linearly varying phase (Section 3.4), i.e.,\nF {I1(x + u)} = F {I1(x)} e−ju·ω = I1(ω)e−ju·ω,\n(8.17)\nwhere ω is the vector-valued angular frequency of the Fourier transform and we use cal-\nligraphic notation I1(ω) = F {I1(x)} to denote the Fourier transform of a signal (Sec-\ntion 3.4).\nAnother useful property of Fourier transforms is that convolution in the spatial domain\ncorresponds to multiplication in the Fourier domain (Section 3.4).6 Thus, the Fourier trans-\nform of the cross-correlation function ECC can be written as\nF {ECC(u)} = F\n(X\ni\nI0(xi)I1(xi + u)\n)\n= F {I0(u)¯∗I1(u)} = I0(ω)I∗\n1(ω), (8.18)\nwhere\nf(u)¯∗g(u) =\nX\ni\nf(xi)g(xi + u)\n(8.19)\nis the correlation function, i.e., the convolution of one signal with the reverse of the other,\nand I∗\n1(ω) is the complex conjugate of I1(ω). This is because convolution is deﬁned as the\nsummation of one signal with the reverse of the other (Section 3.4).\nThus, to efﬁciently evaluate ECC over the range of all possible values of u, we take the\nFourier transforms of both images I0(x) and I1(x), multiply both transforms together (after\nconjugating the second one), and take the inverse transform of the result. The Fast Fourier\nTransform algorithm can compute the transform of an N × M image in O(NM log NM)\noperations (Bracewell 1986). This can be signiﬁcantly faster than the O(N 2M 2) operations\nrequired to do a full search when the full range of image overlaps is considered.\nWhile Fourier-based convolution is often used to accelerate the computation of image\ncorrelations, it can also be used to accelerate the sum of squared differences function (and its\nvariants). Consider the SSD formula given in (8.1). Its Fourier transform can be written as\nF {ESSD(u)}\n=\nF\n(X\ni\n[I1(xi + u) −I0(xi)]2\n)\n=\nδ(ω)\nX\ni\n[I2\n0(xi) + I2\n1(xi)] −2I0(ω)I∗\n1(ω).\n(8.20)\nThus, the SSD function can be computed by taking twice the correlation function and sub-\ntracting it from the sum of the energies in the two images.\n6 In fact, the Fourier shift property (8.17) derives from the convolution theorem by observing that shifting is\nequivalent to convolution with a displaced delta function δ(x −u).",
  "412": "390\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWindowed correlation.\nUnfortunately, the Fourier convolution theorem only applies when\nthe summation over xi is performed over all the pixels in both images, using a circular shift\nof the image when accessing pixels outside the original boundaries. While this is acceptable\nfor small shifts and comparably sized images, it makes no sense when the images overlap by\na small amount or one image is a small subset of the other.\nIn that case, the cross-correlation function should be replaced with a windowed (weighted)\ncross-correlation function,\nEWCC(u)\n=\nX\ni\nw0(xi)I0(xi) w1(xi + u)I1(xi + u),\n(8.21)\n=\n[w0(x)I0(x)]¯∗[w1(x)I1(x)]\n(8.22)\nwhere the weighting functions w0 and w1 are zero outside the valid ranges of the images\nand both images are padded so that circular shifts return 0 values outside the original image\nboundaries.\nAn even more interesting case is the computation of the weighted SSD function intro-\nduced in Equation (8.5),\nEWSSD(u)\n=\nX\ni\nw0(xi)w1(xi + u)[I1(xi + u) −I0(xi)]2.\n(8.23)\nExpanding this as a sum of correlations and deriving the appropriate set of Fourier transforms\nis left for Exercise 8.1.\nThe same kind of derivation can also be applied to the bias–gain corrected sum of squared\ndifference function EBG (8.9). Again, Fourier transforms can be used to efﬁciently compute\nall the correlations needed to perform the linear regression in the bias and gain parameters in\norder to estimate the exposure-compensated difference for each potential shift (Exercise 8.1).\nPhase correlation.\nA variant of regular correlation (8.18) that is sometimes used for motion\nestimation is phase correlation (Kuglin and Hines 1975; Brown 1992). Here, the spectrum of\nthe two signals being matched is whitened by dividing each per-frequency product in (8.18)\nby the magnitudes of the Fourier transforms,\nF {EPC(u)} =\nI0(ω)I∗\n1(ω)\n∥I0(ω)∥∥I1(ω)∥\n(8.24)\nbefore taking the ﬁnal inverse Fourier transform. In the case of noiseless signals with perfect\n(cyclic) shift, we have I1(x + u) = I0(x) and hence, from Equation (8.17), we obtain\nF {I1(x + u)}\n=\nI1(ω)e−2πju·ω = I0(ω) and\nF {EPC(u)}\n=\ne−2πju·ω.\n(8.25)",
  "413": "8.1 Translational alignment\n391\nThe output of phase correlation (under ideal conditions) is therefore a single spike (impulse)\nlocated at the correct value of u, which (in principle) makes it easier to ﬁnd the correct\nestimate.\nPhase correlation has a reputation in some quarters of outperforming regular correlation,\nbut this behavior depends on the characteristics of the signals and noise. If the original images\nare contaminated by noise in a narrow frequency band (e.g., low-frequency noise or peaked\nfrequency “hum”), the whitening process effectively de-emphasizes the noise in these regions.\nHowever, if the original signals have very low signal-to-noise ratio at some frequencies (say,\ntwo blurry or low-textured images with lots of high-frequency noise), the whitening process\ncan actually decrease performance (see Exercise 8.1).\nRecently, gradient cross-correlation has emerged as a promising alternative to phase cor-\nrelation (Argyriou and Vlachos 2003), although further systematic studies are probably war-\nranted. Phase correlation has also been studied by Fleet and Jepson (1990) as a method for\nestimating general optical ﬂow and stereo disparity.\nRotations and scale.\nWhile Fourier-based alignment is mostly used to estimate transla-\ntional shifts between images, it can, under certain limited conditions, also be used to estimate\nin-plane rotations and scales. Consider two images that are related purely by rotation, i.e.,\nI1( ˆRx) = I0(x).\n(8.26)\nIf we re-sample the images into polar coordinates,\n˜I0(r, θ) = I0(r cos θ, r sin θ) and ˜I1(r, θ) = I1(r cos θ, r sin θ),\n(8.27)\nwe obtain\n˜I1(r, θ + ˆθ) = ˜I0(r, θ).\n(8.28)\nThe desired rotation can then be estimated using a Fast Fourier Transform (FFT) shift-based\ntechnique.\nIf the two images are also related by a scale,\nI1(eˆs ˆRx) = I0(x),\n(8.29)\nwe can re-sample into log-polar coordinates,\n˜I0(s, θ) = I0(es cos θ, es sin θ) and ˜I1(s, θ) = I1(es cos θ, es sin θ),\n(8.30)\nto obtain\n˜I1(s + ˆs, θ + ˆθ) = I0(s, θ).\n(8.31)",
  "414": "392\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nI\nx\nei\nΔu I0(xi)\nI1(xi+u)\nJ1(xi+u)\nI0\nI1\nxi\nFigure 8.2\nTaylor series approximation of a function and the incremental computation of\nthe optical ﬂow correction amount. J1(xi + u) is the image gradient at (xi + u) and ei is\nthe current intensity difference.\nIn this case, care must be taken to choose a suitable range of s values that reasonably samples\nthe original image.\nFor images that are also translated by a small amount,\nI1(eˆs ˆRx + t) = I0(x),\n(8.32)\nDe Castro and Morandi (1987) propose an ingenious solution that uses several steps to esti-\nmate the unknown parameters. First, both images are converted to the Fourier domain and\nonly the magnitudes of the transformed images are retained. In principle, the Fourier mag-\nnitude images are insensitive to translations in the image plane (although the usual caveats\nabout border effects apply). Next, the two magnitude images are aligned in rotation and scale\nusing the polar or log-polar representations. Once rotation and scale are estimated, one of the\nimages can be de-rotated and scaled and a regular translational algorithm can be applied to\nestimate the translational shift.\nUnfortunately, this trick only applies when the images have large overlap (small transla-\ntional motion). For more general motion of patches or images, the parametric motion estima-\ntor described in Section 8.2 or the feature-based approaches described in Section 6.1 need to\nbe used.\n8.1.3 Incremental reﬁnement\nThe techniques described up till now can estimate alignment to the nearest pixel (or poten-\ntially fractional pixel if smaller search steps are used). In general, image stabilization and\nstitching applications require much higher accuracies to obtain acceptable results.\nTo obtain better sub-pixel estimates, we can use one of several techniques described by\nTian and Huhns (1986). One possibility is to evaluate several discrete (integer or fractional)\nvalues of (u, v) around the best value found so far and to interpolate the matching score to\nﬁnd an analytic minimum.",
  "415": "8.1 Translational alignment\n393\nA more commonly used approach, ﬁrst proposed by Lucas and Kanade (1981), is to\nperform gradient descent on the SSD energy function (8.1), using a Taylor series expansion\nof the image function (Figure 8.2),\nELK−SSD(u + ∆u)\n=\nX\ni\n[I1(xi + u + ∆u) −I0(xi)]2\n(8.33)\n≈\nX\ni\n[I1(xi + u) + J1(xi + u)∆u −I0(xi)]2\n(8.34)\n=\nX\ni\n[J1(xi + u)∆u + ei]2,\n(8.35)\nwhere\nJ1(xi + u) = ∇I1(xi + u) = (∂I1\n∂x , ∂I1\n∂y )(xi + u)\n(8.36)\nis the image gradient or Jacobian at (xi + u) and\nei = I1(xi + u) −I0(xi),\n(8.37)\nﬁrst introduced in (8.1), is the current intensity error.7 The gradient at a particular sub-pixel\nlocation (xi + u) can be computed using a variety of techniques, the simplest of which is\nto simply take the horizontal and vertical differences between pixels x and x + (1, 0) or\nx + (0, 1). More sophisticated derivatives can sometimes lead to noticeable performance\nimprovements.\nThe linearized form of the incremental update to the SSD error (8.35) is often called the\noptical ﬂow constraint or brightness constancy constraint equation\nIxu + Iyv + It = 0,\n(8.38)\nwhere the subscripts in Ix and Iy denote spatial derivatives, and It is called the temporal\nderivative, which makes sense if we are computing instantaneous velocity in a video se-\nquence. When squared and summed or integrated over a region, it can be used to compute\noptic ﬂow (Horn and Schunck 1981).\nThe above least squares problem (8.35) can be minimized by solving the associated nor-\nmal equations (Appendix A.2),\nA∆u = b\n(8.39)\nwhere\nA =\nX\ni\nJT\n1 (xi + u)J1(xi + u)\n(8.40)\n7 We follow the convention, commonly used in robotics and by Baker and Matthews (2004), that derivatives with\nrespect to (column) vectors result in row vectors, so that fewer transposes are needed in the formulas.",
  "416": "394\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nand\nb = −\nX\ni\neiJT\n1 (xi + u)\n(8.41)\nare called the (Gauss–Newton approximation of the) Hessian and gradient-weighted residual\nvector, respectively.8 These matrices are also often written as\nA =\n\"\nP I2\nx\nP IxIy\nP IxIy\nP I2\ny\n#\nand b = −\n\" P IxIt\nP IyIt\n#\n.\n(8.42)\nThe gradients required for J1(xi + u) can be evaluated at the same time as the image\nwarps required to estimate I1(xi + u) (Section 3.6.1 (3.89)) and, in fact, are often computed\nas a side-product of image interpolation. If efﬁciency is a concern, these gradients can be\nreplaced by the gradients in the template image,\nJ1(xi + u) ≈J0(xi),\n(8.43)\nsince near the correct alignment, the template and displaced target images should look sim-\nilar. This has the advantage of allowing the pre-computation of the Hessian and Jacobian\nimages, which can result in signiﬁcant computational savings (Hager and Belhumeur 1998;\nBaker and Matthews 2004). A further reduction in computation can be obtained by writing\nthe warped image I1(xi + u) used to compute ei in (8.37) as a convolution of a sub-pixel\ninterpolation ﬁlter with the discrete samples in I1 (Peleg and Rav-Acha 2006). Precomput-\ning the inner product between the gradient ﬁeld and shifted version of I1 allows the iterative\nre-computation of ei to be performed in constant time (independent of the number of pixels).\nThe effectiveness of the above incremental update rule relies on the quality of the Taylor\nseries approximation. When far away from the true displacement (say, 1–2 pixels), several\niterations may be needed. It is possible, however, to estimate a value for J1 using a least\nsquares ﬁt to a series of larger displacements in order to increase the range of convergence\n(Jurie and Dhome 2002) or to “learn” a special-purpose recognizer for a given patch (Avi-\ndan 2001; Williams, Blake, and Cipolla 2003; Lepetit, Pilet, and Fua 2006; Hinterstoisser,\nBenhimane, Navab et al. 2008; ¨Ozuysal, Calonder, Lepetit et al. 2010) as discussed in Sec-\ntion 4.1.4.\nA commonly used stopping criterion for incremental updating is to monitor the magnitude\nof the displacement correction ∥u∥and to stop when it drops below a certain threshold (say,\n1/10 of a pixel). For larger motions, it is usual to combine the incremental update rule with a\nhierarchical coarse-to-ﬁne search strategy, as described in Section 8.1.1.\n8 The true Hessian is the full second derivative of the error function E, which may not be positive deﬁnite—see\nSection 6.1.3 and Appendix A.3.",
  "417": "8.1 Translational alignment\n395\nxi\nxi+u\nu\ni\n(a)\n(b)\n(c)\nFigure 8.3\nAperture problems for different image regions, denoted by the orange and red\nL-shaped structures, overlaid in the same image to make it easier to diagram the ﬂow. (a) A\nwindow w(xi) centered at xi (black circle) can uniquely be matched to its corresponding\nstructure at xi +u in the second (red) image. (b) A window centered on the edge exhibits the\nclassic aperture problem, since it can be matched to a 1D family of possible locations. (c) In\na completely textureless region, the matches become totally unconstrained.\nConditioning and aperture problems.\nSometimes, the inversion of the linear system (8.39)\ncan be poorly conditioned because of lack of two-dimensional texture in the patch being\naligned. A commonly occurring example of this is the aperture problem, ﬁrst identiﬁed in\nsome of the early papers on optical ﬂow (Horn and Schunck 1981) and then studied more ex-\ntensively by Anandan (1989). Consider an image patch that consists of a slanted edge moving\nto the right (Figure 8.3). Only the normal component of the velocity (displacement) can be\nreliably recovered in this case. This manifests itself in (8.39) as a rank-deﬁcient matrix A,\ni.e., one whose smaller eigenvalue is very close to zero.9\nWhen Equation (8.39) is solved, the component of the displacement along the edge is very\npoorly conditioned and can result in wild guesses under small noise perturbations. One way\nto mitigate this problem is to add a prior (soft constraint) on the expected range of motions\n(Simoncelli, Adelson, and Heeger 1991; Baker, Gross, and Matthews 2004; Govindu 2006).\nThis can be accomplished by adding a small value to the diagonal of A, which essentially\nbiases the solution towards smaller ∆u values that still (mostly) minimize the squared error.\nHowever, the pure Gaussian model assumed when using a simple (ﬁxed) quadratic prior,\nas in (Simoncelli, Adelson, and Heeger 1991), does not always hold in practice, e.g., because\nof aliasing along strong edges (Triggs 2004). For this reason, it may be prudent to add some\nsmall fraction (say, 5%) of the larger eigenvalue to the smaller one before doing the matrix\ninversion.\n9The matrix A is by construction always guaranteed to be symmetric positive semi-deﬁnite, i.e., it has real\nnon-negative eigenvalues.",
  "418": "396\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 8.4\nSSD surfaces corresponding to three locations (red crosses) in an image:\n(a) highly textured area, strong minimum, low uncertainty; (b) strong edge, aperture prob-\nlem, high uncertainty in one direction; (c) weak texture, no clear minimum, large uncertainty.",
  "419": "8.1 Translational alignment\n397\nUncertainty modeling.\nThe reliability of a particular patch-based motion estimate can be\ncaptured more formally with an uncertainty model. The simplest such model is a covariance\nmatrix, which captures the expected variance in the motion estimate in all possible directions.\nAs discussed in Section 6.1.4 and Appendix B.6, under small amounts of additive Gaussian\nnoise, it can be shown that the covariance matrix Σu is proportional to the inverse of the\nHessian A,\nΣu = σ2\nnA−1,\n(8.44)\nwhere σ2\nn is the variance of the additive Gaussian noise (Anandan 1989; Matthies, Kanade,\nand Szeliski 1989; Szeliski 1989).\nFor larger amounts of noise, the linearization performed by the Lucas–Kanade algorithm\nin (8.35) is only approximate, so the above quantity becomes a Cramer–Rao lower bound on\nthe true covariance. Thus, the minimum and maximum eigenvalues of the Hessian A can now\nbe interpreted as the (scaled) inverse variances in the least-certain and most-certain directions\nof motion. (A more detailed analysis using a more realistic model of image noise is given by\nSteele and Jaynes (2005).) Figure 8.4 shows the local SSD surfaces for three different pixel\nlocations in an image. As you can see, the surface has a clear minimum in the highly textured\nregion and suffers from the aperture problem near the strong edge.\nBias and gain, weighting, and robust error metrics.\nThe Lucas–Kanade update rule can\nalso be applied to the bias–gain equation (8.9) to obtain\nELK−BG(u + ∆u) =\nX\ni\n[J1(xi + u)∆u + ei −αI0(xi) −β]2\n(8.45)\n(Lucas and Kanade 1981; Gennert 1988; Fuh and Maragos 1991; Baker, Gross, and Matthews\n2003). The resulting 4 × 4 system of equations can be solved to simultaneously estimate the\ntranslational displacement update ∆u and the bias and gain parameters β and α.\nA similar formulation can be derived for images (templates) that have a linear appearance\nvariation,\nI1(x + u) ≈I0(x) +\nX\nj\nλjBj(x),\n(8.46)\nwhere the Bj(x) are the basis images and the λj are the unknown coefﬁcients (Hager and\nBelhumeur 1998; Baker, Gross, Ishikawa et al. 2003; Baker, Gross, and Matthews 2003).\nPotential linear appearance variations include illumination changes (Hager and Belhumeur\n1998) and small non-rigid deformations (Black and Jepson 1998).\nA weighted (windowed) version of the Lucas–Kanade algorithm is also possible:\nELK−WSSD(u + ∆u) =\nX\ni\nw0(xi)w1(xi + u)[J1(xi + u)∆u + ei]2.\n(8.47)",
  "420": "398\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nNote that here, in deriving the Lucas–Kanade update from the original weighted SSD function\n(8.5), we have neglected taking the derivative of the w1(xi + u) weighting function with\nrespect to u, which is usually acceptable in practice, especially if the weighting function is a\nbinary mask with relatively few transitions.\nBaker, Gross, Ishikawa et al. (2003) only use the w0(x) term, which is reasonable if the\ntwo images have the same extent and no (independent) cutouts in the overlap region. They\nalso discuss the idea of making the weighting proportional to ∇I(x), which helps for very\nnoisy images, where the gradient itself is noisy. Similar observations, formulated in terms\nof total least squares (Van Huffel and Vandewalle 1991; Van Huffel and Lemmerling 2002),\nhave been made by other researchers studying optical ﬂow (Weber and Malik 1995; Bab-\nHadiashar and Suter 1998b; M¨uhlich and Mester 1998). Lastly, Baker, Gross, Ishikawa et al.\n(2003) show how evaluating Equation (8.47) at just the most reliable (highest gradient) pixels\ndoes not signiﬁcantly reduce performance for large enough images, even if only 5–10% of\nthe pixels are used. (This idea was originally proposed by Dellaert and Collins (1999), who\nused a more sophisticated selection criterion.)\nThe Lucas–Kanade incremental reﬁnement step can also be applied to the robust error\nmetric introduced in Section 8.1,\nELK−SRD(u + ∆u) =\nX\ni\nρ(J1(xi + u)∆u + ei),\n(8.48)\nwhich can be solved using the iteratively reweighted least squares technique described in\nSection 6.1.4.\n8.2 Parametric motion\nMany image alignment tasks, for example image stitching with handheld cameras, require\nthe use of more sophisticated motion models, as described in Section 2.1.2. Since these\nmodels, e.g., afﬁne deformations, typically have more parameters than pure translation, a\nfull search over the possible range of values is impractical. Instead, the incremental Lucas–\nKanade algorithm can be generalized to parametric motion models and used in conjunction\nwith a hierarchical search algorithm (Lucas and Kanade 1981; Rehg and Witkin 1991; Fuh\nand Maragos 1991; Bergen, Anandan, Hanna et al. 1992; Shashua and Toelg 1997; Shashua\nand Wexler 2001; Baker and Matthews 2004).\nFor parametric motion, instead of using a single constant translation vector u, we use\na spatially varying motion ﬁeld or correspondence map, x′(x; p), parameterized by a low-\ndimensional vector p, where x′ can be any of the motion models presented in Section 2.1.2.",
  "421": "8.2 Parametric motion\n399\nThe parametric incremental motion update rule now becomes\nELK−PM(p + ∆p)\n=\nX\ni\n[I1(x′(xi; p + ∆p)) −I0(xi)]2\n(8.49)\n≈\nX\ni\n[I1(x′\ni) + J1(x′\ni)∆p −I0(xi)]2\n(8.50)\n=\nX\ni\n[J1(x′\ni)∆p + ei]2,\n(8.51)\nwhere the Jacobian is now\nJ1(x′\ni) = ∂I1\n∂p = ∇I1(x′\ni)∂x′\n∂p (xi),\n(8.52)\ni.e., the product of the image gradient ∇I1 with the Jacobian of the correspondence ﬁeld,\nJx′ = ∂x′/∂p.\nThe motion Jacobians Jx′ for the 2D planar transformations introduced in Section 2.1.2\nand Table 2.1 are given in Table 6.1. Note how we have re-parameterized the motion matrices\nso that they are always the identity at the origin p = 0. This becomes useful later, when we\ntalk about the compositional and inverse compositional algorithms. (It also makes it easier to\nimpose priors on the motions.)\nFor parametric motion, the (Gauss–Newton) Hessian and gradient-weighted residual vec-\ntor become\nA =\nX\ni\nJTx′(xi)[∇IT\n1 (x′\ni)∇I1(x′\ni)]Jx′(xi)\n(8.53)\nand\nb = −\nX\ni\nJTx′(xi)[ei∇IT\n1 (x′\ni)].\n(8.54)\nNote how the expressions inside the square brackets are the same ones evaluated for the\nsimpler translational motion case (8.40–8.41).\nPatch-based approximation.\nThe computation of the Hessian and residual vectors for\nparametric motion can be signiﬁcantly more expensive than for the translational case. For\nparametric motion with n parameters and N pixels, the accumulation of A and b takes\nO(n2N) operations (Baker and Matthews 2004). One way to reduce this by a signiﬁcant\namount is to divide the image up into smaller sub-blocks (patches) Pj and to only accumulate\nthe simpler 2 × 2 quantities inside the square brackets at the pixel level (Shum and Szeliski\n2000),\nAj\n=\nX\ni∈Pj\n∇IT\n1 (x′\ni)∇I1(x′\ni)\n(8.55)\nbj\n=\nX\ni∈Pj\nei∇IT\n1 (x′\ni).\n(8.56)",
  "422": "400\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe full Hessian and residual can then be approximated as\nA ≈\nX\nj\nJTx′(ˆxj)[\nX\ni∈Pj\n∇IT\n1 (x′\ni)∇I1(x′\ni)]Jx′(ˆxj) =\nX\nj\nJTx′(ˆxj)AjJx′(ˆxj)\n(8.57)\nand\nb ≈−\nX\nj\nJTx′(ˆxj)[\nX\ni∈Pj\nei∇IT\n1 (x′\ni)] = −\nX\nj\nJTx′(ˆxj)bj,\n(8.58)\nwhere ˆxj is the center of each patch Pj (Shum and Szeliski 2000). This is equivalent to\nreplacing the true motion Jacobian with a piecewise-constant approximation. In practice,\nthis works quite well. The relationship of this approximation to feature-based registration is\ndiscussed in Section 9.2.4.\nCompositional approach.\nFor a complex parametric motion such as a homography, the\ncomputation of the motion Jacobian becomes complicated and may involve a per-pixel divi-\nsion. Szeliski and Shum (1997) observed that this can be simpliﬁed by ﬁrst warping the target\nimage I1 according to the current motion estimate x′(x; p),\n˜I1(x) = I1(x′(x; p)),\n(8.59)\nand then comparing this warped image against the template I0(x),\nELK−SS(∆p)\n=\nX\ni\n[˜I1(˜x(xi; ∆p)) −I0(xi)]2\n(8.60)\n≈\nX\ni\n[ ˜J1(xi)∆p + ei]2\n(8.61)\n=\nX\ni\n[∇˜I1(xi)J ˜x(xi)∆p + ei]2.\n(8.62)\nNote that since the two images are assumed to be fairly similar, only an incremental para-\nmetric motion is required, i.e., the incremental motion can be evaluated around p = 0, which\ncan lead to considerable simpliﬁcations. For example, the Jacobian of the planar projective\ntransform (6.19) now becomes\nJ ˜x = ∂˜x\n∂p\n\f\f\f\fp=0\n=\n\"\nx\ny\n1\n0\n0\n0\n−x2\n−xy\n0\n0\n0\nx\ny\n1\n−xy\n−y2\n#\n.\n(8.63)\nOnce the incremental motion ˜x has been computed, it can be prepended to the previously\nestimated motion, which is easy to do for motions represented with transformation matrices,\nsuch as those given in Tables 2.1 and 6.1. Baker and Matthews (2004) call this the forward\ncompositional algorithm, since the target image is being re-warped and the ﬁnal motion esti-\nmates are being composed.",
  "423": "8.2 Parametric motion\n401\nIf the appearance of the warped and template images is similar enough, we can replace\nthe gradient of ˜I1(x) with the gradient of I0(x), as suggested previously (8.43). This has po-\ntentially a big advantage in that it allows the pre-computation (and inversion) of the Hessian\nmatrix A given in Equation (8.53). The residual vector b (8.54) can also be partially precom-\nputed, i.e., the steepest descent images ∇I0(x)J ˜x(x) can precomputed and stored for later\nmultiplication with the e(x) = ˜I1(x)−I0(x) error images (Baker and Matthews 2004). This\nidea was ﬁrst suggested by Hager and Belhumeur (1998) in what Baker and Matthews (2004)\ncall a inverse additive scheme.\nBaker and Matthews (2004) introduce one more variant they call the inverse composi-\ntional algorithm. Rather than (conceptually) re-warping the warped target image ˜I1(x), they\ninstead warp the template image I0(x) and minimize\nELK−BM(∆p)\n=\nX\ni\n[˜I1(xi) −I0(˜x(xi; ∆p))]2\n(8.64)\n≈\nX\ni\n[∇I0(xi)J ˜x(xi)∆p −ei]2.\n(8.65)\nThis is identical to the forward warped algorithm (8.62) with the gradients ∇˜I1(x) replaced\nby the gradients ∇I0(x), except for the sign of ei. The resulting update ∆p is the negative of\nthe one computed by the modiﬁed Equation (8.62) and hence the inverse of the incremental\ntransformation must be prepended to the current transform. Because the inverse composi-\ntional algorithm has the potential of pre-computing the inverse Hessian and the steepest de-\nscent images, this makes it the preferred approach of those surveyed by Baker and Matthews\n(2004). Figure 8.5 (Baker, Gross, Ishikawa et al. 2003) beautifully shows all of the steps\nrequired to implement the inverse compositional algorithm.\nBaker and Matthews (2004) also discuss the advantage of using Gauss–Newton iteration\n(i.e., the ﬁrst-order expansion of the least squares, as above) compared to other approaches\nsuch as steepest descent and Levenberg–Marquardt. Subsequent parts of the series (Baker,\nGross, Ishikawa et al. 2003; Baker, Gross, and Matthews 2003, 2004) discuss more advanced\ntopics such as per-pixel weighting, pixel selection for efﬁciency, a more in-depth discussion of\nrobust metrics and algorithms, linear appearance variations, and priors on parameters. They\nmake for invaluable reading for anyone interested in implementing a highly tuned imple-\nmentation of incremental image registration. Evangelidis and Psarakis (2008) provide some\ndetailed experimental evaluations of these and other related approaches.\n8.2.1 Application: Video stabilization\nVideo stabilization is one of the most widely used applications of parametric motion esti-\nmation (Hansen, Anandan, Dana et al. 1994; Irani, Rousso, and Peleg 1997; Morimoto and",
  "424": "402\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.5\nA schematic overview of the inverse compositional algorithm (copied, with\npermission, from (Baker, Gross, Ishikawa et al. 2003)). Steps 3–6 (light-colored arrows) are\nperformed once as a pre-computation. The main algorithm simply consists of iterating: image\nwarping (Step 1), image differencing (Step 2), image dot products (Step 7), multiplication\nwith the inverse of the Hessian (Step 8), and the update to the warp (Step 9). All of these\nsteps can be performed efﬁciently.",
  "425": "8.2 Parametric motion\n403\nChellappa 1997; Srinivasan, Chellappa, Veeraraghavan et al. 2005). Algorithms for stabiliza-\ntion run inside both hardware devices, such as camcorders and still cameras, and software\npackages for improving the visual quality of shaky videos.\nIn their paper on full-frame video stabilization, Matsushita, Ofek, Ge et al. (2006) give\na nice overview of the three major stages of stabilization, namely motion estimation, motion\nsmoothing, and image warping. Motion estimation algorithms often use a similarity trans-\nform to handle camera translations, rotations, and zooming. The tricky part is getting these\nalgorithms to lock onto the background motion, which is a result of the camera movement,\nwithout getting distracted by independent moving foreground objects. Motion smoothing al-\ngorithms recover the low-frequency (slowly varying) part of the motion and then estimate\nthe high-frequency shake component that needs to be removed. Finally, image warping algo-\nrithms apply the high-frequency correction to render the original frames as if the camera had\nundergone only the smooth motion.\nThe resulting stabilization algorithms can greatly improve the appearance of shaky videos\nbut they often still contain visual artifacts. For example, image warping can result in missing\nborders around the image, which must be cropped, ﬁlled using information from other frames,\nor hallucinated using inpainting techniques (Section 10.5.1). Furthermore, video frames cap-\ntured during fast motion are often blurry. Their appearance can be improved either using\ndeblurring techniques (Section 10.3) or stealing sharper pixels from other frames with less\nmotion or better focus (Matsushita, Ofek, Ge et al. 2006). Exercise 8.3 has you implement\nand test some of these ideas.\nIn situations where the camera is translating a lot in 3D, e.g., when the videographer is\nwalking, an even better approach is to compute a full structure from motion reconstruction\nof the camera motion and 3D scene. A smooth 3D camera path can then be computed and\nthe original video re-rendered using view interpolation with the interpolated 3D point cloud\nserving as the proxy geometry while preserving salient features (Liu, Gleicher, Jin et al.\n2009). If you have access to a camera array instead of a single video camera, you can do even\nbetter using a light ﬁeld rendering approach (Section 13.3) (Smith, Zhang, Jin et al. 2009).\n8.2.2 Learned motion models\nAn alternative to parameterizing the motion ﬁeld with a geometric deformation such as an\nafﬁne transform is to learn a set of basis functions tailored to a particular application (Black,\nYacoob, Jepson et al. 1997). First, a set of dense motion ﬁelds (Section 8.4) is computed from\na set of training videos. Next, singular value decomposition (SVD) is applied to the stack of\nmotion ﬁelds ut(x) to compute the ﬁrst few singular vectors vk(x). Finally, for a new test\nsequence, a novel ﬂow ﬁeld is computed using a coarse-to-ﬁne algorithm that estimates the",
  "426": "404\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 8.6\nLearned parameterized motion ﬁelds for a walking sequence (Black, Yacoob,\nJepson et al. 1997) c⃝1997 IEEE: (a) learned basis ﬂow ﬁelds; (b) plots of motion coefﬁcients\nover time and corresponding estimated motion ﬁelds.\nunknown coefﬁcient ak in the parameterized ﬂow ﬁeld\nu(x) =\nX\nk\nakvk(x).\n(8.66)\nFigure 8.6a shows a set of basis ﬁelds learned by observing videos of walking motions.\nFigure 8.6b shows the temporal evolution of the basis coefﬁcients as well as a few of the\nrecovered parametric motion ﬁelds. Note that similar ideas can also be applied to feature\ntracks (Torresani, Hertzmann, and Bregler 2008), which is a topic we discuss in more detail\nin Sections 4.1.4 and 12.6.4.\n8.3 Spline-based motion\nWhile parametric motion models are useful in a wide variety of applications (such as video\nstabilization and mapping onto planar surfaces), most image motion is too complicated to be\ncaptured by such low-dimensional models.\nTraditionally, optical ﬂow algorithms (Section 8.4) compute an independent motion esti-\nmate for each pixel, i.e., the number of ﬂow vectors computed is equal to the number of input\npixels. The general optical ﬂow analog to Equation (8.1) can thus be written as\nESSD−OF({ui}) =\nX\ni\n[I1(xi + ui) −I0(xi)]2.\n(8.67)",
  "427": "8.3 Spline-based motion\n405\nFigure 8.7 Spline motion ﬁeld: the displacement vectors ui = (ui, vi) are shown as pluses\n(+) and are controlled by the smaller number of control vertices ˆuj = (ˆui, ˆvj), which are\nshown as circles (◦).\nNotice how in the above equation, the number of variables {ui} is twice the number of\nmeasurements, so the problem is underconstrained.\nThe two classic approaches to this problem, which we study in Section 8.4, are to perform\nthe summation over overlapping regions (the patch-based or window-based approach) or to\nadd smoothness terms on the {ui} ﬁeld using regularization or Markov random ﬁelds (Sec-\ntion 3.7). In this section, we describe an alternative approach that lies somewhere between\ngeneral optical ﬂow (independent ﬂow at each pixel) and parametric ﬂow (a small number of\nglobal parameters). The approach is to represent the motion ﬁeld as a two-dimensional spline\ncontrolled by a smaller number of control vertices {ˆuj} (Figure 8.7),\nui =\nX\nj\nˆujBj(xi) =\nX\nj\nˆujwi,j,\n(8.68)\nwhere the Bj(xi) are called the basis functions and are only non-zero over a small ﬁnite sup-\nport interval (Szeliski and Coughlan 1997). We call the wij = Bj(xi) weights to emphasize\nthat the {ui} are known linear combinations of the {ˆuj}. Some commonly used spline basis\nfunctions are shown in Figure 8.8.\nSubstituting the formula for the individual per-pixel ﬂow vectors ui (8.68) into the SSD\nerror metric (8.67) yields a parametric motion formula similar to Equation (8.50). The biggest\ndifference is that the Jacobian J1(x′\ni) (8.52) now consists of the sparse entries in the weight\nmatrix W = [wij].\nIn situations where we know something more about the motion ﬁeld, e.g., when the mo-\ntion is due to a camera moving in a static scene, we can use more specialized motion models.\nFor example, the plane plus parallax model (Section 2.1.5) can be naturally combined with\na spline-based motion representation, where the in-plane motion is represented by a homog-\nraphy (6.19) and the out-of-plane parallax d is represented by a scalar variable at each spline",
  "428": "406\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.8 Sample spline basis functions (Szeliski and Coughlan 1997) c⃝1997 Springer.\nThe block (constant) interpolator/basis corresponds to block-based motion estimation\n(Le Gall 1991). See Section 3.5.1 for more details on spline functions.",
  "429": "8.3 Spline-based motion\n407\n(a)\n(b)\n(c)\n(d)\nFigure 8.9\nQuadtree spline-based motion estimation (Szeliski and Shum 1996) c⃝1996\nIEEE: (a) quadtree spline representation, (b) which can lead to cracks, unless the white nodes\nare constrained to depend on their parents; (c) deformed quadtree spline mesh overlaid on\ngrayscale image; (d) ﬂow ﬁeld visualized as a needle diagram.\ncontrol point (Szeliski and Kang 1995; Szeliski and Coughlan 1997).\nIn many cases, the small number of spline vertices results in a motion estimation problem\nthat is well conditioned. However, if large textureless regions (or elongated edges subject\nto the aperture problem) persist across several spline patches, it may be necessary to add a\nregularization term to make the problem well posed (Section 3.7.1). The simplest way to\ndo this is to directly add squared difference penalties between adjacent vertices in the spline\ncontrol mesh {ˆuj}, as in (3.100). If a multi-resolution (coarse-to-ﬁne) strategy is being used,\nit is important to re-scale these smoothness terms while going from level to level.\nThe linear system corresponding to the spline-based motion estimator is sparse and regu-\nlar. Because it is usually of moderate size, it can often be solved using direct techniques such\nas Cholesky decomposition (Appendix A.4). Alternatively, if the problem becomes too large\nand subject to excessive ﬁll-in, iterative techniques such as hierarchically preconditioned con-\njugate gradient (Szeliski 1990b, 2006b) can be used instead (Appendix A.5).\nBecause of its robustness, spline-based motion estimation has been used for a number\nof applications, including visual effects (Roble 1999) and medical image registration (Sec-\ntion 8.3.1) (Szeliski and Lavall´ee 1996; Kybic and Unser 2003).\nOne disadvantage of the basic technique, however, is that the model does a poor job\nnear motion discontinuities, unless an excessive number of nodes is used. To remedy this\nsituation, Szeliski and Shum (1996) propose using a quadtree representation embedded in the\nspline control grid (Figure 8.9a). Large cells are used to present regions of smooth motion,\nwhile smaller cells are added in regions of motion discontinuities (Figure 8.9c).\nTo estimate the motion, a coarse-to-ﬁne strategy is used. Starting with a regular spline\nimposed over a lower-resolution image, an initial motion estimate is obtained. Spline patches\nwhere the motion is inconsistent, i.e., the squared residual (8.67) is above a threshold, are\nsubdivided into smaller patches. In order to avoid cracks in the resulting motion ﬁeld (Fig-",
  "430": "408\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 8.10\nElastic brain registration (Kybic and Unser 2003) c⃝2003 IEEE: (a) original\nbrain atlas and patient MRI images overlaid in red–green; (b) after elastic registration with\neight user-speciﬁed landmarks (not shown); (c) a cubic B-spline deformation ﬁeld, shown as\na deformed grid.\nure 8.9b), the values of certain nodes in the reﬁned mesh, i.e., those adjacent to larger cells,\nneed to be restricted so that they depend on their parent values. This is most easily accom-\nplished using a hierarchical basis representation for the quadtree spline (Szeliski 1990b) and\nselectively setting some of the hierarchical basis functions to 0, as described in (Szeliski and\nShum 1996).\n8.3.1 Application: Medical image registration\nBecause they excel at representing smooth elastic deformation ﬁelds, spline-based motion\nmodels have found widespread use in medical image registration (Bajcsy and Kovacic 1989;\nSzeliski and Lavall´ee 1996; Christensen, Joshi, and Miller 1997).10 Registration techniques\ncan be used both to track an individual patient’s development or progress over time (a lon-\ngitudinal study) or to match different patient images together to ﬁnd commonalities and de-\ntect variations or pathologies (cross-sectional studies). When different imaging modalities\nare being registered, e.g., computed tomography (CT) scans and magnetic resonance images\n(MRI), mutual information measures of similarity are often necessary (Viola and Wells III\n1997; Maes, Collignon, Vandermeulen et al. 1997).\nKybic and Unser (2003) provide a nice literature review and describe a complete working\nsystem based on representing both the images and the deformation ﬁelds as multi-resolution\nsplines. Figure 8.10 shows an example of the Kybic and Unser system being used to register\na patient’s brain MRI with a labeled brain atlas image. The system can be run in a fully auto-\n10 In computer graphics, such elastic volumetric deformation are known as free-form deformations (Sederberg and\nParry 1986; Coquillart 1990; Celniker and Gossard 1991).",
  "431": "8.4 Optical ﬂow\n409\n(a)\n(b)\n(c)\nFigure 8.11 Octree spline-based image registration of two vertebral surface models (Szeliski\nand Lavall´ee 1996) c⃝1996 Springer: (a) after initial rigid alignment; (b) after elastic align-\nment; (c) a cross-section through the adapted octree spline deformation ﬁeld.\nmatic mode but more accurate results can be obtained by locating a few key landmarks. More\nrecent papers on deformable medical image registration, including performance evaluations,\ninclude (Klein, Staring, and Pluim 2007; Glocker, Komodakis, Tziritas et al. 2008).\nAs with other applications, regular volumetric splines can be enhanced using selective\nreﬁnement. In the case of 3D volumetric image or surface registration, these are known as\noctree splines (Szeliski and Lavall´ee 1996) and have been used to register medical surface\nmodels such as vertebrae and faces from different patients (Figure 8.11).\n8.4 Optical ﬂow\nThe most general (and challenging) version of motion estimation is to compute an indepen-\ndent estimate of motion at each pixel, which is generally known as optical (or optic) ﬂow. As\nwe mentioned in the previous section, this generally involves minimizing the brightness or\ncolor difference between corresponding pixels summed over the image,\nESSD−OF({ui}) =\nX\ni\n[I1(xi + ui) −I0(xi)]2.\n(8.69)\nSince the number of variables {ui} is twice the number of measurements, the problem is\nunderconstrained. The two classic approaches to this problem are to perform the summa-\ntion locally over overlapping regions (the patch-based or window-based approach) or to\nadd smoothness terms on the {ui} ﬁeld using regularization or Markov random ﬁelds (Sec-\ntion 3.7) and to search for a global minimum.\nThe patch-based approach usually involves using a Taylor series expansion of the dis-\nplaced image function (8.35) in order to obtain sub-pixel estimates (Lucas and Kanade 1981).",
  "432": "410\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAnandan (1989) shows how a series of local discrete search steps can be interleaved with\nLucas–Kanade incremental reﬁnement steps in a coarse-to-ﬁne pyramid scheme, which al-\nlows the estimation of large motions, as described in Section 8.1.1. He also analyzes how the\nuncertainty in local motion estimates is related to the eigenvalues of the local Hessian matrix\nAi (8.44), as shown in Figures 8.3–8.4.\nBergen, Anandan, Hanna et al. (1992) develop a uniﬁed framework for describing both\nparametric (Section 8.2) and patch-based optic ﬂow algorithms and provide a nice introduc-\ntion to this topic. After each iteration of optic ﬂow estimation in a coarse-to-ﬁne pyramid,\nthey re-warp one of the images so that only incremental ﬂow estimates are computed (Sec-\ntion 8.1.1). When overlapping patches are used, an efﬁcient implementation is to ﬁrst com-\npute the outer products of the gradients and intensity errors (8.40–8.41) at every pixel and\nthen perform the overlapping window sums using a moving average ﬁlter.11\nInstead of solving for each motion (or motion update) independently, Horn and Schunck\n(1981) develop a regularization-based framework where (8.69) is simultaneously minimized\nover all ﬂow vectors {ui}. In order to constrain the problem, smoothness constraints, i.e.,\nsquared penalties on ﬂow derivatives, are added to the basic per-pixel error metric. Because\nthe technique was originally developed for small motions in a variational (continuous func-\ntion) framework, the linearized brightness constancy constraint corresponding to (8.35), i.e.,\n(8.38), is more commonly written as an analytic integral\nEHS =\nZ\n(Ixu + Iyv + It)2 dx dy,\n(8.70)\nwhere (Ix, Iy) = ∇I1 = J1 and It = ei is the temporal derivative, i.e., the brightness\nchange between images. The Horn and Schunck model can also be viewed as the limiting\ncase of spline-based motion estimation as the splines become 1x1 pixel patches.\nIt is also possible to combine ideas from local and global ﬂow estimation into a single\nframework by using a locally aggregated (as opposed to single-pixel) Hessian as the bright-\nness constancy term (Bruhn, Weickert, and Schn¨orr 2005). Consider the discrete analog\n(8.35) to the analytic global energy (8.70),\nEHSD =\nX\ni\nuT\ni [JiJT\ni ]ui + 2eiJT\ni ui + e2\ni .\n(8.71)\nIf we replace the per-pixel (rank 1) Hessians Ai = [JiJT\ni ] and residuals bi = Jiei with area-\naggregated versions (8.40–8.41), we obtain a global minimization algorithm where region-\nbased brightness constraints are used.\nAnother extension to the basic optic ﬂow model is to use a combination of global (para-\nmetric) and local motion models. For example, if we know that the motion is due to a camera\n11Other smoothing or aggregation ﬁlters can also be used at this stage (Bruhn, Weickert, and Schn¨orr 2005).",
  "433": "8.4 Optical ﬂow\n411\nmoving in a static scene (rigid motion), we can re-formulate the problem as the estimation of\na per-pixel depth along with the parameters of the global camera motion (Adiv 1989; Hanna\n1991; Bergen, Anandan, Hanna et al. 1992; Szeliski and Coughlan 1997; Nir, Bruckstein,\nand Kimmel 2008; Wedel, Cremers, Pock et al. 2009). Such techniques are closely related to\nstereo matching (Chapter 11). Alternatively, we can estimate either per-image or per-segment\nafﬁne motion models combined with per-pixel residual corrections (Black and Jepson 1996;\nJu, Black, and Jepson 1996; Chang, Tekalp, and Sezan 1997; M´emin and P´erez 2002). We\nrevisit this topic in Section 8.5.\nOf course, image brightness may not always be an appropriate metric for measuring ap-\npearance consistency, e.g., when the lighting in an image is varying. As discussed in Sec-\ntion 8.1, matching gradients, ﬁltered images, or other metrics such as image Hessians (sec-\nond derivative measures) may be more appropriate. It is also possible to locally compute the\nphase of steerable ﬁlters in the image, which is insensitive to both bias and gain transforma-\ntions (Fleet and Jepson 1990). Papenberg, Bruhn, Brox et al. (2006) review and explore such\nconstraints and also provide a detailed analysis and justiﬁcation for iteratively re-warping\nimages during incremental ﬂow computation.\nBecause the brightness constancy constraint is evaluated at each pixel independently,\nrather than being summed over patches where the constant ﬂow assumption may be violated,\nglobal optimization approaches tend to perform better near motion discontinuities. This is\nespecially true if robust metrics are used in the smoothness constraint (Black and Anandan\n1996; Bab-Hadiashar and Suter 1998a).12 One popular choice for robust metrics in the L1\nnorm, also known as total variation (TV), which results in a convex energy whose global\nminimum can be found (Bruhn, Weickert, and Schn¨orr 2005; Papenberg, Bruhn, Brox et\nal. 2006). Anisotropic smoothness priors, which apply a different smoothness in the direc-\ntions parallel and perpendicular to the image gradient, are another popular choice (Nagel and\nEnkelmann 1986; Sun, Roth, Lewis et al. 2008; Werlberger, Trobin, Pock et al. 2009). It\nis also possible to learn a set of better smoothness constraints (derivative ﬁlters and robust\nfunctions) from a set of paired ﬂow and intensity images (Sun, Roth, Lewis et al. 2008). Ad-\nditional details on some of these techniques are given by Baker, Black, Lewis et al. (2007)\nand Baker, Scharstein, Lewis et al. (2009).\nBecause of the large, two-dimensional search space in estimating ﬂow, most algorithms\nuse variations of gradient descent and coarse-to-ﬁne continuation methods to minimize the\nglobal energy function. This contrasts starkly with stereo matching (which is an “easier”\none-dimensional disparity estimation problem), where combinatorial optimization techniques\nhave been the method of choice for the last decade.\nFortunately, combinatorial optimization methods based on Markov random ﬁelds are be-\n12 Robust brightness metrics (Section 8.1, (8.2)) can also help improve the performance of window-based ap-\nproaches (Black and Anandan 1996).",
  "434": "412\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.12\nEvaluation of the results of 24 optical ﬂow algorithms, October 2009, http:\n//vision.middlebury.edu/ﬂow/, (Baker, Scharstein, Lewis et al. 2009). By moving the mouse\npointer over an underlined performance score, the user can interactively view the correspond-\ning ﬂow and error maps. Clicking on a score toggles between the computed and ground truth\nﬂows. Next to each score, the corresponding rank in the current column is indicated by a\nsmaller blue number. The minimum (best) score in each column is shown in boldface. The\ntable is sorted by the average rank (computed over all 24 columns, three region masks for each\nof the eight sequences). The average rank serves as an approximate measure of performance\nunder the selected metric/statistic.",
  "435": "8.4 Optical ﬂow\n413\nginning to appear and tend to be among the better-performing methods on the recently re-\nleased optical ﬂow database (Baker, Black, Lewis et al. 2007).13\nExamples of such techniques include the one developed by Glocker, Paragios, Komodakis\net al. (2008), who use a coarse-to-ﬁne strategy with per-pixel 2D uncertainty estimates, which\nare then used to guide the reﬁnement and search at the next ﬁner level. Instead of using gra-\ndient descent to reﬁne the ﬂow estimates, a combinatorial search over discrete displacement\nlabels (which is able to ﬁnd better energy minima) is performed using their Fast-PD algorithm\n(Komodakis, Tziritas, and Paragios 2008).\nLempitsky, Roth, and Rother. (2008) use fusion moves (Lempitsky, Rother, and Blake\n2007) over proposals generated from basic ﬂow algorithms (Horn and Schunck 1981; Lucas\nand Kanade 1981) to ﬁnd good solutions. The basic idea behind fusion moves is to replace\nportions of the current best estimate with hypotheses generated by more basic techniques\n(or their shifted versions) and to alternate them with local gradient descent for better energy\nminimization.\nThe ﬁeld of accurate motion estimation continues to evolve at a rapid pace, with signif-\nicant advances in performance occurring every year. The optical ﬂow evaluation Web site\n(http://vision.middlebury.edu/ﬂow/) is a good source of pointers to high-performing recently\ndeveloped algorithms (Figure 8.12).\n8.4.1 Multi-frame motion estimation\nSo far, we have looked at motion estimation as a two-frame problem, where the goal is to\ncompute a motion ﬁeld that aligns pixels from one image with those in another. In practice,\nmotion estimation is usually applied to video, where a whole sequence of frames is available\nto perform this task.\nOne classic approach to multi-frame motion is to ﬁlter the spatio-temporal volume using\noriented or steerable ﬁlters (Heeger 1988), in a manner analogous to oriented edge detec-\ntion (Section 3.2.3). Figure 8.13 shows two frames from the commonly used ﬂower garden\nsequence, as well as a horizontal slice through the spatio-temporal volume, i.e., the 3D vol-\nume created by stacking all of the video frames together. Because the pixel motion is mostly\nhorizontal, the slopes of individual (textured) pixel tracks, which correspond to their horizon-\ntal velocities, can clearly be seen. Spatio-temporal ﬁltering uses a 3D volume around each\npixel to determine the best orientation in space–time, which corresponds directly to a pixel’s\nvelocity.\nUnfortunately, in order to obtain reasonably accurate velocity estimates everywhere in\nan image, spatio-temporal ﬁlters have moderately large extents, which severely degrades the\nquality of their estimates near motion discontinuities. (This same problem is endemic in\n13 http://vision.middlebury.edu/ﬂow/.",
  "436": "414\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 8.13\nSlice through a spatio-temporal volume (Szeliski 1999) c⃝1999 IEEE: (a–b)\ntwo frames from the ﬂower garden sequence; (c) a horizontal slice through the complete\nspatio-temporal volume, with the arrows indicating locations of potential key frames where\nﬂow is estimated. Note that the colors for the ﬂower garden sequence are incorrect; the correct\ncolors (yellow ﬂowers) are shown in Figure 8.15.\n2D window-based motion estimators.) An alternative to full spatio-temporal ﬁltering is to\nestimate more local spatio-temporal derivatives and use them inside a global optimization\nframework to ﬁll in textureless regions (Bruhn, Weickert, and Schn¨orr 2005; Govindu 2006).\nAnother alternative is to simultaneously estimate multiple motion estimates, while also\noptionally reasoning about occlusion relationships (Szeliski 1999). Figure 8.13c shows schemat-\nically one potential approach to this problem. The horizontal arrows show the locations of\nkeyframes s where motion is estimated, while other slices indicate video frames t whose\ncolors are matched with those predicted by interpolating between the keyframes. Motion es-\ntimation can be cast as a global energy minimization problem that simultaneously minimizes\nbrightness compatibility and ﬂow compatibility terms between keyframes and other frames,\nin addition to using robust smoothness terms.\nThe multi-view framework is potentially even more appropriate for rigid scene motion\n(multi-view stereo) (Section 11.6), where the unknowns at each pixel are disparities and\nocclusion relationships can be determined directly from pixel depths (Szeliski 1999; Kol-\nmogorov and Zabih 2002). However, it may also be applicable to general motion, with the\naddition of models for object accelerations and occlusion relationships.\n8.4.2 Application: Video denoising\nVideo denoising is the process of removing noise and other artifacts such as scratches from\nﬁlm and video (Kokaram 2004). Unlike single image denoising, where the only information\navailable is in the current picture, video denoisers can average or borrow information from\nadjacent frames. However, in order to do this without introducing blur or jitter (irregular\nmotion), they need accurate per-pixel motion estimates.\nExercise 8.7 lists some of the steps required, which include the ability to determine if the",
  "437": "8.5 Layered motion\n415\ncurrent motion estimate is accurate enough to permit averaging with other frames. Gai and\nKang (2009) describe their recently developed restoration process, which involves a series of\nadditional steps to deal with the special characteristics of vintage ﬁlm.\n8.4.3 Application: De-interlacing\nAnother commonly used application of per-pixel motion estimation is video de-interlacing,\nwhich is the process of converting a video taken with alternating ﬁelds of even and odd\nlines to a non-interlaced signal that contains both ﬁelds in each frame (de Haan and Bellers\n1998). Two simple de-interlacing techniques are bob, which copies the line above or below\nthe missing line from the same ﬁeld, and weave, which copies the corresponding line from\nthe ﬁeld before or after. The names come from the visual artifacts generated by these two\nsimple techniques: bob introduces an up-and-down bobbing motion along strong horizontal\nlines; weave can lead to a “zippering” effect along horizontally translating edges. Replacing\nthese copy operators with averages can help but does not completely remove these artifacts.\nA wide variety of improved techniques have been developed for this process, which is\noften embedded in specialized DSP chips found inside video digitization boards in computers\n(since broadcast video is often interlaced, while computer monitors are not). A large class\nof these techniques estimates local per-pixel motions and interpolates the missing data from\nthe information available in spatially and temporally adjacent ﬁelds. Dai, Baker, and Kang\n(2009) review this literature and propose their own algorithm, which selects among seven\ndifferent interpolation functions at each pixel using an MRF framework.\n8.5 Layered motion\nIn many situation, visual motion is caused by the movement of a small number of objects\nat different depths in the scene. In such situations, the pixel motions can be described more\nsuccinctly (and estimated more reliably) if pixels are grouped into appropriate objects or\nlayers (Wang and Adelson 1994).\nFigure 8.14 shows this approach schematically. The motion in this sequence is caused by\nthe translational motion of the checkered background and the rotation of the foreground hand.\nThe complete motion sequence can be reconstructed from the appearance of the foreground\nand background elements, which can be represented as alpha-matted images (sprites or video\nobjects) and the parametric motion corresponding to each layer. Displacing and compositing\nthese layers in back to front order (Section 3.1.3) recreates the original video sequence.\nLayered motion representations not only lead to compact representations (Wang and\nAdelson 1994; Lee, ge Chen, lung Bruce Lin et al. 1997), but they also exploit the infor-\nmation available in multiple video frames, as well as accurately modeling the appearance of",
  "438": "416\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIntensity map\nAlpha map\nVelocity map\nIntensity map\nAlpha map\nVelocity map\nFrame 1\nFrame 2\nFrame 3\nFigure 8.14\nLayered motion estimation framework (Wang and Adelson 1994) c⃝1994\nIEEE: The top two rows describe the two layers, each of which consists of an intensity (color)\nimage, an alpha mask (black=transparent), and a parametric motion ﬁeld. The layers are com-\nposited with different amounts of motion to recreate the video sequence.\npixels near motion discontinuities. This makes them particularly suited as a representation\nfor image-based rendering (Section 13.2.1) (Shade, Gortler, He et al. 1998; Zitnick, Kang,\nUyttendaele et al. 2004) as well as object-level video editing.\nTo compute a layered representation of a video sequence, Wang and Adelson (1994) ﬁrst\nestimate afﬁne motion models over a collection of non-overlapping patches and then cluster\nthese estimates using k-means. They then alternate between assigning pixels to layers and\nrecomputing motion estimates for each layer using the assigned pixels, using a technique\nﬁrst proposed by Darrell and Pentland (1991). Once the parametric motions and pixel-wise\nlayer assignments have been computed for each frame independently, layers are constructed\nby warping and merging the various layer pieces from all of the frames together. Median\nﬁltering is used to produce sharp composite layers that are robust to small intensity variations,\nas well as to infer occlusion relationships between the layers. Figure 8.15 shows the results\nof this process on the ﬂower garden sequence. You can see both the initial and ﬁnal layer\nassignments for one of the frames, as well as the composite ﬂow and the alpha-matted layers\nwith their corresponding ﬂow vectors overlaid.\nIn follow-on work, Weiss and Adelson (1996) use a formal probabilistic mixture model\nto infer both the optimal number of layers and the per-pixel layer assignments. Weiss (1997)",
  "439": "8.5 Layered motion\n417\ncolor image (input frame)\nﬂow\ninitial layers\nﬁnal layers\nlayers with pixel assignments and ﬂow\nFigure 8.15 Layered motion estimation results (Wang and Adelson 1994) c⃝1994 IEEE.\nfurther generalizes this approach by replacing the per-layer afﬁne motion models with smooth\nregularized per-pixel motion estimates, which allows the system to better handle curved and\nundulating layers, such as those seen in most real-world sequences.\nThe above approaches, however, still make a distinction between estimating the motions\nand layer assignments and then later estimating the layer colors. In the system described by\nBaker, Szeliski, and Anandan (1998), the generative model illustrated in Figure 8.14 is gen-\neralized to account for real-world rigid motion scenes. The motion of each frame is described\nusing a 3D camera model and the motion of each layer is described using a 3D plane equation\nplus per-pixel residual depth offsets (the plane plus parallax representation (Section 2.1.5)).\nThe initial layer estimation proceeds in a manner similar to that of Wang and Adelson (1994),\nexcept that rigid planar motions (homographies) are used instead of afﬁne motion models.\nThe ﬁnal model reﬁnement, however, jointly re-optimizes the layer pixel color and opacity\nvalues Ll and the 3D depth, plane, and motion parameters zl, nl, and P t by minimizing the\ndiscrepancy between the re-synthesized and observed motion sequences (Baker, Szeliski, and\nAnandan 1998).\nFigure 8.16 shows the ﬁnal results obtained with this algorithm. As you can see, the\nmotion boundaries and layer assignments are much crisper than those in Figure 8.15. Because\nof the per-pixel depth offsets, the individual layer color values are also sharper than those\nobtained with afﬁne or planar motion models. While the original system of Baker, Szeliski,\nand Anandan (1998) required a rough initial assignment of pixels to layers, Torr, Szeliski,\nand Anandan (2001) describe automated Bayesian techniques for initializing this system and\ndetermining the optimal number of layers.\nLayered motion estimation continues to be an active area of research. Representative pa-\npers in this area include (Sawhney and Ayer 1996; Jojic and Frey 2001; Xiao and Shah 2005;\nKumar, Torr, and Zisserman 2008; Thayananthan, Iwasaki, and Cipolla 2008; Schoenemann\nand Cremers 2008).",
  "440": "418\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.16\nLayered stereo reconstruction (Baker, Szeliski, and Anandan 1998) c⃝1998\nIEEE: (a) ﬁrst and (b) last input images; (c) initial segmentation into six layers; (d) and\n(e) the six layer sprites; (f) depth map for planar sprites (darker denotes closer); front layer\n(g) before and (h) after residual depth estimation. Note that the colors for the ﬂower garden\nsequence are incorrect; the correct colors (yellow ﬂowers) are shown in Figure 8.15.\no\nOf course, layers are not the only way to introduce segmentation into motion estimation.\nA large number of algorithms have been developed that alternate between estimating optic\nﬂow vectors and segmenting them into coherent regions (Black and Jepson 1996; Ju, Black,\nand Jepson 1996; Chang, Tekalp, and Sezan 1997; M´emin and P´erez 2002; Cremers and\nSoatto 2005). Some of the more recent techniques rely on ﬁrst segmenting the input color\nimages and then estimating per-segment motions that produce a coherent motion ﬁeld while\nalso modeling occlusions (Zitnick, Kang, Uyttendaele et al. 2004; Zitnick, Jojic, and Kang\n2005; Stein, Hoiem, and Hebert 2007; Thayananthan, Iwasaki, and Cipolla 2008).\n8.5.1 Application: Frame interpolation\nFrame interpolation is another widely used application of motion estimation, often imple-\nmented in the same circuitry as de-interlacing hardware required to match an incoming video",
  "441": "8.5 Layered motion\n419\nto a monitor’s actual refresh rate. As with de-interlacing, information from novel in-between\nframes needs to be interpolated from preceding and subsequent frames. The best results can\nbe obtained if an accurate motion estimate can be computed at each unknown pixel’s lo-\ncation. However, in addition to computing the motion, occlusion information is critical to\nprevent colors from being contaminated by moving foreground objects that might obscure a\nparticular pixel in a preceding or subsequent frame.\nIn a little more detail, consider Figure 8.13c and assume that the arrows denote keyframes\nbetween which we wish to interpolate additional images. The orientations of the streaks\nin this ﬁgure encode the velocities of individual pixels. If the same motion estimate u0 is\nobtained at location x0 in image I0 as is obtained at location x0 + u0 in image I1, the ﬂow\nvectors are said to be consistent. This motion estimate can be transferred to location x0 +tu0\nin the image It being generated, where t ∈(0, 1) is the time of interpolation. The ﬁnal color\nvalue at pixel x0 + tu0 can be computed as a linear blend,\nIt(x0 + tu0) = (1 −t)I0(x0) + tI1(x0 + u0).\n(8.72)\nIf, however, the motion vectors are different at corresponding locations, some method must\nbe used to determine which is correct and which image contains colors that are occluded.\nThe actual reasoning is even more subtle than this. One example of such an interpolation\nalgorithm, based on earlier work in depth map interpolation (Shade, Gortler, He et al. 1998;\nZitnick, Kang, Uyttendaele et al. 2004) which is the one used in the ﬂow evaluation paper of\nBaker, Black, Lewis et al. (2007); Baker, Scharstein, Lewis et al. (2009). An even higher-\nquality frame interpolation algorithm, which uses gradient-based reconstruction, is presented\nby Mahajan, Huang, Matusik et al. (2009).\n8.5.2 Transparent layers and reﬂections\nA special case of layered motion that occurs quite often is transparent motion, which is usu-\nally caused by reﬂections seen in windows and picture frames (Figures 8.17 and 8.18).\nSome of the early work in this area handles transparent motion by either just estimating\nthe component motions (Shizawa and Mase 1991; Bergen, Burt, Hingorani et al. 1992; Darrell\nand Simoncelli 1993; Irani, Rousso, and Peleg 1994) or by assigning individual pixels to\ncompeting motion layers (Darrell and Pentland 1995; Black and Anandan 1996; Ju, Black,\nand Jepson 1996), which is appropriate for scenes partially seen through a ﬁne occluder\n(e.g., foliage). However, to accurately separate truly transparent layers, a better model for\nmotion due to reﬂections is required. Because of the way that light is both reﬂected from\nand transmitted through a glass surface, the correct model for reﬂections is an additive one,\nwhere each moving layer contributes some intensity to the ﬁnal image (Szeliski, Avidan, and\nAnandan 2000).",
  "442": "420\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 8.17 Light reﬂecting off the transparent glass of a picture frame: (a) ﬁrst image from\nthe input sequence; (b) dominant motion layer min-composite; (c) secondary motion residual\nlayer max-composite; (d–e) ﬁnal estimated picture and reﬂection layers The original images\nare from Black and Anandan (1996), while the separated layers are from Szeliski, Avidan,\nand Anandan (2000) c⃝2000 IEEE.\nIf the motions of the individual layers are known, the recovery of the individual layers is\na simple constrained least squares problem, with the individual layer images are constrained\nto be positive. However, this problem can suffer from extended low-frequency ambiguities,\nespecially if either of the layers lacks dark (black) pixels or the motion is uni-directional. In\ntheir paper, Szeliski, Avidan, and Anandan (2000) show that the simultaneous estimation of\nthe motions and layer values can be obtained by alternating between robustly computing the\nmotion layers and then making conservative (upper- or lower-bound) estimates of the layer\nintensities. The ﬁnal motion and layer estimates can then be polished using gradient descent\non a joint constrained least squares formulation similar to (Baker, Szeliski, and Anandan\n1998), where the over compositing operator is replaced with addition.\nFigures 8.17 and 8.18 show the results of applying these techniques to two different pic-\nture frames with reﬂections. Notice how, in the second sequence, the amount of reﬂected light\nis quite low compared to the transmitted light (the picture of the girl) and yet the algorithm is\nstill able to recover both layers.\nUnfortunately, the simple parametric motion models used in (Szeliski, Avidan, and Anan-\ndan 2000) are only valid for planar reﬂectors and scenes with shallow depth. The extension of\nthese techniques to curved reﬂectors and scenes with signiﬁcant depth has also been studied",
  "443": "8.6 Additional reading\n421\nFigure 8.18 Transparent motion separation (Szeliski, Avidan, and Anandan 2000) c⃝2000\nIEEE: (a) ﬁrst image from input sequence; (b) dominant motion layer min-composite; (c) sec-\nondary motion residual layer max-composite; (d–e) ﬁnal estimated picture and reﬂection lay-\ners. Note that the reﬂected layers in (c) and (e) are doubled in intensity to better show their\nstructure.\n(Swaminathan, Kang, Szeliski et al. 2002; Criminisi, Kang, Swaminathan et al. 2005), as has\nthe extension to scenes with more complex 3D depth (Tsin, Kang, and Szeliski 2006).\n8.6 Additional reading\nSome of the earliest algorithms for motion estimation were developed for motion-compen-\nsated video coding (Netravali and Robbins 1979) and such techniques continue to be used\nin modern coding standards such as MPEG, H.263, and H.264 (Le Gall 1991; Richardson\n2003).14 In computer vision, this ﬁeld was originally called image sequence analysis (Huang\n1981). Some of the early seminal papers include the variational approaches developed by\nHorn and Schunck (1981) and Nagel and Enkelmann (1986), and the patch-based translational\nalignment technique developed by Lucas and Kanade (1981). Hierarchical (coarse-to-ﬁne)\nversions of such algorithms were developed by Quam (1984), Anandan (1989), and Bergen,\nAnandan, Hanna et al. (1992), although they have also long been used in motion estimation\nfor video coding.\nTranslational motion models were generalized to afﬁne motion by Rehg and Witkin (1991),\nFuh and Maragos (1991), and Bergen, Anandan, Hanna et al. (1992) and to quadric refer-\nence surfaces by Shashua and Toelg (1997) and Shashua and Wexler (2001)—see Baker and\nMatthews (2004) for a nice review. Such parametric motion estimation algorithms have found\nwidespread application in video summarization (Teodosio and Bender 1993; Irani and Anan-\ndan 1998), video stabilization (Hansen, Anandan, Dana et al. 1994; Srinivasan, Chellappa,\n14 http://www.itu.int/rec/T-REC-H.264.",
  "444": "422\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nVeeraraghavan et al. 2005; Matsushita, Ofek, Ge et al. 2006), and video compression (Irani,\nHsu, and Anandan 1995; Lee, ge Chen, lung Bruce Lin et al. 1997). Surveys of parametric\nimage registration include those by Brown (1992), Zitov’aa and Flusser (2003), Goshtasby\n(2005), and Szeliski (2006a).\nGood general surveys and comparisons of optic ﬂow algorithms include those by Ag-\ngarwal and Nandhakumar (1988), Barron, Fleet, and Beauchemin (1994), Otte and Nagel\n(1994), Mitiche and Bouthemy (1996), Stiller and Konrad (1999), McCane, Novins, Cran-\nnitch et al. (2001), Szeliski (2006a), and Baker, Black, Lewis et al. (2007). The topic of\nmatching primitives, i.e., pre-transforming images using ﬁltering or other techniques before\nmatching, is treated in a number of papers (Anandan 1989; Bergen, Anandan, Hanna et al.\n1992; Scharstein 1994; Zabih and Woodﬁll 1994; Cox, Roy, and Hingorani 1995; Viola and\nWells III 1997; Negahdaripour 1998; Kim, Kolmogorov, and Zabih 2003; Jia and Tang 2003;\nPapenberg, Bruhn, Brox et al. 2006; Seitz and Baker 2009). Hirschm¨uller and Scharstein\n(2009) compare a number of these approaches and report on their relative performance in\nscenes with exposure differences.\nThe publication of a new benchmark for evaluating optical ﬂow algorithms (Baker, Black,\nLewis et al. 2007) has led to rapid advances in the quality of estimation algorithms, to the\npoint where new datasets may soon become necessary. According to their updated techni-\ncal report (Baker, Scharstein, Lewis et al. 2009), most of the best performing algorithms use\nrobust data and smoothness norms (often L1 TV) and continuous variational optimization\ntechniques, although some techniques use discrete optimization or segmentations (Papen-\nberg, Bruhn, Brox et al. 2006; Trobin, Pock, Cremers et al. 2008; Xu, Chen, and Jia 2008;\nLempitsky, Roth, and Rother. 2008; Werlberger, Trobin, Pock et al. 2009; Lei and Yang 2009;\nWedel, Cremers, Pock et al. 2009).\n8.7 Exercises\nEx 8.1: Correlation\nImplement and compare the performance of the following correlation\nalgorithms:\n• sum of squared differences (8.1)\n• sum of robust differences (8.2)\n• sum of absolute differences (8.3)\n• bias–gain compensated squared differences (8.9)\n• normalized cross-correlation (8.11)",
  "445": "8.7 Exercises\n423\n• windowed versions of the above (8.22–8.23)\n• Fourier-based implementations of the above measures (8.18–8.20)\n• phase correlation (8.24)\n• gradient cross-correlation (Argyriou and Vlachos 2003).\nCompare a few of your algorithms on different motion sequences with different amounts of\nnoise, exposure variation, occlusion, and frequency variations (e.g., high-frequency textures,\nsuch as sand or cloth, and low-frequency images, such as clouds or motion-blurred video).\nSome datasets with illumination variation and ground truth correspondences (horizontal mo-\ntion) can be found at http://vision.middlebury.edu/stereo/data/ (the 2005 and 2006 datasets).\nSome additional ideas, variants, and questions:\n1. When do you think that phase correlation will outperform regular correlation or SSD?\nCan you show this experimentally or justify it analytically?\n2. For the Fourier-based masked or windowed correlation and sum of squared differences,\nthe results should be the same as the direct implementations. Note that you will have\nto expand (8.5) into a sum of pairwise correlations, just as in (8.22). (This is part of the\nexercise.)\n3. For the bias–gain corrected variant of squared differences (8.9), you will also have\nto expand the terms to end up with a 3 × 3 (least squares) system of equations. If\nimplementing the Fast Fourier Transform version, you will need to ﬁgure out how all\nof these entries can be evaluated in the Fourier domain.\n4. (Optional) Implement some of the additional techniques studied by Hirschm¨uller and\nScharstein (2009) and see if your results agree with theirs.\nEx 8.2: Afﬁne registration\nImplement a coarse-to-ﬁne direct method for afﬁne and pro-\njective image alignment.\n1. Does it help to use lower-order (simpler) models at coarser levels of the pyramid\n(Bergen, Anandan, Hanna et al. 1992)?\n2. (Optional) Implement patch-based acceleration (Shum and Szeliski 2000; Baker and\nMatthews 2004).\n3. See the Baker and Matthews (2004) survey for more comparisons and ideas.\nEx 8.3: Stabilization\nWrite a program to stabilize an input video sequence. You should\nimplement the following steps, as described in Section 8.2.1:",
  "446": "424\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Compute the translation (and, optionally, rotation) between successive frames with ro-\nbust outlier rejection.\n2. Perform temporal high-pass ﬁltering on the motion parameters to remove the low-\nfrequency component (smooth the motion).\n3. Compensate for the high-frequency motion, zooming in slightly (a user-speciﬁed amount)\nto avoid missing edge pixels.\n4. (Optional) Do not zoom in, but instead borrow pixels from previous or subsequent\nframes to ﬁll in.\n5. (Optional) Compensate for images that are blurry because of fast motion by “stealing”\nhigher frequencies from adjacent frames.\nEx 8.4: Optical ﬂow\nCompute optical ﬂow (spline-based or per-pixel) between two im-\nages, using one or more of the techniques described in this chapter.\n1. Test your algorithms on the motion sequences available at http://vision.middlebury.\nedu/ﬂow/ or http://people.csail.mit.edu/celiu/motionAnnotation/ and compare your re-\nsults (visually) to those available on these Web sites. If you think your algorithm is\ncompetitive with the best, consider submitting it for formal evaluation.\n2. Visualize the quality of your results by generating in-between images using frame in-\nterpolation (Exercise 8.5).\n3. What can you say about the relative efﬁciency (speed) of your approach?\nEx 8.5: Automated morphing / frame interpolation\nWrite a program to automatically morph\nbetween pairs of images. Implement the following steps, as sketched out in Section 8.5.1 and\nby Baker, Scharstein, Lewis et al. (2009):\n1. Compute the ﬂow both ways (previous exercise). Consider using a multi-frame (n > 2)\ntechnique to better deal with occluded regions.\n2. For each intermediate (morphed) image, compute a set of ﬂow vectors and which im-\nages should be used in the ﬁnal composition.\n3. Blend (cross-dissolve) the images and view with a sequence viewer.\nTry this out on images of your friends and colleagues and see what kinds of morphs you get.\nAlternatively, take a video sequence and do a high-quality slow-motion effect. Compare your\nalgorithm with simple cross-fading.",
  "447": "8.7 Exercises\n425\nEx 8.6: Motion-based user interaction\nWrite a program to compute a low-resolution mo-\ntion ﬁeld in order to interactively control a simple application (Cutler and Turk 1998). For\nexample:\n1. Downsample each image using a pyramid and compute the optical ﬂow (spline-based\nor pixel-based) from the previous frame.\n2. Segment each training video sequence into different “actions” (e.g., hand moving in-\nwards, moving up, no motion) and “learn” the velocity ﬁelds associated with each one.\n(You can simply ﬁnd the mean and variance for each motion ﬁeld or use something\nmore sophisticated, such as a support vector machine (SVM).)\n3. Write a recognizer that ﬁnds successive actions of approximately the right duration and\nhook it up to an interactive application (e.g., a sound generator or a computer game).\n4. Ask your friends to test it out.\nEx 8.7: Video denoising\nImplement the algorithm sketched in Application 8.4.2. Your al-\ngorithm should contain the following steps:\n1. Compute accurate per-pixel ﬂow.\n2. Determine which pixels in the reference image have good matches with other frames.\n3. Either average all of the matched pixels or choose the sharpest image, if trying to\ncompensate for blur. Don’t forget to use regular single-frame denoising techniques as\npart of your solution, (see Section 3.4.4, Section 3.7.3, and Exercise 3.11).\n4. Devise a fall-back strategy for areas where you don’t think the ﬂow estimates are accu-\nrate enough.\nEx 8.8: Motion segmentation\nWrite a program to segment an image into separately mov-\ning regions or to reliably ﬁnd motion boundaries.\nUse the human-assisted motion segmentation database at http://people.csail.mit.edu/celiu/\nmotionAnnotation/ as some of your test data.\nEx 8.9: Layered motion estimation\nDecompose into separate layers (Section 8.5) a video\nsequence of a scene taken with a moving camera:\n1. Find the set of dominant (afﬁne or planar perspective) motions, either by computing\nthem in blocks or ﬁnding a robust estimate and then iteratively re-ﬁtting outliers.\n2. Determine which pixels go with each motion.",
  "448": "426\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Construct the layers by blending pixels from different frames.\n4. (Optional) Add per-pixel residual ﬂows or depths.\n5. (Optional) Reﬁne your estimates using an iterative global optimization technique.\n6. (Optional) Write an interactive renderer to generate in-between frames or view the\nscene from different viewpoints (Shade, Gortler, He et al. 1998).\n7. (Optional) Construct an unwrap mosaic from a more complex scene and use this to do\nsome video editing (Rav-Acha, Kohli, Fitzgibbon et al. 2008).\nEx 8.10: Transparent motion and reﬂection estimation\nTake a video sequence looking\nthrough a window (or picture frame) and see if you can remove the reﬂection in order to\nbetter see what is inside.\nThe steps are described in Section 8.5.2 and by Szeliski, Avidan, and Anandan (2000).\nAlternative approaches can be found in work by Shizawa and Mase (1991), Bergen, Burt,\nHingorani et al. (1992), Darrell and Simoncelli (1993), Darrell and Pentland (1995), Irani,\nRousso, and Peleg (1994), Black and Anandan (1996), and Ju, Black, and Jepson (1996).",
  "449": "Chapter 9\nImage stitching\n9.1\nMotion models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430\n9.1.1\nPlanar perspective motion\n. . . . . . . . . . . . . . . . . . . . . . . 431\n9.1.2\nApplication: Whiteboard and document scanning . . . . . . . . . . . 432\n9.1.3\nRotational panoramas . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n9.1.4\nGap closing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435\n9.1.5\nApplication: Video summarization and compression\n. . . . . . . . . 436\n9.1.6\nCylindrical and spherical coordinates\n. . . . . . . . . . . . . . . . . 438\n9.2\nGlobal alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n9.2.1\nBundle adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n9.2.2\nParallax removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n9.2.3\nRecognizing panoramas\n. . . . . . . . . . . . . . . . . . . . . . . . 446\n9.2.4\nDirect vs. feature-based alignment . . . . . . . . . . . . . . . . . . . 450\n9.3\nCompositing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450\n9.3.1\nChoosing a compositing surface . . . . . . . . . . . . . . . . . . . . 451\n9.3.2\nPixel selection and weighting (de-ghosting) . . . . . . . . . . . . . . 453\n9.3.3\nApplication: Photomontage\n. . . . . . . . . . . . . . . . . . . . . . 459\n9.3.4\nBlending\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459\n9.4\nAdditional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462\n9.5\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463",
  "450": "428\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 9.1\nImage stitching: (a) portion of a cylindrical panorama and (b) a spherical\npanorama constructed from 54 photographs (Szeliski and Shum 1997) c⃝1997 ACM; (c) a\nmulti-image panorama automatically assembled from an unordered photo collection; a multi-\nimage stitch (d) without and (e) with moving object removal (Uyttendaele, Eden, and Szeliski\n2001) c⃝2001 IEEE.",
  "451": "9 Image stitching\n429\nAlgorithms for aligning images and stitching them into seamless photo-mosaics are among\nthe oldest and most widely used in computer vision (Milgram 1975; Peleg 1981). image\nstitching algorithms create the high-resolution photo-mosaics used to produce today’s digital\nmaps and satellite photos. They also come bundled with most digital cameras and can be used\nto create beautiful ultra wide-angle panoramas.\nimage stitching originated in the photogrammetry community, where more manually in-\ntensive methods based on surveyed ground control points or manually registered tie points\nhave long been used to register aerial photos into large-scale photo-mosaics (Slama 1980).\nOne of the key advances in this community was the development of bundle adjustment al-\ngorithms (Section 7.4), which could simultaneously solve for the locations of all of the cam-\nera positions, thus yielding globally consistent solutions (Triggs, McLauchlan, Hartley et al.\n1999). Another recurring problem in creating photo-mosaics is the elimination of visible\nseams, for which a variety of techniques have been developed over the years (Milgram 1975,\n1977; Peleg 1981; Davis 1998; Agarwala, Dontcheva, Agrawala et al. 2004)\nIn ﬁlm photography, special cameras were developed in the 1990s to take ultra-wide-\nangle panoramas, often by exposing the ﬁlm through a vertical slit as the camera rotated on its\naxis (Meehan 1990). In the mid-1990s, image alignment techniques started being applied to\nthe construction of wide-angle seamless panoramas from regular hand-held cameras (Mann\nand Picard 1994; Chen 1995; Szeliski 1996). More recent work in this area has addressed\nthe need to compute globally consistent alignments (Szeliski and Shum 1997; Sawhney and\nKumar 1999; Shum and Szeliski 2000), to remove “ghosts” due to parallax and object move-\nment (Davis 1998; Shum and Szeliski 2000; Uyttendaele, Eden, and Szeliski 2001; Agarwala,\nDontcheva, Agrawala et al. 2004), and to deal with varying exposures (Mann and Picard 1994;\nUyttendaele, Eden, and Szeliski 2001; Levin, Zomet, Peleg et al. 2004; Agarwala, Dontcheva,\nAgrawala et al. 2004; Eden, Uyttendaele, and Szeliski 2006; Kopf, Uyttendaele, Deussen et\nal. 2007).1 These techniques have spawned a large number of commercial stitching products\n(Chen 1995; Sawhney, Kumar, Gendel et al. 1998), of which reviews and comparisons can\nbe found on the Web.2\nWhile most of the earlier techniques worked by directly minimizing pixel-to-pixel dis-\nsimilarities, more recent algorithms usually extract a sparse set of features and match them\nto each other, as described in Chapter 4. Such feature-based approaches to image stitching\nhave the advantage of being more robust against scene movement and are potentially faster,\nif implemented the right way. Their biggest advantage, however, is the ability to “recognize\npanoramas”, i.e., to automatically discover the adjacency (overlap) relationships among an\nunordered set of images, which makes them ideally suited for fully automated stitching of\n1 A collection of some of these papers was compiled by Benosman and Kang (2001) and they are surveyed by\nSzeliski (2006a).\n2 The Photosynth Web site, http://photosynth.net, allows people to create and upload panoramas for free.",
  "452": "430\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\npanoramas taken by casual users (Brown and Lowe 2007).\nWhat, then, are the essential problems in image stitching? As with image alignment, we\nmust ﬁrst determine the appropriate mathematical model relating pixel coordinates in one im-\nage to pixel coordinates in another; Section 9.1 reviews the basic models we have studied and\npresents some new motion models related speciﬁcally to panoramic image stitching. Next,\nwe must somehow estimate the correct alignments relating various pairs (or collections) of\nimages. Chapter 4 discussed how distinctive features can be found in each image and then\nefﬁciently matched to rapidly establish correspondences between pairs of images. Chapter 8\ndiscussed how direct pixel-to-pixel comparisons combined with gradient descent (and other\noptimization techniques) can also be used to estimate these parameters. When multiple im-\nages exist in a panorama, bundle adjustment (Section 7.4) can be used to compute a globally\nconsistent set of alignments and to efﬁciently discover which images overlap one another. In\nSection 9.2, we look at how each of these previously developed techniques can be modiﬁed\nto take advantage of the imaging setups commonly used to create panoramas.\nOnce we have aligned the images, we must choose a ﬁnal compositing surface for warping\nthe aligned images (Section 9.3.1). We also need algorithms to seamlessly cut and blend over-\nlapping images, even in the presence of parallax, lens distortion, scene motion, and exposure\ndifferences (Section 9.3.2–9.3.4).\n9.1 Motion models\nBefore we can register and align images, we need to establish the mathematical relationships\nthat map pixel coordinates from one image to another. A variety of such parametric motion\nmodels are possible, from simple 2D transforms, to planar perspective models, 3D camera\nrotations, lens distortions, and mapping to non-planar (e.g., cylindrical) surfaces.\nWe already covered several of these models in Sections 2.1 and 6.1. In particular, we saw\nin Section 2.1.5 how the parametric motion describing the deformation of a planar surfaced\nas viewed from different positions can be described with an eight-parameter homography\n(2.71) (Mann and Picard 1994; Szeliski 1996). We also saw how a camera undergoing a pure\nrotation induces a different kind of homography (2.72).\nIn this section, we review both of these models and show how they can be applied to dif-\nferent stitching situations. We also introduce spherical and cylindrical compositing surfaces\nand show how, under favorable circumstances, they can be used to perform alignment using\npure translations (Section 9.1.6). Deciding which alignment model is most appropriate for a\ngiven situation or set of data is a model selection problem (Hastie, Tibshirani, and Friedman\n2001; Torr 2002; Bishop 2006; Robert 2007), an important topic we do not cover in this book.",
  "453": "9.1 Motion models\n431\n(a) translation [2 dof]\n(b) afﬁne [6 dof]\n(c) perspective [8 dof]\n(d) 3D rotation [3+ dof]\nFigure 9.2 Two-dimensional motion models and how they can be used for image stitching.\n9.1.1 Planar perspective motion\nThe simplest possible motion model to use when aligning images is to simply translate and\nrotate them in 2D (Figure 9.2a). This is exactly the same kind of motion that you would\nuse if you had overlapping photographic prints. It is also the kind of technique favored by\nDavid Hockney to create the collages that he calls joiners (Zelnik-Manor and Perona 2007;\nNomura, Zhang, and Nayar 2007). Creating such collages, which show visible seams and\ninconsistencies that add to the artistic effect, is popular on Web sites such as Flickr, where they\nmore commonly go under the name panography (Section 6.1.2). Translation and rotation are\nalso usually adequate motion models to compensate for small camera motions in applications\nsuch as photo and video stabilization and merging (Exercise 6.1 and Section 8.2.1).\nIn Section 6.1.3, we saw how the mapping between two cameras viewing a common plane\ncan be described using a 3×3 homography (2.71). Consider the matrix M 10 that arises when\nmapping a pixel in one image to a 3D point and then back onto a second image,\n˜x1 ∼˜\nP 1 ˜\nP\n−1\n0 ˜x0 = M 10˜x0.\n(9.1)\nWhen the last row of the P 0 matrix is replaced with a plane equation ˆn0·p+c0 and points are\nassumed to lie on this plane, i.e., their disparity is d0 = 0, we can ignore the last column of\nM 10 and also its last row, since we do not care about the ﬁnal z-buffer depth. The resulting\nhomography matrix ˜\nH10 (the upper left 3 × 3 sub-matrix of M 10) describes the mapping\nbetween pixels in the two images,\n˜x1 ∼˜\nH10˜x0.\n(9.2)\nThis observation formed the basis of some of the earliest automated image stitching al-\ngorithms (Mann and Picard 1994; Szeliski 1994, 1996). Because reliable feature matching\ntechniques had not yet been developed, these algorithms used direct pixel value matching, i.e.,\ndirect parametric motion estimation, as described in Section 8.2 and Equations (6.19–6.20).",
  "454": "432\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMore recent stitching algorithms ﬁrst extract features and then match them up, often using\nrobust techniques such as RANSAC (Section 6.1.4) to compute a good set of inliers. The ﬁnal\ncomputation of the homography (9.2), i.e., the solution of the least squares ﬁtting problem\ngiven pairs of corresponding features,\nx1 = (1 + h00)x0 + h01y0 + h02\nh20x0 + h21y0 + 1\nand y1 = h10x0 + (1 + h11)y0 + h12\nh20x0 + h21y0 + 1\n,\n(9.3)\nuses iterative least squares, as described in Section 6.1.3 and Equations (6.21–6.23).\n9.1.2 Application: Whiteboard and document scanning\nThe simplest image-stitching application is to stitch together a number of image scans taken\non a ﬂatbed scanner. Say you have a large map, or a piece of child’s artwork, that is too large\nto ﬁt on your scanner. Simply take multiple scans of the document, making sure to overlap\nthe scans by a large enough amount to ensure that there are enough common features. Next,\ntake successive pairs of images that you know overlap, extract features, match them up, and\nestimate the 2D rigid transform (2.16),\nxk+1 = Rkxk + tk,\n(9.4)\nthat best matches the features, using two-point RANSAC, if necessary, to ﬁnd a good set\nof inliers. Then, on a ﬁnal compositing surface (aligned with the ﬁrst scan, for example),\nresample your images (Section 3.6.1) and average them together. Can you see any potential\nproblems with this scheme?\nOne complication is that a 2D rigid transformation is non-linear in the rotation angle θ,\nso you will have to either use non-linear least squares or constrain R to be orthonormal, as\ndescribed in Section 6.1.3.\nA bigger problem lies in the pairwise alignment process. As you align more and more\npairs, the solution may drift so that it is no longer globally consistent. In this case, a global op-\ntimization procedure, as described in Section 9.2, may be required. Such global optimization\noften requires a large system of non-linear equations to be solved, although in some cases,\nsuch as linearized homographies (Section 9.1.3) or similarity transforms (Section 6.1.2), reg-\nular least squares may be an option.\nA slightly more complex scenario is when you take multiple overlapping handheld pic-\ntures of a whiteboard or other large planar object (He and Zhang 2005; Zhang and He 2007).\nHere, the natural motion model to use is a homography, although a more complex model that\nestimates the 3D rigid motion relative to the plane (plus the focal length, if unknown), could\nin principle be used.",
  "455": "9.1 Motion models\n433\nΠ∞:\n(0,0,0,1)·p= 0\nR10\nx1 = (x1,y1,f1)\n~\nx0 = (x0,y0,f0)\n~\nFigure 9.3 Pure 3D camera rotation. The form of the homography (mapping) is particularly\nsimple and depends only on the 3D rotation matrix and focal lengths.\n9.1.3 Rotational panoramas\nThe most typical case for panoramic image stitching is when the camera undergoes a pure ro-\ntation. Think of standing at the rim of the Grand Canyon. Relative to the distant geometry in\nthe scene, as you snap away, the camera is undergoing a pure rotation, which is equivalent to\nassuming that all points are very far from the camera, i.e., on the plane at inﬁnity (Figure 9.3).\nSetting t0 = t1 = 0, we get the simpliﬁed 3 × 3 homography\n˜\nH10 = K1R1R−1\n0 K−1\n0\n= K1R10K−1\n0 ,\n(9.5)\nwhere Kk = diag(fk, fk, 1) is the simpliﬁed camera intrinsic matrix (2.59), assuming that\ncx = cy = 0, i.e., we are indexing the pixels starting from the optical center (Szeliski 1996).\nThis can also be re-written as\n\n\nx1\ny1\n1\n\n∼\n\n\nf1\nf1\n1\n\nR10\n\n\nf −1\n0\nf −1\n0\n1\n\n\n\n\nx0\ny0\n1\n\n\n(9.6)\nor\n\n\nx1\ny1\nf1\n\n∼R10\n\n\nx0\ny0\nf0\n\n,\n(9.7)\nwhich reveals the simplicity of the mapping equations and makes all of the motion parameters\nexplicit. Thus, instead of the general eight-parameter homography relating a pair of images,\nwe get the three-, four-, or ﬁve-parameter 3D rotation motion models corresponding to the\ncases where the focal length f is known, ﬁxed, or variable (Szeliski and Shum 1997).3 Es-\ntimating the 3D rotation matrix (and, optionally, focal length) associated with each image is\n3 An initial estimate of the focal lengths can be obtained using the intrinsic calibration techniques described in\nSection 6.3.4 or from EXIF tags.",
  "456": "434\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nintrinsically more stable than estimating a homography with a full eight degrees of freedom,\nwhich makes this the method of choice for large-scale image stitching algorithms (Szeliski\nand Shum 1997; Shum and Szeliski 2000; Brown and Lowe 2007).\nGiven this representation, how do we update the rotation matrices to best align two over-\nlapping images? Given a current estimate for the homography ˜\nH10 in (9.5), the best way to\nupdate R10 is to prepend an incremental rotation matrix R(ω) to the current estimate R10\n(Szeliski and Shum 1997; Shum and Szeliski 2000),\n˜\nH(ω) = K1R(ω)R10K−1\n0\n= [K1R(ω)K−1\n1 ][K1R10K−1\n0 ] = D ˜\nH10.\n(9.8)\nNote that here we have written the update rule in the compositional form, where the in-\ncremental update D is prepended to the current homography ˜\nH10. Using the small-angle\napproximation to R(ω) given in (2.35), we can write the incremental update matrix as\nD = K1R(ω)K−1\n1\n≈K1(I + [ω]×)K−1\n1\n=\n\n\n1\n−ωz\nf1ωy\nωz\n1\n−f1ωx\n−ωy/f1\nωx/f1\n1\n\n.\n(9.9)\nNotice how there is now a nice one-to-one correspondence between the entries in the D\nmatrix and the h00, . . . , h21 parameters used in Table 6.1 and Equation (6.19), i.e.,\n(h00, h01, h02, h00, h11, h12, h20, h21) = (0, −ωz, f1ωy, ωz, 0, −f1ωx, −ωy/f1, ωx/f1).\n(9.10)\nWe can therefore apply the chain rule to Equations (6.24 and 9.10) to obtain\n\"\nˆx′ −x\nˆy′ −y\n#\n=\n\"\n−xy/f1\nf1 + x2/f1\n−y\n−(f1 + y2/f1)\nxy/f1\nx\n# \n\nωx\nωy\nωz\n\n,\n(9.11)\nwhich give us the linearized update equations needed to estimate ω = (ωx, ωy, ωz).4 Notice\nthat this update rule depends on the focal length f1 of the target view and is independent\nof the focal length f0 of the template view. This is because the compositional algorithm\nessentially makes small perturbations to the target. Once the incremental rotation vector ω\nhas been computed, the R1 rotation matrix can be updated using R1 ←R(ω)R1.\nThe formulas for updating the focal length estimates are a little more involved and are\ngiven in (Shum and Szeliski 2000). We will not repeat them here, since an alternative up-\ndate rule, based on minimizing the difference between back-projected 3D rays, is given in\nSection 9.2.1. Figure 9.4 shows the alignment of four images under the 3D rotation motion\nmodel.\n4 This is the same as the rotational component of instantaneous rigid ﬂow (Bergen, Anandan, Hanna et al. 1992)\nand the update equations given by Szeliski and Shum (1997) and Shum and Szeliski (2000).",
  "457": "9.1 Motion models\n435\nFigure 9.4\nFour images taken with a hand-held camera registered using a 3D rotation mo-\ntion model (Szeliski and Shum 1997) c⃝1997 ACM. Notice how the homographies, rather\nthan being arbitrary, have a well-deﬁned keystone shape whose width increases away from\nthe origin.\n9.1.4 Gap closing\nThe techniques presented in this section can be used to estimate a series of rotation matrices\nand focal lengths, which can be chained together to create large panoramas. Unfortunately,\nbecause of accumulated errors, this approach will rarely produce a closed 360◦panorama.\nInstead, there will invariably be either a gap or an overlap (Figure 9.5).\nWe can solve this problem by matching the ﬁrst image in the sequence with the last one.\nThe difference between the two rotation matrix estimates associated with the repeated ﬁrst\nindicates the amount of misregistration. This error can be distributed evenly across the whole\nsequence by taking the quotient of the two quaternions associated with these rotations and\ndividing this “error quaternion” by the number of images in the sequence (assuming relatively\nconstant inter-frame rotations). We can also update the estimated focal length based on the\namount of misregistration. To do this, we ﬁrst convert the error quaternion into a gap angle,\nθg and then update the focal length using the equation f ′ = f(1 −θg/360◦).\nFigure 9.5a shows the end of registered image sequence and the ﬁrst image. There is a\nbig gap between the last image and the ﬁrst which are in fact the same image. The gap is\n32◦because the wrong estimate of focal length (f = 510) was used. Figure 9.5b shows the\nregistration after closing the gap with the correct focal length (f = 468). Notice that both\nmosaics show very little visual misregistration (except at the gap), yet Figure 9.5a has been\ncomputed using a focal length that has 9% error. Related approaches have been developed by\nHartley (1994b), McMillan and Bishop (1995), Stein (1995), and Kang and Weiss (1997) to\nsolve the focal length estimation problem using pure panning motion and cylindrical images.",
  "458": "436\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 9.5 Gap closing (Szeliski and Shum 1997) c⃝1997 ACM: (a) A gap is visible when\nthe focal length is wrong (f = 510). (b) No gap is visible for the correct focal length\n(f = 468).\nUnfortunately, this particular gap-closing heuristic only works for the kind of “one-dimensional”\npanorama where the camera is continuously turning in the same direction. In Section 9.2, we\ndescribe a different approach to removing gaps and overlaps that works for arbitrary camera\nmotions.\n9.1.5 Application: Video summarization and compression\nAn interesting application of image stitching is the ability to summarize and compress videos\ntaken with a panning camera. This application was ﬁrst suggested by Teodosio and Bender\n(1993), who called their mosaic-based summaries salient stills. These ideas were then ex-\ntended by Irani, Hsu, and Anandan (1995), Kumar, Anandan, Irani et al. (1995), and Irani and\nAnandan (1998) to additional applications, such as video compression and video indexing.\nWhile these early approaches used afﬁne motion models and were therefore restricted to long\nfocal lengths, the techniques were generalized by Lee, ge Chen, lung Bruce Lin et al. (1997)\nto full eight-parameter homographies and incorporated into the MPEG-4 video compression\nstandard, where the stitched background layers were called video sprites (Figure 9.6).\nWhile video stitching is in many ways a straightforward generalization of multiple-image\nstitching (Steedly, Pal, and Szeliski 2005; Baudisch, Tan, Steedly et al. 2006), the potential\npresence of large amounts of independent motion, camera zoom, and the desire to visualize\ndynamic events impose additional challenges. For example, moving foreground objects can\noften be removed using median ﬁltering. Alternatively, foreground objects can be extracted\ninto a separate layer (Sawhney and Ayer 1996) and later composited back into the stitched\npanoramas, sometimes as multiple instances to give the impressions of a “Chronophotograph”",
  "459": "9.1 Motion models\n437\n+\n+\n+ · · · +\n=\nFigure 9.6 Video stitching the background scene to create a single sprite image that can be\ntransmitted and used to re-create the background in each frame (Lee, ge Chen, lung Bruce Lin\net al. 1997) c⃝1997 IEEE.\n(Massey and Bender 1996) and sometimes as video overlays (Irani and Anandan 1998).\nVideos can also be used to create animated panoramic video textures (Section 13.5.2), in\nwhich different portions of a panoramic scene are animated with independently moving video\nloops (Agarwala, Zheng, Pal et al. 2005; Rav-Acha, Pritch, Lischinski et al. 2005), or to shine\n“video ﬂashlights” onto a composite mosaic of a scene (Sawhney, Arpa, Kumar et al. 2002).\nVideo can also provide an interesting source of content for creating panoramas taken from\nmoving cameras. While this invalidates the usual assumption of a single point of view (opti-\ncal center), interesting results can still be obtained. For example, the VideoBrush system of\nSawhney, Kumar, Gendel et al. (1998) uses thin strips taken from the center of the image to\ncreate a panorama taken from a horizontally moving camera. This idea can be generalized\nto other camera motions and compositing surfaces using the concept of mosaics on adap-\ntive manifold (Peleg, Rousso, Rav-Acha et al. 2000), and also used to generate panoramic\nstereograms (Peleg, Ben-Ezra, and Pritch 2001). Related ideas have been used to create\npanoramic matte paintings for multi-plane cel animation (Wood, Finkelstein, Hughes et al.\n1997), for creating stitched images of scenes with parallax (Kumar, Anandan, Irani et al.\n1995), and as 3D representations of more complex scenes using multiple-center-of-projection\nimages (Rademacher and Bishop 1998) and multi-perspective panoramas (Rom´an, Garg, and\nLevoy 2004; Rom´an and Lensch 2006; Agarwala, Agrawala, Cohen et al. 2006).\nAnother interesting variant on video-based panoramas are concentric mosaics (Section 13.3.3)\n(Shum and He 1999). Here, rather than trying to produce a single panoramic image, the com-\nplete original video is kept and used to re-synthesize views (from different camera origins)\nusing ray remapping (light ﬁeld rendering), thus endowing the panorama with a sense of 3D",
  "460": "438\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np = (X,Y,Z)\nx = (sinθ,h,cosθ)\nθ\nh\nx\ny\np = (X,Y,Z)\nx = (sinθ cosφ, sinφ,\ncosθ cosφ)\nθ\nφ\nx\ny\n(a)\n(b)\nFigure 9.7 Projection from 3D to (a) cylindrical and (b) spherical coordinates.\ndepth. The same data set can also be used to explicitly reconstruct the depth using multi-\nbaseline stereo (Peleg, Ben-Ezra, and Pritch 2001; Li, Shum, Tang et al. 2004; Zheng, Kang,\nCohen et al. 2007).\n9.1.6 Cylindrical and spherical coordinates\nAn alternative to using homographies or 3D motions to align images is to ﬁrst warp the images\ninto cylindrical coordinates and then use a pure translational model to align them (Chen 1995;\nSzeliski 1996). Unfortunately, this only works if the images are all taken with a level camera\nor with a known tilt angle.\nAssume for now that the camera is in its canonical position, i.e., its rotation matrix is the\nidentity, R = I, so that the optical axis is aligned with the z axis and the y axis is aligned\nvertically. The 3D ray corresponding to an (x, y) pixel is therefore (x, y, f).\nWe wish to project this image onto a cylindrical surface of unit radius (Szeliski 1996).\nPoints on this surface are parameterized by an angle θ and a height h, with the 3D cylindrical\ncoordinates corresponding to (θ, h) given by\n(sin θ, h, cos θ) ∝(x, y, f),\n(9.12)\nas shown in Figure 9.7a. From this correspondence, we can compute the formula for the\nwarped or mapped coordinates (Szeliski and Shum 1997),\nx′\n=\nsθ = s tan−1 x\nf ,\n(9.13)\ny′\n=\nsh = s\ny\np\nx2 + f 2 ,\n(9.14)\nwhere s is an arbitrary scaling factor (sometimes called the radius of the cylinder) that can be\nset to s = f to minimize the distortion (scaling) near the center of the image.5 The inverse of\n5 The scale can also be set to a larger or smaller value for the ﬁnal compositing surface, depending on the desired\noutput panorama resolution—see Section 9.3.",
  "461": "9.1 Motion models\n439\nthis mapping equation is given by\nx\n=\nf tan θ = f tan x′\ns ,\n(9.15)\ny\n=\nh\np\nx2 + f 2 = y′\ns f\nq\n1 + tan2 x′/s = f y′\ns sec x′\ns .\n(9.16)\nImages can also be projected onto a spherical surface (Szeliski and Shum 1997), which\nis useful if the ﬁnal panorama includes a full sphere or hemisphere of views, instead of just\na cylindrical strip. In this case, the sphere is parameterized by two angles (θ, φ), with 3D\nspherical coordinates given by\n(sin θ cos φ, sin φ, cos θ cos φ) ∝(x, y, f),\n(9.17)\nas shown in Figure 9.7b.6 The correspondence between coordinates is now given by (Szeliski\nand Shum 1997):\nx′\n=\nsθ = s tan−1 x\nf ,\n(9.18)\ny′\n=\nsφ = s tan−1\ny\np\nx2 + f 2 ,\n(9.19)\nwhile the inverse is given by\nx\n=\nf tan θ = f tan x′\ns ,\n(9.20)\ny\n=\np\nx2 + f 2 tan φ = tan y′\ns f\nq\n1 + tan2 x′/s = f tan y′\ns sec x′\ns .\n(9.21)\nNote that it may be simpler to generate a scaled (x, y, z) direction from Equation (9.17)\nfollowed by a perspective division by z and a scaling by f.\nCylindrical image stitching algorithms are most commonly used when the camera is\nknown to be level and only rotating around its vertical axis (Chen 1995). Under these condi-\ntions, images at different rotations are related by a pure horizontal translation.7 This makes\nit attractive as an initial class project in an introductory computer vision course, since the\nfull complexity of the perspective alignment algorithm (Sections 6.1, 8.2, and 9.1.3) can be\navoided. Figure 9.8 shows how two cylindrically warped images from a leveled rotational\npanorama are related by a pure translation (Szeliski and Shum 1997).\nProfessional panoramic photographers often use pan-tilt heads that make it easy to control\nthe tilt and to stop at speciﬁc detents in the rotation angle. Motorized rotation heads are also\n6 Note that these are not the usual spherical coordinates, ﬁrst presented in Equation (2.8). Here, the y axis points\nat the north pole instead of the z axis, since we are used to viewing images taken horizontally, i.e., with the y axis\npointing in the direction of the gravity vector.\n7Small vertical tilts can sometimes be compensated for with vertical translations.",
  "462": "440\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 9.8 A cylindrical panorama (Szeliski and Shum 1997) c⃝1997 ACM: (a) two cylin-\ndrically warped images related by a horizontal translation; (b) part of a cylindrical panorama\ncomposited from a sequence of images.\nFigure 9.9\nA spherical panorama constructed from 54 photographs (Szeliski and Shum\n1997) c⃝1997 ACM.\nsometimes used for the acquisition of larger panoramas (Kopf, Uyttendaele, Deussen et al.\n2007).8 Not only do they ensure a uniform coverage of the visual ﬁeld with a desired amount\nof image overlap but they also make it possible to stitch the images using cylindrical or\nspherical coordinates and pure translations. In this case, pixel coordinates (x, y, f) must ﬁrst\nbe rotated using the known tilt and panning angles before being projected into cylindrical\nor spherical coordinates (Chen 1995). Having a roughly known panning angle also makes it\neasier to compute the alignment, since the rough relative positioning of all the input images is\nknown ahead of time, enabling a reduced search range for alignment. Figure 9.9 shows a full\n3D rotational panorama unwrapped onto the surface of a sphere (Szeliski and Shum 1997).\nOne ﬁnal coordinate mapping worth mentioning is the polar mapping, where the north\n8See also http://gigapan.org.",
  "463": "9.2 Global alignment\n441\npole lies along the optical axis rather than the vertical axis,\n(cos θ sin φ, sin θ sin φ, cos φ) = s (x, y, z).\n(9.22)\nIn this case, the mapping equations become\nx′\n=\nsφ cos θ = sx\nr tan−1 r\nz ,\n(9.23)\ny′\n=\nsφ sin θ = sy\nr tan−1 r\nz ,\n(9.24)\nwhere r =\np\nx2 + y2 is the radial distance in the (x, y) plane and sφ plays a similar role\nin the (x′, y′) plane. This mapping provides an attractive visualization surface for certain\nkinds of wide-angle panoramas and is also a good model for the distortion induced by ﬁsheye\nlenses, as discussed in Section 2.1.6. Note how for small values of (x, y), the mapping\nequations reduce to x′ ≈sx/z, which suggests that s plays a role similar to the focal length\nf.\n9.2 Global alignment\nSo far, we have discussed how to register pairs of images using a variety of motion models. In\nmost applications, we are given more than a single pair of images to register. The goal is then\nto ﬁnd a globally consistent set of alignment parameters that minimize the mis-registration\nbetween all pairs of images (Szeliski and Shum 1997; Shum and Szeliski 2000; Sawhney and\nKumar 1999; Coorg and Teller 2000).\nIn this section, we extend the pairwise matching criteria (6.2, 8.1, and 8.50) to a global\nenergy function that involves all of the per-image pose parameters (Section 9.2.1). Once\nwe have computed the global alignment, we often need to perform local adjustments, such\nas parallax removal, to reduce double images and blurring due to local mis-registrations\n(Section 9.2.2). Finally, if we are given an unordered set of images to register, we need to\ndiscover which images go together to form one or more panoramas. This process of panorama\nrecognition is described in Section 9.2.3.\n9.2.1 Bundle adjustment\nOne way to register a large number of images is to add new images to the panorama one\nat a time, aligning the most recent image with the previous ones already in the collection\n(Szeliski and Shum 1997) and discovering, if necessary, which images it overlaps (Sawhney\nand Kumar 1999). In the case of 360◦panoramas, accumulated error may lead to the presence\nof a gap (or excessive overlap) between the two ends of the panorama, which can be ﬁxed",
  "464": "442\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nby stretching the alignment of all the images using a process called gap closing (Szeliski and\nShum 1997). However, a better alternative is to simultaneously align all the images using a\nleast-squares framework to correctly distribute any mis-registration errors.\nThe process of simultaneously adjusting pose parameters for a large collection of overlap-\nping images is called bundle adjustment in the photogrammetry community (Triggs, McLauch-\nlan, Hartley et al. 1999). In computer vision, it was ﬁrst applied to the general structure from\nmotion problem (Szeliski and Kang 1994) and then later specialized for panoramic image\nstitching (Shum and Szeliski 2000; Sawhney and Kumar 1999; Coorg and Teller 2000).\nIn this section, we formulate the problem of global alignment using a feature-based ap-\nproach, since this results in a simpler system. An equivalent direct approach can be obtained\neither by dividing images into patches and creating a virtual feature correspondence for each\none (as discussed in Section 9.2.4 and by Shum and Szeliski (2000)) or by replacing the\nper-feature error metrics with per-pixel metrics.\nConsider the feature-based alignment problem given in Equation (6.2), i.e.,\nEpairwise−LS =\nX\ni\n∥ri∥2 = ∥˜x′\ni(xi; p) −ˆx′\ni∥2.\n(9.25)\nFor multi-image alignment, instead of having a single collection of pairwise feature corre-\nspondences, {(xi, ˆx′\ni)}, we have a collection of n features, with the location of the ith feature\npoint in the jth image denoted by xij and its scalar conﬁdence (i.e., inverse variance) denoted\nby cij.9 Each image also has some associated pose parameters.\nIn this section, we assume that this pose consists of a rotation matrix Rj and a focal\nlength fj, although formulations in terms of homographies are also possible (Szeliski and\nShum 1997; Sawhney and Kumar 1999). The equation mapping a 3D point xi into a point\nxij in frame j can be re-written from Equations (2.68) and (9.5) as\n˜xij ∼KjRjxi and xi ∼R−1\nj K−1\nj\n˜xij,\n(9.26)\nwhere Kj = diag(fj, fj, 1) is the simpliﬁed form of the calibration matrix. The motion\nmapping a point xij from frame j into a point xik in frame k is similarly given by\n˜xik ∼˜\nHkj ˜xij = KkRkR−1\nj K−1\nj\n˜xij.\n(9.27)\nGiven an initial set of {(Rj, fj)} estimates obtained from chaining pairwise alignments, how\ndo we reﬁne these estimates?\nOne approach is to directly extend the pairwise energy Epairwise−LS (9.25) to a multiview\nformulation,\nEall−pairs−2D =\nX\ni\nX\njk\ncijcik∥˜xik(ˆxij; Rj, fj, Rk, fk) −ˆxik∥2,\n(9.28)\n9 Features that are not seen in image j have cij = 0. We can also use 2 × 2 inverse covariance matrices Σ−1\nij in\nplace of cij, as shown in Equation (6.11).",
  "465": "9.2 Global alignment\n443\nwhere the ˜xik function is the predicted location of feature i in frame k given by (9.27),\nˆxij is the observed location, and the “2D” in the subscript indicates that an image-plane\nerror is being minimized (Shum and Szeliski 2000). Note that since ˜xik depends on the ˆxij\nobserved value, we actually have an errors-in-variable problem, which in principle requires\nmore sophisticated techniques than least squares to solve (Van Huffel and Lemmerling 2002;\nMatei and Meer 2006). However, in practice, if we have enough features, we can directly\nminimize the above quantity using regular non-linear least squares and obtain an accurate\nmulti-frame alignment.\nWhile this approach works well in practice, it suffers from two potential disadvantages.\nFirst, since a summation is taken over all pairs with corresponding features, features that are\nobserved many times are overweighted in the ﬁnal solution. (In effect, a feature observed m\ntimes gets counted\n\u0000m\n2\n\u0001\ntimes instead of m times.) Second, the derivatives of ˜xik with respect\nto the {(Rj, fj)} are a little cumbersome, although using the incremental correction to Rj\nintroduced in Section 9.1.3 makes this more tractable.\nAn alternative way to formulate the optimization is to use true bundle adjustment, i.e., to\nsolve not only for the pose parameters {(Rj, fj)} but also for the 3D point positions {xi},\nEBA−2D =\nX\ni\nX\nj\ncij∥˜xij(xi; Rj, fj) −ˆxij∥2,\n(9.29)\nwhere ˜xij(xi; Rj, fj) is given by (9.26). The disadvantage of full bundle adjustment is that\nthere are more variables to solve for, so each iteration and also the overall convergence may\nbe slower. (Imagine how the 3D points need to “shift” each time some rotation matrices are\nupdated.) However, the computational complexity of each linearized Gauss–Newton step can\nbe reduced using sparse matrix techniques (Section 7.4.1) (Szeliski and Kang 1994; Triggs,\nMcLauchlan, Hartley et al. 1999; Hartley and Zisserman 2004).\nAn alternative formulation is to minimize the error in 3D projected ray directions (Shum\nand Szeliski 2000), i.e.,\nEBA−3D =\nX\ni\nX\nj\ncij∥˜xi(ˆxij; Rj, fj) −xi∥2,\n(9.30)\nwhere ˜xi(xij; Rj, fj) is given by the second half of (9.26). This has no particular advantage\nover (9.29). In fact, since errors are being minimized in 3D ray space, there is a bias towards\nestimating longer focal lengths, since the angles between rays become smaller as f increases.\nHowever, if we eliminate the 3D rays xi, we can derive a pairwise energy formulated in\n3D ray space (Shum and Szeliski 2000),\nEall−pairs−3D =\nX\ni\nX\njk\ncijcik∥˜xi(ˆxij; Rj, fj) −˜xi(ˆxik; Rk, fk)∥2.\n(9.31)",
  "466": "444\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis results in the simplest set of update equations (Shum and Szeliski 2000), since the fk can\nbe folded into the creation of the homogeneous coordinate vector as in Equation (9.7). Thus,\neven though this formula over-weights features that occur more frequently, it is the method\nused by Shum and Szeliski (2000) and Brown, Szeliski, and Winder (2005). In order to reduce\nthe bias towards longer focal lengths, we multiply each residual (3D error) by\np\nfjfk, which\nis similar to projecting the 3D rays into a “virtual camera” of intermediate focal length.\nUp vector selection.\nAs mentioned above, there exists a global ambiguity in the pose of the\n3D cameras computed by the above methods. While this may not appear to matter, people\nprefer that the ﬁnal stitched image is “upright” rather than twisted or tilted. More concretely,\npeople are used to seeing photographs displayed so that the vertical (gravity) axis points\nstraight up in the image. Consider how you usually shoot photographs: while you may pan\nand tilt the camera any which way, you usually keep the horizontal edge of your camera (its\nx-axis) parallel to the ground plane (perpendicular to the world gravity direction).\nMathematically, this constraint on the rotation matrices can be expressed as follows. Re-\ncall from Equation (9.26) that the 3D to 2D projection is given by\n˜xik ∼KkRkxi.\n(9.32)\nWe wish to post-multiply each rotation matrix Rk by a global rotation Rg such that the pro-\njection of the global y-axis, ˆ= (0, 1, 0) is perpendicular to the image x-axis, ˆı = (1, 0, 0).10\nThis constraint can be written as\nˆıT RkRgˆ= 0\n(9.33)\n(note that the scaling by the calibration matrix is irrelevant here). This is equivalent to re-\nquiring that the ﬁrst row of Rk, rk0 = ˆıT Rk be perpendicular to the second column of Rg,\nrg1 = Rgˆ. This set of constraints (one per input image) can be written as a least squares\nproblem,\nrg1 = arg min\nr\nX\nk\n(rT rk0)2 = arg min\nr rT\n\"X\nk\nrk0rT\nk0\n#\nr.\n(9.34)\nThus, rg1 is the smallest eigenvector of the scatter or moment matrix spanned by the indi-\nvidual camera rotation x-vectors, which should generally be of the form (c, 0, s) when the\ncameras are upright.\nTo fully specify the Rg global rotation, we need to specify one additional constraint. This\nis related to the view selection problem discussed in Section 9.3.1. One simple heuristic is to\n10 Note that here we use the convention common in computer graphics that the vertical world axis corresponds to\ny. This is a natural choice if we wish the rotation matrix associated with a “regular” image taken horizontally to be\nthe identity, rather than a 90◦rotation around the x-axis.",
  "467": "9.2 Global alignment\n445\nprefer the average z-axis of the individual rotation matrices, k = P\nk ˆk\nT Rk to be close to\nthe world z-axis, rg2 = Rgˆk. We can therefore compute the full rotation matrix Rg in three\nsteps:\n1. rg1 = min eigenvector (P\nk rk0rT\nk0);\n2. rg0 = N((P\nk rk2) × rg1);\n3. rg2 = rg0 × rg1,\nwhere N(v) = v/∥v∥normalizes a vector v.\n9.2.2 Parallax removal\nOnce we have optimized the global orientations and focal lengths of our cameras, we may ﬁnd\nthat the images are still not perfectly aligned, i.e., the resulting stitched image looks blurry\nor ghosted in some places. This can be caused by a variety of factors, including unmodeled\nradial distortion, 3D parallax (failure to rotate the camera around its optical center), small\nscene motions such as waving tree branches, and large-scale scene motions such as people\nmoving in and out of pictures.\nEach of these problems can be treated with a different approach. Radial distortion can be\nestimated (potentially ahead of time) using one of the techniques discussed in Section 2.1.6.\nFor example, the plumb-line method (Brown 1971; Kang 2001; El-Melegy and Farag 2003)\nadjusts radial distortion parameters until slightly curved lines become straight, while mosaic-\nbased approaches adjust them until mis-registration is reduced in image overlap areas (Stein\n1997; Sawhney and Kumar 1999).\n3D parallax can be handled by doing a full 3D bundle adjustment, i.e., by replacing the\nprojection equation (9.26) used in Equation (9.29) with Equation (2.68), which models cam-\nera translations. The 3D positions of the matched feature points and cameras can then be si-\nmultaneously recovered, although this can be signiﬁcantly more expensive than parallax-free\nimage registration. Once the 3D structure has been recovered, the scene could (in theory) be\nprojected to a single (central) viewpoint that contains no parallax. However, in order to do\nthis, dense stereo correspondence needs to be performed (Section 11.3) (Li, Shum, Tang et al.\n2004; Zheng, Kang, Cohen et al. 2007), which may not be possible if the images contain only\npartial overlap. In that case, it may be necessary to correct for parallax only in the overlap\nareas, which can be accomplished using a multi-perspective plane sweep (MPPS) algorithm\n(Kang, Szeliski, and Uyttendaele 2004; Uyttendaele, Criminisi, Kang et al. 2004).\nWhen the motion in the scene is very large, i.e., when objects appear and disappear com-\npletely, a sensible solution is to simply select pixels from only one image at a time as the\nsource for the ﬁnal composite (Milgram 1977; Davis 1998; Agarwala, Dontcheva, Agrawala",
  "468": "446\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\net al. 2004), as discussed in Section 9.3.2. However, when the motion is reasonably small (on\nthe order of a few pixels), general 2D motion estimation (optical ﬂow) can be used to perform\nan appropriate correction before blending using a process called local alignment (Shum and\nSzeliski 2000; Kang, Uyttendaele, Winder et al. 2003). This same process can also be used\nto compensate for radial distortion and 3D parallax, although it uses a weaker motion model\nthan explicitly modeling the source of error and may, therefore, fail more often or introduce\nunwanted distortions.\nThe local alignment technique introduced by Shum and Szeliski (2000) starts with the\nglobal bundle adjustment (9.31) used to optimize the camera poses. Once these have been\nestimated, the desired location of a 3D point xi can be estimated as the average of the back-\nprojected 3D locations,\n¯xi ∼\nX\nj\ncij ˜xi(ˆxij; Rj, fj)\n,X\nj\ncij ,\n(9.35)\nwhich can be projected into each image j to obtain a target location ¯xij. The difference\nbetween the target locations ¯xij and the original features xij provide a set of local motion\nestimates\nuij = ¯xij −xij,\n(9.36)\nwhich can be interpolated to form a dense correction ﬁeld uj(xj). In their system, Shum and\nSzeliski (2000) use an inverse warping algorithm where the sparse −uij values are placed at\nthe new target locations ¯xij, interpolated using bilinear kernel functions (Nielson 1993) and\nthen added to the original pixel coordinates when computing the warped (corrected) image.\nIn order to get a reasonably dense set of features to interpolate, Shum and Szeliski (2000)\nplace a feature point at the center of each patch (the patch size controls the smoothness in\nthe local alignment stage), rather than relying of features extracted using an interest operator\n(Figure 9.10).\nAn alternative approach to motion-based de-ghosting was proposed by Kang, Uytten-\ndaele, Winder et al. (2003), who estimate dense optical ﬂow between each input image and a\ncentral reference image. The accuracy of the ﬂow vector is checked using a photo-consistency\nmeasure before a given warped pixel is considered valid and is used to compute a high dy-\nnamic range radiance estimate, which is the goal of their overall algorithm. The requirement\nfor a reference image makes their approach less applicable to general image mosaicing, al-\nthough an extension to this case could certainly be envisaged.\n9.2.3 Recognizing panoramas\nThe ﬁnal piece needed to perform fully automated image stitching is a technique to recognize\nwhich images actually go together, which Brown and Lowe (2007) call recognizing panora-",
  "469": "9.2 Global alignment\n447\n(a)\n(b)\n(c)\nFigure 9.10\nDeghosting a mosaic with motion parallax (Shum and Szeliski 2000) c⃝2000\nIEEE: (a) composite with parallax; (b) after a single deghosting step (patch size 32); (c) after\nmultiple steps (sizes 32, 16 and 8).\nmas. If the user takes images in sequence so that each image overlaps its predecessor and\nalso speciﬁes the ﬁrst and last images to be stitched, bundle adjustment combined with the\nprocess of topology inference can be used to automatically assemble a panorama (Sawhney\nand Kumar 1999). However, users often jump around when taking panoramas, e.g., they\nmay start a new row on top of a previous one, jump back to take a repeat shot, or create\n360◦panoramas where end-to-end overlaps need to be discovered. Furthermore, the ability\nto discover multiple panoramas taken by a user over an extended period of time can be a big\nconvenience.\nTo recognize panoramas, Brown and Lowe (2007) ﬁrst ﬁnd all pairwise image overlaps\nusing a feature-based method and then ﬁnd connected components in the overlap graph to\n“recognize” individual panoramas (Figure 9.11). The feature-based matching stage ﬁrst ex-\ntracts scale invariant feature transform (SIFT) feature locations and feature descriptors (Lowe\n2004) from all the input images and places them in an indexing structure, as described in Sec-\ntion 4.1.3. For each image pair under consideration, the nearest matching neighbor is found\nfor each feature in the ﬁrst image, using the indexing structure to rapidly ﬁnd candidates and\nthen comparing feature descriptors to ﬁnd the best match. RANSAC is used to ﬁnd a set of in-\nlier matches; pairs of matches are used to hypothesize similarity motion models that are then\nused to count the number of inliers. (A more recent RANSAC algorithm tailored speciﬁcally\nfor rotational panoramas is described by Brown, Hartley, and Nist´er (2007).)\nIn practice, the most difﬁcult part of getting a fully automated stitching algorithm to\nwork is deciding which pairs of images actually correspond to the same parts of the scene.\nRepeated structures such as windows (Figure 9.12) can lead to false matches when using\na feature-based approach. One way to mitigate this problem is to perform a direct pixel-\nbased comparison between the registered images to determine if they actually are different\nviews of the same scene. Unfortunately, this heuristic may fail if there are moving objects\nin the scene (Figure 9.13). While there is no magic bullet for this problem, short of full\nscene understanding, further improvements can likely be made by applying domain-speciﬁc",
  "470": "448\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 9.11\nRecognizing panoramas (Brown, Szeliski, and Winder 2005), ﬁgures cour-\ntesy of Matthew Brown: (a) input images with pairwise matches; (b) images grouped into\nconnected components (panoramas); (c) individual panoramas registered and blended into\nstitched composites.",
  "471": "9.2 Global alignment\n449\nFigure 9.12\nMatching errors (Brown, Szeliski, and Winder 2004): accidental matching of\nseveral features can lead to matches between pairs of images that do not actually overlap.\nFigure 9.13\nValidation of image matches by direct pixel error comparison can fail when the\nscene contains moving objects (Uyttendaele, Eden, and Szeliski 2001) c⃝2001 IEEE.",
  "472": "450\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nheuristics, such as priors on typical camera motions as well as machine learning techniques\napplied to the problem of match validation.\n9.2.4 Direct vs. feature-based alignment\nGiven that there exist these two approaches to aligning images, which is preferable?\nEarly feature-based methods would get confused in regions that were either too textured\nor not textured enough. The features would often be distributed unevenly over the images,\nthereby failing to match image pairs that should have been aligned. Furthermore, establishing\ncorrespondences relied on simple cross-correlation between patches surrounding the feature\npoints, which did not work well when the images were rotated or had foreshortening due to\nhomographies.\nToday, feature detection and matching schemes are remarkably robust and can even be\nused for known object recognition from widely separated views (Lowe 2004). Features not\nonly respond to regions of high “cornerness” (F¨orstner 1986; Harris and Stephens 1988) but\nalso to “blob-like” regions (Lowe 2004), and uniform areas (Matas, Chum, Urban et al. 2004;\nTuytelaars and Van Gool 2004). Furthermore, because they operate in scale-space and use a\ndominant orientation (or orientation invariant descriptors), they can match images that differ\nin scale, orientation, and even foreshortening. Our own experience in working with feature-\nbased approaches is that if the features are well distributed over the image and the descriptors\nreasonably designed for repeatability, enough correspondences to permit image stitching can\nusually be found (Brown, Szeliski, and Winder 2005).\nThe biggest disadvantage of direct pixel-based alignment techniques is that they have a\nlimited range of convergence. Even though they can be used in a hierarchical (coarse-to-\nﬁne) estimation framework, in practice it is hard to use more than two or three levels of a\npyramid before important details start to be blurred away.11 For matching sequential frames\nin a video, direct approaches can usually be made to work. However, for matching partially\noverlapping images in photo-based panoramas or for image collections where the contrast or\ncontent varies too much, they fail too often to be useful and feature-based approaches are\ntherefore preferred.\n9.3 Compositing\nOnce we have registered all of the input images with respect to each other, we need to decide\nhow to produce the ﬁnal stitched mosaic image. This involves selecting a ﬁnal compositing\nsurface (ﬂat, cylindrical, spherical, etc.) and view (reference image). It also involves selecting\n11 Fourier-based correlation (Szeliski 1996; Szeliski and Shum 1997) can extend this range but requires cylindrical\nimages or motion prediction to be useful.",
  "473": "9.3 Compositing\n451\nwhich pixels contribute to the ﬁnal composite and how to optimally blend these pixels to\nminimize visible seams, blur, and ghosting.\nIn this section, we review techniques that address these problems, namely compositing\nsurface parameterization, pixel and seam selection, blending, and exposure compensation.\nMy emphasis is on fully automated approaches to the problem. Since the creation of high-\nquality panoramas and composites is as much an artistic endeavor as a computational one,\nvarious interactive tools have been developed to assist this process (Agarwala, Dontcheva,\nAgrawala et al. 2004; Li, Sun, Tang et al. 2004; Rother, Kolmogorov, and Blake 2004).\nSome of these are covered in more detail in Section 10.4.\n9.3.1 Choosing a compositing surface\nThe ﬁrst choice to be made is how to represent the ﬁnal image. If only a few images are\nstitched together, a natural approach is to select one of the images as the reference and to\nthen warp all of the other images into its reference coordinate system. The resulting com-\nposite is sometimes called a ﬂat panorama, since the projection onto the ﬁnal surface is still\na perspective projection, and hence straight lines remain straight (which is often a desirable\nattribute).12\nFor larger ﬁelds of view, however, we cannot maintain a ﬂat representation without ex-\ncessively stretching pixels near the border of the image. (In practice, ﬂat panoramas start\nto look severely distorted once the ﬁeld of view exceeds 90◦or so.) The usual choice for\ncompositing larger panoramas is to use a cylindrical (Chen 1995; Szeliski 1996) or spherical\n(Szeliski and Shum 1997) projection, as described in Section 9.1.6. In fact, any surface used\nfor environment mapping in computer graphics can be used, including a cube map, which\nrepresents the full viewing sphere with the six square faces of a cube (Greene 1986; Szeliski\nand Shum 1997). Cartographers have also developed a number of alternative methods for\nrepresenting the globe (Bugayevskiy and Snyder 1995).\nThe choice of parameterization is somewhat application dependent and involves a trade-\noff between keeping the local appearance undistorted (e.g., keeping straight lines straight)\nand providing a reasonably uniform sampling of the environment. Automatically making\nthis selection and smoothly transitioning between representations based on the extent of the\npanorama is an active area of current research (Kopf, Uyttendaele, Deussen et al. 2007).\nAn interesting recent development in panoramic photography has been the use of stereo-\ngraphic projections looking down at the ground (in an outdoor scene) to create “little planet”\nrenderings.13\n12 Recently, some techniques have been developed to straighten curved lines in cylindrical and spherical panora-\nmas (Carroll, Agrawala, and Agarwala 2009; Kopf, Lischinski, Deussen et al. 2009).\n13 These are inspired by The Little Prince by Antoine De Saint-Exupery. Go to http://www.ﬂickr.com and search",
  "474": "452\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nView selection.\nOnce we have chosen the output parameterization, we still need to deter-\nmine which part of the scene will be centered in the ﬁnal view. As mentioned above, for a ﬂat\ncomposite, we can choose one of the images as a reference. Often, a reasonable choice is the\none that is geometrically most central. For example, for rotational panoramas represented as\na collection of 3D rotation matrices, we can choose the image whose z-axis is closest to the\naverage z-axis (assuming a reasonable ﬁeld of view). Alternatively, we can use the average\nz-axis (or quaternion, but this is trickier) to deﬁne the reference rotation matrix.\nFor larger, e.g., cylindrical or spherical, panoramas, we can use the same heuristic if a\nsubset of the viewing sphere has been imaged. In the case of full 360◦panoramas, a better\nchoice might be to choose the middle image from the sequence of inputs, or sometimes the\nﬁrst image, assuming this contains the object of greatest interest. In all of these cases, having\nthe user control the ﬁnal view is often highly desirable. If the “up vector” computation de-\nscribed in Section 9.2.1 is working correctly, this can be as simple as panning over the image\nor setting a vertical “center line” for the ﬁnal panorama.\nCoordinate transformations.\nAfter selecting the parameterization and reference view, we\nstill need to compute the mappings between the input and output pixels coordinates.\nIf the ﬁnal compositing surface is ﬂat (e.g., a single plane or the face of a cube map)\nand the input images have no radial distortion, the coordinate transformation is the simple\nhomography described by (9.5). This kind of warping can be performed in graphics hardware\nby appropriately setting texture mapping coordinates and rendering a single quadrilateral.\nIf the ﬁnal composite surface has some other analytic form (e.g., cylindrical or spherical),\nwe need to convert every pixel in the ﬁnal panorama into a viewing ray (3D point) and then\nmap it back into each image according to the projection (and optionally radial distortion)\nequations. This process can be made more efﬁcient by precomputing some lookup tables,\ne.g., the partial trigonometric functions needed to map cylindrical or spherical coordinates to\n3D coordinates or the radial distortion ﬁeld at each pixel. It is also possible to accelerate this\nprocess by computing exact pixel mappings on a coarser grid and then interpolating these\nvalues.\nWhen the ﬁnal compositing surface is a texture-mapped polyhedron, a slightly more so-\nphisticated algorithm must be used. Not only do the 3D and texture map coordinates have to\nbe properly handled, but a small amount of overdraw outside the triangle footprints in the tex-\nture map is necessary, to ensure that the texture pixels being interpolated during 3D rendering\nhave valid values (Szeliski and Shum 1997).\nSampling issues.\nWhile the above computations can yield the correct (fractional) pixel\naddresses in each input image, we still need to pay attention to sampling issues. For example,\nfor “little planet projection”.",
  "475": "9.3 Compositing\n453\nif the ﬁnal panorama has a lower resolution than the input images, pre-ﬁltering the input\nimages is necessary to avoid aliasing. These issues have been extensively studied in both the\nimage processing and computer graphics communities. The basic problem is to compute the\nappropriate pre-ﬁlter, which depends on the distance (and arrangement) between neighboring\nsamples in a source image. As discussed in Sections 3.5.2 and 3.6.1, various approximate\nsolutions, such as MIP mapping (Williams 1983) or elliptically weighted Gaussian averaging\n(Greene and Heckbert 1986) have been developed in the graphics community. For highest\nvisual quality, a higher order (e.g., cubic) interpolator combined with a spatially adaptive pre-\nﬁlter may be necessary (Wang, Kang, Szeliski et al. 2001). Under certain conditions, it may\nalso be possible to produce images with a higher resolution than the input images using the\nprocess of super-resolution (Section 10.3).\n9.3.2 Pixel selection and weighting (de-ghosting)\nOnce the source pixels have been mapped onto the ﬁnal composite surface, we must still\ndecide how to blend them in order to create an attractive-looking panorama. If all of the\nimages are in perfect registration and identically exposed, this is an easy problem, i.e., any\npixel or combination will do. However, for real images, visible seams (due to exposure\ndifferences), blurring (due to mis-registration), or ghosting (due to moving objects) can occur.\nCreating clean, pleasing-looking panoramas involves both deciding which pixels to use\nand how to weight or blend them. The distinction between these two stages is a little ﬂuid,\nsince per-pixel weighting can be thought of as a combination of selection and blending. In\nthis section, we discuss spatially varying weighting, pixel selection (seam placement), and\nthen more sophisticated blending.\nFeathering and center-weighting.\nThe simplest way to create a ﬁnal composite is to sim-\nply take an average value at each pixel,\nC(x) =\nX\nk\nwk(x)˜Ik(x)\n,X\nk\nwk(x) ,\n(9.37)\nwhere ˜Ik(x) are the warped (re-sampled) images and wk(x) is 1 at valid pixels and 0 else-\nwhere. On computer graphics hardware, this kind of summation can be performed in an\naccumulation buffer (using the A channel as the weight).\nSimple averaging usually does not work very well, since exposure differences, mis-\nregistrations, and scene movement are all very visible (Figure 9.14a). If rapidly moving\nobjects are the only problem, taking a median ﬁlter (which is a kind of pixel selection opera-\ntor) can often be used to remove them (Figure 9.14b) (Irani and Anandan 1998). Conversely,\ncenter-weighting (discussed below) and minimum likelihood selection (Agarwala, Dontcheva,",
  "476": "454\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 9.14\nFinal composites computed by a variety of algorithms (Szeliski 2006a): (a)\naverage, (b) median, (c) feathered average, (d) p-norm p = 10, (e) Voronoi, (f) weighted\nROD vertex cover with feathering, (g) graph cut seams with Poisson blending and (h) with\npyramid blending.",
  "477": "9.3 Compositing\n455\nAgrawala et al. 2004) can sometimes be used to retain multiple copies of a moving object\n(Figure 9.17).\nA better approach to averaging is to weight pixels near the center of the image more\nheavily and to down-weight pixels near the edges. When an image has some cutout regions,\ndown-weighting pixels near the edges of both cutouts and the image is preferable. This can\nbe done by computing a distance map or grassﬁre transform,\nwk(x) = arg min\ny {∥y∥| ˜Ik(x + y) is invalid },\n(9.38)\nwhere each valid pixel is tagged with its Euclidean distance to the nearest invalid pixel (Sec-\ntion 3.3.3). The Euclidean distance map can be efﬁciently computed using a two-pass raster\nalgorithm (Danielsson 1980; Borgefors 1986).\nWeighted averaging with a distance map is often called feathering (Szeliski and Shum\n1997; Chen and Klette 1999; Uyttendaele, Eden, and Szeliski 2001) and does a reasonable job\nof blending over exposure differences. However, blurring and ghosting can still be problems\n(Figure 9.14c). Note that weighted averaging is not the same as compositing the individual\nimages with the classic over operation (Porter and Duff 1984; Blinn 1994a), even when using\nthe weight values (normalized to sum up to one) as alpha (translucency) channels. This is\nbecause the over operation attenuates the values from more distant surfaces and, hence, is not\nequivalent to a direct sum.\nOne way to improve feathering is to raise the distance map values to some large power,\ni.e., to use wp\nk(x) in Equation (9.37). The weighted averages then become dominated by\nthe larger values, i.e., they act somewhat like a p-norm. The resulting composite can often\nprovide a reasonable tradeoff between visible exposure differences and blur (Figure 9.14d).\nIn the limit as p →∞, only the pixel with the maximum weight is selected,\nC(x) = ˜Il(x)(x),\n(9.39)\nwhere\nl = arg max\nk\nwk(x)\n(9.40)\nis the label assignment or pixel selection function that selects which image to use at each\npixel. This hard pixel selection process produces a visibility mask-sensitive variant of the fa-\nmiliar Voronoi diagram, which assigns each pixel to the nearest image center in the set (Wood,\nFinkelstein, Hughes et al. 1997; Peleg, Rousso, Rav-Acha et al. 2000). The resulting com-\nposite, while useful for artistic guidance and in high-overlap panoramas (manifold mosaics)\ntends to have very hard edges with noticeable seams when the exposures vary (Figure 9.14e).\nXiong and Turkowski (1998) use this Voronoi idea (local maximum of the grassﬁre trans-\nform) to select seams for Laplacian pyramid blending (which is discussed below). However,",
  "478": "456\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 9.15\nComputation of regions of difference (RODs) (Uyttendaele, Eden, and Szeliski\n2001) c⃝2001 IEEE: (a) three overlapping images with a moving face; (b) corresponding\nRODs; (c) graph of coincident RODs.\nsince the seam selection is performed sequentially as new images are added in, some artifacts\ncan occur.\nOptimal seam selection.\nComputing the Voronoi diagram is one way to select the seams\nbetween regions where different images contribute to the ﬁnal composite. However, Voronoi\nimages totally ignore the local image structure underlying the seam.\nA better approach is to place the seams in regions where the images agree, so that tran-\nsitions from one source to another are not visible. In this way, the algorithm avoids “cutting\nthrough” moving objects where a seam would look unnatural (Davis 1998). For a pair of\nimages, this process can be formulated as a simple dynamic program starting from one edge\nof the overlap region and ending at the other (Milgram 1975, 1977; Davis 1998; Efros and\nFreeman 2001).\nWhen multiple images are being composited, the dynamic program idea does not readily\ngeneralize. (For square texture tiles being composited sequentially, Efros and Freeman (2001)\nrun a dynamic program along each of the four tile sides.)\nTo overcome this problem, Uyttendaele, Eden, and Szeliski (2001) observed that, for\nwell-registered images, moving objects produce the most visible artifacts, namely translu-\ncent looking ghosts. Their system therefore decides which objects to keep and which ones\nto erase. First, the algorithm compares all overlapping input image pairs to determine re-\ngions of difference (RODs) where the images disagree. Next, a graph is constructed with the\nRODs as vertices and edges representing ROD pairs that overlap in the ﬁnal composite (Fig-\nure 9.15). Since the presence of an edge indicates an area of disagreement, vertices (regions)\nmust be removed from the ﬁnal composite until no edge spans a pair of remaining vertices.\nThe smallest such set can be computed using a vertex cover algorithm. Since several such\ncovers may exist, a weighted vertex cover is used instead, where the vertex weights are com-\nputed by summing the feather weights in the ROD (Uyttendaele, Eden, and Szeliski 2001).\nThe algorithm therefore prefers removing regions that are near the edge of the image, which\nreduces the likelihood that partially visible objects will appear in the ﬁnal composite. (It is",
  "479": "9.3 Compositing\n457\nFigure 9.16\nPhotomontage (Agarwala, Dontcheva, Agrawala et al. 2004) c⃝2004 ACM.\nFrom a set of ﬁve source images (of which four are shown on the left), Photomontage quickly\ncreates a composite family portrait in which everyone is smiling and looking at the camera\n(right). Users simply ﬂip through the stack and coarsely draw strokes using the designated\nsource image objective over the people they wish to add to the composite. The user-applied\nstrokes and computed regions (middle) are color-coded by the borders of the source images\non the left.\nalso possible to infer which object in a region of difference is the foreground object by the\n“edginess” (pixel differences) across the ROD boundary, which should be higher when an\nobject is present (Herley 2005).) Once the desired excess regions of difference have been\nremoved, the ﬁnal composite can be created by feathering (Figure 9.14f).\nA different approach to pixel selection and seam placement is described by Agarwala,\nDontcheva, Agrawala et al. (2004). Their system computes the label assignment that opti-\nmizes the sum of two objective functions. The ﬁrst is a per-pixel image objective that deter-\nmines which pixels are likely to produce good composites,\nCD =\nX\nx\nD(x, l(x)),\n(9.41)\nwhere D(x, l) is the data penalty associated with choosing image l at pixel x. In their system,\nusers can select which pixels to use by “painting” over an image with the desired object or\nappearance, which sets D(x, l) to a large value for all labels l other than the one selected\nby the user (Figure 9.16). Alternatively, automated selection criteria can be used, such as\nmaximum likelihood, which prefers pixels that occur repeatedly in the background (for object\nremoval), or minimum likelihood for objects that occur infrequently, i.e., for moving object\nretention. Using a more traditional center-weighted data term tends to favor objects that are\ncentered in the input images (Figure 9.17).\nThe second term is a seam objective that penalizes differences in labelings between adja-\ncent images,\nCS =\nX\n(x,y)∈N\nS(x, y, l(x), l(y)),\n(9.42)",
  "480": "458\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 9.17\nSet of ﬁve photos tracking a snowboarder’s jump stitched together into a seam-\nless composite. Because the algorithm prefers pixels near the center of the image, multiple\ncopies of the boarder are retained.\nwhere S(x, y, lx, ly) is the image-dependent interaction penalty or seam cost of placing a\nseam between pixels x and y, and N is the set of N4 neighboring pixels. For example,\nthe simple color-based seam penalty used in (Kwatra, Sch¨odl, Essa et al. 2003; Agarwala,\nDontcheva, Agrawala et al. 2004) can be written as\nS(x, y, lx, ly) = ∥˜Ilx(x) −˜Ily(x)∥+ ∥˜Ilx(y) −˜Ily(y)∥.\n(9.43)\nMore sophisticated seam penalties can also look at image gradients or the presence of image\nedges (Agarwala, Dontcheva, Agrawala et al. 2004). Seam penalties are widely used in other\ncomputer vision applications such as stereo matching (Boykov, Veksler, and Zabih 2001) to\ngive the labeling function its coherence or smoothness. An alternative approach, which places\nseams along strong consistent edges in overlapping images using a watershed computation is\ndescribed by Soille (2006).\nThe sum of these two objective functions gives rise to a Markov random ﬁeld (MRF),\nfor which good optimization algorithms are described in Sections 3.7.2 and 5.5 and Ap-\npendix B.5. For label computations of this kind, the α-expansion algorithm developed by\nBoykov, Veksler, and Zabih (2001) works particularly well (Szeliski, Zabih, Scharstein et al.\n2008).\nFor the result shown in Figure 9.14g, Agarwala, Dontcheva, Agrawala et al. (2004) use\na large data penalty for invalid pixels and 0 for valid pixels. Notice how the seam placement\nalgorithm avoids regions of difference, including those that border the image and that might\nresult in objects being cut off. Graph cuts (Agarwala, Dontcheva, Agrawala et al. 2004) and",
  "481": "9.3 Compositing\n459\nvertex cover (Uyttendaele, Eden, and Szeliski 2001) often produce similar looking results,\nalthough the former is signiﬁcantly slower since it optimizes over all pixels, while the latter\nis more sensitive to the thresholds used to determine regions of difference.\n9.3.3 Application: Photomontage\nWhile image stitching is normally used to composite partially overlapping photographs, it\ncan also be used to composite repeated shots of a scene taken with the aim of obtaining the\nbest possible composition and appearance of each element.\nFigure 9.16 shows the Photomontage system developed by Agarwala, Dontcheva, Agrawala\net al. (2004), where users draw strokes over a set of pre-aligned images to indicate which re-\ngions they wish to keep from each image. Once the system solves the resulting multi-label\ngraph cut (9.41–9.42), the various pieces taken from each source photo are blended together\nusing a variant of Poisson image blending (9.44–9.46). Their system can also be used to au-\ntomatically composite an all-focus image from a series of bracketed focus images (Hasinoff,\nKutulakos, Durand et al. 2009) or to remove wires and other unwanted elements from sets of\nphotographs. Exercise 9.10 has you implement this system and try out some of its variants.\n9.3.4 Blending\nOnce the seams between images have been determined and unwanted objects removed, we\nstill need to blend the images to compensate for exposure differences and other mis-alignments.\nThe spatially varying weighting (feathering) previously discussed can often be used to accom-\nplish this. However, it is difﬁcult in practice to achieve a pleasing balance between smoothing\nout low-frequency exposure variations and retaining sharp enough transitions to prevent blur-\nring (although using a high exponent in feathering can help).\nLaplacian pyramid blending.\nAn attractive solution to this problem is the Laplacian pyra-\nmid blending technique developed by Burt and Adelson (1983b), which we discussed in Sec-\ntion 3.5.5. Instead of using a single transition width, a frequency-adaptive width is used by\ncreating a band-pass (Laplacian) pyramid and making the transition widths within each level\na function of the level, i.e., the same width in pixels. In practice, a small number of levels,\ni.e., as few as two (Brown and Lowe 2007), may be adequate to compensate for differences\nin exposure. The result of applying this pyramid blending is shown in Figure 9.14h.\nGradient domain blending.\nAn alternative approach to multi-band image blending is to\nperform the operations in the gradient domain. Reconstructing images from their gradient\nﬁelds has a long history in computer vision (Horn 1986), starting originally with work in",
  "482": "460\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 9.18\nPoisson image editing (P´erez, Gangnet, and Blake 2003) c⃝2003 ACM: (a)\nThe dog and the two children are chosen as source images to be pasted into the destination\nswimming pool. (b) Simple pasting fails to match the colors at the boundaries, whereas (c)\nPoisson image blending masks these differences.\nbrightness constancy (Horn 1974), shape from shading (Horn and Brooks 1989), and photo-\nmetric stereo (Woodham 1981). More recently, related ideas have been used for reconstruct-\ning images from their edges (Elder and Goldberg 2001), removing shadows from images\n(Weiss 2001), separating reﬂections from a single image (Levin, Zomet, and Weiss 2004;\nLevin and Weiss 2007), and tone mapping high dynamic range images by reducing the mag-\nnitude of image edges (gradients) (Fattal, Lischinski, and Werman 2002).\nP´erez, Gangnet, and Blake (2003) show how gradient domain reconstruction can be used\nto do seamless object insertion in image editing applications (Figure 9.18). Rather than copy-\ning pixels, the gradients of the new image fragment are copied instead. The actual pixel values\nfor the copied area are then computed by solving a Poisson equation that locally matches the\ngradients while obeying the ﬁxed Dirichlet (exact matching) conditions at the seam bound-\nary. P´erez, Gangnet, and Blake (2003) show that this is equivalent to computing an additive\nmembrane interpolant of the mismatch between the source and destination images along the\nboundary.14 In earlier work, Peleg (1981) also proposed adding a smooth function to enforce\nconsistency along the seam curve.\nAgarwala, Dontcheva, Agrawala et al. (2004) extended this idea to a multi-source formu-\nlation, where it no longer makes sense to talk of a destination image whose exact pixel values\nmust be matched at the seam. Instead, each source image contributes its own gradient ﬁeld\nand the Poisson equation is solved using Neumann boundary conditions, i.e., dropping any\nequations that involve pixels outside the boundary of the image.\n14 The membrane interpolant is known to have nicer interpolation properties for arbitrary-shaped constraints than\nfrequency-domain interpolants (Nielson 1993).",
  "483": "9.3 Compositing\n461\nRather than solving the Poisson partial differential equations, Agarwala, Dontcheva, Agrawala\net al. (2004) directly minimize a variational problem,\nmin\nC(x) ∥∇C(x) −∇˜Il(x)(x)∥2.\n(9.44)\nThe discretized form of this equation is a set of gradient constraint equations\nC(x + ˆı) −C(x)\n=\n˜Il(x)(x + ˆı) −˜Il(x)(x) and\n(9.45)\nC(x + ˆ) −C(x)\n=\n˜Il(x)(x + ˆ) −˜Il(x)(x),\n(9.46)\nwhere ˆı = (1, 0) and ˆ= (0, 1) are unit vectors in the x and y directions.15 They then solve\nthe associated sparse least squares problem. Since this system of equations is only deﬁned\nup to an additive constraint, Agarwala, Dontcheva, Agrawala et al. (2004) ask the user to\nselect the value of one pixel. In practice, a better choice might be to weakly bias the solution\ntowards reproducing the original color values.\nIn order to accelerate the solution of this sparse linear system, Fattal, Lischinski, and\nWerman (2002) use multigrid, whereas Agarwala, Dontcheva, Agrawala et al. (2004) use\nhierarchical basis preconditioned conjugate gradient descent (Szeliski 1990b, 2006b) (Ap-\npendix A.5). In subsequent work, Agarwala (2007) shows how using a quadtree represen-\ntation for the solution can further accelerate the computation with minimal loss in accuracy,\nwhile Szeliski, Uyttendaele, and Steedly (2008) show how representing the per-image offset\nﬁelds using even coarser splines is even faster. This latter work also argues that blending\nin the log domain, i.e., using multiplicative rather than additive offsets, is preferable, as it\nmore closely matches texture contrasts across seam boundaries. The resulting seam blending\nworks very well in practice (Figure 9.14h), although care must be taken when copying large\ngradient values near seams so that a “double edge” is not introduced.\nCopying gradients directly from the source images after seam placement is just one ap-\nproach to gradient domain blending. The paper by Levin, Zomet, Peleg et al. (2004) examines\nseveral different variants of this approach, which they call Gradient-domain Image STitching\n(GIST). The techniques they examine include feathering (blending) the gradients from the\nsource images, as well as using an L1 norm in performing the reconstruction of the image\nfrom the gradient ﬁeld, rather than using an L2 norm as in Equation (9.44). Their preferred\ntechnique is the L1 optimization of a feathered (blended) cost function on the original image\ngradients (which they call GIST1-l1). Since L1 optimization using linear programming can\nbe slow, they develop a faster iterative median-based algorithm in a multigrid framework.\nVisual comparisons between their preferred approach and what they call optimal seam on\nthe gradients (which is equivalent to the approach of Agarwala, Dontcheva, Agrawala et al.\n(2004)) show similar results, while signiﬁcantly improving on pyramid blending and feather-\ning algorithms.\n15 At seam locations, the right hand side is replaced by the average of the gradients in the two source images.",
  "484": "462\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nExposure compensation.\nPyramid and gradient domain blending can do a good job of\ncompensating for moderate amounts of exposure differences between images.\nHowever,\nwhen the exposure differences become large, alternative approaches may be necessary.\nUyttendaele, Eden, and Szeliski (2001) iteratively estimate a local correction between\neach source image and a blended composite. First, a block-based quadratic transfer function is\nﬁt between each source image and an initial feathered composite. Next, transfer functions are\naveraged with their neighbors to get a smoother mapping and per-pixel transfer functions are\ncomputed by splining (interpolating) between neighboring block values. Once each source\nimage has been smoothly adjusted, a new feathered composite is computed and the process is\nrepeated (typically three times). The results shown by Uyttendaele, Eden, and Szeliski (2001)\ndemonstrate that this does a better job of exposure compensation than simple feathering and\ncan handle local variations in exposure due to effects such as lens vignetting.\nUltimately, however, the most principled way to deal with exposure differences is to stitch\nimages in the radiance domain, i.e., to convert each image into a radiance image using its\nexposure value and then create a stitched, high dynamic range image, as discussed in Sec-\ntion 10.2 (Eden, Uyttendaele, and Szeliski 2006).\n9.4 Additional reading\nThe literature on image stitching dates back to work in the photogrammetry community in\nthe 1970s (Milgram 1975, 1977; Slama 1980). In computer vision, papers started appearing\nin the early 1980s (Peleg 1981), while the development of fully automated techniques came\nabout a decade later (Mann and Picard 1994; Chen 1995; Szeliski 1996; Szeliski and Shum\n1997; Sawhney and Kumar 1999; Shum and Szeliski 2000). Those techniques used direct\npixel-based alignment but feature-based approaches are now the norm (Zoghlami, Faugeras,\nand Deriche 1997; Capel and Zisserman 1998; Cham and Cipolla 1998; Badra, Qumsieh, and\nDudek 1998; McLauchlan and Jaenicke 2002; Brown and Lowe 2007). A collection of some\nof these papers can be found in the book by Benosman and Kang (2001). Szeliski (2006a)\nprovides a comprehensive survey of image stitching, on which the material in this chapter is\nbased.\nHigh-quality techniques for optimal seam ﬁnding and blending are another important\ncomponent of image stitching systems. Important developments in this ﬁeld include work by\nMilgram (1977), Burt and Adelson (1983b), Davis (1998), Uyttendaele, Eden, and Szeliski\n(2001),P´erez, Gangnet, and Blake (2003), Levin, Zomet, Peleg et al. (2004), Agarwala,\nDontcheva, Agrawala et al. (2004), Eden, Uyttendaele, and Szeliski (2006), and Kopf, Uyt-\ntendaele, Deussen et al. (2007).\nIn addition to the merging of multiple overlapping photographs taken for aerial or ter-",
  "485": "9.5 Exercises\n463\nrestrial panoramic image creation, stitching techniques can be used for automated white-\nboard scanning (He and Zhang 2005; Zhang and He 2007), scanning with a mouse (Nakao,\nKashitani, and Kaneyoshi 1998), and retinal image mosaics (Can, Stewart, Roysam et al.\n2002). They can also be applied to video sequences (Teodosio and Bender 1993; Irani, Hsu,\nand Anandan 1995; Kumar, Anandan, Irani et al. 1995; Sawhney and Ayer 1996; Massey\nand Bender 1996; Irani and Anandan 1998; Sawhney, Arpa, Kumar et al. 2002; Agarwala,\nZheng, Pal et al. 2005; Rav-Acha, Pritch, Lischinski et al. 2005; Steedly, Pal, and Szeliski\n2005; Baudisch, Tan, Steedly et al. 2006) and can even be used for video compression (Lee,\nge Chen, lung Bruce Lin et al. 1997).\n9.5 Exercises\nEx 9.1: Direct pixel-based alignment\nTake a pair of images, compute a coarse-to-ﬁne afﬁne\nalignment (Exercise 8.2) and then blend them using either averaging (Exercise 6.2) or a Lapla-\ncian pyramid (Exercise 3.20). Extend your motion model from afﬁne to perspective (homog-\nraphy) to better deal with rotational mosaics and planar surfaces seen under arbitrary motion.\nEx 9.2: Featured-based stitching\nExtend your feature-based alignment technique from Ex-\nercise 6.2 to use a full perspective model and then blend the resulting mosaic using either\naveraging or more sophisticated distance-based feathering (Exercise 9.9).\nEx 9.3: Cylindrical strip panoramas\nTo generate cylindrical or spherical panoramas from\na horizontally panning (rotating) camera, it is best to use a tripod. Set your camera up to take\na series of 50% overlapped photos and then use the following steps to create your panorama:\n1. Estimate the amount of radial distortion by taking some pictures with lots of long\nstraight lines near the edges of the image and then using the plumb-line method from\nExercise 6.10.\n2. Compute the focal length either by using a ruler and paper, as in Figure 6.7 (Debevec,\nWenger, Tchou et al. 2002) or by rotating your camera on the tripod, overlapping the\nimages by exactly 0% and counting the number of images it takes to make a 360◦\npanorama.\n3. Convert each of your images to cylindrical coordinates using (9.12–9.16).\n4. Line up the images with a translational motion model using either a direct pixel-based\ntechnique, such as coarse-to-ﬁne incremental or an FFT, or a feature-based technique.\n5. (Optional) If doing a complete 360◦panorama, align the ﬁrst and last images. Compute\nthe amount of accumulated vertical mis-registration and re-distribute this among the\nimages.",
  "486": "464\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n6. Blend the resulting images using feathering or some other technique.\nEx 9.4: Coarse alignment\nUse FFT or phase correlation (Section 8.1.2) to estimate the\ninitial alignment between successive images. How well does this work? Over what range of\noverlaps? If it does not work, does aligning sub-sections (e.g., quarters) do better?\nEx 9.5: Automated mosaicing\nUse feature-based alignment with four-point RANSAC for\nhomographies (Section 6.1.3, Equations (6.19–6.23)) or three-point RANSAC for rotational\nmotions (Brown, Hartley, and Nist´er 2007) to match up all pairs of overlapping images.\nMerge these pairwise estimates together by ﬁnding a spanning tree of pairwise relations.\nVisualize the resulting global alignment, e.g., by displaying a blend of each image with all\nother images that overlap it.\nFor greater robustness, try multiple spanning trees (perhaps randomly sampled based on\nthe conﬁdence in pairwise alignments) to see if you can recover from bad pairwise matches\n(Zach, Klopschitz, and Pollefeys 2010). As a measure of ﬁtness, count how many pairwise\nestimates are consistent with the global alignment.\nEx 9.6: Global optimization\nUse the initialization from the previous algorithm to perform\na full bundle adjustment over all of the camera rotations and focal lengths, as described in\nSection 7.4 and by Shum and Szeliski (2000). Optionally, estimate radial distortion parame-\nters as well or support ﬁsheye lenses (Section 2.1.6).\nAs in the previous exercise, visualize the quality of your registration by creating compos-\nites of each input image with its neighbors, optionally blinking between the original image\nand the composite to better see mis-alignment artifacts.\nEx 9.7: De-ghosting\nUse the results of the previous bundle adjustment to predict the loca-\ntion of each feature in a consensus geometry. Use the difference between the predicted and\nactual feature locations to correct for small mis-registrations, as described in Section 9.2.2\n(Shum and Szeliski 2000).\nEx 9.8: Compositing surface\nChoose a compositing surface (Section 9.3.1), e.g., a single\nreference image extended to a larger plane, a sphere represented using cylindrical or spherical\ncoordinates, a stereographic “little planet” projection, or a cube map.\nProject all of your images onto this surface and blend them with equal weighting, for now\n(just to see where the original image seams are).\nEx 9.9: Feathering and blending\nCompute a feather (distance) map for each warped source\nimage and use these maps to blend the warped images.\nAlternatively, use Laplacian pyramid blending (Exercise 3.20) or gradient domain blend-\ning.",
  "487": "9.5 Exercises\n465\nEx 9.10: Photomontage and object removal\nImplement a “PhotoMontage” system in which\nusers can indicate desired or unwanted regions in pre-registered images using strokes or other\nprimitives (such as bounding boxes).\n(Optional) Devise an automatic moving objects remover (or “keeper”) by analyzing which\ninconsistent regions are more or less typical given some consensus (e.g., median ﬁltering) of\nthe aligned images. Figure 9.17 shows an example where the moving object was kept. Try\nto make this work for sequences with large amounts of overlaps and consider averaging the\nimages to make the moving object look more ghosted.",
  "488": "466\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "489": "Chapter 10\nComputational photography\n10.1 Photometric calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\n10.1.1 Radiometric response function . . . . . . . . . . . . . . . . . . . . . 470\n10.1.2 Noise level estimation\n. . . . . . . . . . . . . . . . . . . . . . . . . 473\n10.1.3 Vignetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474\n10.1.4 Optical blur (spatial response) estimation . . . . . . . . . . . . . . . 476\n10.2 High dynamic range imaging . . . . . . . . . . . . . . . . . . . . . . . . . . 479\n10.2.1 Tone mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487\n10.2.2 Application: Flash photography . . . . . . . . . . . . . . . . . . . . 494\n10.3 Super-resolution and blur removal . . . . . . . . . . . . . . . . . . . . . . . 497\n10.3.1 Color image demosaicing\n. . . . . . . . . . . . . . . . . . . . . . . 502\n10.3.2 Application: Colorization\n. . . . . . . . . . . . . . . . . . . . . . . 504\n10.4 Image matting and compositing . . . . . . . . . . . . . . . . . . . . . . . . . 505\n10.4.1 Blue screen matting . . . . . . . . . . . . . . . . . . . . . . . . . . . 507\n10.4.2 Natural image matting . . . . . . . . . . . . . . . . . . . . . . . . . 509\n10.4.3 Optimization-based matting\n. . . . . . . . . . . . . . . . . . . . . . 513\n10.4.4 Smoke, shadow, and ﬂash matting . . . . . . . . . . . . . . . . . . . 516\n10.4.5 Video matting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518\n10.5 Texture analysis and synthesis\n. . . . . . . . . . . . . . . . . . . . . . . . . 518\n10.5.1 Application: Hole ﬁlling and inpainting . . . . . . . . . . . . . . . . 521\n10.5.2 Application: Non-photorealistic rendering . . . . . . . . . . . . . . . 522\n10.6 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n10.7 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526",
  "490": "468\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 10.1\nComputational photography: (a) merging multiple exposures to create high\ndynamic range images (Debevec and Malik 1997) c⃝1997 ACM; (b) merging ﬂash and non-\nﬂash photographs; (Petschnigg, Agrawala, Hoppe et al. 2004) c⃝2004 ACM; (c) image mat-\nting and compositing; (Chuang, Curless, Salesin et al. 2001) c⃝2001 IEEE; (d) hole ﬁlling\nwith inpainting (Criminisi, P´erez, and Toyama 2004) c⃝2004 IEEE.",
  "491": "10 Computational photography\n469\nStitching multiple images into wide ﬁeld of view panoramas, which we covered in Chapter 9,\nallows us create photographs that could not be captured with a regular camera. This is just\none instance of computational photography, where image analysis and processing algorithms\nare applied to one or more photographs to create images that go beyond the capabilities of\ntraditional imaging systems. Some of these techniques are now being incorporated directly\ninto digital still cameras. For example, some of the newer digital still cameras have sweep\npanorama modes and take multiple shots in low-light conditions to reduce image noise.\nIn this chapter, we cover a number of additional computational photography algorithms.\nWe begin with a review of photometric image calibration (Section 10.1), i.e., the measurement\nof camera and lens responses, which is a prerequisite for many of the algorithms we describe\nlater. We then discuss high dynamic range imaging (Section 10.2), which captures the full\nrange of brightness in a scene through the use of multiple exposures (Figure 10.1a). We also\ndiscuss tone mapping operators, which map rich images back into regular display devices,\nsuch as screens and printers, as well as algorithms that merge ﬂash and regular images to\nobtain better exposures (Figure 10.1b).\nNext, we discuss how the resolution of images can be improved either by merging mul-\ntiple photographs together or using sophisticated image priors (Section 10.3). This includes\nalgorithms for extracting full-color images from the patterned Bayer mosaics present in most\ncameras.\nIn Section 10.4, we discuss algorithms for cutting pieces of images from one photograph\nand pasting them into others (Figure 10.1c). In Section 10.5, we describe how to generate\nnovel textures from real-world samples for applications such as ﬁlling holes in images (Fig-\nure 10.1d). We close with a brief overview of non-photorealistic rendering (Section 10.5.2),\nwhich can turn regular photographs into artistic renderings that resemble traditional drawings\nand paintings.\nOne topic that we do not cover extensively in this book is novel computational sensors,\noptics, and cameras. A nice survey can be found in an article by Nayar (2006), a recently\npublished book by Raskar and Tumblin (2010), and more recent research papers (Levin,\nFergus, Durand et al. 2007). Some related discussion can also be found in Sections 10.2\nand 13.3.\nA good general-audience introduction to computational photography can be found in the\narticle by Hayes (2008) as well as survey papers by Nayar (2006), Cohen and Szeliski (2006),\nLevoy (2006), and Debevec (2006).1 Raskar and Tumblin (2010) give extensive coverage of\ntopics in this area, with particular emphasis on computational cameras and sensors. The\nsub-ﬁeld of high dynamic range imaging has its own book discussing research in this area\n(Reinhard, Ward, Pattanaik et al. 2005), as well as a wonderful book aimed more at profes-\n1 See also the two special issue journals edited by Bimber (2006) and Durand and Szeliski (2007).",
  "492": "470\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nsional photographers (Freeman 2008).2 A good survey of image matting is provided by Wang\nand Cohen (2007a).\nThere are also several courses on computational photography where the instructors have\nprovided extensive on-line materials, e.g., Fr´edo Durand’s Computation Photography course\nat MIT,3 Alyosha Efros’ class at Carnegie Mellon,4 Marc Levoy’s class at Stanford,5 and a\nseries of SIGGRAPH courses on Computational Photography.6\n10.1 Photometric calibration\nBefore we can successfully merge multiple photographs, we need to characterize the func-\ntions that map incoming irradiance into pixel values and also the amounts of noise present\nin each image. In this section, we examine three components of the imaging pipeline (Fig-\nure 10.2) that affect this mapping.\nThe ﬁrst is the radiometric response function (Mitsunaga and Nayar 1999), which maps\nphotons arriving at the lens into digital values stored in the image ﬁle (Section 10.1.1). The\nsecond is vignetting, which darkens pixel values near the periphery of images, especially at\nlarge apertures (Section 10.1.3). The third is the point spread function, which characterizes\nthe blur induced by the lens, anti-aliasing ﬁlters, and ﬁnite sensor areas (Section 10.1.4).7 The\nmaterial in this section builds on the image formation processes described in Sections 2.2.3\nand 2.3.3, so if it has been a while since you looked at those sections, please go back and\nreview them.\n10.1.1 Radiometric response function\nAs we can see in Figure 10.2, a number of factors affect how the intensity of light arriving\nat the lens ends up being mapped into stored digital values. Let us ignore for now any non-\nuniform attenuation that may occur inside the lens, which we cover in Section 10.1.3.\nThe ﬁrst factors to affect this mapping are the aperture and shutter speed (Section 2.3),\nwhich can be modeled as global multipliers on the incoming light, most conveniently mea-\nsured in exposure values (log2 brightness ratios). Next, the analog to digital (A/D) converter\non the sensing chip applies an electronic gain, usually controlled by the ISO setting on your\ncamera. While in theory this gain is linear, as with any electronics non-linearities may be\n2 Gulbins and Gulbins (2009) discuss related photographic techniques.\n3 MIT 6.815/6.865, http://stellar.mit.edu/S/course/6/sp08/6.815/materials.html.\n4 CMU 15-463, http://graphics.cs.cmu.edu/courses/15-463/.\n5 Stanford CS 448A, http://graphics.stanford.edu/courses/cs448a-10/.\n6 http://web.media.mit.edu/∼raskar/photo/.\n7 Additional photometric camera and lens effects include sensor glare, blooming, and chromatic aberration, which\ncan also be thought of as a spectrally varying form of geometric aberration (Section 2.2.3).",
  "493": "10.1 Photometric calibration\n471\nScene\nRadiance\nDSP\nRAW\nSensor chip\nCamera Body\nOptics\nAperture\nSensor\n(CCD/CMOS)\nADC\nDemosaic\n(Sharpen)\nWhite \nBalance\nGamma/curve\nCompress\nShutter\nGain\n(ISO)\nJPEG\n(a)\nADC\nRAW\nDSP\nSensor chip\nCamera Body\nScene \nRadiance\nOptics\nAperture\nSensor\n(CCD/CMOS)\nDemosaic\n(Sharpen)\nWhite \nBalance\nGamma/curve\nCompress\nShutter\nGain\n(ISO)\nJPEG\nBlur kern. & RD\nF-stop & Vignette\nExposure T\nAA CFA\nNoise\nISO Gain\nQ1\n?\n?\nRGB Gain\nQ2\nGamma & S-curve\n(b)\nFigure 10.2 Image sensing pipeline: (a) block diagram showing the various sources of noise\nas well as the typical digital post-processing steps; (b) equivalent signal transforms, including\nconvolution, gain, and noise injection. The abbreviations are: RD = radial distortion, AA =\nanti-aliasing ﬁlter, CFA = color ﬁlter array, Q1 and Q2 = quantization noise.",
  "494": "472\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 10.3 Radiometric response calibration: (a) typical camera response function, show-\ning the mapping between incoming log irradiance (exposure) and output eight-bit pixel val-\nues, for one color channel (Debevec and Malik 1997) c⃝1997 ACM; (b) color checker chart.\npresent (either unintentionally or by design). Ignoring, for now, photon noise, on-chip noise,\nampliﬁer noise, and quantization noise, which we discuss shortly, you can often assume that\nthe mapping between incoming light and the values stored in a RAW camera ﬁle (if your\ncamera supports this) is roughly linear.\nIf images are being stored in the more common JPEG format, the camera’s digital signal\nprocessor (DSP) next performs Bayer pattern demosaicing (Sections 2.3.2 and 10.3.1), which\nis a mostly linear (but often non-stationary) process. Some sharpening is also often applied at\nthis stage. Next, the color values are multiplied by different constants (or sometimes a 3 × 3\ncolor twist matrix) to perform color balancing, i.e., to move the white point closer to pure\nwhite. Finally, a standard gamma is applied to the intensities in each color channel and the\ncolors are converted into YCbCr format before being transformed by a DCT, quantized, and\nthen compressed into the JPEG format (Section 2.3.3). Figure 10.2 shows all of these steps\nin pictorial form.\nGiven the complexity of all of this processing, it is difﬁcult to model the camera response\nfunction (Figure 10.3a), i.e., the mapping between incoming irradiance and digital RGB val-\nues, from ﬁrst principles. A more practical approach is to calibrate the camera by measuring\ncorrespondences between incoming light and ﬁnal values.\nThe most accurate, but most expensive, approach is to use an integrating sphere, which is\na large (typically 1m diameter) sphere carefully painted on the inside with white matte paint.\nAn accurately calibrated light at the top controls the amount of radiance inside the sphere\n(which is constant everywhere because of the sphere’s radiometry) and a small opening at the\nside allows for a camera/lens combination to be mounted. By slowly varying the current going\ninto the light, an accurate correspondence can be established between incoming radiance and",
  "495": "10.1 Photometric calibration\n473\nmeasured pixel values. The vignetting and noise characteristics of the camera can also be\nsimultaneously determined.\nA more practical alternative is to use a calibration chart (Figure 10.3b) such as the Mac-\nbeth or Munsell ColorChecker Chart.8 The biggest problem with this approach is to ensure\nuniform lighting. One approach is to use a large dark room with a high-quality light source\nfar away from (and perpendicular to) the chart. Another is to place the chart outdoors away\nfrom any shadows. (The results will differ under these two conditions, because the color of\nthe illuminant will be different).\nThe easiest approach is probably to take multiple exposures of the same scene while the\ncamera is on a tripod and to recover the response function by simultaneously estimating the\nincoming irradiance at each pixel and the response curve (Mann and Picard 1995; Debevec\nand Malik 1997; Mitsunaga and Nayar 1999). This approach is discussed in more detail in\nSection 10.2 on high dynamic range imaging.\nIf all else fails, i.e., you just have one or more unrelated photos, you can use an Interna-\ntional Color Consortium (ICC) proﬁle for the camera (Fairchild 2005).9 Even more simply,\nyou can just assume that the response is linear if they are RAW ﬁles and that the images have\na γ = 2.2 non-linearity (plus clipping) applied to each RGB channel if they are JPEG images.\n10.1.2 Noise level estimation\nIn addition to knowing the camera response function, it is also often important to know the\namount of noise being injected under a particular camera setting (e.g., ISO/gain level). The\nsimplest characterization of noise is a single standard deviation, usually measured in gray\nlevels, independent of pixel value. A more accurate model can be obtained by estimating\nthe noise level as a function of pixel value (Figure 10.4), which is known as the noise level\nfunction (Liu, Szeliski, Kang et al. 2008).\nAs with the camera response function, the simplest way to estimate these quantities is in\nthe lab, using either an integrating sphere or a calibration chart. The noise can be estimated\neither at each pixel independently, by taking repeated exposures and computing the temporal\nvariance in the measurements (Healey and Kondepudy 1994), or over regions, by assuming\nthat pixel values should all be the same within some region (e.g., inside a color checker\nsquare) and computing a spatial variance.\nThis approach can be generalized to photos where there are regions of constant or slowly\nvarying intensity (Liu, Szeliski, Kang et al. 2008). First, segment the image into such regions\nand ﬁt a constant or linear function inside each region. Next, measure the (spatial) standard\ndeviation of the differences between the noisy input pixels and the smooth ﬁtted function\n8 http://www.xrite.com.\n9 See also the ICC Information on Proﬁles, http://www.color.org/info proﬁles2.xalter.",
  "496": "474\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.4\nNoise level function estimates obtained from a single color photograph (Liu,\nSzeliski, Kang et al. 2008) c⃝2008 IEEE. The colored curves are the estimated NLF ﬁt as the\nprobabilistic lower envelope of the measured deviations between the noisy piecewise-smooth\nimages. The ground truth NLFs obtained by averaging 29 images are shown in gray.\naway from large gradients and region boundaries. Plot these as a function of output level for\neach color channel, as shown in Figure 10.4. Finally, ﬁt a lower envelope to this distribution\nin order to ignore pixels or deviations that are outliers. A fully Bayesian approach to this\nproblem that models the statistical distribution of each quantity is presented by (Liu, Szeliski,\nKang et al. 2008). A simpler approach, which should produce useful results in most cases,\nis to ﬁt a low-dimensional function (e.g., positive valued B-spline) to the lower envelope (see\nExercise 10.2).\nIn more recent work, Matsushita and Lin (2007) present a technique for simultaneously\nestimating a camera’s response and noise level functions based on skew (asymmetries) in\nlevel-dependent noise distributions. Their paper also contains extensive references to previ-\nous work in these areas.\n10.1.3 Vignetting\nA common problem with using wide-angle and wide-aperture lenses is that the image tends\nto darken in the corners (Figure 10.5a). This problem is generally known as vignetting and\ncomes in several different forms, including natural, optical, and mechanical vignetting (Sec-\ntion 2.2.3) (Ray 2002). As with radiometric response function calibration, the most accurate\nway to calibrate vignetting is to use an integrating sphere or a picture of a uniformly colored\nand illuminated blank wall.\nAn alternative approach is to stitch a panoramic scene and to assume that the true radiance\nat each pixel comes from the central portion of each input image. This is easier to do if\nthe radiometric response function is already known (e.g., by shooting in RAW mode) and\nif the exposure is kept constant. If the response function, image exposures, and vignetting\nfunction are unknown, they can still be recovered by optimizing a large least squares ﬁtting",
  "497": "10.1 Photometric calibration\n475\nFigure 10.5\nSingle image vignetting correction (Zheng, Yu, Kang et al. 2008) c⃝2008\nIEEE: (a) original image with strong visible vignetting; (b) vignetting compensation as de-\nscribed by Zheng, Zhou, Georgescu et al. (2006); (c–d) vignetting compensation as described\nby Zheng, Yu, Kang et al. (2008).\n(a)\n(b)\n(c)\n(d)\nFigure 10.6\nSimultaneous estimation of vignetting, exposure, and radiometric response\n(Goldman 2011) c⃝2011 IEEE: (a) original average of the input images; (b) after compen-\nsating for vignetting; (c) using gradient domain blending only (note the remaining mottled\nlook); (d) after both vignetting compensation and blending.\nproblem (Litvinov and Schechner 2005; Goldman 2011). Figure 10.6 shows an example of\nsimultaneously estimating the vignetting, exposure, and radiometric response function from\na set of overlapping photographs (Goldman 2011). Note that unless vignetting is modeled\nand compensated, regular gradient-domain image blending (Section 9.3.4) will not create an\nattractive image.\nIf only a single image is available, vignetting can be estimated by looking for slow con-\nsistent intensity variations in the radial direction. The original algorithm proposed by Zheng,\nLin, and Kang (2006) ﬁrst pre-segmented the image into smoothly varying regions and then\nperformed an analysis inside each region. Instead of pre-segmenting the image, Zheng, Yu,\nKang et al. (2008) compute the radial gradients at all the pixels and use the asymmetry in\nthis distribution (since gradients away from the center are, on average, slightly negative) to",
  "498": "476\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nestimate the vignetting. Figure 10.5 shows the results of applying each of these algorithms\nto an image with a large amount of vignetting. Exercise 10.3 has you implement some of the\nabove techniques.\n10.1.4 Optical blur (spatial response) estimation\nOne ﬁnal characteristic of imaging systems that you should calibrate is the spatial response\nfunction, which encodes the optical blur that gets convolved with the incoming image to pro-\nduce the point-sampled image. The shape of the convolution kernel, which is also known as\npoint spread function (PSF) or optical transfer function, depends on several factors, including\nlens blur and radial distortion (Section 2.2.3), anti-aliasing ﬁlters in front of the sensor, and\nthe shape and extent of each active pixel area (Section 2.3) (Figure 10.2). A good estimate of\nthis function is required for applications such as multi-image super-resolution and de-blurring\n(Section 10.3).\nIn theory, one could estimate the PSF by simply observing an inﬁnitely small point light\nsource everywhere in the image. Creating an array of samples by drilling through a dark plate\nand backlighting with a very bright light source is difﬁcult in practice.\nA more practical approach is to observe an image composed of long straight lines or\nbars, since these can be ﬁtted to arbitrary precision. Because the location of a horizontal\nor vertical edge can be aliased during acquisition, slightly slanted edges are preferred. The\nproﬁle and locations of such edges can be estimated to sub-pixel precision, which makes it\npossible to estimate the PSF at sub-pixel resolutions (Reichenbach, Park, and Narayanswamy\n1991; Burns and Williams 1999; Williams and Burns 2001; Goesele, Fuchs, and Seidel 2003).\nThe thesis by Murphy (2005) contains a nice survey of all aspects of camera calibration,\nincluding the spatial frequency response (SFR), spatial uniformity, tone reproduction, color\nreproduction, noise, dynamic range, color channel registration, and depth of ﬁeld. It also\nincludes a description of a slant-edge calibration algorithm called sfrmat2.\nThe slant-edge technique can be used to recover a 1D projection of the 2D PSF, e.g.,\nslightly vertical edges are used to recover the horizontal line spread function (LSF) (Williams\n1999). The LSF is then often converted into the Fourier domain and its magnitude plotted as a\none-dimensional modulation transfer function (MTF), which indicates which image frequen-\ncies are lost (blurred) and aliased during the acquisition process (Section 2.3.1). For most\ncomputational photography applications, it is preferable to directly estimate the full 2D PSF,\nsince it can be hard to recover from its projections (Williams 1999).\nFigure 10.7 shows a pattern containing edges at all orientations, which can be used to\ndirectly recover a two-dimensional PSF. First, corners in the pattern are located by extracting\nedges in the sensed image, linking them, and ﬁnding the intersections of the circular arcs.\nNext, the ideal pattern, whose analytic form is known, is warped (using a homography) to",
  "499": "10.1 Photometric calibration\n477\nFigure 10.7 Calibration pattern with edges equally distributed at all orientations that can be\nused for PSF and radial distortion estimation (Joshi, Szeliski, and Kriegman 2008) c⃝2008\nIEEE. A portion of an actual sensed image is shown in the middle and a close-up of the ideal\npattern is on the right.\nﬁt the central portion of the input image and its intensities are adjusted to ﬁt the ones in\nthe sensed image. If desired, the pattern can be rendered at a higher resolution than the input\nimage, which enables the estimation of the PSF to sub-pixel resolution (Figure 10.8a). Finally\na large linear least squares system is solved to recover the unknown PSF kernel K,\nK = arg min\nK ∥B −D(I ∗K)∥2,\n(10.1)\nwhere B is the sensed (blurred) image, I is the predicted (sharp) image, and D is an optional\ndownsampling operator that matches the resolution of the ideal and sensed images (Joshi,\nSzeliski, and Kriegman 2008). In terms of the notation (3.75) introduced in Section 3.4.3,\nthis could also be written as\nb = arg min\nb\n∥o −D(s ∗b)∥2,\n(10.2)\nwhere o is the observed image, s is the sharp image, and b is the blur kernel.\nIf the process of estimating the PSF is done locally in overlapping patches of the image,\nit can also be used to estimate the radial distortion and chromatic aberration induced by the\nlens (Figure 10.8b). Because the homography mapping the ideal target to the sensed image\nis estimated in the central (undistorted) part of the image, any (per-channel) shifts induced\nby the optics manifest themselves as a displacement in the PSF centers.10 Compensating\nfor these shifts eliminates both the achromatic radial distortion and the inter-channel shifts\nthat result in visible chromatic aberration. The color-dependent blurring caused by chromatic\naberration (Figure 2.21) can also be removed using the de-blurring techniques discussed in\n10 This process confounds the distinction between geometric and photometric calibration. In principle, any ge-\nometric distortion could be modeled by spatially varying displaced PSFs. In practice, it is easier to fold any large\nshifts into the geometric correction component.",
  "500": "478\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n−2\n−1\n0\n1\n2\n−2\n−1\n0 \n1 \n2 \n(a)\n(b)\n(c)\nFigure 10.8 Point spread function estimation using a calibration target (Joshi, Szeliski, and\nKriegman 2008) c⃝2008 IEEE. (a) Sub-pixel PSFs at successively higher resolutions (note\nthe interaction between the square sensing area and the circular lens blur). (b) The radial\ndistortion and chromatic aberration can also be estimated and removed. (c) PSF for a mis-\nfocused (blurred) lens showing some diffraction and vignetting effects in the corners.\nSection 10.3. Figure 10.8b shows how the radial distortion and chromatic aberration manifest\nthemselves as elongated and displaced PSFs, along with the result of removing these effects\nin a region of the calibration target.\nThe local 2D PSF estimation technique can also be used to estimate vignetting. Fig-\nure 10.8c shows how the mechanical vignetting manifests itself as clipping of the PSF in the\ncorners of the image. In order for the overall dimming associated with vignetting to be prop-\nerly captured, the modiﬁed intensities of the ideal pattern need to be extrapolated from the\ncenter, which is best done with a uniformly illuminated target.\nWhen working with RAW Bayer-pattern images, the correct way to estimate the PSF is\nto only evaluate the least squares terms in (10.1) at sensed pixel values, while interpolating\nthe ideal image to all values. For JPEG images, you should linearize your intensities ﬁrst,\ne.g., remove the gamma and any other non-linearities in your estimated radiometric response\nfunction.\nWhat if you have an image that was taken with an uncalibrated camera? Can you still\nrecover the PSF an use it to correct the image? In fact, with a slight modiﬁcation, the previous\nalgorithms still work.\nInstead of assuming a known calibration image, you can detect strong elongated edges\nand ﬁt ideal step edges in such regions (Figure 10.9b), resulting in the sharp image shown",
  "501": "10.2 High dynamic range imaging\n479\nMin\nMax\nR\nR\nValid  Region\n(a)\n(b)\n(c)\n(d)\nFigure 10.9\nEstimating the PSF without using a calibration pattern (Joshi, Szeliski, and\nKriegman 2008) c⃝2008 IEEE: (a) Input image with blue cross-section (proﬁle) location, (b)\nProﬁle of sensed and predicted step edges, (c–d) Locations and values of the predicted colors\nnear the edge locations.\nin Figure 10.9d. For every pixel that is surrounded by a complete set of valid estimated\nneighbors (green pixels in Figure 10.9c), apply the least squares formula (10.1) to estimate\nthe kernel K. The resulting locally estimated PSFs can be used to correct for chromatic\naberration (since the relative displacements between per-channel PSFs can be computed), as\nshown by Joshi, Szeliski, and Kriegman (2008).\nExercise 10.4 provides some more detailed instructions for implementing and testing\nedge-based PSF estimation algorithms. An alternative approach, which does not require the\nexplicit detection of edges but uses image statistics (gradient distributions) instead, is pre-\nsented by Fergus, Singh, Hertzmann et al. (2006).\n10.2 High dynamic range imaging\nAs we mentioned earlier in this chapter, registered images taken at different exposures can be\nused to calibrate the radiometric response function of a camera. More importantly, they can\nhelp you create well-exposed photographs under challenging conditions, such as brightly lit",
  "502": "480\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.10\nSample indoor image where the areas outside the window are overexposed\nand inside the room are too dark.\n1\n1,500\n25,000\n400,000\n2,000,000\nFigure 10.11 Relative brightness of different scenes, ranging from 1 inside a dark room lit\nby a monitor to 2,000,000 looking at the sun. Photos courtesy of Paul Debevec.\nscenes where any single exposure contains saturated (overexposed) and dark (underexposed)\nregions (Figure 10.10). This problem is quite common, because the natural world contains a\nrange of radiance values that is far greater than can be captured with any photographic sensor\nor ﬁlm (Figure 10.11). Taking a set of bracketed exposures (exposures taken by a camera\nin automatic exposure bracketing (AEB) mode to deliberately under- and over-expose the\nimage) gives you the material from which to create a properly exposed photograph, as shown\nin Figure 10.12 (Reinhard, Ward, Pattanaik et al. 2005; Freeman 2008; Gulbins and Gulbins\n2009; Hasinoff, Durand, and Freeman 2010).\nWhile it is possible to combine pixels from different exposures directly into a ﬁnal com-\n+\n+\n⇒\nFigure 10.12\nA bracketed set of shots (using the camera’s automatic exposure bracketing\n(AEB) mode) and the resulting high dynamic range (HDR) composite.",
  "503": "10.2 High dynamic range imaging\n481\nposite (Burt and Kolczynski 1993; Mertens, Kautz, and Reeth 2007), this approach runs the\nrisk of creating contrast reversals and halos. Instead, the more common approach is to pro-\nceed in three stages:\n1. Estimate the radiometric response function from the aligned images.\n2. Estimate a radiance map by selecting or blending pixels from different exposures.\n3. Tone map the resulting high dynamic range (HDR) image back into a displayable\ngamut.\nThe idea behind estimating the radiometric response function is relatively straightforward\n(Mann and Picard 1995; Debevec and Malik 1997; Mitsunaga and Nayar 1999; Reinhard,\nWard, Pattanaik et al. 2005). Suppose you take three sets of images at different exposures\n(shutter speeds), say at ±2 exposure values.11 If we were able to determine the irradiance\n(exposure) Ei at each pixel (2.101), we could plot it against the measured pixel value zij for\neach exposure time tj, as shown in Figure 10.13.\nUnfortunately, we do not know the irradiance values Ei, so these have to be estimated\nat the same time as the radiometric response function f, which can be written (Debevec and\nMalik 1997) as\nzij = f(Ei tj),\n(10.3)\nwhere tj is the exposure time for the jth image. The inverse response curve f −1 is given by\nf −1(zij) = Ei tj.\n(10.4)\nTaking logarithms of both sides (base 2 is convenient, as we can now measure quantities in\nEVs), we obtain\ng(zij) = log f −1(zij) = log Ei + log tj,\n(10.5)\nwhere g = log f −1 (which maps pixel values zij into log irradiance) is the curve we are\nestimating (Figure 10.13 turned on its side).\nDebevec and Malik (1997) assume that the exposure times tj are known. (Recall that\nthese can be obtained from a camera’s EXIF tags, but that they actually follow a power of 2\nprogression . . . , 1/128, 1/64, 1/32, 1/16, 1/8, . . . instead of the marked . . . , 1/125, 1/60, 1/30,\n1/15, 1/8, . . . values—see Exercise 2.5.) The unknowns are therefore the per-pixel exposures\nEi and the response values gk = g(k), where g can be discretized according to the 256\npixel values commonly observed in eight-bit images. (The response curves are calibrated\nseparately for each color channel.)\n11 Changing the shutter speed is preferable to changing the aperture, as the latter can modify the vignetting and\nfocus. Using ±2 “f-stops” (technically, exposure values, or EVs, since f-stops refer to apertures) is usually the right\ncompromise between capturing a good dynamic range and having properly exposed pixels everywhere.",
  "504": "482\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nlog Exposure\nPixel value\n3\n1\n2\nlog Exposure\nPixel value\nFigure 10.13 Radiometric calibration using multiple exposures (Debevec and Malik 1997).\nCorresponding pixel values are plotted as functions of log exposures (irradiance). The curves\non the left are shifted to account for each pixel’s unknown radiance until they all line up into\na single smooth curve.\nIn order to make the response curve smooth, Debevec and Malik (1997) add a second-\norder smoothness constraint\nλ\nX\nk\ng′′(k)2 = λ\nX\n[g(k −1) −2g(k) + g(k + 1)]2,\n(10.6)\nwhich is similar to the one used in snakes (5.3). Since pixel values are more reliable in the\nmiddle of their range (and the g function becomes singular near saturation values), they also\nadd a weighting (hat) function w(k) that decays to zero at both ends of the pixel value range,\nw(z) =\n(\nz −zmin\nz ≤(zmin + zmax)/2\nzmax −z\nz > (zmin + zmax)/2.\n(10.7)\nPutting all of these terms together, they obtain a least squares problem in the unknowns\n{gk} and {Ei},\nE =\nX\ni\nX\nj\nw(zi,j)[g(zi,j) −log Ei −log tj]2 + λ\nX\nk\nw(k)g′′(k)2.\n(10.8)\n(In order to remove the overall shift ambiguity in the response curve and irradiance values,\nthe middle of the response curve is set to 0.) Debevec and Malik (1997) show how this can\nbe implemented in 21 lines of MATLAB code, which partially accounts for the popularity of\ntheir technique.\nWhile Debevec and Malik (1997) assume that the exposure times tj are known exactly,\nthere is no reason why these additional variables cannot be thrown into the least squares\nproblem, constraining their ﬁnal estimated values to lie close to their nominal values ˆtj with\nan extra term η P\nj(tj −ˆtj)2.",
  "505": "10.2 High dynamic range imaging\n483\n(a)\n(b)\nFigure 10.14\nRecovered response function and radiance image for a real digital camera\n(DCS460) (Debevec and Malik 1997) c⃝1997 ACM.\nFigure 10.14 shows the recovered radiometric response function for a digital camera along\nwith select (relative) radiance values in the overall radiance map. Figure 10.15 shows the\nbracketed input images captured on color ﬁlm and the corresponding radiance map.\nWhile Debevec and Malik (1997) use a general second-order smooth curve g to parame-\nterize their response curve, Mann and Picard (1995) use a three-parameter function\nf(E) = α + βEγ,\n(10.9)\nwhile Mitsunaga and Nayar (1999) use a low-order (N ≤10) polynomial for the inverse\nresponse function g. Pal, Szeliski, Uyttendaele et al. (2004) derive a Bayesian model that\nestimates an independent smooth response function for each image, which can better model\nthe more sophisticated (and hence less predictable) automatic contrast and tone adjustment\nperformed in today’s digital cameras.\nOnce the response function has been estimated, the second step in creating high dynamic\nrange photographs is to merge the input images into a composite radiance map. If the re-\nsponse function and images were known exactly, i.e., if they were noise free, you could use\nany non-saturated pixel value to estimate the corresponding radiance by mapping it through\nthe inverse response curve E = g(z).\nUnfortunately, pixels are noisy, especially under low-light conditions when fewer photons\narrive at the sensor. To compensate for this, Mann and Picard (1995) use the derivative of\nthe response function as a weight in determining the ﬁnal radiance estimate, since “ﬂatter”\nregions of the curve tell us less about the incoming irradiance. Debevec and Malik (1997)\nuse a hat function (10.7) which accentuates mid-tone pixels while avoiding saturated values.\nMitsunaga and Nayar (1999) show that in order to maximize the signal-to-noise ratio (SNR),",
  "506": "484\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.15\nBracketed set of exposures captured with a ﬁlm camera and the resulting\nradiance image displayed in pseudocolor (Debevec and Malik 1997) c⃝1997 ACM.\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 10.16\nMerging multiple exposures to create a high dynamic range composite (Kang,\nUyttendaele, Winder et al. 2003): (a–c) three different exposures; (d) merging the exposures\nusing classic algorithms (note the ghosting due to the horse’s head movement); (e) merging\nthe exposures with motion compensation.",
  "507": "10.2 High dynamic range imaging\n485\n(a)\n(b)\n(c)\nFigure 10.17 HDR merging with large amounts of motion (Eden, Uyttendaele, and Szeliski\n2006) c⃝2006 IEEE: (a) registered bracketed input images; (b) results after the ﬁrst pass of\nimage selection: reference labels, image, and tone-mapped image; (c) results after the second\npass of image selection: ﬁnal labels, compressed HDR image, and tone-mapped image\nthe weighting function must emphasize both higher pixel values and larger gradients in the\ntransfer function, i.e.,\nw(z) = g(z)/g′(z),\n(10.10)\nwhere the weights w are used to form the ﬁnal irradiance estimate\nlog Ei =\nP\nj w(zij)[g(zij) −log tj]\nP\nj w(zij)\n.\n(10.11)\nExercise 10.1 has you implement one of the radiometric response function calibration tech-\nniques and then use it to create radiance maps.\nUnder real-world conditions, casually acquired images may not be perfectly registered\nand may contain moving objects. Ward (2003) uses a global (parametric) transform to align\nthe input images, while Kang, Uyttendaele, Winder et al. (2003) present an algorithm that\ncombines global registration with local motion estimation (optical ﬂow) to accurately align\nthe images before blending their radiance estimates (Figure 10.16). Since the images may",
  "508": "486\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.18 Fuji SuperCCD high dynamic range image sensor. The paired large and small\nactive areas provide two different effective exposures.\nhave widely different exposures, care must be taken when estimating the motions, which must\nthemselves be checked for consistency to avoid the creation of ghosts and object fragments.\nEven this approach, however, may not work when the camera is simultaneously undergo-\ning large panning motions and exposure changes, which is a common occurrence in casually\nacquired panoramas. Under such conditions, different parts of the image may be seen at one\nor more exposures. Devising a method to blend all of these different sources while avoid-\ning sharp transitions and dealing with scene motion is a challenging problem. One approach\nis to ﬁrst ﬁnd a consensus mosaic and to then selectively compute radiances in under- and\nover-exposed regions (Eden, Uyttendaele, and Szeliski 2006), as shown in Figure 10.17.\nRecently, some cameras, such as the Sony α550 and Pentax K-7, have started integrating\nmultiple exposure merging and tone mapping directly into the camera body. In the future,\nthe need to compute high dynamic range images from multiple exposures may be eliminated\nby advances in camera sensor technology (Figure 10.18) (Yang, El Gamal, Fowler et al.\n1999; Nayar and Mitsunaga 2000; Nayar and Branzoi 2003; Kang, Uyttendaele, Winder et\nal. 2003; Narasimhan and Nayar 2005; Tumblin, Agrawal, and Raskar 2005). However, the\nneed to blend such images and to tone map them to lower-gamut displays is likely to remain.\nHDR image formats.\nBefore we discuss techniques for mapping HDR images back to a\ndisplayable gamut, we should discuss the commonly used formats for storing HDR images.\nIf storage space is not an issue, storing each of the R, G, and B values as a 32-bit IEEE\nﬂoat is the best solution. The commonly used Portable PixMap (.ppm) format, which supports\nboth uncompressed ASCII and raw binary encodings of values, can be extended to a Portable\nFloatMap (.pfm) format by modifying the header. TIFF also supports full ﬂoating point\nvalues.\nA more compact representation is the Radiance format (.pic, .hdr) (Ward 1994), which\nuses a single common exponent and per-channel mantissas (10.19b). An intermediate encod-",
  "509": "10.2 High dynamic range imaging\n487\n96 bits / pixel\nsign\nexponent\nmantissa\n(a)\n32 bits / pixel\nred\ngreen\nblue\nexponent\n(b)\n48 bits / pixel\nsign exponent\nmantissa\n(c)\nFigure 10.19\nHDR image encoding formats: (a) Portable PixMap (.ppm); (b) Radiance\n(.pic, .hdr); (c) OpenEXR (.exr).\ning, OpenEXR from ILM,12 uses 16-bit ﬂoats for each channel (10.19c), which is a format\nsupported natively on most modern GPUs. Ward (2004) describes these and other data for-\nmats such as LogLuv (Larson 1998) in more detail, as do the books by Reinhard, Ward,\nPattanaik et al. (2005) and Freeman (2008). An even more recent HDR image format is the\nJPEG XR standard.13\n10.2.1 Tone mapping\nOnce a radiance map has been computed, it is usually necessary to display it on a lower gamut\n(i.e., eight-bit) screen or printer. A variety of tone mapping techniques has been developed for\nthis purpose, which involve either computing spatially varying transfer functions or reducing\nimage gradients to ﬁt the available dynamic range (Reinhard, Ward, Pattanaik et al. 2005).\nThe simplest way to compress a high dynamic range radiance image into a low dynamic\nrange gamut is to use a global transfer curve (Larson, Rushmeier, and Piatko 1997). Fig-\nure 10.20 shows one such example, where a gamma curve is used to map an HDR image back\n12 http://www.openexr.net/.\n13 http://www.itu.int/rec/T-REC-T.832-200903-I/en.",
  "510": "488\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 10.20\nGlobal tone mapping: (a) input HDR image, linearly mapped; (b) gamma\napplied to each color channel independently; (c) gamma applied to intensity (colors are\nless washed out). Original HDR image courtesy of Paul Debevec, http://ict.debevec.org/\n∼debevec/Research/HDR/. Processed images courtesy of Fr´edo Durand, MIT 6.815/6.865\ncourse on Computational Photography.\ninto a displayable gamut. If gamma is applied separately to each channel (Figure 10.20b), the\ncolors become muted (less saturated), since higher-valued color channels contribute less (pro-\nportionately) to the ﬁnal color. Splitting the image up into its luminance and chrominance\n(say, L*a*b*) components (Section 2.3.2), applying the global mapping to the luminance\nchannel, and then reconstituting a color image works better (Figure 10.20c).\nUnfortunately, when the image has a really wide range of exposures, this global approach\nstill fails to preserve details in regions with widely varying exposures. What is needed, in-\nstead, is something akin to the dodging and burning performed by photographers in the dark-\nroom. Mathematically, this is similar to dividing each pixel by the average brightness in a\nregion around that pixel.\nFigure 10.21 shows how this process works. As before, the image is split into its lumi-\nnance and chrominance channels. The log luminance image\nH(x, y) = log L(x, y)\n(10.12)\nis then low-pass ﬁltered to produce a base layer\nHL(x, y) = B(x, y) ∗H(x, y),\n(10.13)\nand a high-pass detail layer\nHH(x, y) = H(x, y) −HL(x, y).\n(10.14)\nThe base layer is then contrast reduced by scaling to the desired log-luminance range,\nH′\nH(x, y) = s HH(x, y)\n(10.15)",
  "511": "10.2 High dynamic range imaging\n489\nand added to the detail layer to produce the new log-luminance image\nI(x, y) = H′\nH(x, y) + HL(x, y),\n(10.16)\nwhich can then be exponentiated to produce the tone-mapped (compressed) luminance im-\nage. Note that this process is equivalent to dividing each luminance value by (a monotonic\nmapping of) the average log-luminance value in a region around that pixel.\nFigure 10.21 shows the low-pass and high-pass log luminance image and the resulting\ntone-mapped color image. Note how the detail layer has visible halos around the high-\ncontrast edges, which are visible in the ﬁnal tone-mapped image. This is because linear\nﬁltering, which is not edge preserving, produces halos in the detail layer (Figure 10.23).\nThe solution to this problem is to use an edge-preserving ﬁlter to create the base layer. Du-\nrand and Dorsey (2002) study a number of such edge-preserving ﬁlters, including anisotropic\nand robust anisotropic diffusion, and select bilateral ﬁltering (Section 3.3.1) as their edge-\npreserving ﬁlter. (A more recent paper by Farbman, Fattal, Lischinski et al. (2008) argues\nin favor of using a weighted least squares (WLF) ﬁlter as an alternative to the bilateral ﬁlter\nand Paris, Kornprobst, Tumblin et al. (2008) reviews bilateral ﬁltering and its applications\nin computer vision and computational photography.) Figure 10.22 shows how replacing the\nlinear low-pass ﬁlter with a bilateral ﬁlter produces tone-mapped images with no visible ha-\nlos. Figure 10.24 summarizes the complete information ﬂow in this process, starting with\nthe decomposition into log luminance and chrominance images, bilateral ﬁltering, contrast\nreduction, and re-composition into the ﬁnal output image.\nAn alternative to compressing the base layer is to compress its derivatives, i.e., the gra-\ndient of the log-luminance image (Fattal, Lischinski, and Werman 2002). Figure 10.25 illus-\ntrates this process. The log-luminance image is differentiated to obtain a gradient image\nH′(x, y) = ∇H(x, y).\n(10.17)\nThis gradient image is then attenuated by a spatially varying attenuation function Φ(x, y),\nG(x, y) = H′(x, y) Φ(x, y).\n(10.18)\nThe attenuation function I(x, y) is designed to attenuate large-scale brightness changes (Fig-\nure 10.26a) and is designed to take into account gradients at different spatial scales (Fattal,\nLischinski, and Werman 2002).\nAfter attenuation, the resulting gradient ﬁeld is re-integrated by solving a ﬁrst-order vari-\national (least squares) problem,\nmin\nZ Z\n∥∇I(x, y) −G(x, y)∥2dx dy\n(10.19)",
  "512": "490\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 10.21 Local tone mapping using linear ﬁlters: (a) low-pass and high-pass ﬁltered log\nluminance images and color (chrominance) image; (b) resulting tone-mapped image (after at-\ntenuating the low-pass log luminance image) shows visible halos around the trees. Processed\nimages courtesy of Fr´edo Durand, MIT 6.815/6.865 course on Computational Photography.\n(a)\n(b)\nFigure 10.22 Local tone mapping using bilateral ﬁlter (Durand and Dorsey 2002): (a) low-\npass and high-pass bilateral ﬁltered log luminance images and color (chrominance) image;\n(b) resulting tone-mapped image (after attenuating the low-pass log luminance image) shows\nno halos. Processed images courtesy of Fr´edo Durand, MIT 6.815/6.865 course on Compu-\ntational Photography.",
  "513": "10.2 High dynamic range imaging\n491\nFigure 10.23 Gaussian vs. bilateral ﬁltering (Petschnigg, Agrawala, Hoppe et al. 2004) c⃝\n2004 ACM: A Gaussian low-pass ﬁlter blurs across all edges and therefore creates strong\npeaks and valleys in the detail image that cause halos. The bilateral ﬁlter does not smooth\nacross strong edges and thereby reduces halos while still capturing detail.\nFigure 10.24\nLocal tone mapping using bilateral ﬁlter (Durand and Dorsey 2002): sum-\nmary of algorithm workﬂow. Images courtesy of Fr´edo Durand, MIT 6.815/6.865 course on\nComputational Photography.",
  "514": "492\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nlog\nexp\ndiff\nint.\ncompress\n2500:1\n7.5:1\nH(x)\nI(x)\nH’(x)\nG(x)\nFigure 10.25\nGradient domain tone mapping (Fattal, Lischinski, and Werman 2002) c⃝\n2002 ACM. The original image with a dynamic range of 2415:1 is ﬁrst converted into the log\ndomain, H(x), and its gradients are computed, H′(x). These are attenuated (compressed)\nbased on local contrast, G(x), and integrated to produce the new logarithmic exposure image\nI(x), which is exponentiated to produce the ﬁnal intensity image, whose dynamic range is\n7.5:1.\nto obtain the compressed log-luminance image I(x, y). This least squares problem is the\nsame that was used for Poisson blending (Section 9.3.4) and was ﬁrst introduced in our study\nof regularization (Section 3.7.1, 3.100). It can efﬁciently be solved using techniques such\nas multigrid and hierarchical basis preconditioning (Fattal, Lischinski, and Werman 2002;\nSzeliski 2006b; Farbman, Fattal, Lischinski et al. 2008). Once the new luminance image has\nbeen computed, it is combined with the original color image using\nCout =\n\u0012Cin\nLin\n\u0013s\nLout,\n(10.20)\nwhere C = (R, G, B) and Lin and Lout are the original and compressed luminance images.\nThe exponent s controls the saturation of the colors and is typically in the range s ∈[0.4, 0.6].\nFigure 10.26b shows the ﬁnal tone-mapped color image, which shows no visible halos despite\nthe extremely large variation in input radiance values.\nYet another alternative to these two approaches is to perform the local dodging and burn-\ning using a locally scale-selective operator (Reinhard, Stark, Shirley et al. 2002). Figure 10.27\nshows how such a scale selection operator can determine a radius (scale) that only includes",
  "515": "10.2 High dynamic range imaging\n493\n(a)\n(b)\nFigure 10.26\nGradient domain tone mapping (Fattal, Lischinski, and Werman 2002) c⃝\n2002 ACM: (a) attenuation map, with darker values corresponding to more attenuation; (b)\nﬁnal tone-mapped image.\nsimilar color values within the inner circle while avoiding much brighter values in the sur-\nrounding circle. In practice, a difference of Gaussians normalized by the inner Gaussian\nresponse is evaluated over a range of scales, and the largest scale whose metric is below a\nthreshold is selected (Reinhard, Stark, Shirley et al. 2002).\nWhat all of these techniques have in common is that they adaptively attenuate or brighten\ndifferent regions of the image so that they can be displayed in a limited gamut without loss of\ncontrast. Lischinski, Farbman, Uyttendaele et al. (2006b) introduce an interactive technique\nthat performs this operation by interpolating a set of sparse user-drawn adjustments (strokes\nand associated exposure value corrections) to a piecewise-continuous exposure correction\nmap (Figure 10.28). The interpolation is performed by minimizing a locally weighted least\nsquare (WLS) variational problem,\nmin\nZ Z\nwd(x, y)∥f(x, y) −g(x, y)∥2dx dy + λ\nZ Z\nws(x, y)∥∇f(x, y)∥2dx dy,\n(10.21)\nwhere g(x, y) and f(x, y) are the input and output log exposure (attenuation) maps (Fig-\nure 10.28). The data weighting term wd(x, y) is 1 at stroke locations and 0 elsewhere. The\nsmoothness weighting term ws(x, y) is inversely proportional to the log-luminance gradient,\nws =\n1\n∥∇H∥α + ϵ\n(10.22)\nand hence encourages the f(x, y) map to be smoother in low-gradient areas than along high-\ngradient discontinuities.14 The same approach can also be used for fully automated tone map-\n14 In practice, the x and y discrete derivatives are weighted separately (Lischinski, Farbman, Uyttendaele et al.",
  "516": "494\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.27\nScale selection for tone mapping (Reinhard, Stark, Shirley et al. 2002) c⃝\n2002 ACM.\nping by setting target exposure values at each pixel and allowing the weighted least squares\nto convert these into piecewise smooth adjustment maps.\nThe weighted least squares algorithm, which was originally developed for image coloriza-\ntion applications (Levin, Lischinski, and Weiss 2004), has recently been applied to general\nedge-preserving smoothing in applications such as contrast enhancement (Bae, Paris, and\nDurand 2006) and tone mapping (Farbman, Fattal, Lischinski et al. 2008) where the bilateral\nﬁltering was previously used. It can also be used to perform HDR merging and tone mapping\nsimultaneously (Raman and Chaudhuri 2007, 2009).\nGiven the wide range of locally adaptive tone mapping algorithms that have been devel-\noped, which ones should be used in practice? Freeman (2008) provides a great discussion\nof commercially available algorithms, their artifacts, and the parameters that can be used to\ncontrol them. He also has a wealth of tips for HDR photography and workﬂow. I highly rec-\nommend his book for anyone contemplating additional research (or personal photography) in\nthis area.\n10.2.2 Application: Flash photography\nWhile high dynamic range imaging combines images of a scene taken at different exposures,\nit is also possible to combine ﬂash and non-ﬂash images to achieve better exposure and color\nbalance and to reduce noise (Eisemann and Durand 2004; Petschnigg, Agrawala, Hoppe et\nal. 2004).\nThe problem with ﬂash images is that the color is often unnatural (it fails to capture the\nambient illumination), there may be strong shadows or specularities, and there is a radial\nfalloff in brightness away from the camera (Figures 10.1b and 10.29a). Non-ﬂash photos\n2006b). Their default parameter settings are λ = 0.2, α = 1, and ϵ = 0.0001.",
  "517": "10.2 High dynamic range imaging\n495\n(a)\n(b)\nFigure 10.28 Interactive local tone mapping (Lischinski, Farbman, Uyttendaele et al. 2006b)\nc⃝2006 ACM: (a) user-drawn strokes with associated exposure values g(x, y) (b) correspond-\ning piecewise-smooth exposure adjustment map f(x, y).\n(a)\n(b)\n(c)\n(d)\nFigure 10.29 Detail transfer in ﬂash/no-ﬂash photography (Petschnigg, Agrawala, Hoppe et\nal. 2004) c⃝2004 ACM: (a) details of input ambient A and ﬂash F images; (b) joint bilaterally\nﬁltered no-ﬂash image ANR; (c) detail layer F Detail computed from the ﬂash image F; (d)\nﬁnal merged image AFinal.",
  "518": "496\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntaken under low light conditions often suffer from excessive noise (because of the high ISO\ngains and low photon counts) and blur (due to longer exposures). Is there some way to\ncombine a non-ﬂash photo taken just before the ﬂash goes off with the ﬂash photo to produce\nan image with good color values, sharpness, and low noise?15\nPetschnigg, Agrawala, Hoppe et al. (2004) approach this problem by ﬁrst ﬁltering the no-\nﬂash (ambient) image A with a variant of the bilateral ﬁlter called the joint bilateral ﬁlter16\nin which the range kernel (3.36)\nr(i, j, k, l) = exp\n\u0012\n−∥f(i, j) −f(k, l)∥2\n2σ2r\n\u0013\n(10.23)\nis evaluated on the ﬂash image F instead of the ambient image A, since the ﬂash image is less\nnoisy and hence has more reliable edges (Figure 10.29b). Because the contents of the ﬂash\nimage can be unreliable inside and at the boundaries of shadows and specularities, these are\ndetected and a regular bilaterally ﬁltered image ABase is used instead (Figure 10.30).\nThe second stage of their algorithm computes a ﬂash detail image\nF Detail =\nF + ϵ\nF Base + ϵ,\n(10.24)\nwhere F Base is a bilaterally ﬁltered version of the ﬂash image F and ϵ = 0.02. This detail im-\nage (Figure 10.29c) encodes details that may have been ﬁltered away from the noise-reduced\nno-ﬂash image ANR, as well as additional details created by the ﬂash camera, which often\nadd crispness. The detail image is used to modulate the noise-reduced ambient image ANR\nto produce the ﬁnal results\nAFinal = (1 −M)ANRF Detail + MABase\n(10.25)\nshown in Figures 10.1b and 10.29d.\nEisemann and Durand (2004) present an alternative algorithm that shares some of the\nsame basic concepts. Both papers are well worth reading and contrasting (Exercise 10.6).\nFlash images can also be used for a variety of additional applications such as extracting\nmore reliable foreground mattes of objects (Raskar, Tan, Feris et al. 2004; Sun, Li, Kang et al.\n2006). Flash photography is just one instance of the more general topic of active illumination,\nwhich is discussed in more detail by Raskar and Tumblin (2010).\n15 In fact, the discontinued FujiFilm FinePix F40fd camera takes a pair of ﬂash and no ﬂash images in quick\nsuccession; however, it only lets you decide to keep one of them.\n16 Eisemann and Durand (2004) call this the cross bilateral ﬁlter.",
  "519": "10.3 Super-resolution and blur removal\n497\nFigure 10.30\nFlash/no-ﬂash photography algorithm (Petschnigg, Agrawala, Hoppe et al.\n2004) c⃝2004 ACM. The ambient (no-ﬂash) image A is ﬁltered with a regular bilateral ﬁlter\nto produce ABase, which is used in shadow and specularity regions, and a joint bilaterally\nﬁltered noise reduced image ANR. The ﬂash image F is bilaterally ﬁltered to produce a\nbase image F Base and a detail (ratio) image F Detail, which is used to modulate the de-\nnoised ambient image. The shadow/specularity mask M is computed by comparing linearized\nversions of the ﬂash and no-ﬂash images.\n10.3 Super-resolution and blur removal\nWhile high dynamic range imaging enables us to obtain an image with a larger dynamic\nrange than a single regular image, super-resolution enables us to create images with higher\nspatial resolution and less noise than regular camera images (Chaudhuri 2001; Park, Park,\nand Kang 2003; Capel and Zisserman 2003; Capel 2004; van Ouwerkerk 2006). Most com-\nmonly, super-resolution refers to the process of aligning and combining several input images\nto produce such high-resolution composites (Irani and Peleg 1991; Cheeseman, Kanefsky,\nHanson et al. 1993; Pickup, Capel, Roberts et al. 2009). However, some newer techniques\ncan super-resolve a single image (Freeman, Jones, and Pasztor 2002; Baker and Kanade 2002;\nFattal 2007) and are hence closely related to techniques for removing blur (Sections 3.4.3 and\n3.4.4).\nThe most principled way to formulate the super-resolution problem is to write down the\nstochastic image formation equations and image priors and to then use Bayesian inference to\nrecover the super-resolved (original) sharp image. We can do this by generalizing the image",
  "520": "498\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nformation equations (3.75) used for image deblurring (Section 3.4.3), which we also used\nin (10.2) for blur kernel (PSF) estimation (Section 10.1.4). In this case, we have several ob-\nserved images {ok(x)}, as well as an image warping function ˆhk(x) for each observed image\n(Figure 3.47). Combining all of these elements, we get the (noisy) observation equations17\nok(x) = D{b(x) ∗s(ˆhk(x))} + nk(x),\n(10.26)\nwhere D is the downsampling operator, which operates after the super-resolved (sharp)\nwarped image s(ˆhk(x)) has been convolved with the blur kernel b(x). The above image\nformation equations lead to the following least squares problem,\nX\nk\n∥ok(x) −D{bk(x) ∗s(ˆhk(x))}∥2.\n(10.27)\nIn most super-resolution algorithms, the alignment (warping) ˆhk is estimated using one of\nthe input frames as the reference frame; either feature-based (Section 6.1.3) or direct (image-\nbased) (Section 8.2) parametric alignment techniques can be used. (A few algorithms, such\nas those described by Schultz and Stevenson (1996) or Capel (2004) use dense (per-pixel\nﬂow) estimates.) A better approach is to re-compute the alignment by directly minimizing\n(10.27) once an initial estimate of s(x) has been computed (Hardie, Barnard, and Armstrong\n1997) or to marginalize out the motion parameters altogether (Pickup, Capel, Roberts et al.\n2007)—see also the work of Protter and Elad (2009) for some related video super-resolution\nwork.\nThe point spread function (blur kernel) bk is either inferred from knowledge of the image\nformation process (e.g., the amount of motion or defocus blur and the camera sensor optics)\nor calibrated from a test image or the observed images {ok} using one of the techniques\ndescribed in Section 10.1.4. The problem of simultaneously inferring the blur kernel and the\nsharp image is known as blind image deconvolution (Kundur and Hatzinakos 1996; Levin\n2006).18\nGiven an estimate of ˆhk and bk(x), (10.27) can be re-written using matrix/vector notation\nas a large sparse least squares problem in the unknown values of the super-resolved pixels s,\nX\nk\n∥ok −DBkW ks∥2.\n(10.28)\n17 It is also possible to add an unknown bias–gain term to each observation (Capel 2004), as was done for motion\nestimation in (8.8).\n18 Notice that there is a chicken-and-egg problem if both the blur kernel and the super-resolved image are un-\nknown. This can be “broken” either using structural assumptions about the sharp image, e.g., the presence of edges\n(Joshi, Szeliski, and Kriegman 2008) or prior models for the image, such as edge sparsity (Fergus, Singh, Hertzmann\net al. 2006).",
  "521": "10.3 Super-resolution and blur removal\n499\n(Recall from (3.89) that once the warping function ˆhk is known, values of s(ˆhk(x)) depend\nlinearly on those in s(x).) An efﬁcient way to solve this least squares problem is to use\npreconditioned conjugate gradient descent (Capel 2004), although some earlier algorithms,\nsuch as the one developed by Irani and Peleg (1991), used regular gradient descent (also\nknown as iterative back projection (IBP), in the computed tomography literature).\nThe above formulation assumes that warping can be expressed as a simple (sinc or bicu-\nbic) interpolated resampling of the super-resolved sharp image, followed by a stationary\n(spatially invariant) blurring (PSF) and area integration process. However, if the surface is\nseverely foreshortened, we have to take into account the spatially varying ﬁltering that occurs\nduring the image warping (Section 3.6.1), before we can then model the PSF induced by the\noptics and camera sensor (Wang, Kang, Szeliski et al. 2001; Capel 2004).\nHow well does this least squares (MLE) approach to super-resolution work? In practice,\nthis depends a lot on the amount of blur and aliasing in the camera optics, as well as the accu-\nracy in the motion and PSF estimates (Baker and Kanade 2002; Jiang, Wong, and Bao 2003;\nCapel 2004). Less blurring and more aliasing means that there is more (aliased) high fre-\nquency information available to be recovered. However, because the least squares (maximum\nlikelihood) formulation uses no image prior, a lot of high-frequency noise can be introduced\ninto the solution (Figure 10.31c).\nFor this reason, most super-resolution algorithms assume some form of image prior. The\nsimplest of these is to place a penalty on the image derivatives similar to Equations (3.105\nand 3.113), e.g.,\nX\n(i,j)\nρp(s(i, j) −s(i + 1, j)) + ρp(s(i, j) −s(i, j + 1)).\n(10.29)\nAs discussed in Section 3.7.2, when ρp is quadratic, this is a form of Tikhonov regulariza-\ntion (Section 3.7.1), and the overall problem is still linear least squares. The resulting prior\nimage model is a Gaussian Markov random ﬁeld (GMRF), which can be extended to other\n(e.g., diagonal) differences, as in (Capel 2004) (Figure 10.31).\nUnfortunately, GMRFs tend to produce solutions with visible ripples, which can also\nbe interpreted as increased noise sensitivity in middle frequencies (Exercise 3.17). A bet-\nter image prior is a robust prior that encourages piecewise continuous solutions (Black and\nRangarajan 1996), see Appendix B.3. Examples of such priors include the Huber potential\n(Schultz and Stevenson 1996; Capel and Zisserman 2003), which is a blend of a Gaussian\nwith a longer-tailed Laplacian, and the even sparser (heavier-tailed) hyper-Laplacians used\nby Levin, Fergus, Durand et al. (2007) and Krishnan and Fergus (2009). It is also possible to\nlearn the parameters for such priors using cross-validation (Capel 2004; Pickup 2007).\nWhile sparse (robust) derivative priors can reduce rippling effects and increase edge\nsharpness, they cannot hallucinate higher-frequency texture or details. To do this, a train-",
  "522": "500\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 10.31 Super-resolution results using a variety of image priors (Capel 2001): (a) Low-\nres ROI (bicubic 3× zoom); (b) average image; (c) MLE @ 1.25× pixel-zoom; (d) simple\n∥x∥2 prior (λ = 0.004); (e) GMRF (λ = 0.003); (f) HMRF (λ = 0.01, α = 0.04). 10\nimages are used as input and a 3× super-resolved image is produced in each case, except for\nthe MLE result in (c).\n(a)\n(b)\n(c)\nFigure 10.32 Example-based super-resolution: (a) original 32 × 32 low-resolution image;\n(b) example-based super-resolved 256 × 256 image (Freeman, Jones, and Pasztor 2002) c⃝\n2002 IEEE; (c) upsampling via imposed edge statistics (Fattal 2007) c⃝2007 ACM.",
  "523": "10.3 Super-resolution and blur removal\n501\ning set of sample images can be used to ﬁnd plausible mappings between low-frequency\noriginals and the missing higher frequencies. Inspired by some of the example-based texture\nsynthesis algorithms we discuss in Section 10.5, the example-based super-resolution algo-\nrithm developed by Freeman, Jones, and Pasztor (2002) uses training images to learn the\nmapping between local texture patches and missing higher-frequency details. To ensure that\noverlapping patches are similar in appearance, a Markov random ﬁeld is used and optimized\nusing either belief propagation (Freeman, Pasztor, and Carmichael 2000) or a raster-scan de-\nterministic variant (Freeman, Jones, and Pasztor 2002). Figure 10.32 shows the results of\nhallucinating missing details using this approach and compares these results to a more recent\nalgorithm by Fattal (2007). This latter algorithm learns to predict oriented gradient magni-\ntudes in the ﬁner resolution image based on a pixel’s location relative to the nearest detected\nedge along with the corresponding edge statistics (magnitude and width). It is also possible\nto combine sparse (robust) derivative priors with example-based super-resolution, as shown\nby Tappen, Russell, and Freeman (2003).\nAn alternative (but closely related) form of hallucination is to recognize the parts of a\ntraining database of images to which a low-resolution pixel might correspond. In their work,\nBaker and Kanade (2002) use local derivative-of-Gaussian ﬁlter responses as features and\nthen match parent structure vectors in a manner similar to De Bonet (1997).19 The high-\nfrequency gradient at each recognized training image location is then used as a constraint on\nthe super-resolved image, along with the usual reconstruction (prediction) equation (10.27).\nFigure 10.33 shows the result of hallucinating higher-resolution faces from lower-resolution\ninputs; Baker and Kanade (2002) also show examples of super-resolving known-font text.\nExercise 10.7 gives more details on how to implement and test one or more of these super-\nresolution techniques.\nUnder favorable conditions, super-resolution and related upsampling techniques can in-\ncrease the resolution of a well-photographed image or image collection. When the input\nimages are blurry to start with, the best one can often hope for is to reduce the amount of blur.\nThis problem is closely related super-resolution, with the biggest differences being that the\nblur kernel b is usually much larger and the downsampling factor D is unity. A large literature\non image deblurring exists; some of the more recent publications with nice literature reviews\ninclude those by Fergus, Singh, Hertzmann et al. (2006), Yuan, Sun, Quan et al. (2008), and\nJoshi, Zitnick, Szeliski et al. (2009). It is also possible to reduce blur by combining sharp (but\nnoisy) images with blurrier (but cleaner) images (Yuan, Sun, Quan et al. 2007), take lots of\nquick exposures20 (Hasinoff and Kutulakos 2008; Hasinoff, Kutulakos, Durand et al. 2009;\nHasinoff, Durand, and Freeman 2010), or use coded aperture techniques to simultaneously\n19 For face super-resolution, where all the images are pre-aligned, only corresponding pixels in different images\nare examined.\n20 The SONY DSC-WX1 takes multiple shots to produce better low-light photos.",
  "524": "502\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.33 Recognition-based super-resolution (Baker and Kanade 2002) c⃝2002 IEEE.\nThe Hallucinated column shows the results of the recognition-based algorithm compared to\nthe regularization-based approach of Hardie, Barnard, and Armstrong (1997).\nestimate depth and reduce blur (Levin, Fergus, Durand et al. 2007; Zhou, Lin, and Nayar\n2009).\n10.3.1 Color image demosaicing\nA special case of super-resolution, which is used daily in most digital still cameras, is the\nprocess of demosaicing samples from a color ﬁlter array (CFA) into a full-color RGB image.\nFigure 10.34 shows the most commonly used CFA known as the Bayer pattern, which has\ntwice as many green (G) sensors as red and blue sensors.\nThe process of going from the known CFA pixels values to the full RGB image is quite\nchallenging. Unlike regular super-resolution, where small errors in guessing unknown values\nusually show up as blur or aliasing, demosaicing artifacts often produce spurious colors or\nhigh-frequency patterned zippering, which are quite visible to the eye (Figure 10.35b).\nOver the years, a variety of techniques have been developed for image demosaicing (Kim-\nmel 1999). Bennett, Uyttendaele, Zitnick et al. (2006) present a recently developed algorithm\nalong with some good references, while Longere, Delahunt, Zhang et al. (2002) and Tappen,\nRussell, and Freeman (2003) compare some previously developed techniques using percep-\ntually motivated metrics. To reduce the zippering effect, most techniques use the edge or",
  "525": "10.3 Super-resolution and blur removal\n503\n(a)\n(b)\nrgB\nrGb\nrgB\nrGb\nrGb\nRgb\nrGb\nRgb\nrgB\nrGb\nrgB\nrGb\nrGb\nRgb\nrGb\nRgb\nB\nG\nB\nG\nG\nR\nG\nR\nG\nB\nG\nR\nG\nR\nB\nG\nFigure 10.34 Bayer RGB pattern: (a) color ﬁlter array layout; (b) interpolated pixel values,\nwith unknown (guessed) values shown as lower case.\n(a)\n(b)\n(c)\n(d)\nFigure 10.35 CFA demosaicing results (Bennett, Uyttendaele, Zitnick et al. 2006) c⃝2006\nSpringer: (a) original full-resolution image (a color subsampled version is used as the input\nto the algorithms); (b) bilinear interpolation results, showing color fringing near the tip of the\nblue crayon and zippering near its left (vertical) edge; (c) the high-quality linear interpolation\nresults of Malvar, He, and Cutler (2004) (note the strong halo/checkerboard artifacts on the\nyellow crayon); (d) using the local two-color prior of Bennett, Uyttendaele, Zitnick et al.\n(2006).",
  "526": "504\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.36\nTwo-color model computed from a collection of local 5 × 5 neighborhoods\n(Bennett, Uyttendaele, Zitnick et al. 2006) c⃝2006 Springer. After two-means clustering\nand reprojection along the line joining the two dominant colors (red dots), the majority of the\npixels fall near the ﬁtted line. The distribution along the line, projected along the RGB axes,\nis peaked at 0 and 1, the two dominant colors.\ngradient information from the green channel, which is more reliable because it is sampled\nmore densely, to infer plausible values for the red and blue channels, which are more sparsely\nsampled.\nTo reduce color fringing, some techniques perform a color space analysis, e.g., using\nmedian ﬁltering on color opponent channels (Longere, Delahunt, Zhang et al. 2002). The\napproach of Bennett, Uyttendaele, Zitnick et al. (2006) locally forms a two-color model from\nan initial demosaicing result, using a moving 5 × 5 window to ﬁnd the two dominant colors\n(Figure 10.36).21\nOnce the local color model has been estimated at each pixel, a Bayesian approach is\nthen used to encourage pixel values to lie along each color line and to cluster around the\ndominant color values, which reduces halos (Figure 10.35d). The Bayesian approach also\nsupports the simultaneous application of demosaicing and super-resolution, i.e., multiple CFA\ninputs can be merged into a higher-quality full-color image, which becomes more important\nas additional processing becomes incorporated into today’s cameras.\n10.3.2 Application: Colorization\nAlthough not strictly an example of super-resolution, the process of colorization, i.e., manu-\nally adding colors to a “black and white” (grayscale) image, is another example of a sparse\ninterpolation problem. In most applications of colorization, the user draws some scribbles in-\ndicating the desired colors in certain regions (Figure 10.37a) and the system interpolates the\n21 Previous work on locally linear color models (Klinker, Shafer, and Kanade 1990; Omer and Werman 2004)\nfocuses on color and illumination variation within a single material, whereas Bennett, Uyttendaele, Zitnick et al.\n(2006) use the two-color model to describe variations across color (material) edges.",
  "527": "10.4 Image matting and compositing\n505\n(a)\n(b)\n(c)\nFigure 10.37 Colorization using optimization (Levin, Lischinski, and Weiss 2004) c⃝2004\nACM: (a) grayscale image some color scribbles overlaid; (b) resulting colorized image; (c)\noriginal color image from which the grayscale image and the chrominance values for the\nscribbles were derived. Original photograph by Rotem Weiss.\nspeciﬁed chrominance (u, v) values to the whole image, which are then re-combined with the\ninput luminance channel to produce a ﬁnal colorized image, as shown in Figure 10.37b. In the\nsystem developed by Levin, Lischinski, and Weiss (2004), the interpolation is performed us-\ning locally weighted regularization (3.100), where the local smoothness weights are inversely\nproportional to luminance gradients. This approach to locally weighted regularization has\ninspired later algorithms for high dynamic range tone mapping (Lischinski, Farbman, Uyt-\ntendaele et al. 2006a), see Section 10.2.1, as well as other applications of the weighted least\nsquares (WLS) formulation (Farbman, Fattal, Lischinski et al. 2008). An alternative approach\nto performing the sparse chrominance interpolation based on geodesic (edge-aware) distance\nfunctions has been developed by Yatziv and Sapiro (2006).\n10.4 Image matting and compositing\nImage matting and compositing is the process of cutting a foreground object out of one image\nand pasting it against a new background (Smith and Blinn 1996; Wang and Cohen 2007a).\nIt is commonly used in television and ﬁlm production to composite a live actor in front of\ncomputer-generated imagery such as weather maps or 3D virtual characters and scenery\n(Wright 2006; Brinkmann 2008).\nWe have already seen a number of tools for interactively segmenting objects in an image,\nincluding snakes (Section 5.1.1), scissors (Section 5.1.3), and GrabCut segmentation (Sec-\ntion 5.5). While these techniques can generate reasonable pixel-accurate segmentations, they\nfail to capture the subtle interplay of foreground and background colors at mixed pixels along\nthe boundary (Szeliski and Golland 1999) (Figure 10.38a).\nIn order to successfully copy a foreground object from one image to another without",
  "528": "506\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.38\nSoftening a hard segmentation boundary (border matting) (Rother, Kol-\nmogorov, and Blake 2004) c⃝2004 ACM: (a) the region surrounding a segmentation bound-\nary where pixels of mixed foreground and background colors are visible; (b) pixel values\nalong the boundary are used to compute a soft alpha matte; (c) at each point along the curve\nt, a displacement ∆and a width σ are estimated.\nvisible discretization artifacts, we need to pull a matte, i.e., to estimate a soft opacity channel\nα and the uncontaminated foreground colors F from the input composite image C. Recall\nfrom Section 3.1.3 (Figure 3.4) that the compositing equation (3.8) can be written as\nC = (1 −α)B + αF.\n(10.30)\nThis operator attenuates the inﬂuence of the background image B by a factor (1 −α) and\nthen adds in the (partial) color values corresponding to the foreground element F.\nWhile the compositing operation is easy to implement, the reverse matting operation of\nestimating F, α, and B given an input image C is much more challenging (Figure 10.39).\nTo see why, observe that while the composite pixel color C provides three measurements,\nthe F, α, and B unknowns have a total of seven degrees of freedom. Devising techniques to\nestimate these unknowns despite the underconstrained nature of the problem is the essence of\nimage matting.\nIn this section, we review a number of image matting techniques. We begin with blue\nscreen matting, which assumes that the background is a constant known color, and discuss its\nvariants, two-screen matting (when multiple backgrounds can be used) and difference matting\n(where the known background is arbitrary). We then discuss local variants of natural image\nmatting, where both the foreground and background are unknown. In these applications, it is\nusual to ﬁrst specify a trimap, i.e., a three-way labeling of the image into foreground, back-\nground, and unknown regions (Figure 10.39b). Next, we present some global optimization\napproaches to natural image matting. Finally, we discuss variants on the matting problem,\nincluding shadow matting, ﬂash matting, and environment matting.",
  "529": "10.4 Image matting and compositing\n507\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 10.39 Natural image matting (Chuang, Curless, Salesin et al. 2001) c⃝2001 IEEE:\n(a) input image with a “natural” (non-constant) background; (b) hand-drawn trimap—gray\nindicates unknown regions; (c) extracted alpha map; (d) extracted (premultiplied) foreground\ncolors; (e) composite over a new background.\n10.4.1 Blue screen matting\nBlue screen matting involves ﬁlming an actor (or object) in front of a constant colored back-\nground. While originally bright blue was the preferred color, bright green is now more com-\nmonly used (Wright 2006; Brinkmann 2008). Smith and Blinn (1996) discuss a number of\ntechniques for blue screen matting, which are mostly described in patents rather than in the\nopen research literature. Early techniques used linear combination of object color channels\nwith user-tuned parameters to estimate the opacity α.\nChuang, Curless, Salesin et al. (2001) describe a newer technique called Mishima’s al-\ngorithm, which involves ﬁtting two polyhedral surfaces (centered at the mean background\ncolor), separating the foreground and background color distributions and then measuring the\nrelative distance of a novel color to these surfaces to estimate α (Figure 10.41e). While this\ntechnique works well in many studio settings, it can still suffer from blue spill, where translu-\ncent pixels around the edges of an object acquire some of the background blue coloration\n(Figure 10.40).\nTwo-screen matting.\nIn their paper, Smith and Blinn (1996) also introduce an algorithm\ncalled triangulation matting that uses more than one known background color to over-constrain\nthe equations required to estimate the opacity α and foreground color F.",
  "530": "508\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.40\nBlue-screen matting results (Chuang, Curless, Salesin et al. 2001) c⃝2001\nIEEE. Mishima’s method produces visible blue spill (color fringing in the hair), while\nChuang’s Bayesian matting approach produces accurate results.\nFor example, consider in the compositing equation (10.30) setting the background color\nto black, i.e., B = 0. The resulting composite image C is therefore equal to αF. Replacing\nthe background color with a different known non-zero value B now results in\nC −αF = (1 −α)B,\n(10.31)\nwhich is an overconstrained set of (color) equations for estimating α. In practice, B should\nbe chosen so as not to saturate C and, for best accuracy, several values of B should be used.\nIt is also important that colors be linearized before processing, which is the case for all image\nmatting algorithms. Papers that generate ground truth alpha mattes for evaluation purposes\nnormally use these techniques to obtain accurate matte estimates (Chuang, Curless, Salesin\net al. 2001; Wang and Cohen 2007b; Levin, Acha, and Lischinski 2008; Rhemann, Rother,\nRav-Acha et al. 2008; Rhemann, Rother, Wang et al. 2009).22 Exercise 10.8 has you do this\nas well.\n22 See the alpha matting evaluation Web site at http://alphamatting.com/.",
  "531": "10.4 Image matting and compositing\n509\nDifference matting.\nA related approach when the background is irregular but known is\ncalled difference matting (Wright 2006; Brinkmann 2008). It is most commonly used when\nthe actor or object is ﬁlmed against a static background, e.g., for ofﬁce videoconferencing,\nperson tracking applications (Toyama, Krumm, Brumitt et al. 1999), or to produce silhou-\nettes for volumetric 3D reconstruction techniques (Section 11.6.2) (Szeliski 1993; Seitz and\nDyer 1997; Seitz, Curless, Diebel et al. 2006). It can also be used with a panning camera\nwhere the background is composited from frames where the foreground has been removed\nusing a garbage matte (Section 10.4.5) (Chuang, Agarwala, Curless et al. 2002). Another\nrecent application is the detection of visual continuity errors in ﬁlms, i.e., differences in the\nbackground when a shot is re-taken at later time (Pickup and Zisserman 2009).\nIn the case where the foreground and background motions can both be speciﬁed with\nparametric transforms, high-quality mattes can be extracted using a generalization of triangu-\nlation matting (Wexler, Fitzgibbon, and Zisserman 2002). When frames need to be processed\nindependently, however, the results are often of poor quality (Figure 10.42). In such cases,\nusing a pair of stereo cameras as input can dramatically improve the quality of the results\n(Criminisi, Cross, Blake et al. 2006; Yin, Criminisi, Winn et al. 2007).\n10.4.2 Natural image matting\nThe most general version of image matting is when nothing is known about the background\nexcept, perhaps, for a rough segmentation of the scene into foreground, background, and\nunknown regions, which is known as the trimap (Figure 10.39b). Some recent techniques,\nhowever, relax this requirement and allow the user to just draw a few strokes or scribbles in\nthe image, see Figures 10.45 and 10.46 (Wang and Cohen 2005; Wang, Agrawala, and Cohen\n2007; Levin, Lischinski, and Weiss 2008; Rhemann, Rother, Rav-Acha et al. 2008; Rhemann,\nRother, and Gelautz 2008). Fully automated single image matting results have also been\nreported (Levin, Acha, and Lischinski 2008; Singaraju, Rother, and Rhemann 2009). The\nsurvey paper by Wang and Cohen (2007a) has detailed descriptions and comparisons of all of\nthese techniques, a selection of which are described brieﬂy below.\nA relatively simple algorithm for performing natural image matting is Knockout, as de-\nscribed by Chuang, Curless, Salesin et al. (2001) and illustrated in Figure 10.41f. In this\nalgorithm, the nearest known foreground and background pixels (in image space) are deter-\nmined and then blended with neighboring known pixels to produce a per-pixel foreground F\nand background B color estimate. The background color is then adjusted so that the measured\ncolor C lies on the line between F and B. Finally, opacity α is estimated on a per-channel\nbasis, and the three estimates are combined based on per-channel color differences. (This is\nan approximation to the least squares solution for α.) Figure 10.42 shows that Knockout has\nproblems when the background consists of more than one dominant local color.",
  "532": "510\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMishima\nKnockout\nRuzon–Tomasi\nBayesian\nFigure 10.41\nImage matting algorithms (Chuang, Curless, Salesin et al. 2001) c⃝2001\nIEEE. Mishima’s algorithm models global foreground and background color distribution as\npolyhedral surfaces centered around the mean background (blue) color. Knockout uses a lo-\ncal color estimate of foreground and background for each pixel and computes α along each\ncolor axis. Ruzon and Tomasi’s algorithm locally models foreground and background colors\nand variances. Chuang et al.’s Bayesian matting approach computes a MAP estimate of (frac-\ntional) foreground color and opacity given the local foreground and background distributions.\nMore accurate matting results can be obtained if we treat the foreground and background\ncolors as distributions sampled over some region (Figure 10.41g–h). Ruzon and Tomasi\n(2000) model local color distributions as mixtures of (uncorrelated) Gaussians and compute\nthese models in strips. They then ﬁnd the pairing of mixture components F and B that best\ndescribes the observed color C, compute the α as the relative distance between these means,\nand adjust the estimates of F and B so they are collinear with C.\nChuang, Curless, Salesin et al. (2001) and Hillman, Hannah, and Renshaw (2001) use\nfull 3 × 3 color covariance matrices to model mixtures of correlated Gaussians, and compute\nestimates independently for each pixel. Matte extraction proceeds in strips starting from\nknown color values growing into the unknown regions, so that recently computed F and B\ncolors can be used in later stages.\nTo estimate the most likely value of an unknown pixel’s opacity and (unmixed) foreground\nand background colors, Chuang et al. use a fully Bayesian formulation that maximizes\nP(F, B, α|C) = P(C|F, B, α)P(F)P(B)P(α)/P(C).\n(10.32)",
  "533": "10.4 Image matting and compositing\n511\nFigure 10.42 Natural image matting results (Chuang, Curless, Salesin et al. 2001) c⃝2001\nIEEE. Difference matting and Knockout both perform poorly on this kind of background,\nwhile the more recent natural image matting techniques perform well. Chuang et al.’s results\nare slightly smoother and closer to the ground truth.",
  "534": "512\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis is equivalent to minimizing the negative log likelihood\nL(F, B, α|C) = L(C|F, B, α) + L(F) + L(B) + L(α)\n(10.33)\n(dropping the L(C) term since it is constant).\nLet us examine each of these terms in turn. The ﬁrst, L(C|F, B, α), is the likelihood that\npixel color C was observed given values for the unknowns (F, B, α). If we assume Gaussian\nnoise in our observation with variance σ2\nC, this negative log likelihood (data term) is\nL(C) = 1/2∥C −[αF + (1 −α)B]∥2/σ2\nC,\n(10.34)\nas illustrated in Figure 10.41h.\nThe second term, L(F), corresponds to the likelihood that a particular foreground color F\ncomes from the mixture of Gaussians distribution. After partitioning the sample foreground\ncolors into clusters, a weighted mean and covariance is computed, where the weights are\nproportional to a given foreground pixel’s opacity and distance from the unknown pixel. The\nnegative log likelihood for each cluster is thus given by\nL(F) = (F −F)T Σ−1\nF (F −F).\n(10.35)\nA similar method is used to estimate unknown background color distributions. If the back-\nground is already known, i.e., for blue screen or difference matting applications, its measured\ncolor value and variance are used instead.\nAn alternative to modeling the foreground and background color distributions as mixtures\nof Gaussians is to keep around the original color samples and to compute the most likely\npairings that explain the observed color C (Wang and Cohen 2005, 2007b). These techniques\nare described in more detail in (Wang and Cohen 2007a).\nIn their Bayesian matting paper, Chuang, Curless, Salesin et al. (2001) assume a constant\n(non-informative) distribution for L(α). More recent papers assume this distribution to be\nmore peaked around 0 and 1, or sometimes use Markov random ﬁelds (MRFs) to deﬁne a\nglobal correlated prior on P(α) (Wang and Cohen 2007a).\nTo compute the most likely estimates for (F, B, α), the Bayesian matting algorithm alter-\nnates between computing (F, B) and α, since each of these problems is quadratic and hence\ncan be solved as a small linear system. When several color clusters are estimated, the most\nlikely pairing of foreground and background color clusters is used.\nBayesian image matting produces results that improve on the original natural image mat-\nting algorithm by Ruzon and Tomasi (2000), as can be seen in Figure 10.42. However, com-\npared to more recent techniques (Wang and Cohen 2007a), its performance is not as good for\ncomplex background or inaccurate trimaps (Figure 10.44).",
  "535": "10.4 Image matting and compositing\n513\n10.4.3 Optimization-based matting\nAn alternative to estimating each pixel’s opacity and foreground color independently is to use\nglobal optimization to compute a matte that takes into account correlations between neigh-\nboring α values. Two examples of this are border matting in the GrabCut interactive segmen-\ntation system (Rother, Kolmogorov, and Blake 2004) and Poisson Matting (Sun, Jia, Tang et\nal. 2004).\nBorder matting ﬁrst dilates the region around the binary segmentation produced by Grab-\nCut (Section 5.5) and then solves for a sub-pixel boundary location ∆and a blur width σ for\nevery point along the boundary (Figure 10.38). Smoothness in these parameters along the\nboundary is enforced using regularization and the optimization is performed using dynamic\nprogramming. While this technique can obtain good results for smooth boundaries, such as a\nperson’s face, it has difﬁculty with ﬁne details, such as hair.\nPoisson matting (Sun, Jia, Tang et al. 2004) assumes a known foreground and background\ncolor for each pixel in the trimap (as with Bayesian matting). However, instead of indepen-\ndently estimating each α value, it assumes that the gradient of the alpha matte and the gradient\nof the color image are related by\n∇α =\nF −B\n∥F −B∥2 · ∇C,\n(10.36)\nwhich can be derived by taking gradients of both sides of (10.30) and assuming that the\nforeground and background vary slowly. The per-pixel gradient estimates are then integrated\ninto a continuous α(x) ﬁeld using the regularization (least squares) technique ﬁrst described\nin Section 3.7.1 (3.100) and subsequently used in Poisson blending (Section 9.3.4, 9.44) and\ngradient-based dynamic range compression mapping (Section 10.2.1, 10.19). This technique\nworks well when good foreground and background color estimates are available and these\ncolors vary slowly.\nInstead of computing per-pixel foreground and background colors, Levin, Lischinski, and\nWeiss (2008) assume only that these color distribution can locally be well approximated as\nmixtures of two colors, which is known as the color line model (Figure 10.43a–c). Under this\nassumption, a closed-form estimate for α at each pixel i in a (say, 3 × 3) window Wk is given\nby\nαi = ak · (Ci −B0) = ak · C + bk,\n(10.37)\nwhere Ci is the pixel color treated as a three-vector, B0 is any pixel along the background\ncolor line, and ak is the vector joining the two closest points on the foreground and back-\nground color lines, as shown in Figure 10.43c. (Note that the geometric derivation shown\nin this ﬁgure is an alternative to the algebraic derivation presented by Levin, Lischinski, and\nWeiss (2008).) Minimizing the deviations of the alpha values αi from their respective color",
  "536": "514\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nak\nα=0\nα=0.5\nα=1\nB0\nCi\n(d)\nCi\nμk\nΣk\nCj\nFigure 10.43 Color line matting (Levin, Lischinski, and Weiss 2008): (a) local 3 × 3 patch\nof colors; (b) potential assignment of α values; (c) foreground and background color lines,\nthe vector ak joining their closest points of intersection, and the family of parallel planes of\nconstant α values, αi = ak ·(Ci −B0); (d) a scatter plot of sample colors and the deviations\nfrom the mean µk for two sample colors Ci and Cj.\nline models (10.37) over all overlapping windows Wk in the image gives rise to the cost\nEα =\nX\nk\n X\ni∈Wk\n(αi −ak · Ci −bk)2 + ϵ∥ak∥\n!\n,\n(10.38)\nwhere the ϵ term is used to regularize the value of ak in the case where the two color distri-\nbutions overlap (i.e., in constant α regions).\nBecause this formula is quadratic in the unknowns {(ak, bk)}, they can be eliminated\ninside each window Wk, leading to a ﬁnal energy\nEα = αT Lα,\n(10.39)\nwhere the entries in the L matrix are given by\nLij =\nX\nk:i∈Wk∧j∈Wk\n\u0012\nδij −1\nM\n\u0010\n1 + (Ci −µk)T ˆΣ\n−1\nk (Cj −µk)\n\u0011\u0013\n,\n(10.40)\nwhere M = |Wk| is the number of pixels in each (overlapping) window, µk is the mean color\nof the pixels in window Wk, and ˆΣk is the 3 × 3 covariance of the pixel colors plus ϵ/MI.\nFigure 10.43d shows the intuition behind the entries in this afﬁnity matrix, which is called\nthe matting Laplacian. Note how when two pixels Ci and Cj in Wk point in opposite direc-\ntions away from the mean µk, their weighted dot product is close to −1, and so their afﬁnity\nbecomes close to 0. Pixels close to each other in color space (and hence with similar expected\nα values) will have afﬁnities close to −2/M.\nMinimizing the quadratic energy (10.39) constrained by the known values of α = {0, 1}\nat scribbles only requires the solution of a sparse set of linear equations, which is why the",
  "537": "10.4 Image matting and compositing\n515\nFigure 10.44 Comparative matting results for a medium accuracy trimap. Wang and Cohen\n(2007a) describe the individual techniques being compared.\nauthors call their technique a closed-form solution to natural image matting. Once α has\nbeen computed, the foreground and background colors are estimated using a least squares\nminimization of the compositing equation (10.30) regularized with a spatially varying ﬁrst-\norder smoothness,\nEB,F =\nX\ni\n∥Ci −[α + Fi + (1 −αi)Bi]∥2 + λ|∇αi|(∥∇Fi∥2 + ∥∇Bi∥2),\n(10.41)\nwhere the |∇αi| weight is applied separately for the x and y components of the F and B\nderivatives (Levin, Lischinski, and Weiss 2008).\nLaplacian (closed-form) matting is just one of many optimization-based techniques sur-\nveyed and compared by Wang and Cohen (2007a). Some of these techniques use alternative\nformulations for the afﬁnities or smoothness terms on the α matte, alternative estimation\ntechniques such as belief propagation, or alternative representations (e.g., local histograms)\nfor modeling local foreground and background color distributions (Wang and Cohen 2005,\n2007b,c). Some of these techniques also provide real-time results as the user draws a contour\nline or sparse set of scribbles (Wang, Agrawala, and Cohen 2007; Rhemann, Rother, Rav-\nAcha et al. 2008) or even pre-segment the image into a small number of mattes that the user\ncan select with simple clicks (Levin, Acha, and Lischinski 2008).\nFigure 10.44 shows the results of running a number of the surveyed algorithms on a region\nof toy animal fur where a trimap has been speciﬁed, while Figure 10.45 shows results for\ntechniques that can produce mattes with only a few scribbles as input. Figure 10.46 shows\na result for an even more recent algorithm (Rhemann, Rother, Rav-Acha et al. 2008) that\nclaims to outperform all of the techniques surveyed by Wang and Cohen (2007a).\nPasting.\nOnce a matte has been pulled from an image, it is usually composited directly\nover the new background, unless the seams between the cutout and background regions are",
  "538": "516\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 10.45\nComparative matting results with scribble-based inputs. Wang and Cohen\n(2007a) describe the individual techniques being compared.\nFigure 10.46\nStroke-based segmentation result (Rhemann, Rother, Rav-Acha et al. 2008)\nc⃝2008 IEEE.\nto be hidden, in which case Poisson blending (P´erez, Gangnet, and Blake 2003) can be used\n(Section 9.3.4).\nIn the latter case, it is helpful if the matte boundary passes through regions that either\nhave little texture or look similar in the old and new images. Papers by Jia, Sun, Tang et al.\n(2006) and Wang and Cohen (2007c) explain how to do this.\n10.4.4 Smoke, shadow, and ﬂash matting\nIn addition to matting out solid objects with fractional boundaries, it is also possible to matte\nout translucent media such as smoke (Chuang, Agarwala, Curless et al. 2002). Starting with\na video sequence, each pixel is modeled as a linear combination of its (unknown) background\ncolor and a constant foreground (smoke) color that is common to all pixels. Voting in color",
  "539": "10.4 Image matting and compositing\n517\n(a)\n(b)\n(c)\n(d)\nFigure 10.47\nSmoke matting (Chuang, Agarwala, Curless et al. 2002) c⃝2002 ACM: (a)\ninput video frame; (b) after removing the foreground object; (c) estimated alpha matte; (d)\ninsertion of new objects into the background.\nFigure 10.48 Shadow matting (Chuang, Goldman, Curless et al. 2003) c⃝2003 ACM. In-\nstead of simply darkening the new scene with the shadow (c), shadow matting correctly dims\nthe lit scene with the new shadow and drapes the shadow over 3D geometry (d).\nspace is used to estimate this foreground color and the distance along each color line is used\nto estimate the per-pixel temporally varying alpha (Figure 10.47).\nExtracting and re-inserting shadows is also possible using a related technique (Chuang,\nGoldman, Curless et al. 2003). Here, instead of assuming a constant foreground color, each\npixel is assumed to vary between its fully lit and fully shadowed colors, which can be esti-\nmated by taking (robust) minimum and maximum values over time as a shadow passes over\nthe scene (Exercise 10.9). The resulting fractional shadow matte can be used to re-project\nthe shadow into a new scene. If the destination scene has a non-planar geometry, it can be\nscanned by waving a straight stick shadow across the scene. The new shadow matte can then\nbe warped with the computed deformation ﬁeld to have it drape correctly over the new scene\n(Figure 10.48).\nThe quality and reliability of matting algorithms can also be enhanced using more sophis-\nticated acquisition systems. For example, taking a ﬂash and non-ﬂash image pair supports\nthe reliable extraction of foreground mattes, which show up as regions of large illumination\nchange between the two images (Sun, Li, Kang et al. 2006). Taking simultaneous video\nstreams focused at different distances (McGuire, Matusik, Pﬁster et al. 2005) or using multi-\ncamera arrays (Joshi, Matusik, and Avidan 2006) are also good approaches to producing",
  "540": "518\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nhigh-quality mattes. These techniques are described in more detail in (Wang and Cohen\n2007a).\nLastly, photographing a refractive object in front of a number of patterned backgrounds\nallows the object to be placed in novel 3D environments. These environment matting tech-\nniques (Zongker, Werner, Curless et al. 1999; Chuang, Zongker, Hindorff et al. 2000) are\ndiscussed in Section 13.4.\n10.4.5 Video matting\nWhile regular single-frame matting techniques such as blue or green screen matting (Smith\nand Blinn 1996; Wright 2006; Brinkmann 2008) can be applied to video sequences, the pres-\nence of moving objects can sometimes make the matting process easier, as portions of the\nbackground may get revealed in preceding or subsequent frames.\nChuang, Agarwala, Curless et al. (2002) describe a nice approach to this video matting\nproblem, where foreground objects are ﬁrst removed using a conservative garbage matte and\nthe resulting background plates are aligned and composited to yield a high-quality back-\nground estimate. They also describe how trimaps drawn at sparse keyframes can be inter-\npolated to in-between frames using bi-direction optic ﬂow. Alternative approaches to video\nmatting, such as rotoscoping, which involves drawing and tracking curves in video sequences\n(Agarwala, Hertzmann, Seitz et al. 2004), are discussed in the matting survey paper by Wang\nand Cohen (2007a).\n10.5 Texture analysis and synthesis\nWhile texture analysis and synthesis may not at ﬁrst seem like computational photography\ntechniques, they are, in fact, widely used to repair defects, such as small holes, in images or\nto create non-photorealistic painterly renderings from regular photographs.\nThe problem of texture synthesis can be formulated as follows: given a small sample of\na “texture” (Figure 10.49a), generate a larger similar-looking image (Figure 10.49b). As you\ncan imagine, for certain sample textures, this problem can be quite challenging.\nTraditional approaches to texture analysis and synthesis try to match the spectrum of the\nsource image while generating shaped noise. Matching the frequency characteristics, which\nis equivalent to matching spatial correlations, is in itself not sufﬁcient. The distributions of\nthe responses at different frequencies must also match. Heeger and Bergen (1995) develop an\nalgorithm that alternates between matching the histograms of multi-scale (steerable pyramid)\nresponses and matching the ﬁnal image histogram. Portilla and Simoncelli (2000) improve\non this technique by also matching pairwise statistics across scale and orientations. De Bonet\n(1997) uses a coarse-to-ﬁne strategy to ﬁnd locations in the source texture with a similar",
  "541": "10.5 Texture analysis and synthesis\n519\nradishes\nlots more radishes\nrocks\nyogurt\n(a)\n(b)\n(c)\nFigure 10.49 Texture synthesis: (a) given a small patch of texture, the task is to synthesize\n(b) a similar-looking larger patch; (c) other semi-structured textures that are challenging to\nsynthesize. (Images courtesy of Alyosha Efros.)\nparent structure, i.e., similar multi-scale oriented ﬁlter responses, and then randomly chooses\none of these matching locations as the current sample value.\nMore recent texture synthesis algorithms sequentially generate texture pixels by looking\nfor neighborhoods in the source texture that are similar to the currently synthesized image\n(Efros and Leung 1999). Consider the (as yet) unknown pixel p in the partially constructed\ntexture on the left side of Figure 10.50. Since some of its neighboring pixels have been\nalready been synthesized, we can look for similar partial neighborhoods in the sample texture\nimage on the right and randomly select one of these as the new value of p. This process\ncan be repeated down the new image either in a raster fashion or by scanning around the\nperiphery (“onion peeling”) when ﬁlling holes, as discussed in (Section 10.5.1). In their\nactual implementation, Efros and Leung (1999) ﬁnd the most similar neighborhood and then\ninclude all other neighborhoods within a d = (1 + ϵ) distance, with ϵ = 0.1. They also\noptionally weight the random pixel selections by the similarity metric d.\nTo accelerate this process and improve its visual quality, Wei and Levoy (2000) extend\nthis technique using a coarse-to-ﬁne generation process, where coarser levels of the pyramid,\nwhich have already been synthesized, are also considered during the matching (De Bonet\n1997). To accelerate the nearest neighbor ﬁnding, tree-structured vector quantization is used.\nEfros and Freeman (2001) propose an alternative acceleration and visual quality improve-",
  "542": "520\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np\nnon-parametric\nsampling\nInput image\nOutput image\nFigure 10.50\nTexture synthesis using non-parametric sampling (Efros and Leung 1999).\nThe value of the newest pixel p is randomly chosen from similar local (partial) patches in the\nsource texture (input image). (Figure courtesy of Alyosha Efros.)\np\nInput image\nnon-parametric\nsampling\nB\nOutput image\nB1\nB2\nMinimal error\nboundary cut\nFigure 10.51\nTexture synthesis by image quilting (Efros and Freeman 2001). Instead of\ngenerating a single pixel at a time, larger blocks are copied from the source texture. The tran-\nsitions in the overlap regions between the selected blocks are then optimized using dynamic\nprogramming. (Figure courtesy of Alyosha Efros.)\nment technique. Instead of synthesizing a single pixel at a time, overlapping square blocks\nare selected using similarity with previously synthesized regions (Figure 10.51). Once the\nappropriate blocks have been selected, the seam between newly overlapping blocks is deter-\nmined using dynamic programming. (Full graph cut seam selection is not required, since only\none seam location per row is needed for a vertical boundary.) Because this process involves\nselecting small patches and them stitching them together, Efros and Freeman (2001) call their\nsystem image quilting. Komodakis and Tziritas (2007b) present an MRF-based version of\nthis block synthesis algorithm that uses a new, efﬁcient version of loopy belief propagation\nthey call “Priority-BP”.",
  "543": "10.5 Texture analysis and synthesis\n521\n(a)\n(b)\n(c)\n(d)\nFigure 10.52\nImage inpainting (hole ﬁlling): (a–b) propagation along isophote directions\n(Bertalmio, Sapiro, Caselles et al. 2000) c⃝2000 ACM; (c–d) exemplar-based inpainting\nwith conﬁdence-based ﬁlling order (Criminisi, P´erez, and Toyama 2004).\n10.5.1 Application: Hole ﬁlling and inpainting\nFilling holes left behind when objects or defects are excised from photographs, which is\nknown as inpainting, is one of the most common applications of texture synthesis. Such\ntechniques are used not only to remove unwanted people or interlopers from photographs\n(King 1997) but also to ﬁx small defects in old photos and movies (scratch removal) or to\nremove wires holding props or actors in mid-air during ﬁlming (wire removal). Bertalmio,\nSapiro, Caselles et al. (2000) solve the problem by propagating pixel values along isophote\n(constant-value) directions interleaved with some anisotropic diffusion steps (Figure 10.52a–\nb). Telea (2004) develops a faster technique that uses the fast marching method from level\nsets (Section 5.1.4). However, these techniques will not hallucinate texture in the missing\nregions. Bertalmio, Vese, Sapiro et al. (2003) augment their earlier technique by adding\nsynthetic texture to the inﬁlled regions.\nThe example-based (non-parametric) texture generation techniques discussed in the pre-\nvious section can also be used by ﬁlling the holes from the outside in (the “onion-peel” or-\ndering). However, this approach may fail to propagate strong oriented structures. Criminisi,\nP´erez, and Toyama (2004) use exemplar-based texture synthesis where the order of synthesis\nis determined by the strength of the gradient along the region boundary (Figures 10.1d and\n10.52c–d). Sun, Yuan, Jia et al. (2004) present a related approach where the user draws in-\nteractive lines to indicate where structures should be preferentially propagated. Additional\ntechniques related to these approaches include those developed by Drori, Cohen-Or, and\nYeshurun (2003), Kwatra, Sch¨odl, Essa et al. (2003), Kwatra, Essa, Bobick et al. (2005),\nWilczkowiak, Brostow, Tordoff et al. (2005), Komodakis and Tziritas (2007b), and Wexler,\nShechtman, and Irani (2007).\nMost hole ﬁlling algorithms borrow small pieces of the original image to ﬁll in the holes.\nWhen a large database of source images is available, e.g., when images are taken from a",
  "544": "522\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 10.53 Texture transfer (Efros and Freeman 2001) c⃝2001 ACM: (a) reference (tar-\nget) image; (b) source texture; (c) image (partially) rendered using the texture.\nphoto sharing site or the Internet, it is sometimes possible to copy a single contiguous image\nregion to ﬁll the hole. Hays and Efros (2007) present such a technique, which uses image\ncontext and boundary compatibility to select the source image, which is then blended with\nthe original (holey) image using graph cuts and Poisson blending. This technique is discussed\nin more detail in Section 14.4.4 and Figure 14.46.\n10.5.2 Application: Non-photorealistic rendering\nTwo more applications of the exemplar-based texture synthesis ideas are texture transfer\n(Efros and Freeman 2001) and image analogies (Hertzmann, Jacobs, Oliver et al. 2001),\nwhich are both examples of non-photorealistic rendering (Gooch and Gooch 2001).\nIn addition to using a source texture image, texture transfer also takes a reference (or\ntarget) image, and tries to match certain characteristics of the target image with the newly\nsynthesized image. For example, the new image being rendered in Figure 10.53c not only\ntries to satisfy the usual similarity constraints with the source texture in Figure 10.53b, but it\nalso tries to match the luminance characteristics of the reference image. Efros and Freeman\n(2001) mention that blurred image intensities or local image orientation angles are alternative\nquantities that could be matched.\nHertzmann, Jacobs, Oliver et al. (2001) formulate the following problem:\nGiven a pair of images A and A′ (the unﬁltered and ﬁltered source images, re-\nspectively), along with some additional unﬁltered target image B, synthesize a\nnew ﬁltered target image B′ such that\nA : A′ :: B : B′.",
  "545": "10.5 Texture analysis and synthesis\n523\nA\nA′\nB\nB′\nFigure 10.54 Image analogies (Hertzmann, Jacobs, Oliver et al. 2001) c⃝2001 ACM. Given\nan example pair of a source image A and its rendered (ﬁltered) version A′, generate the\nrendered version B′ from another unﬁltered source image B.\nInstead of having the user program a certain non-photorealistic rendering effect, it is sufﬁcient\nto supply the system with examples of before and after images, and let the system synthesize\nthe novel image using exemplar-based synthesis, as shown in Figure 10.54.\nThe algorithm used to solve image analogies proceeds in a manner analogous to the tex-\nture synthesis algorithms of (Efros and Leung 1999; Wei and Levoy 2000). Once Gaus-\nsian pyramids have been computed for all of the source and reference images, the algorithm\nlooks for neighborhoods in the source ﬁltered pyramids generated from A′ that are simi-\nlar to the partially constructed neighborhood in B′, while at the same time having similar\nmulti-resolution appearances at corresponding locations in A and B. As with texture trans-\nfer, appearance characteristics can include not only (blurred) color or luminance values but\nalso orientations.\nThis general framework allows image analogies to be applied to a variety of rendering\ntasks. In addition to exemplar-based non-photorealistic rendering, image analogies can be\nused for traditional texture synthesis, super-resolution, and texture transfer (using the same\ntextured image for both A and A′). If only the ﬁltered (rendered) image A′ is available, as\nis the case with paintings, the missing reference image A can be hallucinated using a smart\n(edge preserving) blur operator. Finally, it is possible to train a system to perform texture-by-\nnumbers by manually painting over a natural image with pseudocolors corresponding to pix-\nels’ semantic meanings, e.g., water, trees, and grass (Figure 10.55a–b). The resulting system\ncan then convert a novel sketch into a fully rendered synthetic photograph (Figure 10.55c–d).\nIn more recent work, Cheng, Vishwanathan, and Zhang (2008) add ideas from image quilting\n(Efros and Freeman 2001) and MRF inference (Komodakis, Tziritas, and Paragios 2008) to\nthe basic image analogies algorithm, while Ramanarayanan and Bala (2007) recast this pro-\ncess as energy minimization, which means it can also be viewed as a conditional random ﬁeld\n(Section 3.7.2), and devise an efﬁcient algorithm to ﬁnd a good minimum.\nMore traditional ﬁltering and feature detection techniques can also be used for non-",
  "546": "524\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nOriginal A′\nPainted A\nNovel painted B\nNovel textured B′\nFigure 10.55 Texture-by-numbers (Hertzmann, Jacobs, Oliver et al. 2001) c⃝2001 ACM.\nGiven a textured image A′ and a hand-labeled (painted) version A, synthesize a new image\nB′ given just the painted version B.\n(a)\n(b)\nFigure 10.56\nNon-photorealistic abstraction of photographs: (a) DeCarlo and Santella\n(2002) c⃝2002 ACM and (b) Farbman, Fattal, Lischinski et al. (2008) c⃝2008 ACM.\nphotorealistic rendering.23 For example, pen-and-ink illustration (Winkenbach and Salesin\n1994) and painterly rendering techniques (Litwinowicz 1997) use local color, intensity, and\norientation estimates as an input to their procedural rendering algorithms. Techniques for\nstylizing and simplifying photographs and video (DeCarlo and Santella 2002; Winnem¨oller,\nOlsen, and Gooch 2006; Farbman, Fattal, Lischinski et al. 2008), as in Figure 10.56, use\ncombinations of edge-preserving blurring (Section 3.3.1) and edge detection and enhance-\nment (Section 4.2.3).\n10.6 Additional reading\nA good overview of computational photography can be found in the book by Raskar and\nTumblin (2010), survey articles by Nayar (2006), Cohen and Szeliski (2006), Levoy (2006),\n23 For a good selection of papers, see the Symposia on Non-Photorealistic Animation and Rendering (NPAR) at\nhttp://www.npar.org/.",
  "547": "10.6 Additional reading\n525\nDebevec (2006), and Hayes (2008), as well as two special journal issues edited by Bimber\n(2006) and Durand and Szeliski (2007). Notes from the courses on computational photog-\nraphy mentioned at the beginning of this chapter are another great source of material and\nreferences.24\nThe sub-ﬁeld of high dynamic range imaging has its own book discussing research in this\narea (Reinhard, Ward, Pattanaik et al. 2005), as well as some books describing related pho-\ntographic techniques (Freeman 2008; Gulbins and Gulbins 2009). Algorithms for calibrating\nthe radiometric response function of a camera can be found in articles by Mann and Picard\n(1995), Debevec and Malik (1997), and Mitsunaga and Nayar (1999).\nThe subject of tone mapping is treated extensively in (Reinhard, Ward, Pattanaik et al.\n2005). Representative papers from the large volume of literature on this topic include those\nby Tumblin and Rushmeier (1993), Larson, Rushmeier, and Piatko (1997), Pattanaik, Ferw-\nerda, Fairchild et al. (1998), Tumblin and Turk (1999), Durand and Dorsey (2002), Fattal,\nLischinski, and Werman (2002), Reinhard, Stark, Shirley et al. (2002), Lischinski, Farbman,\nUyttendaele et al. (2006b), and Farbman, Fattal, Lischinski et al. (2008).\nThe literature on super-resolution is quite extensive (Chaudhuri 2001; Park, Park, and\nKang 2003; Capel and Zisserman 2003; Capel 2004; van Ouwerkerk 2006). The term super-\nresolution usually describes techniques for aligning and merging multiple images to produce\nhigher-resolution composites (Keren, Peleg, and Brada 1988; Irani and Peleg 1991; Cheese-\nman, Kanefsky, Hanson et al. 1993; Mann and Picard 1994; Chiang and Boult 1996; Bascle,\nBlake, and Zisserman 1996; Capel and Zisserman 1998; Smelyanskiy, Cheeseman, Maluf et\nal. 2000; Capel and Zisserman 2000; Pickup, Capel, Roberts et al. 2009; Gulbins and Gul-\nbins 2009). However, single-image super-resolution techniques have also been developed\n(Freeman, Jones, and Pasztor 2002; Baker and Kanade 2002; Fattal 2007).\nA good survey on image matting is given by Wang and Cohen (2007a). Representative\npapers, which include extensive comparisons with previous work, include those by Chuang,\nCurless, Salesin et al. (2001), Wang and Cohen (2007b), Levin, Acha, and Lischinski (2008),\nRhemann, Rother, Rav-Acha et al. (2008), and Rhemann, Rother, Wang et al. (2009).\nThe literature on texture synthesis and hole ﬁlling includes traditional approaches to tex-\nture synthesis, which try to match image statistics between source and destination images\n(Heeger and Bergen 1995; De Bonet 1997; Portilla and Simoncelli 2000), as well as newer\napproaches, which search for matching neighborhoods or patches inside the source sample\n(Efros and Leung 1999; Wei and Levoy 2000; Efros and Freeman 2001). In a similar vein,\ntraditional approaches to hole ﬁlling involve the solution of local variational (smooth continu-\nation) problems (Bertalmio, Sapiro, Caselles et al. 2000; Bertalmio, Vese, Sapiro et al. 2003;\n24 MIT 6.815/6.865, http://stellar.mit.edu/S/course/6/sp08/6.815/materials.html, CMU 15-463, http://graphics.cs.\ncmu.edu/courses/15-463/2008 fall/, Stanford CS 448A, http://graphics.stanford.edu/courses/cs448a-08-spring/, and\nSIGGRAPH courses, http://web.media.mit.edu/∼raskar/photo/.",
  "548": "526\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTelea 2004). More recent techniques use data-driven texture synthesis approaches (Drori,\nCohen-Or, and Yeshurun 2003; Kwatra, Sch¨odl, Essa et al. 2003; Criminisi, P´erez, and\nToyama 2004; Sun, Yuan, Jia et al. 2004; Kwatra, Essa, Bobick et al. 2005; Wilczkowiak,\nBrostow, Tordoff et al. 2005; Komodakis and Tziritas 2007b; Wexler, Shechtman, and Irani\n2007).\n10.7 Exercises\nEx 10.1: Radiometric calibration\nImplement one of the multi-exposure radiometric cali-\nbration algorithms described in Section 10.2 (Debevec and Malik 1997; Mitsunaga and Nayar\n1999; Reinhard, Ward, Pattanaik et al. 2005). This calibration will be useful in a number of\ndifferent applications, such as stitching images or stereo matching with different exposures\nand shape from shading.\n1. Take a series of bracketed images with your camera on a tripod. If your camera has\nan automatic exposure bracketing (AEB) mode, taking three images may be sufﬁcient\nto calibrate most of your camera’s dynamic range, especially if your scene has a lot of\nbright and dark regions. (Shooting outdoors or through a window on a sunny day is\nbest.)\n2. If your images are not taken on a tripod, ﬁrst perform a global alignment (similarity\ntransform).\n3. Estimate the radiometric response function using one of the techniques cited above.\n4. Estimate the high dynamic range radiance image by selecting or blending pixels from\ndifferent exposures (Debevec and Malik 1997; Mitsunaga and Nayar 1999; Eden, Uyt-\ntendaele, and Szeliski 2006).\n5. Repeat your calibration experiments under different conditions, e.g., indoors under in-\ncandescent light, to get a sense for the range of color balancing effects that your camera\nimposes.\n6. If your camera supports RAW and JPEG mode, calibrate both sets of images simulta-\nneously and to each other (the radiance at each pixel will correspond). See if you can\ncome up with a model for what your camera does, e.g., whether it treats color balance\nas a diagonal or full 3 × 3 matrix multiply, whether it uses non-linearities in addition\nto gamma, whether it sharpens the image while “developing” the JPEG image, etc.\n7. Develop an interactive viewer to change the exposure of an image based on the average\nexposure of a region around the mouse. (One variant is to show the adjusted image",
  "549": "10.7 Exercises\n527\ninside a window around the mouse. Another is to adjust the complete image based on\nthe mouse position.)\n8. Implement a tone mapping operator (Exercise 10.5) and use this to map your radiance\nimage to a displayable gamut.\nEx 10.2: Noise level function\nDetermine your camera’s noise level function using either\nmultiple shots or by analyzing smooth regions.\n1. Set up your camera on a tripod looking at a calibration target or a static scene with a\ngood variation in input levels and colors. (Check your camera’s histogram to ensure\nthat all values are being sampled.)\n2. Take repeated images of the same scene (ideally with a remote shutter release) and\naverage them to compute the variance at each pixel. Discarding pixels near high gra-\ndients (which are affected by camera motion), plot for each color channel the standard\ndeviation at each pixel as a function of its output value.\n3. Fit a lower envelope to these measurements and use this as your noise level function.\nHow much variation do you see in the noise as a function of input level? How much of\nthis is signiﬁcant, i.e., away from ﬂat regions in your camera response function where\nyou do not want to be sampling anyway?\n4. (Optional) Using the same images, develop a technique that segments the image into\nnear-constant regions (Liu, Szeliski, Kang et al. 2008). (This is easier if you are pho-\ntographing a calibration chart.) Compute the deviations for each region from a single\nimage and use them to estimate the NLF. How does this compare to the multi-image\ntechnique, and how stable are your estimates from image to image?\nEx 10.3: Vignetting\nEstimate the amount of vignetting in some of your lenses using one of\nthe following three techniques (or devise one of your choosing):\n1. Take an image of a large uniform intensity region (well-illuminated wall or blue sky—\nbut be careful of brightness gradients) and ﬁt a radial polynomial curve to estimate the\nvignetting.\n2. Construct a center-weighted panorama and compare these pixel values to the input im-\nage values to estimate the vignetting function. Weight pixels in slowly varying regions\nmore highly, as small misalignments will give large errors at high gradients. Option-\nally estimate the radiometric response function as well (Litvinov and Schechner 2005;\nGoldman 2011).",
  "550": "528\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Analyze the radial gradients (especially in low-gradient regions) and ﬁt the robust\nmeans of these gradients to the derivative of the vignetting function, as described by\nZheng, Yu, Kang et al. (2008).\nFor the parametric form of your vignetting function, you can either use a simple radial func-\ntion, e.g.,\nf(r) = 1 + α1r + α2r2 + · · ·\n(10.42)\nor one of the specialized equations developed by Kang and Weiss (2000) and Zheng, Lin, and\nKang (2006).\nIn all of these cases, be sure that you are using linearized intensity measurements, by\nusing either RAW images or images linearized through a radiometric response function, or at\nleast images where the gamma curve has been removed.\n(Optional) What happens if you forget to undo the gamma before ﬁtting a (multiplicative)\nvignetting function?\nEx 10.4: Optical blur (PSF) estimation\nCompute the optical PSF either using a known\ntarget (Figure 10.7) or by detecting and ﬁtting step edges (Section 10.1.4) (Joshi, Szeliski,\nand Kriegman 2008).\n1. Detect strong edges to sub-pixel precision.\n2. Fit a local proﬁle to each oriented edge and ﬁll these pixels into an ideal target image,\neither at image resolution or at a higher resolution (Figure 10.9c–d).\n3. Use least squares (10.1) at valid pixels to estimate the PSF kernel K, either globally or\nin locally overlapping sub-regions of the image.\n4. Visualize the recovered PSFs and use them to remove chromatic aberration or de-blur\nthe image.\nEx 10.5: Tone mapping\nImplement one of the tone mapping algorithms discussed in Sec-\ntion 10.2.1 (Durand and Dorsey 2002; Fattal, Lischinski, and Werman 2002; Reinhard, Stark,\nShirley et al. 2002; Lischinski, Farbman, Uyttendaele et al. 2006b) or any of the numer-\nous additional algorithms discussed by Reinhard, Ward, Pattanaik et al. (2005) and http:\n//stellar.mit.edu/S/course/6/sp08/6.815/materials.html.\n(Optional) Compare your algorithm to local histogram equalization (Section 3.1.4).\nEx 10.6: Flash enhancement\nDevelop an algorithm to combine ﬂash and non-ﬂash pho-\ntographs to best effect. You can use ideas from Eisemann and Durand (2004) and Petschnigg,\nAgrawala, Hoppe et al. (2004) or anything else you think might work well.",
  "551": "10.7 Exercises\n529\nEx 10.7: Super-resolution\nImplement one or more super-resolution algorithms and com-\npare their performance.\n1. Take a set of photographs of the same scene using a hand-held camera (to ensure that\nthere is some jitter between the photographs).\n2. Determine the PSF for the images you are trying to super-resolve using one of the\ntechniques in Exercise 10.4.\n3. Alternatively, simulate a collection of lower-resolution images by taking a high-quality\nphotograph (avoid those with compression artifacts) and applying your own pre-ﬁlter\nkernel and downsampling.\n4. Estimate the relative motion between the images using a parametric translation and\nrotation motion estimation algorithm (Sections 6.1.3 or 8.2).\n5. Implement a basic least squares super-resolution algorithm by minimizing the differ-\nence between the observed and downsampled images (10.27–10.28).\n6. Add in a gradient image prior, either as another least squares term or as a robust term\nthat can be minimized using iteratively reweighted least squares (Appendix A.3).\n7. (Optional) Implement one of the example-based super-resolution techniques, where\nmatching against a set of exemplar images is used either to infer higher-frequency\ninformation to be added to the reconstruction (Freeman, Jones, and Pasztor 2002)\nor higher-frequency gradients to be matched in the super-resolved image (Baker and\nKanade 2002).\n8. (Optional) Use local edge statistic information to improve the quality of the super-\nresolved image (Fattal 2007).\nEx 10.8: Image matting\nDevelop an algorithm for pulling a foreground matte from natural\nimages, as described in Section 10.4.\n1. Make sure that the images you are taking are linearized (Exercise 10.1 and Section 10.1)\nand that your camera exposure is ﬁxed (full manual mode), at least when taking multi-\nple shots of the same scene.\n2. To acquire ground truth data, place your object in front of a computer monitor and\ndisplay a variety of solid background colors as well as some natural imagery.\n3. Remove your object and re-display the same images to acquire known background\ncolors.",
  "552": "530\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n4. Use triangulation matting (Smith and Blinn 1996) to estimate the ground truth opacities\nα and pre-multiplied foreground colors αF for your objects.\n5. Implement one or more of the natural image matting algorithms described in Sec-\ntion 10.4 and compare your results to the ground truth values you computed. Alter-\nnatively, use the matting test images published on http://alphamatting.com/.\n6. (Optional) Run your algorithms on other images taken with the same calibrated camera\n(or other images you ﬁnd interesting).\nEx 10.9: Smoke and shadow matting\nExtract smoke or shadow mattes from one scene\nand insert them into another (Chuang, Agarwala, Curless et al. 2002; Chuang, Goldman,\nCurless et al. 2003).\n1. Take a still or video sequence of images with and without some intermittent smoke and\nshadows. (Remember to linearize your images before proceeding with any computa-\ntions.)\n2. For each pixel, ﬁt a line to the observed color values.\n3. If performing smoke matting, robustly compute the intersection of these lines to obtain\nthe smoke color estimate. Then, estimate the background color as the other extremum\n(unless you already took a smoke-free background image).\nIf performing shadow matting, compute robust shadow (minimum) and lit (maximum)\nvalues for each pixel.\n4. Extract the smoke or shadow mattes from each frame as the fraction between these two\nvalues (background and smoke or shadowed and lit).\n5. Scan a new (destination) scene or modify the original background with an image editor.\n6. Re-insert the smoke or shadow matte, along with any other foreground objects you may\nhave extracted.\n7. (Optional) Using a series of cast stick shadows, estimate the deformation ﬁeld for the\ndestination scene in order to correctly warp (drape) the shadows across the new ge-\nometry. (This is related to the shadow scanning technique developed by Bouguet and\nPerona (1999) and implemented in Exercise 12.2.)\n8. (Optional) Chuang, Goldman, Curless et al. (2003) only demonstrated their technique\nfor planar source geometries. Can you extend their technique to capture shadows ac-\nquired from an irregular source geometry?",
  "553": "10.7 Exercises\n531\n9. (Optional) Can you change the direction of the shadow, i.e., simulate the effect of\nchanging the light source direction?\nEx 10.10: Texture synthesis\nImplement one of the texture synthesis or hole ﬁlling algo-\nrithms presented in Section 10.5. Here is one possible procedure:\n1. Implement the basic Efros and Leung (1999) algorithm, i.e., starting from the outside\n(for hole ﬁlling) or in raster order (for texture synthesis), search for a similar neighbor-\nhood in the source texture image, and copy that pixel.\n2. Add in the Wei and Levoy (2000) extension of generating the pixels in a coarse-to-ﬁne\nfashion, i.e., generate a lower-resolution synthetic texture (or ﬁlled image), and use this\nas a guide for matching regions in the ﬁner resolution version.\n3. Add in the Criminisi, P´erez, and Toyama (2004) idea of prioritizing pixels to be ﬁlled\nby some function of the local structure (gradient or orientation strength).\n4. Extend any of the above algorithms by selecting sub-blocks in the source texture and\nusing optimization to determine the seam between the new block and the existing image\nthat it overlaps (Efros and Freeman 2001).\n5. (Optional) Implement one of the isophote (smooth continuation) inpainting algorithms\n(Bertalmio, Sapiro, Caselles et al. 2000; Telea 2004).\n6. (Optional) Add the ability to supply a target (reference) image (Efros and Freeman\n2001) or to provide sample ﬁltered or unﬁltered (reference and rendered) images (Hertz-\nmann, Jacobs, Oliver et al. 2001), see Section 10.5.2.\nEx 10.11: Colorization\nImplement the Levin, Lischinski, and Weiss (2004) colorization al-\ngorithm that is sketched out in Section 10.3.2 and Figure 10.37. Find some historic monochrome\nphotographs and some modern color ones. Write an interactive tool that lets you “pick” col-\nors from a modern photo and paint over the old one. Tune the algorithm parameters to give\nyou good results. Are you pleased with the results? Can you think of ways to make them\nlook more “antique”, e.g., with softer (less saturated and edgy) colors?",
  "554": "532\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "555": "Chapter 11\nStereo correspondence\n11.1 Epipolar geometry\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\n11.1.1 Rectiﬁcation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538\n11.1.2 Plane sweep . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540\n11.2 Sparse correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543\n11.2.1 3D curves and proﬁles . . . . . . . . . . . . . . . . . . . . . . . . . 543\n11.3 Dense correspondence\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545\n11.3.1 Similarity measures . . . . . . . . . . . . . . . . . . . . . . . . . . . 546\n11.4 Local methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548\n11.4.1 Sub-pixel estimation and uncertainty . . . . . . . . . . . . . . . . . . 550\n11.4.2 Application: Stereo-based head tracking . . . . . . . . . . . . . . . . 551\n11.5 Global optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552\n11.5.1 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 554\n11.5.2 Segmentation-based techniques\n. . . . . . . . . . . . . . . . . . . . 556\n11.5.3 Application: Z-keying and background replacement . . . . . . . . . . 558\n11.6 Multi-view stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558\n11.6.1 Volumetric and 3D surface reconstruction . . . . . . . . . . . . . . . 562\n11.6.2 Shape from silhouettes . . . . . . . . . . . . . . . . . . . . . . . . . 567\n11.7 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570\n11.8 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571",
  "556": "534\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 11.1\nStereo reconstruction techniques can convert (a–b) a pair of images into (c)\na depth map (http://vision.middlebury.edu/stereo/data/scenes2003/) or (d–e) a sequence of\nimages into (f) a 3D model (http://vision.middlebury.edu/mview/data/). (g) An analytical\nstereo plotter, courtesy of Kenney Aerial Mapping, Inc., can generate (h) contour plots.",
  "557": "11 Stereo correspondence\n535\nStereo matching is the process of taking two or more images and estimating a 3D model of\nthe scene by ﬁnding matching pixels in the images and converting their 2D positions into\n3D depths. In Chapters 6–7, we described techniques for recovering camera positions and\nbuilding sparse 3D models of scenes or objects. In this chapter, we address the question\nof how to build a more complete 3D model, e.g., a sparse or dense depth map that assigns\nrelative depths to pixels in the input images. We also look at the topic of multi-view stereo\nalgorithms that produce complete 3D volumetric or surface-based object models.\nWhy are people interested in stereo matching? From the earliest inquiries into visual per-\nception, it was known that we perceive depth based on the differences in appearance between\nthe left and right eye.1 As a simple experiment, hold your ﬁnger vertically in front of your\neyes and close each eye alternately. You will notice that the ﬁnger jumps left and right relative\nto the background of the scene. The same phenomenon is visible in the image pair shown in\nFigure 11.1a–b, in which the foreground objects shift left and right relative to the background.\nAs we will shortly see, under simple imaging conﬁgurations (both eyes or cameras look-\ning straight ahead), the amount of horizontal motion or disparity is inversely proportional to\nthe distance from the observer. While the basic physics and geometry relating visual disparity\nto scene structure are well understood (Section 11.1), automatically measuring this disparity\nby establishing dense and accurate inter-image correspondences is a challenging task.\nThe earliest stereo matching algorithms were developed in the ﬁeld of photogrammetry\nfor automatically constructing topographic elevation maps from overlapping aerial images.\nPrior to this, operators would use photogrammetric stereo plotters, which displayed shifted\nversions of such images to each eye and allowed the operator to ﬂoat a dot cursor around con-\nstant elevation contours (Figure 11.1g). The development of fully automated stereo matching\nalgorithms was a major advance in this ﬁeld, enabling much more rapid and less expensive\nprocessing of aerial imagery (Hannah 1974; Hsieh, McKeown, and Perlant 1992).\nIn computer vision, the topic of stereo matching has been one of the most widely stud-\nied and fundamental problems (Marr and Poggio 1976; Barnard and Fischler 1982; Dhond\nand Aggarwal 1989; Scharstein and Szeliski 2002; Brown, Burschka, and Hager 2003; Seitz,\nCurless, Diebel et al. 2006), and continues to be one of the most active research areas. While\nphotogrammetric matching concentrated mainly on aerial imagery, computer vision applica-\ntions include modeling the human visual system (Marr 1982), robotic navigation and manip-\nulation (Moravec 1983; Konolige 1997; Thrun, Montemerlo, Dahlkamp et al. 2006), as well\nas view interpolation and image-based rendering (Figure 11.2a–d), 3D model building (Fig-\nure 11.2e–f and h–j), and mixing live action with computer-generated imagery (Figure 11.2g).\nIn this chapter, we describe the fundamental principles behind stereo matching, following\n1 The word stereo comes from the Greek for solid; stereo vision is how we perceive solid shape (Koenderink\n1990).",
  "558": "536\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\nFigure 11.2 Applications of stereo vision: (a) input image, (b) computed depth map, and (c)\nnew view generation from multi-view stereo (Matthies, Kanade, and Szeliski 1989) c⃝1989\nSpringer; (d) view morphing between two images (Seitz and Dyer 1996) c⃝1996 ACM; (e–f)\n3D face modeling (images courtesy of Fr´ed´eric Devernay); (g) z-keying live and computer-\ngenerated imagery (Kanade, Yoshida, Oda et al. 1996) c⃝1996 IEEE; (h–j) building 3D\nsurface models from multiple video streams in Virtualized Reality (Kanade, Rander, and\nNarayanan 1997).",
  "559": "11.1 Epipolar geometry\n537\nthe general taxonomy proposed by Scharstein and Szeliski (2002). We begin in Section 11.1\nwith a review of the geometry of stereo image matching, i.e., how to compute for a given\npixel in one image the range of possible locations the pixel might appear at in the other\nimage, i.e., its epipolar line. We describe how to pre-warp images so that corresponding\nepipolar lines are coincident (rectiﬁcation). We also describe a general resampling algorithm\ncalled plane sweep that can be used to perform multi-image stereo matching with arbitrary\ncamera conﬁgurations.\nNext, we brieﬂy survey techniques for the sparse stereo matching of interest points and\nedge-like features (Section 11.2). We then turn to the main topic of this chapter, namely the\nestimation of a dense set of pixel-wise correspondences in the form of a disparity map (Fig-\nure 11.1c). This involves ﬁrst selecting a pixel matching criterion (Section 11.3) and then\nusing either local area-based aggregation (Section 11.4) or global optimization (Section 11.5)\nto help disambiguate potential matches. In Section 11.6, we discuss multi-view stereo meth-\nods that aim to reconstruct a complete 3D model instead of just a single disparity image\n(Figure 11.1d–f).\n11.1 Epipolar geometry\nGiven a pixel in one image, how can we compute its correspondence in the other image? In\nChapter 8, we saw that a variety of search techniques can be used to match pixels based on\ntheir local appearance as well as the motions of neighboring pixels. In the case of stereo\nmatching, however, we have some additional information available, namely the positions and\ncalibration data for the cameras that took the pictures of the same static scene (Section 7.2).\nHow can we exploit this information to reduce the number of potential correspondences,\nand hence both speed up the matching and increase its reliability? Figure 11.3a shows how a\npixel in one image x0 projects to an epipolar line segment in the other image. The segment\nis bounded at one end by the projection of the original viewing ray at inﬁnity p∞and at the\nother end by the projection of the original camera center c0 into the second camera, which\nis known as the epipole e1. If we project the epipolar line in the second image back into the\nﬁrst, we get another line (segment), this time bounded by the other corresponding epipole\ne0. Extending both line segments to inﬁnity, we get a pair of corresponding epipolar lines\n(Figure 11.3b), which are the intersection of the two image planes with the epipolar plane\nthat passes through both camera centers c0 and c1 as well as the point of interest p (Faugeras\nand Luong 2001; Hartley and Zisserman 2004).",
  "560": "538\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\np\nx1\nx0\n(R,t)\np∞\ne1\ne0\nc0\nc1\n       epipolar plane\np∞\np\n(R,t)\nc0\nc1\nepipolar\nlines\nx\n0\ne0\ne1\nx1\nl1\nl0\n(a)\n(b)\nFigure 11.3\nEpipolar geometry: (a) epipolar line segment corresponding to one ray; (b)\ncorresponding set of epipolar lines and their epipolar plane.\n11.1.1 Rectiﬁcation\nAs we saw in Section 7.2, the epipolar geometry for a pair of cameras is implicit in the\nrelative pose and calibrations of the cameras, and can easily be computed from seven or more\npoint matches using the fundamental matrix (or ﬁve or more points for the calibrated essential\nmatrix) (Zhang 1998a,b; Faugeras and Luong 2001; Hartley and Zisserman 2004). Once this\ngeometry has been computed, we can use the epipolar line corresponding to a pixel in one\nimage to constrain the search for corresponding pixels in the other image. One way to do this\nis to use a general correspondence algorithm, such as optical ﬂow (Section 8.4), but to only\nconsider locations along the epipolar line (or to project any ﬂow vectors that fall off back onto\nthe line).\nA more efﬁcient algorithm can be obtained by ﬁrst rectifying (i.e, warping) the input\nimages so that corresponding horizontal scanlines are epipolar lines (Loop and Zhang 1999;\nFaugeras and Luong 2001; Hartley and Zisserman 2004).2 Afterwards, it is possible to match\nhorizontal scanlines independently or to shift images horizontally while computing matching\nscores (Figure 11.4).\nA simple way to rectify the two images is to ﬁrst rotate both cameras so that they are\nlooking perpendicular to the line joining the camera centers c0 and c1. Since there is a de-\ngree of freedom in the tilt, the smallest rotations that achieve this should be used. Next, to\ndetermine the desired twist around the optical axes, make the up vector (the camera y axis)\n2 This makes most sense if the cameras are next to each other, although by rotating the cameras, rectiﬁcation can\nbe performed on any pair that is not verged too much or has too much of a scale change. In those latter cases, using\nplane sweep (below) or hypothesizing small planar patch locations in 3D (Goesele, Snavely, Curless et al. 2007) may\nbe preferable.",
  "561": "11.1 Epipolar geometry\n539\n(a)\n(b)\n(c)\n(d)\nFigure 11.4\nThe multi-stage stereo rectiﬁcation algorithm of Loop and Zhang (1999) c⃝\n1999 IEEE. (a) Original image pair overlaid with several epipolar lines; (b) images trans-\nformed so that epipolar lines are parallel; (c) images rectiﬁed so that epipolar lines are hori-\nzontal and in vertial correspondence; (d) ﬁnal rectiﬁcation that minimizes horizontal distor-\ntions.\nperpendicular to the camera center line. This ensures that corresponding epipolar lines are\nhorizontal and that the disparity for points at inﬁnity is 0. Finally, re-scale the images, if nec-\nessary, to account for different focal lengths, magnifying the smaller image to avoid aliasing.\n(The full details of this procedure can be found in Fusiello, Trucco, and Verri (2000) and Ex-\nercise 11.1.) Note that in general, it is not possible to rectify an arbitrary collection of images\nsimultaneously unless their optical centers are collinear, although rotating the cameras so that\nthey all point in the same direction reduces the inter-camera pixel movements to scalings and\ntranslations.\nThe resulting standard rectiﬁed geometry is employed in a lot of stereo camera setups and\nstereo algorithms, and leads to a very simple inverse relationship between 3D depths Z and\ndisparities d,\nd = f B\nZ ,\n(11.1)\nwhere f is the focal length (measured in pixels), B is the baseline, and\nx′ = x + d(x, y), y′ = y\n(11.2)\ndescribes the relationship between corresponding pixel coordinates in the left and right im-\nages (Bolles, Baker, and Marimont 1987; Okutomi and Kanade 1993; Scharstein and Szeliski",
  "562": "540\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 11.5\nSlices through a typical disparity space image (DSI) (Scharstein and Szeliski\n2002) c⃝2002 Springer: (a) original color image; (b) ground truth disparities; (c–e) three\n(x, y) slices for d = 10, 16, 21; (f) an (x, d) slice for y = 151 (the dashed line in (b)).\nVarious dark (matching) regions are visible in (c–e), e.g., the bookshelves, table and cans,\nand head statue, and three disparity levels can be seen as horizontal lines in (f). The dark\nbands in the DSIs indicate regions that match at this disparity. (Smaller dark regions are often\nthe result of textureless regions.) Additional examples of DSIs are discussed by Bobick and\nIntille (1999).\n2002).3 The task of extracting depth from a set of images then becomes one of estimating the\ndisparity map d(x, y).\nAfter rectiﬁcation, we can easily compare the similarity of pixels at corresponding lo-\ncations (x, y) and (x′, y′) = (x + d, y) and store them in a disparity space image (DSI)\nC(x, y, d) for further processing (Figure 11.5). The concept of the disparity space (x, y, d)\ndates back to early work in stereo matching (Marr and Poggio 1976), while the concept of a\ndisparity space image (volume) is generally associated with Yang, Yuille, and Lu (1993) and\nIntille and Bobick (1994).\n11.1.2 Plane sweep\nAn alternative to pre-rectifying the images before matching is to sweep a set of planes through\nthe scene and to measure the photoconsistency of different images as they are re-projected\nonto these planes (Figure 11.6). This process is commonly known as the plane sweep algo-\nrithm (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).\nAs we saw in Section 2.1.5, where we introduced projective depth (also known as plane\nplus parallax (Kumar, Anandan, and Hanna 1994; Sawhney 1994; Szeliski and Coughlan\n3 The term disparity was ﬁrst introduced in the human vision literature to describe the difference in location\nof corresponding features seen by the left and right eyes (Marr 1982). Horizontal disparity is the most commonly\nstudied phenomenon, but vertical disparity is possible if the eyes are verged.",
  "563": "11.1 Epipolar geometry\n541\nVirtual camera\nd\nx\ny\nInput  image k\nu\nv\nHomography:\n  u = H x\nx\ny\nk\nd\nk\n(a)\n(b)\nFigure 11.6 Sweeping a set of planes through a scene (Szeliski and Golland 1999) c⃝1999\nSpringer: (a) The set of planes seen from a virtual camera induces a set of homographies in\nany other source (input) camera image. (b) The warped images from all the other cameras can\nbe stacked into a generalized disparity space volume ˜I(x, y, d, k) indexed by pixel location\n(x, y), disparity d, and camera k.\n1997)), the last row of a full-rank 4 × 4 projection matrix ˜\nP can be set to an arbitrary plane\nequation p3 = s3[ˆn0|c0]. The resulting four-dimensional projective transform (collineation)\n(2.68) maps 3D world points p = (X, Y, Z, 1) into screen coordinates xs = (xs, ys, 1, d),\nwhere the projective depth (or parallax) d (2.66) is 0 on the reference plane (Figure 2.11).\nSweeping d through a series of disparity hypotheses, as shown in Figure 11.6a, corre-\nsponds to mapping each input image into the virtual camera ˜\nP deﬁning the disparity space\nthrough a series of homographies (2.68–2.71),\n˜xk ∼˜\nP k ˜\nP\n−1xs = ˜\nHk˜x + tkd = ( ˜\nHk + tk[0 0 d])˜x,\n(11.3)\nas shown in Figure 2.12b, where ˜xk and ˜x are the homogeneous pixel coordinates in the\nsource and virtual (reference) images (Szeliski and Golland 1999). The members of the fam-\nily of homographies ˜\nHk(d) = ˜\nHk + tk[0 0 d], which are parametererized by the addition of\na rank-1 matrix, are related to each other through a planar homology (Hartley and Zisserman\n2004, A5.2).\nThe choice of virtual camera and parameterization is application dependent and is what\ngives this framework a lot of its ﬂexibility. In many applications, one of the input cameras\n(the reference camera) is used, thus computing a depth map that is registered with one of the\ninput images and which can later be used for image-based rendering (Sections 13.1 and 13.2).\nIn other applications, such as view interpolation for gaze correction in video-conferencing",
  "564": "542\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Section 11.4.2) (Ott, Lewis, and Cox 1993; Criminisi, Shotton, Blake et al. 2003), a camera\ncentrally located between the two input cameras is preferable, since it provides the needed\nper-pixel disparities to hallucinate the virtual middle image.\nThe choice of disparity sampling, i.e., the setting of the zero parallax plane and the scaling\nof integer disparities, is also application dependent, and is usually set to bracket the range of\ninterest, i.e., the working volume, while scaling disparities to sample the image in pixel (or\nsub-pixel) shifts. For example, when using stereo vision for obstacle avoidance in robot\nnavigation, it is most convenient to set up disparity to measure per-pixel elevation above the\nground (Ivanchenko, Shen, and Coughlan 2009).\nAs each input image is warped onto the current planes parameterized by disparity d, it\ncan be stacked into a generalized disparity space image ˜I(x, y, d, k) for further processing\n(Figure 11.6b) (Szeliski and Golland 1999). In most stereo algorithms, the photoconsistency\n(e.g., sum of squared or robust differences) with respect to the reference image Ir is calculated\nand stored in the DSI\nC(x, y, d) =\nX\nk\nρ(˜I(x, y, d, k) −Ir(x, y)).\n(11.4)\nHowever, it is also possible to compute alternative statistics such as robust variance, focus,\nor entropy (Section 11.3.1) (Vaish, Szeliski, Zitnick et al. 2006) or to use this representation\nto reason about occlusions (Szeliski and Golland 1999; Kang and Szeliski 2004). The gen-\neralized DSI will come in particularly handy when we come back to the topic of multi-view\nstereo in Section 11.6.\nOf course, planes are not the only surfaces that can be used to deﬁne a 3D sweep through\nthe space of interest. Cylindrical surfaces, especially when coupled with panoramic photog-\nraphy (Chapter 9), are often used (Ishiguro, Yamamoto, and Tsuji 1992; Kang and Szeliski\n1997; Shum and Szeliski 1999; Li, Shum, Tang et al. 2004; Zheng, Kang, Cohen et al. 2007).\nIt is also possible to deﬁne other manifold topologies, e.g., ones where the camera rotates\naround a ﬁxed axis (Seitz 2001).\nOnce the DSI has been computed, the next step in most stereo correspondence algorithms\nis to produce a univalued function in disparity space d(x, y) that best describes the shape of\nthe surfaces in the scene. This can be viewed as ﬁnding a surface embedded in the disparity\nspace image that has some optimality property, such as lowest cost and best (piecewise)\nsmoothness (Yang, Yuille, and Lu 1993). Figure 11.5 shows examples of slices through a\ntypical DSI. More ﬁgures of this kind can be found in the paper by Bobick and Intille (1999).",
  "565": "11.2 Sparse correspondence\n543\n11.2 Sparse correspondence\nEarly stereo matching algorithms were feature-based, i.e., they ﬁrst extracted a set of poten-\ntially matchable image locations, using either interest operators or edge detectors, and then\nsearched for corresponding locations in other images using a patch-based metric (Hannah\n1974; Marr and Poggio 1979; Mayhew and Frisby 1980; Baker and Binford 1981; Arnold\n1983; Grimson 1985; Ohta and Kanade 1985; Bolles, Baker, and Marimont 1987; Matthies,\nKanade, and Szeliski 1989; Hsieh, McKeown, and Perlant 1992; Bolles, Baker, and Hannah\n1993). This limitation to sparse correspondences was partially due to computational resource\nlimitations, but was also driven by a desire to limit the answers produced by stereo algorithms\nto matches with high certainty. In some applications, there was also a desire to match scenes\nwith potentially very different illuminations, where edges might be the only stable features\n(Collins 1996). Such sparse 3D reconstructions could later be interpolated using surface ﬁt-\nting algorithms such as those discussed in Sections 3.7.1 and 12.3.1.\nMore recent work in this area has focused on ﬁrst extracting highly reliable features and\nthen using these as seeds to grow additional matches (Zhang and Shan 2000; Lhuillier and\nQuan 2002). Similar approaches have also been extended to wide baseline multi-view stereo\nproblems and combined with 3D surface reconstruction (Lhuillier and Quan 2005; Strecha,\nTuytelaars, and Van Gool 2003; Goesele, Snavely, Curless et al. 2007) or free-space reasoning\n(Taylor 2003), as described in more detail in Section 11.6.\n11.2.1 3D curves and proﬁles\nAnother example of sparse correspondence is the matching of proﬁle curves (or occluding\ncontours), which occur at the boundaries of objects (Figure 11.7) and at interior self occlu-\nsions, where the surface curves away from the camera viewpoint.\nThe difﬁculty in matching proﬁle curves is that in general, the locations of proﬁle curves\nvary as a function of camera viewpoint. Therefore, matching curves directly in two images\nand then triangulating these matches can lead to erroneous shape measurements. Fortunately,\nif three or more closely spaced frames are available, it is possible to ﬁt a local circular arc to\nthe locations of corresponding edgels (Figure 11.7a) and therefore obtain semi-dense curved\nsurface meshes directly from the matches (Figures 11.7c and g). Another advantage of match-\ning such curves is that they can be used to reconstruct surface shape for untextured surfaces,\nso long as there is a visible difference between foreground and background colors.\nOver the years, a number of different techniques have been developed for reconstructing\nsurface shape from proﬁle curves (Giblin and Weiss 1987; Cipolla and Blake 1992; Vaillant\nand Faugeras 1992; Zheng 1994; Boyer and Berger 1997; Szeliski and Weiss 1998). Cipolla\nand Giblin (2000) describe many of these techniques, as well as related topics such as in-",
  "566": "544\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 11.7 Surface reconstruction from occluding contours (Szeliski and Weiss 1998) c⃝\n2002 Springer: (a) circular arc ﬁtting in the epipolar plane; (b) synthetic example of an el-\nlipsoid with a truncated side and elliptic surface markings; (c) partially reconstructed surface\nmesh seen from an oblique and top-down view; (d) real-world image sequence of a soda can\non a turntable; (e) extracted edges; (f) partially reconstructed proﬁle curves; (g) partially re-\nconstructed surface mesh. (Partial reconstructions are shown so as not to clutter the images.)\nferring camera motion from proﬁle curve sequences. Below, we summarize the approach\ndeveloped by Szeliski and Weiss (1998), which assumes a discrete set of images, rather than\nformulating the problem in a continuous differential framework.\nLet us assume that the camera is moving smoothly enough that the local epipolar geometry\nvaries slowly, i.e., the epipolar planes induced by the successive camera centers and an edgel\nunder consideration are nearly co-planar. The ﬁrst step in the processing pipeline is to extract\nand link edges in each of the input images (Figures 11.7b and e). Next, edgels in successive\nimages are matched using pairwise epipolar geometry, proximity and (optionally) appearance.\nThis provides a linked set of edges in the spatio-temporal volume, which is sometimes called\nthe weaving wall (Baker 1989).\nTo reconstruct the 3D location of an individual edgel, along with its local in-plane normal\nand curvature, we project the viewing rays corresponding to its neighbors onto the instanta-\nneous epipolar plane deﬁned by the camera center, the viewing ray, and the camera velocity,\nas shown in Figure 11.7a. We then ﬁt an osculating circle to the projected lines, parameteriz-",
  "567": "11.3 Dense correspondence\n545\ning the circle by its centerpoint c = (xc, yc) and radius r,\ncixc + siyc + r = di,\n(11.5)\nwhere ci = ˆti · ˆt0 and si = −ˆti · ˆn0 are the cosine and sine of the angle between viewing ray\ni and the central viewing ray 0, and di = (qi −q0)· ˆn0 is the perpendicular distance between\nviewing ray i and the local origin q0, which is a point chosen on the central viewing ray close\nto the line intersections (Szeliski and Weiss 1998). The resulting set of linear equations can\nbe solved using least squares, and the quality of the solution (residual error) can be used to\ncheck for erroneous correspondences.\nThe resulting set of 3D points, along with their spatial (in-image) and temporal (between-\nimage) neighbors, form a 3D surface mesh with local normal and curvature estimates (Fig-\nures 11.7c and g). Note that whenever a curve is due to a surface marking or a sharp crease\nedge, rather than a smooth surface proﬁle curve, this shows up as a 0 or small radius of curva-\nture. Such curves result in isolated 3D space curves, rather than elements of smooth surface\nmeshes, but can still be incorporated into the 3D surface model during a later stage of surface\ninterpolation (Section 12.3.1).\n11.3 Dense correspondence\nWhile sparse matching algorithms are still occasionally used, most stereo matching algo-\nrithms today focus on dense correspondence, since this is required for applications such as\nimage-based rendering or modeling. This problem is more challenging than sparse corre-\nspondence, since inferring depth values in textureless regions requires a certain amount of\nguesswork. (Think of a solid colored background seen through a picket fence. What depth\nshould it be?)\nIn this section, we review the taxonomy and categorization scheme for dense correspon-\ndence algorithms ﬁrst proposed by Scharstein and Szeliski (2002). The taxonomy consists\nof a set of algorithmic “building blocks” from which a large set of algorithms can be con-\nstructed. It is based on the observation that stereo algorithms generally perform some subset\nof the following four steps:\n1. matching cost computation;\n2. cost (support) aggregation;\n3. disparity computation and optimization; and\n4. disparity reﬁnement.",
  "568": "546\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFor example, local (window-based) algorithms (Section 11.4), where the disparity com-\nputation at a given point depends only on intensity values within a ﬁnite window, usually\nmake implicit smoothness assumptions by aggregating support. Some of these algorithms\ncan cleanly be broken down into steps 1, 2, 3. For example, the traditional sum-of-squared-\ndifferences (SSD) algorithm can be described as:\n1. The matching cost is the squared difference of intensity values at a given disparity.\n2. Aggregation is done by summing the matching cost over square windows with constant\ndisparity.\n3. Disparities are computed by selecting the minimal (winning) aggregated value at each\npixel.\nSome local algorithms, however, combine steps 1 and 2 and use a matching cost that is based\non a support region, e.g. normalized cross-correlation (Hannah 1974; Bolles, Baker, and Han-\nnah 1993) and the rank transform (Zabih and Woodﬁll 1994) and other ordinal measures (Bhat\nand Nayar 1998). (This can also be viewed as a preprocessing step; see (Section 11.3.1).)\nGlobal algorithms, on the other hand, make explicit smoothness assumptions and then\nsolve a a global optimization problem (Section 11.5). Such algorithms typically do not per-\nform an aggregation step, but rather seek a disparity assignment (step 3) that minimizes a\nglobal cost function that consists of data (step 1) terms and smoothness terms. The main dis-\ntinctions among these algorithms is the minimization procedure used, e.g., simulated anneal-\ning (Marroquin, Mitter, and Poggio 1987; Barnard 1989), probabilistic (mean-ﬁeld) diffusion\n(Scharstein and Szeliski 1998), expectation maximization (EM) (Birchﬁeld, Natarajan, and\nTomasi 2007), graph cuts (Boykov, Veksler, and Zabih 2001), or loopy belief propagation\n(Sun, Zheng, and Shum 2003), to name just a few.\nIn between these two broad classes are certain iterative algorithms that do not explicitly\nspecify a global function to be minimized, but whose behavior mimics closely that of iterative\noptimization algorithms (Marr and Poggio 1976; Zitnick and Kanade 2000). Hierarchical\n(coarse-to-ﬁne) algorithms resemble such iterative algorithms, but typically operate on an\nimage pyramid where results from coarser levels are used to constrain a more local search at\nﬁner levels (Witkin, Terzopoulos, and Kass 1987; Quam 1984; Bergen, Anandan, Hanna et\nal. 1992).\n11.3.1 Similarity measures\nThe ﬁrst component of any dense stereo matching algorithm is a similarity measure that\ncompares pixel values in order to determine how likely they are to be in correspondence. In\nthis section, we brieﬂy review the similarity measures introduced in Section 8.1 and mention a",
  "569": "11.3 Dense correspondence\n547\nfew others that have been developed speciﬁcally for stereo matching (Scharstein and Szeliski\n2002; Hirschm¨uller and Scharstein 2009).\nThe most common pixel-based matching costs include sums of squared intensity differ-\nences (SSD) (Hannah 1974) and absolute intensity differences (SAD) (Kanade 1994). In\nthe video processing community, these matching criteria are referred to as the mean-squared\nerror (MSE) and mean absolute difference (MAD) measures; the term displaced frame dif-\nference is also often used (Tekalp 1995).\nMore recently, robust measures (8.2), including truncated quadratics and contaminated\nGaussians, have been proposed (Black and Anandan 1996; Black and Rangarajan 1996;\nScharstein and Szeliski 1998). These measures are useful because they limit the inﬂuence\nof mismatches during aggregation. Vaish, Szeliski, Zitnick et al. (2006) compare a number\nof such robust measures, including a new one based on the entropy of the pixel values at a\nparticular disparity hypothesis (Zitnick, Kang, Uyttendaele et al. 2004), which is particularly\nuseful in multi-view stereo.\nOther traditional matching costs include normalized cross-correlation (8.11) (Hannah\n1974; Bolles, Baker, and Hannah 1993; Evangelidis and Psarakis 2008), which behaves\nsimilarly to sum-of-squared-differences (SSD), and binary matching costs (i.e., match or no\nmatch) (Marr and Poggio 1976), based on binary features such as edges (Baker and Binford\n1981; Grimson 1985) or the sign of the Laplacian (Nishihara 1984). Because of their poor\ndiscriminability, simple binary matching costs are no longer used in dense stereo matching.\nSome costs are insensitive to differences in camera gain or bias, for example gradient-\nbased measures (Seitz 1989; Scharstein 1994), phase and ﬁlter-bank responses (Marr and\nPoggio 1979; Kass 1988; Jenkin, Jepson, and Tsotsos 1991; Jones and Malik 1992), ﬁlters\nthat remove regular or robust (bilaterally ﬁltered) means (Ansar, Castano, and Matthies 2004;\nHirschm¨uller and Scharstein 2009), dense feature descriptor (Tola, Lepetit, and Fua 2010),\nand non-parametric measures such as rank and census transforms (Zabih and Woodﬁll 1994),\nordinal measures (Bhat and Nayar 1998), or entropy (Zitnick, Kang, Uyttendaele et al. 2004;\nZitnick and Kang 2007). The census transform, which converts each pixel inside a moving\nwindow into a bit vector representing which neighbors are above or below the central pixel,\nwas found by Hirschm¨uller and Scharstein (2009) to be quite robust against large-scale, non-\nstationary exposure and illumination changes.\nIt is also possible to correct for differing global camera characteristics by performing\na preprocessing or iterative reﬁnement step that estimates inter-image bias–gain variations\nusing global regression (Gennert 1988), histogram equalization (Cox, Roy, and Hingorani\n1995), or mutual information (Kim, Kolmogorov, and Zabih 2003; Hirschm¨uller 2008). Lo-\ncal, smoothly varying compensation ﬁelds have also been proposed (Strecha, Tuytelaars, and\nVan Gool 2003; Zhang, McMillan, and Yu 2006).\nIn order to compensate for sampling issues, i.e., dramatically different pixel values in",
  "570": "548\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nhigh-frequency areas, Birchﬁeld and Tomasi (1998) proposed a matching cost that is less sen-\nsitive to shifts in image sampling. Rather than just comparing pixel values shifted by integral\namounts (which may miss a valid match), they compare each pixel in the reference image\nagainst a linearly interpolated function of the other image. More detailed studies of these\nand additional matching costs are explored in (Szeliski and Scharstein 2004; Hirschm¨uller\nand Scharstein 2009). In particular, if you expect there to be signiﬁcant exposure or appear-\nance variation between images that you are matching, some of the more robust measures\nthat performed well in the evaluation by Hirschm¨uller and Scharstein (2009), such as the\ncensus transform (Zabih and Woodﬁll 1994), ordinal measures (Bhat and Nayar 1998), bi-\nlateral subtraction (Ansar, Castano, and Matthies 2004), or hierarchical mutual information\n(Hirschm¨uller 2008), should be used.\n11.4 Local methods\nLocal and window-based methods aggregate the matching cost by summing or averaging\nover a support region in the DSI C(x, y, d).4 A support region can be either two-dimensional\nat a ﬁxed disparity (favoring fronto-parallel surfaces), or three-dimensional in x-y-d space\n(supporting slanted surfaces). Two-dimensional evidence aggregation has been implemented\nusing square windows or Gaussian convolution (traditional), multiple windows anchored at\ndifferent points, i.e., shiftable windows (Arnold 1983; Fusiello, Roberto, and Trucco 1997;\nBobick and Intille 1999), windows with adaptive sizes (Okutomi and Kanade 1992; Kanade\nand Okutomi 1994; Kang, Szeliski, and Chai 2001; Veksler 2001, 2003), windows based on\nconnected components of constant disparity (Boykov, Veksler, and Zabih 1998), or the re-\nsults of color-based segmentation (Yoon and Kweon 2006; Tombari, Mattoccia, Di Stefano\net al. 2008). Three-dimensional support functions that have been proposed include limited\ndisparity difference (Grimson 1985), limited disparity gradient (Pollard, Mayhew, and Frisby\n1985), Prazdny’s coherence principle (Prazdny 1985), and the more recent work (which in-\ncludes visibility and occlusion reasoning) by Zitnick and Kanade (2000).\nAggregation with a ﬁxed support region can be performed using 2D or 3D convolution,\nC(x, y, d) = w(x, y, d) ∗C0(x, y, d),\n(11.6)\nor, in the case of rectangular windows, using efﬁcient moving average box-ﬁlters (Sec-\ntion 3.2.2) (Kanade, Yoshida, Oda et al. 1996; Kimura, Shinbo, Yamaguchi et al. 1999).\nShiftable windows can also be implemented efﬁciently using a separable sliding min-ﬁlter\n(Figure 11.8) (Scharstein and Szeliski 2002, Section 4.2). Selecting among windows of dif-\nferent shapes and sizes can be performed more efﬁciently by ﬁrst computing a summed area\n4 For two recent surveys and comparisons of such techniques, please see the work of Gong, Yang, Wang et al.\n(2007) and Tombari, Mattoccia, Di Stefano et al. (2008).",
  "571": "11.4 Local methods\n549\nFigure 11.8 Shiftable window (Scharstein and Szeliski 2002) c⃝2002 Springer. The effect\nof trying all 3 × 3 shifted windows around the black pixel is the same as taking the minimum\nmatching score across all centered (non-shifted) windows in the same neighborhood. (For\nclarity, only three of the neighboring shifted windows are shown here.)\n(a)\n(b)\n(c)\n(d)\nFigure 11.9\nAggregation window sizes and weights adapted to image content (Tombari,\nMattoccia, Di Stefano et al. 2008) c⃝2008 IEEE: (a) original image with selected evaluation\npoints; (b) variable windows (Veksler 2003); (c) adaptive weights (Yoon and Kweon 2006);\n(d) segmentation-based (Tombari, Mattoccia, and Di Stefano 2007). Notice how the adaptive\nweights and segmentation-based techniques adapt their support to similarly colored pixels.\ntable (Section 3.2.3, 3.30–3.32) (Veksler 2003). Selecting the right window is important,\nsince windows must be large enough to contain sufﬁcient texture and yet small enough so\nthat they do not straddle depth discontinuities (Figure 11.9). An alternative method for ag-\ngregation is iterative diffusion, i.e., repeatedly adding to each pixel’s cost the weighted values\nof its neighboring pixels’ costs (Szeliski and Hinton 1985; Shah 1993; Scharstein and Szeliski\n1998).\nOf the local aggregation methods compared by Gong, Yang, Wang et al. (2007) and\nTombari, Mattoccia, Di Stefano et al. (2008), the fast variable window approach of Vek-\nsler (2003) and the locally weighting approach developed by Yoon and Kweon (2006) con-\nsistently stood out as having the best tradeoff between performance and speed.5 The local\nweighting technique, in particular, is interesting because, instead of using square windows\nwith uniform weighting, each pixel within an aggregation window inﬂuences the ﬁnal match-\n5 More recent and extensive results from Tombari, Mattoccia, Di Stefano et al. (2008) can be found at http:\n//www.vision.deis.unibo.it/spe/SPEHome.aspx.",
  "572": "550\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ning cost based on its color similarity and spatial distance, just as in bilinear ﬁltering (Fig-\nure 11.9c). (In fact, their aggregation step is closely related to doing a joint bilateral ﬁlter\non the color/disparity image, except that it is done symmetrically in both reference and target\nimages.) The segmentation-based aggregation method of Tombari, Mattoccia, and Di Stefano\n(2007) did even better, although a fast implementation of this algorithm does not yet exist.\nIn local methods, the emphasis is on the matching cost computation and cost aggregation\nsteps. Computing the ﬁnal disparities is trivial: simply choose at each pixel the disparity\nassociated with the minimum cost value. Thus, these methods perform a local “winner-\ntake-all” (WTA) optimization at each pixel. A limitation of this approach (and many other\ncorrespondence algorithms) is that uniqueness of matches is only enforced for one image\n(the reference image), while points in the other image might match multiple points, unless\ncross-checking and subsequent hole ﬁlling is used (Fua 1993; Hirschm¨uller and Scharstein\n2009).\n11.4.1 Sub-pixel estimation and uncertainty\nMost stereo correspondence algorithms compute a set of disparity estimates in some dis-\ncretized space, e.g., for integer disparities (exceptions include continuous optimization tech-\nniques such as optical ﬂow (Bergen, Anandan, Hanna et al. 1992) or splines (Szeliski and\nCoughlan 1997)). For applications such as robot navigation or people tracking, these may be\nperfectly adequate. However for image-based rendering, such quantized maps lead to very\nunappealing view synthesis results, i.e., the scene appears to be made up of many thin shear-\ning layers. To remedy this situation, many algorithms apply a sub-pixel reﬁnement stage after\nthe initial discrete correspondence stage. (An alternative is to simply start with more discrete\ndisparity levels (Szeliski and Scharstein 2004).)\nSub-pixel disparity estimates can be computed in a variety of ways, including iterative\ngradient descent and ﬁtting a curve to the matching costs at discrete disparity levels (Ryan,\nGray, and Hunt 1980; Lucas and Kanade 1981; Tian and Huhns 1986; Matthies, Kanade,\nand Szeliski 1989; Kanade and Okutomi 1994). This provides an easy way to increase the\nresolution of a stereo algorithm with little additional computation. However, to work well,\nthe intensities being matched must vary smoothly, and the regions over which these estimates\nare computed must be on the same (correct) surface.\nRecently, some questions have been raised about the advisability of ﬁtting correlation\ncurves to integer-sampled matching costs (Shimizu and Okutomi 2001). This situation may\neven be worse when sampling-insensitive dissimilarity measures are used (Birchﬁeld and\nTomasi 1998). These issues are explored in more depth by Szeliski and Scharstein (2004).\nBesides sub-pixel computations, there are other ways of post-processing the computed\ndisparities. Occluded areas can be detected using cross-checking, i.e., comparing left-to-",
  "573": "11.4 Local methods\n551\n(a)\n(b)\n(c)\nFigure 11.10 Uncertainty in stereo depth estimation (Szeliski 1991b): (a) input image; (b)\nestimated depth map (blue is closer); (c) estimated conﬁdence(red is higher). As you can see,\nmore textured areas have higher conﬁdence.\nright and right-to-left disparity maps (Fua 1993). A median ﬁlter can be applied to clean\nup spurious mismatches, and holes due to occlusion can be ﬁlled by surface ﬁtting or by\ndistributing neighboring disparity estimates (Birchﬁeld and Tomasi 1999; Scharstein 1999;\nHirschm¨uller and Scharstein 2009).\nAnother kind of post-processing, which can be useful in later processing stages, is to asso-\nciate conﬁdences with per-pixel depth estimates (Figure 11.10), which can be done by looking\nat the curvature of the correlation surface, i.e., how strong the minimum in the DSI image is\nat the winning disparity. Matthies, Kanade, and Szeliski (1989) show that under the assump-\ntion of small noise, photometrically calibrated images, and densely sampled disparities, the\nvariance of a local depth estimate can be estimated as\nV ar(d) = σ2\nI\na ,\n(11.7)\nwhere a is the curvature of the DSI as a function of d, which can be measured using a local\nparabolic ﬁt or by squaring all the horizontal gradients in the window, and σ2\nI is the vari-\nance of the image noise, which can be estimated from the minimum SSD score. (See also\nSection 6.1.4, (8.44), and Appendix B.6.)\n11.4.2 Application: Stereo-based head tracking\nA common application of real-time stereo algorithms is for tracking the position of a user\ninteracting with a computer or game system. The use of stereo can dramatically improve\nthe reliability of such a system compared to trying to use monocular color and intensity\ninformation (Darrell, Gordon, Harville et al. 2000). Once recovered, this information can\nbe used in a variety of applications, including controlling a virtual environment or game,",
  "574": "552\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncorrecting the apparent gaze during video conferencing, and background replacement. We\ndiscuss the ﬁrst two applications below and defer the discussion of background replacement\nto Section 11.5.3.\nThe use of head tracking to control a user’s virtual viewpoint while viewing a 3D object\nor environment on a computer monitor is sometimes called ﬁsh tank virtual reality, since the\nuser is observing a 3D world as if it were contained inside a ﬁsh tank (Ware, Arthur, and\nBooth 1993). Early versions of these systems used mechanical head tracking devices and\nstereo glasses. Today, such systems can be controlled using stereo-based head tracking and\nstereo glasses can be replaced with autostereoscopic displays. Head tracking can also be used\nto construct a “virtual mirror”, where the user’s head can be modiﬁed in real-time using a\nvariety of visual effects (Darrell, Baker, Crow et al. 1997).\nAnother application of stereo head tracking and 3D reconstruction is in gaze correction\n(Ott, Lewis, and Cox 1993). When a user participates in a desktop video-conference or video\nchat, the camera is usually placed on top of the monitor. Since the person is gazing at a\nwindow somewhere on the screen, it appears as if they are looking down and away from the\nother participants, instead of straight at them. Replacing the single camera with two or more\ncameras enables a virtual view to be constructed right at the position where they are looking\nresulting in virtual eye contact. Real-time stereo matching is used to construct an accurate 3D\nhead model and view interpolation (Section 13.1) is used to synthesize the novel in-between\nview (Criminisi, Shotton, Blake et al. 2003).\n11.5 Global optimization\nGlobal stereo matching methods perform some optimization or iteration steps after the dis-\nparity computation phase and often skip the aggregation step altogether, because the global\nsmoothness constraints perform a similar function. Many global methods are formulated in\nan energy-minimization framework, where, as we saw in Sections 3.7 (3.100–3.102) and 8.4,\nthe objective is to ﬁnd a solution d that minimizes a global energy,\nE(d) = Ed(d) + λEs(d).\n(11.8)\nThe data term, Ed(d), measures how well the disparity function d agrees with the input image\npair. Using our previously deﬁned disparity space image, we deﬁne this energy as\nEd(d) =\nX\n(x,y)\nC(x, y, d(x, y)),\n(11.9)\nwhere C is the (initial or aggregated) matching cost DSI.\nThe smoothness term Es(d) encodes the smoothness assumptions made by the algorithm.\nTo make the optimization computationally tractable, the smoothness term is often restricted",
  "575": "11.5 Global optimization\n553\nto measuring only the differences between neighboring pixels’ disparities,\nEs(d) =\nX\n(x,y)\nρ(d(x, y) −d(x + 1, y)) + ρ(d(x, y) −d(x, y + 1)),\n(11.10)\nwhere ρ is some monotonically increasing function of disparity difference. It is also possi-\nble to use larger neighborhoods, such as N8, which can lead to better boundaries (Boykov\nand Kolmogorov 2003), or to use second-order smoothness terms (Woodford, Reid, Torr et\nal. 2008), but such terms require more complex optimization techniques. An alternative to\nsmoothness functionals is to use a lower-dimensional representation such as splines (Szeliski\nand Coughlan 1997).\nIn standard regularization (Section 3.7.1), ρ is a quadratic function, which makes d smooth\neverywhere and may lead to poor results at object boundaries. Energy functions that do not\nhave this problem are called discontinuity-preserving and are based on robust ρ functions\n(Terzopoulos 1986b; Black and Rangarajan 1996). The seminal paper by Geman and Ge-\nman (1984) gave a Bayesian interpretation of these kinds of energy functions and proposed a\ndiscontinuity-preserving energy function based on Markov random ﬁelds (MRFs) and addi-\ntional line processes, which are additional binary variables that control whether smoothness\npenalties are enforced or not. Black and Rangarajan (1996) show how independent line pro-\ncess variables can be replaced by robust pairwise disparity terms.\nThe terms in Es can also be made to depend on the intensity differences, e.g.,\nρd(d(x, y) −d(x + 1, y)) · ρI(∥I(x, y) −I(x + 1, y)∥),\n(11.11)\nwhere ρI is some monotonically decreasing function of intensity differences that lowers\nsmoothness costs at high-intensity gradients. This idea (Gamble and Poggio 1987; Fua 1993;\nBobick and Intille 1999; Boykov, Veksler, and Zabih 2001) encourages disparity discontinu-\nities to coincide with intensity or color edges and appears to account for some of the good\nperformance of global optimization approaches. While most researchers set these functions\nheuristically, Scharstein and Pal (2007) show how the free parameters in such conditional\nrandom ﬁelds (Section 3.7.2, (3.118)) can be learned from ground truth disparity maps.\nOnce the global energy has been deﬁned, a variety of algorithms can be used to ﬁnd a\n(local) minimum. Traditional approaches associated with regularization and Markov random\nﬁelds include continuation (Blake and Zisserman 1987), simulated annealing (Geman and\nGeman 1984; Marroquin, Mitter, and Poggio 1987; Barnard 1989), highest conﬁdence ﬁrst\n(Chou and Brown 1990), and mean-ﬁeld annealing (Geiger and Girosi 1991).\nMore recently, max-ﬂow and graph cut methods have been proposed to solve a special\nclass of global optimization problems (Roy and Cox 1998; Boykov, Veksler, and Zabih 2001;\nIshikawa 2003). Such methods are more efﬁcient than simulated annealing and have produced\ngood results, as have techniques based on loopy belief propagation (Sun, Zheng, and Shum",
  "576": "554\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n2003; Tappen and Freeman 2003). Appendix B.5 and a recent survey paper on MRF inference\n(Szeliski, Zabih, Scharstein et al. 2008) discuss and compare such techniques in more detail.\nWhile global optimization techniques currently produce the best stereo matching results,\nthere are some alternative approaches worth studying.\nCooperative algorithms.\nCooperative algorithms, inspired by computational models of hu-\nman stereo vision, were among the earliest methods proposed for disparity computation (Dev\n1974; Marr and Poggio 1976; Marroquin 1983; Szeliski and Hinton 1985; Zitnick and Kanade\n2000). Such algorithms iteratively update disparity estimates using non-linear operations that\nresult in an overall behavior similar to global optimization algorithms. In fact, for some of\nthese algorithms, it is possible to explicitly state a global function that is being minimized\n(Scharstein and Szeliski 1998).\nCoarse-to-ﬁne and incremental warping.\nMost of today’s best algorithms ﬁrst enumer-\nate all possible matches at all possible disparities and then select the best set of matches in\nsome way. Faster approaches can sometimes be obtained using methods inspired by classic\n(inﬁnitesimal) optical ﬂow computation. Here, images are successively warped and disparity\nestimates incrementally updated until a satisfactory registration is achieved. These techniques\nare most often implemented within a coarse-to-ﬁne hierarchical reﬁnement framework (Quam\n1984; Bergen, Anandan, Hanna et al. 1992; Barron, Fleet, and Beauchemin 1994; Szeliski\nand Coughlan 1997).\n11.5.1 Dynamic programming\nA different class of global optimization algorithm is based on dynamic programming. While\nthe 2D optimization of Equation (11.8) can be shown to be NP-hard for common classes\nof smoothness functions (Veksler 1999), dynamic programming can ﬁnd the global mini-\nmum for independent scanlines in polynomial time. Dynamic programming was ﬁrst used\nfor stereo vision in sparse, edge-based methods (Baker and Binford 1981; Ohta and Kanade\n1985). More recent approaches have focused on the dense (intensity-based) scanline match-\ning problem (Belhumeur 1996; Geiger, Ladendorf, and Yuille 1992; Cox, Hingorani, Rao et\nal. 1996; Bobick and Intille 1999; Birchﬁeld and Tomasi 1999). These approaches work by\ncomputing the minimum-cost path through the matrix of all pairwise matching costs between\ntwo corresponding scanlines, i.e., through a horizontal slice of the DSI. Partial occlusion is\nhandled explicitly by assigning a group of pixels in one image to a single pixel in the other\nimage. Figure 11.11 schematically shows how DP works, while Figure 11.5f shows a real\nDSI slice over which the DP is applied.",
  "577": "11.5 Global optimization\n555\nc\nd\ne\nf\ng\nk\na\nLeft scanline\ni\nRight scanline\na\nc\nf\ng\nj\nk\nh\nb\nM\nL\nR\nR\nR\nM\nL\nL\nM\nM\nM\nk\nd\n1\n2\n3\n2\n8\n10\nm\n1\n2\n3\n4\n0\nLeft\nn\n1\n2\n3\n4\nRight\nCyclopean\nDisparity\n6\n4\n(a)\n(b)\nFigure 11.11 Stereo matching using dynamic programming, as illustrated by (a) Scharstein\nand Szeliski (2002) c⃝2002 Springer and (b) Kolmogorov, Criminisi, Blake et al. (2006). c⃝\n2006 IEEE. For each pair of corresponding scanlines, a minimizing path through the matrix\nof all pairwise matching costs (DSI) is selected. Lowercase letters (a–k) symbolize the inten-\nsities along each scanline. Uppercase letters represent the selected path through the matrix.\nMatches are indicated by M, while partially occluded points (which have a ﬁxed cost) are\nindicated by L or R, corresponding to points only visible in the left or right images, respec-\ntively. Usually, only a limited disparity range is considered (0–4 in the ﬁgure, indicated by\nthe non-shaded squares). The representation in (a) allows for diagonal moves while the rep-\nresentation in (b) does not. Note that these diagrams, which use the Cyclopean representation\nof depth, i.e., depth relative to a camera between the two input cameras, show an “unskewed”\nx-d slice through the DSI.\nTo implement dynamic programming for a scanline y, each entry (state) in a 2D cost\nmatrix D(m, n) is computed by combining its DSI value\nC′(m, n) = C(m + n, m −n, y)\n(11.12)\nwith one of its predecessor cost values.\nUsing the representation shown in Figure 11.11a,\nwhich allows for “diagonal” moves, the aggregated match costs can be recursively computed\nas\nD(m, n, M)\n=\nmin(D(m−1, n−1, M), D(m−1, n, L), D(m−1, n−1, R)) + C′(m, n)\nD(m, n, L)\n=\nmin(D(m−1, n−1, M), D(m−1, n, L)) + O\n(11.13)\nD(m, n, R)\n=\nmin(D(m, n−1, M), D(m, n−1, R)) + O,\nwhere O is a per-pixel occlusion cost. The aggregation rules corresponding to Figure 11.11b\nare given by Kolmogorov, Criminisi, Blake et al. (2006), who also use a two-state foreground–\nbackground model for bi-layer segmentation.",
  "578": "556\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nProblems with dynamic programming stereo include the selection of the right cost for\noccluded pixels and the difﬁculty of enforcing inter-scanline consistency, although several\nmethods propose ways of addressing the latter (Ohta and Kanade 1985; Belhumeur 1996;\nCox, Hingorani, Rao et al. 1996; Bobick and Intille 1999; Birchﬁeld and Tomasi 1999;\nKolmogorov, Criminisi, Blake et al. 2006). Another problem is that the dynamic program-\nming approach requires enforcing the monotonicity or ordering constraint (Yuille and Poggio\n1984). This constraint requires that the relative ordering of pixels on a scanline remain the\nsame between the two views, which may not be the case in scenes containing narrow fore-\nground objects.\nAn alternative to traditional dynamic programming, introduced by Scharstein and Szeliski\n(2002), is to neglect the vertical smoothness constraints in (11.10) and simply optimize in-\ndependent scanlines in the global energy function (11.8), which can easily be done using a\nrecursive algorithm,\nD(x, y, d) = C(x, y, d) + min\nd′ {D(x −1, y, d′) + ρd(d −d′)} .\n(11.14)\nThe advantage of this scanline optimization algorithm is that it computes the same represen-\ntation and minimizes a reduced version of the same energy function as the full 2D energy\nfunction (11.8). Unfortunately, it still suffers from the same streaking artifacts as dynamic\nprogramming.\nA much better approach is to evaluate the cumulative cost function (11.14) from multiple\ndirections, e.g, from the eight cardinal directions, N, E, W, S, NE, SE, SW, NW (Hirschm¨uller\n2008). The resulting semi-global optimization performs quite well and is extremely efﬁcient\nto implement.\nEven though dynamic programming and scanline optimization algorithms do not gen-\nerally produce the most accurate stereo reconstructions, when combined with sophisticated\naggregation strategies, they can produce very fast and high-quality results.\n11.5.2 Segmentation-based techniques\nWhile most stereo matching algorithms perform their computations on a per-pixel basis, some\nof the more recent techniques ﬁrst segment the images into regions and then try to label each\nregion with a disparity.\nFor example, Tao, Sawhney, and Kumar (2001) segment the reference image, estimate\nper-pixel disparities using a local technique, and then do local plane ﬁts inside each segment\nbefore applying smoothness constraints between neighboring segments. Zitnick, Kang, Uyt-\ntendaele et al. (2004) and Zitnick and Kang (2007) use over-segmentation to mitigate initial\nbad segmentations. After a set of initial cost values for each segment has been stored into\na disparity space distribution (DSD), iterative relaxation (or loopy belief propagation, in the",
  "579": "11.5 Global optimization\n557\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 11.12 Segmentation-based stereo matching (Zitnick, Kang, Uyttendaele et al. 2004)\nc⃝2004 ACM: (a) input color image; (b) color-based segmentation; (c) initial disparity es-\ntimates; (d) ﬁnal piecewise-smoothed disparities; (e) MRF neighborhood deﬁned over the\nsegments in the disparity space distribution (Zitnick and Kang 2007) c⃝2007 Springer.\n(a)\n(b)\nFigure 11.13\nStereo matching with adaptive over-segmentation and matting (Taguchi,\nWilburn, and Zitnick 2008) c⃝2008 IEEE: (a) segment boundaries are reﬁned during the\noptimization, leading to more accurate results (e.g., the thin green leaf in the bottom row); (b)\nalpha mattes are extracted at segment boundaries, which leads to visually better compositing\nresults (middle column).\nmore recent work of Zitnick and Kang (2007)) is used to adjust the disparity estimates for\neach segment, as shown in Figure 11.12. Taguchi, Wilburn, and Zitnick (2008) reﬁne the\nsegment shapes as part of the optimization process, which leads to much improved results, as\nshown in Figure 11.13.\nEven more accurate results are obtained by Klaus, Sormann, and Karner (2006), who ﬁrst\nsegment the reference image using mean shift, run a small (3 × 3) SAD plus gradient SAD\n(weighted by cross-checking) to get initial disparity estimates, ﬁt local planes, re-ﬁt with\nglobal planes, and then run a ﬁnal MRF on plane assignments with loopy belief propagation.\nWhen the algorithm was ﬁrst introduced in 2006, it was the top ranked algorithm on the\nevaluation site at http://vision.middlebury.edu/stereo; in early 2010, it still had the top rank\non the new evaluation datasets.\nThe highest ranked algorithm, by Wang and Zheng (2008), follows a similar approach of\nsegmenting the image, doing local plane ﬁts, and then performing cooperative optimization",
  "580": "558\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nof neighboring plane ﬁt parameters. Another highly ranked algorithm, by Yang, Wang, Yang\net al. (2009), uses the color correlation approach of Yoon and Kweon (2006) and hierarchical\nbelief propagation to obtain an initial set of disparity estimates. After left–right consistency\nchecking to detect occluded pixels, the data terms for low-conﬁdence and occluded pixels\nare recomputed using segmentation-based plane ﬁts and one or more rounds of hierarchical\nbelief propagation are used to obtain the ﬁnal disparity estimates.\nAnother important ability of segmentation-based stereo algorithms, which they share with\nalgorithms that use explicit layers (Baker, Szeliski, and Anandan 1998; Szeliski and Golland\n1999) or boundary extraction (Hasinoff, Kang, and Szeliski 2006), is the ability to extract\nfractional pixel alpha mattes at depth discontinuities (Bleyer, Gelautz, Rother et al. 2009).\nThis ability is crucial when attempting to create virtual view interpolation without clinging\nboundary or tearing artifacts (Zitnick, Kang, Uyttendaele et al. 2004) and also to seamlessly\ninsert virtual objects (Taguchi, Wilburn, and Zitnick 2008), as shown in Figure 11.13b.\nSince new stereo matching algorithms continue to be introduced every year, it is a good\nidea to periodically check the Middlebury evaluation site at http://vision.middlebury.edu/\nstereo for a listing of the most recent algorithms to be evaluated.\n11.5.3 Application: Z-keying and background replacement\nAnother application of real-time stereo matching is z-keying, which is the process of seg-\nmenting a foreground actor from the background using depth information, usually for the\npurpose of replacing the background with some computer-generated imagery, as shown in\nFigure 11.2g.\nOriginally, z-keying systems required expensive custom-built hardware to produce the\ndesired depth maps in real time and were, therefore, restricted to broadcast studio applica-\ntions (Kanade, Yoshida, Oda et al. 1996; Iddan and Yahav 2001). Off-line systems were also\ndeveloped for estimating 3D multi-viewpoint geometry from video streams (Section 13.5.4)\n(Kanade, Rander, and Narayanan 1997; Carranza, Theobalt, Magnor et al. 2003; Zitnick,\nKang, Uyttendaele et al. 2004; Vedula, Baker, and Kanade 2005). Recent advances in highly\naccurate real-time stereo matching, however, now make it possible to perform z-keying on\nregular PCs, enabling desktop videoconferencing applications such as those shown in Fig-\nure 11.14 (Kolmogorov, Criminisi, Blake et al. 2006).\n11.6 Multi-view stereo\nWhile matching pairs of images is a useful way of obtaining depth information, matching\nmore images can lead to even better results. In this section, we review not only techniques for",
  "581": "11.6 Multi-view stereo\n559\nFigure 11.14\nBackground replacement using z-keying with a bi-layer segmentation algo-\nrithm (Kolmogorov, Criminisi, Blake et al. 2006) c⃝2006 IEEE.\ncreating complete 3D object models, but also simpler techniques for improving the quality of\ndepth maps using multiple source images.\nAs we saw in our discussion of plane sweep (Section 11.1.2), it is possible to resample\nall neighboring k images at each disparity hypothesis d into a generalized disparity space\nvolume ˜I(x, y, d, k). The simplest way to take advantage of these additional images is to sum\nup their differences from the reference image Ir as in (11.4),\nC(x, y, d) =\nX\nk\nρ(˜I(x, y, d, k) −Ir(x, y)).\n(11.15)\nThis is the basis of the well-known sum of summed-squared-difference (SSSD) and SSAD\napproaches (Okutomi and Kanade 1993; Kang, Webb, Zitnick et al. 1995), which can be ex-\ntended to reason about likely patterns of occlusion (Nakamura, Matsuura, Satoh et al. 1996).\nMore recent work by Gallup, Frahm, Mordohai et al. (2008) show how to adapt the base-\nlines used to the expected depth in order to get the best tradeoff between geometric accuracy\n(wide baseline) and robustness to occlusion (narrow baseline). Alternative multi-view cost\nmetrics include measures such as synthetic focus sharpness and the entropy of the pixel color\ndistribution (Vaish, Szeliski, Zitnick et al. 2006).\nA useful way to visualize the multi-frame stereo estimation problem is to examine the\nepipolar plane image (EPI) formed by stacking corresponding scanlines from all the images,\nas shown in Figures 8.13c and 11.15 (Bolles, Baker, and Marimont 1987; Baker and Bolles\n1989; Baker 1989). As you can see in Figure 11.15, as a camera translates horizontally (in a\nstandard horizontally rectiﬁed geometry), objects at different depths move sideways at a rate\ninversely proportional to their depth (11.1).6 Foreground objects occlude background objects,\nwhich can be seen as EPI-strips (Criminisi, Kang, Swaminathan et al. 2005) occluding other\n6 The four-dimensional generalization of the EPI is the light ﬁeld, which we study in Section 13.3. In principle,",
  "582": "560\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nA\nE\nD\nC\nB\nleft\nmiddle\nright\nF\nx\nt\n(a)\n(b)\nFigure 11.15 Epipolar plane image (EPI) (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996\nACM and a schematic EPI (Kang, Szeliski, and Chai 2001) c⃝2001 IEEE. (a) The Lumigraph\n(light ﬁeld) (Section 13.3) is the 4D space of all light rays passing through a volume of space.\nTaking a 2D slice results in all of the light rays embedded in a plane and is equivalent to a\nscanline taken from a stacked EPI volume. Objects at different depths move sideways with\nvelocities (slopes) proportional to their inverse depth. Occlusion (and translucency) effects\ncan easily be seen in this representation. (b) The EPI corresponding to Figure 11.16 showing\nthe three images (middle, left, and right) as slices through the EPI volume. The spatially and\ntemporally shifted window around the black pixel is indicated by the rectangle, showing the\nright image is not being used in matching.\nstrips in the EPI. If we are given a dense enough set of images, we can ﬁnd such strips and\nreason about their relationships in order to both reconstruct the 3D scene and make inferences\nabout translucent objects (Tsin, Kang, and Szeliski 2006) and specular reﬂections (Swami-\nnathan, Kang, Szeliski et al. 2002; Criminisi, Kang, Swaminathan et al. 2005). Alternatively,\nwe can treat the series of images as a set of sequential observations and merge them using\nKalman ﬁltering (Matthies, Kanade, and Szeliski 1989) or maximum likelihood inference\n(Cox 1994).\nWhen fewer images are available, it becomes necessary to fall back on aggregation tech-\nniques such as sliding windows or global optimization. With additional input images, how-\never, the likelihood of occlusions increases. It is therefore prudent to adjust not only the best\nwindow locations using a shiftable window approach, as shown in Figure 11.16a, but also to\noptionally select a subset of neighboring frames in order to discount those images where the\nregion of interest is occluded, as shown in Figure 11.16b (Kang, Szeliski, and Chai 2001).\nthere is enough information in a light ﬁeld to recover both the shape and the BRDF of objects (Soatto, Yezzi, and Jin\n2003), although relatively little progress has been made to date on this topic.",
  "583": "11.6 Multi-view stereo\n561\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nA\nB C\nD E\nF\nleft\nmiddle\nright\n(a)\n(b)\nFigure 11.16 Spatio-temporally shiftable windows (Kang, Szeliski, and Chai 2001) c⃝2001\nIEEE: A simple three-image sequence (the middle image is the reference image), which has\na moving frontal gray square (marked F) and a stationary background. Regions B, C, D, and\nE are partially occluded. (a) A regular SSD algorithm will make mistakes when matching\npixels in these regions (e.g. the window centered on the black pixel in region B) and in\nwindows straddling depth discontinuities (the window centered on the white pixel in region\nF). (b) Shiftable windows help mitigate the problems in partially occluded regions and near\ndepth discontinuities. The shifted window centered on the white pixel in region F matches\ncorrectly in all frames. The shifted window centered on the black pixel in region B matches\ncorrectly in the left image, but requires temporal selection to disable matching the right image.\nFigure 11.15b shows an EPI corresponding to this sequence and describes in more detail how\ntemporal selection works.\nFigure11.15b shows how such spatio-temporal selection or shifting of windows corresponds\nto selecting the most likely un-occluded volumetric region in the epipolar plane image vol-\nume.\nThe results of applying these techniques to the multi-frame ﬂower garden image sequence\nare shown in Figure 11.17, which compares the results of using regular (non-shifted) SSSD\nwith spatially shifted windows and full spatio-temporal window selection.\n(The task of\napplying stereo to a rigid scene ﬁlmed with a moving camera is sometimes called motion\nstereo). Similar improvements from using spatio-temporal selection are reported by (Kang\nand Szeliski 2004) and are evident even when local measurements are combined with global\noptimization.\nWhile computing a depth map from multiple inputs outperforms pairwise stereo match-\ning, even more dramatic improvements can be obtained by estimating multiple depth maps\nsimultaneously (Szeliski 1999; Kang and Szeliski 2004). The existence of multiple depth\nmaps enables more accurate reasoning about occlusions, as regions which are occluded in\none image may be visible (and matchable) in others. The multi-view reconstruction problem\ncan be formulated as the simultaneous estimation of depth maps at key frames (Figure 8.13c)\nwhile maximizing not only photoconsistency and piecewise disparity smoothness but also the\nconsistency between disparity estimates at different frames. While Szeliski (1999) and Kang",
  "584": "562\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 11.17 Local (5 × 5 window-based) matching results (Kang, Szeliski, and Chai 2001)\nc⃝2001 IEEE: (a) window that is not spatially perturbed (centered); (b) spatially perturbed\nwindow; (c) using the best ﬁve of 10 neighboring frames; (d) using the better half sequence.\nNotice how the results near the tree trunk are improved using temporal selection.\nand Szeliski (2004) use soft (penalty-based) constraints to encourage multiple disparity maps\nto be consistent, Kolmogorov and Zabih (2002) show how such consistency measures can\nbe encoded as hard constraints, which guarantee that the multiple depth maps are not only\nsimilar but actually identical in overlapping regions. Newer algorithms that simultaneously\nestimate multiple disparity maps include papers by Maitre, Shinagawa, and Do (2008) and\nZhang, Jia, Wong et al. (2008).\nA closely related topic to multi-frame stereo estimation is scene ﬂow, in which multiple\ncameras are used to capture a dynamic scene. The task is then to simultaneously recover the\n3D shape of the object at every instant in time and to estimate the full 3D motion of every\nsurface point between frames. Representative papers in this area include those by Vedula,\nBaker, Rander et al. (2005), Zhang and Kambhamettu (2003), Pons, Keriven, and Faugeras\n(2007), Huguet and Devernay (2007), and Wedel, Rabe, Vaudrey et al. (2008). Figure 11.18a\nshows an image of the 3D scene ﬂow for the tango dancer shown in Figure 11.2h–j, while\nFigure 11.18b shows 3D scene ﬂows captured from a moving vehicle for the purpose of\nobstacle avoidance. In addition to supporting mensuration and safety applications, scene\nﬂow can be used to support both spatial and temporal view interpolation (Section 13.5.4), as\ndemonstrated by Vedula, Baker, and Kanade (2005).\n11.6.1 Volumetric and 3D surface reconstruction\nAccording to Seitz, Curless, Diebel et al. (2006):\nThe goal of multi-view stereo is to reconstruct a complete 3D object model from\na collection of images taken from known camera viewpoints.\nThe most challenging but potentially most useful variant of multi-view stereo reconstruc-\ntion is to create globally consistent 3D models. This topic has a long history in computer\nvision, starting with surface mesh reconstruction techniques such as the one developed by",
  "585": "11.6 Multi-view stereo\n563\n(a)\n(b)\nFigure 11.18 Three-dimensional scene ﬂow: (a) computed from a multi-camera dome sur-\nrounding the dancer shown in Figure 11.2h–j (Vedula, Baker, Rander et al. 2005) c⃝2005\nIEEE; (b) computed from stereo cameras mounted on a moving vehicle (Wedel, Rabe, Vau-\ndrey et al. 2008) c⃝2008 Springer.\nFua and Leclerc (1995) (Figure 11.19a). A variety of approaches and representations have\nbeen used to solve this problem, including 3D voxel representations (Seitz and Dyer 1999;\nSzeliski and Golland 1999; De Bonet and Viola 1999; Kutulakos and Seitz 2000; Eisert, Stein-\nbach, and Girod 2000; Slabaugh, Culbertson, Slabaugh et al. 2004; Sinha and Pollefeys 2005;\nVogiatzis, Hernandez, Torr et al. 2007; Hiep, Keriven, Pons et al. 2009), level sets (Faugeras\nand Keriven 1998; Pons, Keriven, and Faugeras 2007), polygonal meshes (Fua and Leclerc\n1995; Narayanan, Rander, and Kanade 1998; Hernandez and Schmitt 2004; Furukawa and\nPonce 2009), and multiple depth maps (Kolmogorov and Zabih 2002). Figure 11.19 shows\nrepresentative examples of 3D object models reconstructed using some of these techniques.\nIn order to organize and compare all these techniques, Seitz, Curless, Diebel et al. (2006)\ndeveloped a six-point taxonomy that can help classify algorithms according to the scene rep-\nresentation, photoconsistency measure, visibility model, shape priors, reconstruction algo-\nrithm, and initialization requirements they use. Below, we summarize some of these choices\nand list a few representative papers. For more details, please consult the full survey paper\n(Seitz, Curless, Diebel et al. 2006) and the evaluation Web site, http://vision.middlebury.edu/\nmview/, which contains pointers to even more recent papers and results.\nScene representation.\nOne of the more popular 3D representations is a uniform grid of 3D\nvoxels,7 which can be reconstructed using a variety of carving (Seitz and Dyer 1999; Kutu-\nlakos and Seitz 2000) or optimization (Sinha and Pollefeys 2005; Vogiatzis, Hernandez, Torr\net al. 2007; Hiep, Keriven, Pons et al. 2009) techniques. Level set techniques (Section 5.1.4)\nalso operate on a uniform grid but, instead of representing a binary occupancy map, they\nrepresent the signed distance to the surface (Faugeras and Keriven 1998; Pons, Keriven, and\nFaugeras 2007), which can encode a ﬁner level of detail. Polygonal meshes are another pop-\n7 For outdoor scenes that go to inﬁnity, a non-uniform gridding of space may be preferable (Slabaugh, Culbertson,\nSlabaugh et al. 2004).",
  "586": "564\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 11.19\nMulti-view stereo algorithms: (a) surface-based stereo (Fua and Leclerc\n1995); (b) voxel coloring (Seitz and Dyer 1999) c⃝1999 Springer; (c) depth map merg-\ning (Narayanan, Rander, and Kanade 1998); (d) level set evolution (Faugeras and Keriven\n1998) c⃝1998 IEEE; (e) silhouette and stereo fusion (Hernandez and Schmitt 2004) c⃝2004\nElsevier; (f) multi-view image matching (Pons, Keriven, and Faugeras 2005) c⃝2005 IEEE;\n(g) volumetric graph cut (Vogiatzis, Torr, and Cipolla 2005) c⃝2005 IEEE; (h) carved visual\nhulls (Furukawa and Ponce 2009) c⃝2009 Springer.\nular representation (Fua and Leclerc 1995; Narayanan, Rander, and Kanade 1998; Isidoro\nand Sclaroff 2003; Hernandez and Schmitt 2004; Furukawa and Ponce 2009; Hiep, Keriven,\nPons et al. 2009). Meshes are the standard representation used in computer graphics and\nalso readily support the computation of visibility and occlusions. Finally, as we discussed in\nthe previous section, multiple depth maps can also be used (Szeliski 1999; Kolmogorov and\nZabih 2002; Kang and Szeliski 2004). Many algorithms also use more than a single represen-\ntation, e.g., they may start by computing multiple depth maps and then merge them into a 3D\nobject model (Narayanan, Rander, and Kanade 1998; Furukawa and Ponce 2009; Goesele,\nCurless, and Seitz 2006; Goesele, Snavely, Curless et al. 2007; Furukawa, Curless, Seitz et\nal. 2010).",
  "587": "11.6 Multi-view stereo\n565\nPhotoconsistency measure.\nAs we discussed in (Section 11.3.1), a variety of similarity\nmeasures can be used to compare pixel values in different images, including measures that\ntry to discount illumination effects or be less sensitive to outliers. In multi-view stereo, algo-\nrithms have a choice of computing these measures directly on the surface of the model, i.e., in\nscene space, or projecting pixel values from one image (or from a textured model) back into\nanother image, i.e., in image space. (The latter corresponds more closely to a Bayesian ap-\nproach, since input images are noisy measurements of the colored 3D model.) The geometry\nof the object, i.e., its distance to each camera and its local surface normal, when available, can\nbe used to adjust the matching windows used in the computation to account for foreshortening\nand scale change (Goesele, Snavely, Curless et al. 2007).\nVisibility model.\nA big advantage that multi-view stereo algorithms have over single-depth-\nmap approaches is their ability to reason in a principled manner about visibility and occlu-\nsions. Techniques that use the current state of the 3D model to predict which surface pixels\nare visible in each image (Kutulakos and Seitz 2000; Faugeras and Keriven 1998; Vogiatzis,\nHernandez, Torr et al. 2007; Hiep, Keriven, Pons et al. 2009) are classiﬁed as using geometric\nvisibility models in the taxonomy of Seitz, Curless, Diebel et al. (2006). Techniques that se-\nlect a neighboring subset of image to match are called quasi-geometric (Narayanan, Rander,\nand Kanade 1998; Kang and Szeliski 2004; Hernandez and Schmitt 2004), while techniques\nthat use traditional robust similarity measures are called outlier-based. While full geometric\nreasoning is the most principled and accurate approach, it can be very slow to evaluate and\ndepends on the evolving quality of the current surface estimate to predict visibility, which can\nbe a bit of a chicken-and-egg problem, unless conservative assumptions are used, as they are\nby Kutulakos and Seitz (2000).\nShape priors.\nBecause stereo matching is often underconstrained, especially in texture-\nless regions, most matching algorithms adopt (either explicitly or implicitly) some form of\nprior model for the expected shape. Many of the techniques that rely on optimization use a\n3D smoothness or area-based photoconsistency constraint, which, because of the natural ten-\ndency of smooth surfaces to shrink inwards, often results in a minimal surface prior (Faugeras\nand Keriven 1998; Sinha and Pollefeys 2005; Vogiatzis, Hernandez, Torr et al. 2007). Ap-\nproaches that carve away the volume of space often stop once a photoconsistent solution is\nfound (Seitz and Dyer 1999; Kutulakos and Seitz 2000), which corresponds to a maximal sur-\nface bias, i.e., these techniques tend to over-estimate the true shape. Finally, multiple depth\nmap approaches often adopt traditional image-based smoothness (regularization) constraints.\nReconstruction algorithm.\nThe details of how the actual reconstruction algorithm pro-\nceeds is where the largest variety—and greatest innovation—in multi-view stereo algorithms",
  "588": "566\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncan be found.\nSome approaches use global optimization deﬁned over a three-dimensional photoconsis-\ntency volume to recover a complete surface. Approaches based on graph cuts use polynomial\ncomplexity binary segmentation algorithms to recover the object model deﬁned on the voxel\ngrid (Sinha and Pollefeys 2005; Vogiatzis, Hernandez, Torr et al. 2007; Hiep, Keriven, Pons\net al. 2009). Level set approaches use a continuous surface evolution to ﬁnd a good mini-\nmum in the conﬁguration space of potential surfaces and therefore require a reasonably good\ninitialization (Faugeras and Keriven 1998; Pons, Keriven, and Faugeras 2007). In order for\nthe photoconsistency volume to be meaningful, matching costs need to be computed in some\nrobust fashion, e.g., using sets of limited views or by aggregating multiple depth maps.\nAn alternative approach to global optimization is to sweep through the 3D volume while\ncomputing both photoconsistency and visibility simultaneously. The voxel coloring algorithm\nof Seitz and Dyer (1999) performs a front-to-back plane sweep. On every plane, any voxels\nthat are sufﬁciently photoconsistent are labeled as part of the object. The corresponding\npixels in the source images can then be “erased”, since they are already accounted for, and\ntherefore do not contribute to further photoconsistency computations. (A similar approach,\nalbeit without the front-to-back sweep order, is used by Szeliski and Golland (1999).) The\nresulting 3D volume, under noise- and resampling-free conditions, is guaranteed to produce\nboth a photoconsistent 3D model and to enclose whatever true 3D object model generated the\nimages.\nUnfortunately, voxel coloring is only guaranteed to work if all of the cameras lie on the\nsame side of the sweep planes, which is not possible in general ring conﬁgurations of cameras.\nKutulakos and Seitz (2000) generalize voxel coloring to space carving, where subsets of\ncameras that satisfy the voxel coloring constraint are iteratively selected and the 3D voxel\ngrid is alternately carved away along different axes.\nAnother popular approach to multi-view stereo is to ﬁrst independently compute multiple\ndepth maps and then merge these partial maps into a complete 3D model. Approaches to\ndepth map merging, which are discussed in more detail in Section 12.2.1, include signed\ndistance functions (Curless and Levoy 1996), used by Goesele, Curless, and Seitz (2006),\nand Poisson surface reconstruction (Kazhdan, Bolitho, and Hoppe 2006), used by Goesele,\nSnavely, Curless et al. (2007). It is also possible to reconstruct sparser representations, such\nas 3D points and lines, and to interpolate them to full 3D surfaces (Section 12.3.1) (Taylor\n2003).\nInitialization requirements.\nOne ﬁnal element discussed by Seitz, Curless, Diebel et al.\n(2006) is the varying degrees of initialization required by different algorithms. Because some\nalgorithms reﬁne or evolve a rough 3D model, they require a reasonably accurate (or over-\ncomplete) initial model, which can often be obtained by reconstructing a volume from object",
  "589": "11.6 Multi-view stereo\n567\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 11.20 The multi-view stereo data sets captured by Seitz, Curless, Diebel et al. (2006)\nc⃝2006 Springer. Only (a) and (b) are currently used for evaluation.\nsilhouettes, as discussed in Section 11.6.2. However, if the algorithm performs a global op-\ntimization (Kolev, Klodt, Brox et al. 2009; Kolev and Cremers 2009), this dependence on\ninitialization is not an issue.\nEmpirical evaluation.\nIn order to evaluate the large number of design alternatives in multi-\nview stereo, Seitz, Curless, Diebel et al. (2006) collected a dataset of calibrated images using\na spherical gantry. A representative image from each of the six datasets is shown in Fig-\nure 11.20, although only the ﬁrst two datasets have as yet been fully processed and used for\nevaluation. Figure 11.21 shows the results of running seven different algorithms on the tem-\nple dataset. As you can see, most of the techniques do an impressive job of capturing the ﬁne\ndetails in the columns, although it is also clear that the techniques employ differing amounts\nof smoothing to achieve these results.\nSince the publication of the survey by Seitz, Curless, Diebel et al. (2006), the ﬁeld of\nmulti-view stereo has continued to advance at a rapid pace (Strecha, Fransens, and Van\nGool 2006; Hernandez, Vogiatzis, and Cipolla 2007; Habbecke and Kobbelt 2007; Furukawa\nand Ponce 2007; Vogiatzis, Hernandez, Torr et al. 2007; Goesele, Snavely, Curless et al.\n2007; Sinha, Mordohai, and Pollefeys 2007; Gargallo, Prados, and Sturm 2007; Merrell, Ak-\nbarzadeh, Wang et al. 2007; Zach, Pock, and Bischof 2007b; Furukawa and Ponce 2008;\nHornung, Zeng, and Kobbelt 2008; Bradley, Boubekeur, and Heidrich 2008; Zach 2008;\nCampbell, Vogiatzis, Hern´andez et al. 2008; Kolev, Klodt, Brox et al. 2009; Hiep, Keriven,\nPons et al. 2009; Furukawa, Curless, Seitz et al. 2010). The multi-view stereo evaluation site,\nhttp://vision.middlebury.edu/mview/, provides quantitative results for these algorithms along\nwith pointers to where to ﬁnd these papers.\n11.6.2 Shape from silhouettes\nIn many situations, performing a foreground–background segmentation of the object of in-\nterest is a good way to initialize or ﬁt a 3D model (Grauman, Shakhnarovich, and Darrell",
  "590": "568\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 11.21 Reconstruction results (details) for seven algorithms (Hernandez and Schmitt\n2004; Furukawa and Ponce 2009; Pons, Keriven, and Faugeras 2005; Goesele, Curless, and\nSeitz 2006; Vogiatzis, Torr, and Cipolla 2005; Tran and Davis 2002; Kolmogorov and Zabih\n2002) evaluated by Seitz, Curless, Diebel et al. (2006) on the 47-image Temple Ring dataset.\nThe numbers underneath each detail image are the accuracy of each of these techniques mea-\nsured in millimeters.\n2003; Vlasic, Baran, Matusik et al. 2008) or to impose a convex set of constraints on multi-\nview stereo (Kolev and Cremers 2008). Over the years, a number of techniques have been\ndeveloped to reconstruct a 3D volumetric model from the intersection of the binary silhou-\nettes projected into 3D. The resulting model is called a visual hull (or sometimes a line hull),\nanalogous with the convex hull of a set of points, since the volume is maximal with respect\nto the visual silhouettes and surface elements are tangent to the viewing rays (lines) along\nthe silhouette boundaries (Laurentini 1994). It is also possible to carve away a more accu-\nrate reconstruction using multi-view stereo (Sinha and Pollefeys 2005) or by analyzing cast\nshadows (Savarese, Andreetto, Rushmeier et al. 2007).\nSome techniques ﬁrst approximate each silhouette with a polygonal representation and\nthen intersect the resulting faceted conical regions in three-space to produce polyhedral mod-\nels (Baumgart 1974; Martin and Aggarwal 1983; Matusik, Buehler, and McMillan 2001),\nwhich can later be reﬁned using triangular splines (Sullivan and Ponce 1998). Other ap-\nproaches use voxel-based representations, usually encoded as octrees (Samet 1989), because\nof the resulting space–time efﬁciency. Figures 11.22a–b show an example of a 3D octree\nmodel and its associated colored tree, where black nodes are interior to the model, white",
  "591": "11.6 Multi-view stereo\n569\n(a)\n(b)\n(c)\n(d)\nFigure 11.22\nVolumetric octree reconstruction from binary silhouettes (Szeliski 1993) c⃝\n1993 Elsevier: (a) octree representation and its corresponding (b) tree structure; (c) input\nimage of an object on a turntable; (d) computed 3D volumetric octree model.\nnodes are exterior, and gray nodes are of mixed occupancy. Examples of octree-based re-\nconstruction approaches include those by Potmesil (1987), Noborio, Fukada, and Arimoto\n(1988), Srivasan, Liang, and Hackwood (1990), and Szeliski (1993).\nThe approach of Szeliski (1993) ﬁrst converts each binary silhouette into a one-sided\nvariant of a distance map, where each pixel in the map indicates the largest square that is\ncompletely inside (or outside) the silhouette. This makes it fast to project an octree cell\ninto the silhouette to conﬁrm whether it is completely inside or outside the object, so that\nit can be colored black, white, or left as gray (mixed) for further reﬁnement on a smaller\ngrid. The octree construction algorithm proceeds in a coarse-to-ﬁne manner, ﬁrst building an\noctree at a relatively coarse resolution, and then reﬁning it by revisiting and subdividing all\nthe input images for the gray (mixed) cells whose occupancy has not yet been determined.\nFigure 11.22d shows the resulting octree model computed from a coffee cup rotating on a\nturntable.\nMore recent work on visual hull computation borrows ideas from image-based rendering,\nand is hence called an image-based visual hull (Matusik, Buehler, Raskar et al. 2000). Instead\nof precomputing a global 3D model, an image-based visual hull is recomputed for each new",
  "592": "570\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nviewpoint, by successively intersecting viewing ray segments with the binary silhouettes in\neach image. This not only leads to a fast computation algorithm but also enables fast texturing\nof the recovered model with color values from the input images. This approach can also\nbe combined with high-quality deformable templates to capture and re-animate whole body\nmotion (Vlasic, Baran, Matusik et al. 2008).\n11.7 Additional reading\nThe ﬁeld of stereo correspondence and depth estimation is one of the oldest and most widely\nstudied topics in computer vision. A number of good surveys have been written over the years\n(Marr and Poggio 1976; Barnard and Fischler 1982; Dhond and Aggarwal 1989; Scharstein\nand Szeliski 2002; Brown, Burschka, and Hager 2003; Seitz, Curless, Diebel et al. 2006) and\nthey can serve as good guides to this extensive literature.\nBecause of computational limitations and the desire to ﬁnd appearance-invariant cor-\nrespondences, early algorithms often focused on ﬁnding sparse correspondences (Hannah\n1974; Marr and Poggio 1979; Mayhew and Frisby 1980; Baker and Binford 1981; Arnold\n1983; Grimson 1985; Ohta and Kanade 1985; Bolles, Baker, and Marimont 1987; Matthies,\nKanade, and Szeliski 1989; Hsieh, McKeown, and Perlant 1992; Bolles, Baker, and Hannah\n1993).\nThe topic of computing epipolar geometry and pre-rectifying images is covered in Sec-\ntions 7.2 and 11.1 and is also treated in textbooks on multi-view geometry (Faugeras and\nLuong 2001; Hartley and Zisserman 2004) and articles speciﬁcally on this topic (Torr and\nMurray 1997; Zhang 1998a,b). The concepts of the disparity space and disparity space im-\nage are often associated with the seminal work by Marr (1982) and the papers of Yang, Yuille,\nand Lu (1993) and Intille and Bobick (1994). The plane sweep algorithm was ﬁrst popular-\nized by Collins (1996) and then generalized to a full arbitrary projective setting by Szeliski\nand Golland (1999) and Saito and Kanade (1999). Plane sweeps can also be formulated using\ncylindrical surfaces (Ishiguro, Yamamoto, and Tsuji 1992; Kang and Szeliski 1997; Shum\nand Szeliski 1999; Li, Shum, Tang et al. 2004; Zheng, Kang, Cohen et al. 2007) or even more\ngeneral topologies (Seitz 2001).\nOnce the topology for the cost volume or DSI has been set up, we need to compute the\nactual photoconsistency measures for each pixel and potential depth. A wide range of such\nmeasures have been proposed, as discussed in Section 11.3.1. Some of these are compared in\nrecent surveys and evaluations of matching costs (Scharstein and Szeliski 2002; Hirschm¨uller\nand Scharstein 2009).\nTo compute an actual depth map from these costs, some form of optimization or selection\ncriterion must be used. The simplest of these are sliding windows of various kinds, which",
  "593": "11.8 Exercises\n571\nare discussed in Section 11.4 and surveyed by Gong, Yang, Wang et al. (2007) and Tombari,\nMattoccia, Di Stefano et al. (2008). More commonly, global optimization frameworks are\nused to compute the best disparity ﬁeld, as described in Section 11.5. These techniques\ninclude dynamic programming and truly global optimization algorithms, such as graph cuts\nand loopy belief propagation. Because the literature on this is so extensive, it is described in\nmore detail in Section 11.5. A good place to ﬁnd pointers to the latest results in this ﬁeld is\nthe Middlebury Stereo Vision Page at http://vision.middlebury.edu/stereo.\nAlgorithms for multi-view stereo typically fall into two categories. The ﬁrst include al-\ngorithms that compute traditional depth maps using several images for computing photocon-\nsistency measures (Okutomi and Kanade 1993; Kang, Webb, Zitnick et al. 1995; Nakamura,\nMatsuura, Satoh et al. 1996; Szeliski and Golland 1999; Kang, Szeliski, and Chai 2001;\nVaish, Szeliski, Zitnick et al. 2006; Gallup, Frahm, Mordohai et al. 2008). Optionally, some\nof these techniques compute multiple depth maps and use additional constraints to encourage\nthe different depth maps to be consistent (Szeliski 1999; Kolmogorov and Zabih 2002; Kang\nand Szeliski 2004; Maitre, Shinagawa, and Do 2008; Zhang, Jia, Wong et al. 2008).\nThe second category consists of papers that compute true 3D volumetric or surface-based\nobject models. Again, because of the large number of papers published on this topic, rather\nthan citing them here, we refer you to the material in Section 11.6.1, the survey by Seitz,\nCurless, Diebel et al. (2006), and the on-line evaluation Web site at http://vision.middlebury.\nedu/mview/.\n11.8 Exercises\nEx 11.1: Stereo pair rectiﬁcation\nImplement the following simple algorithm (Section 11.1.1):\n1. Rotate both cameras so that they are looking perpendicular to the line joining the two\ncamera centers c0 and c1. The smallest rotation can be computed from the cross prod-\nuct between the original and desired optical axes.\n2. Twist the optical axes so that the horizontal axis of each camera looks in the direction\nof the other camera. (Again, the cross product between the current x-axis after the ﬁrst\nrotation and the line joining the cameras gives the rotation.)\n3. If needed, scale up the smaller (less detailed) image so that it has the same resolution\n(and hence line-to-line correspondence) as the other image.\nNow compare your results to the algorithm proposed by Loop and Zhang (1999). Can you\nthink of situations where their approach may be preferable?",
  "594": "572\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 11.2: Rigid direct alignment\nModify your spline-based or optical ﬂow motion estima-\ntor from Exercise 8.4 to use epipolar geometry, i.e. to only estimate disparity.\n(Optional) Extend your algorithm to simultaneously estimate the epipolar geometry (with-\nout ﬁrst using point correspondences) by estimating a base homography corresponding to a\nreference plane for the dominant motion and then an epipole for the residual parallax (mo-\ntion).\nEx 11.3: Shape from proﬁles\nReconstruct a surface model from a series of edge images\n(Section 11.2.1).\n1. Extract edges and link them (Exercises 4.7–4.8).\n2. Based on previously computed epipolar geometry, match up edges in triplets (or longer\nsets) of images.\n3. Reconstruct the 3D locations of the curves using osculating circles (11.5).\n4. Render the resulting 3D surface model as a sparse mesh, i.e., drawing the reconstructed\n3D proﬁle curves and links between 3D points in neighboring images with similar\nosculating circles.\nEx 11.4: Plane sweep\nImplement a plane sweep algorithm (Section 11.1.2).\nIf the images are already pre-rectiﬁed, this consists simply of shifting images relative to\neach other and comparing pixels. If the images are not pre-rectiﬁed, compute the homography\nthat resamples the target image into the reference image’s coordinate system for each plane.\nEvaluate a subset of the following similarity measures (Section 11.3.1) and compare their\nperformance by visualizing the disparity space image (DSI), which should be dark for pixels\nat correct depths:\n• squared difference (SD);\n• absolute difference (AD);\n• truncated or robust measures;\n• gradient differences;\n• rank or census transform (the latter usually performs better);\n• mutual information from a pre-computed joint density function.\nConsider using the Birchﬁeld and Tomasi (1998) technique of comparing ranges between\nneighboring pixels (different shifted or warped images). Also, try pre-compensating images\nfor bias or gain variations using one or more of the techniques discussed in Section 11.3.1.",
  "595": "11.8 Exercises\n573\nEx 11.5: Aggregation and window-based stereo\nImplement one or more of the matching\ncost aggregation strategies described in Section 11.4:\n• convolution with a box or Gaussian kernel;\n• shifting window locations by applying a min ﬁlter (Scharstein and Szeliski 2002);\n• picking a window that maximizes some match-reliability metric (Veksler 2001, 2003);\n• weighting pixels by their similarity to the central pixel (Yoon and Kweon 2006).\nOnce you have aggregated the costs in the DSI, pick the winner at each pixel (winner-take-\nall), and then optionally perform one or more of the following post-processing steps:\n1. compute matches both ways and pick only the reliable matches (draw the others in\nanother color);\n2. tag matches that are unsure (whose conﬁdence is too low);\n3. ﬁll in the matches that are unsure from neighboring values;\n4. reﬁne your matches to sub-pixel disparity by either ﬁtting a parabola to the DSI values\naround the winner or by using an iteration of Lukas–Kanade.\nEx 11.6: Optimization-based stereo\nCompute the disparity space image (DSI) volume us-\ning one of the techniques you implemented in Exercise 11.4 and then implement one (or more)\nof the global optimization techniques described in Section 11.5 to compute the depth map.\nPotential choices include:\n• dynamic programming or scanline optimization (relatively easy);\n• semi-global optimization (Hirschm¨uller 2008), which is a simple extension of scanline\noptimization and performs well;\n• graph cuts using alpha expansions (Boykov, Veksler, and Zabih 2001), for which you\nwill need to ﬁnd a max-ﬂow or min-cut algorithm (http://vision.middlebury.edu/stereo);\n• loopy belief propagation (Appendix B.5.3).\nEvaluate your algorithm by running it on the Middlebury stereo data sets.\nHow well does your algorithm do against local aggregation (Yoon and Kweon 2006)?\nCan you think of some extensions or modiﬁcations to make it even better?",
  "596": "574\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 11.7: View interpolation, revisited\nCompute a dense depth map using one of the tech-\nniques you developed above and use it (or, better yet, a depth map for each source image) to\ngenerate smooth in-between views from a stereo data set.\nCompare your results against using the ground truth depth data (if available).\nWhat kinds of artifacts do you see? Can you think of ways to reduce them?\nMore details on implementing such algorithms can be found in Section 13.1 and Exercises\n13.1–13.4.\nEx 11.8: Multi-frame stereo\nExtend one of your previous techniques to use multiple input\nframes (Section 11.6) and try to improve the results you obtained with just two views.\nIf helpful, try using temporal selection (Kang and Szeliski 2004) to deal with the increased\nnumber of occlusions in multi-frame data sets.\nYou can also try to simultaneously estimate multiple depth maps and make them consis-\ntent (Kolmogorov and Zabih 2002; Kang and Szeliski 2004).\nTest your algorithms out on some standard multi-view data sets.\nEx 11.9: Volumetric stereo\nImplement voxel coloring (Seitz and Dyer 1999) as a simple\nextension to the plane sweep algorithm you implemented in Exercise 11.4.\n1. Instead of computing the complete DSI all at once, evaluate each plane one at a time\nfrom front to back.\n2. Tag every voxel whose photoconsistency is below a certain threshold as being part of\nthe object and remember its average (or robust) color (Seitz and Dyer 1999; Eisert,\nSteinbach, and Girod 2000; Kutulakos 2000; Slabaugh, Culbertson, Slabaugh et al.\n2004).\n3. Erase the input pixels corresponding to tagged voxels in the input images, e.g., by\nsetting their alpha value to 0 (or to some reduced number, depending on occupancy).\n4. As you evaluate the next plane, use the source image alpha values to modify your\nphotoconsistency score, e.g., only consider pixels that have full alpha or weight pixels\nby their alpha values.\n5. If the cameras are not all on the same side of your plane sweeps, use space carving\n(Kutulakos and Seitz 2000) to cycle through different subsets of source images while\ncarving away the volume from different directions.\nEx 11.10: Depth map merging\nUse the technique you developed for multi-frame stereo in\nExercise 11.8 or a different technique, such as the one described by Goesele, Snavely, Curless\net al. (2007), to compute a depth map for every input image.",
  "597": "11.8 Exercises\n575\nMerge these depth maps into a coherent 3D model, e.g., using Poisson surface reconstruc-\ntion (Kazhdan, Bolitho, and Hoppe 2006).\nEx 11.11: Shape from silhouettes\nBuild a silhouette-based volume reconstruction algo-\nrithm (Section 11.6.2). Use an octree or some other representation of your choosing.",
  "598": "576\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "599": "Chapter 12\n3D reconstruction\n12.1 Shape from X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580\n12.1.1 Shape from shading and photometric stereo . . . . . . . . . . . . . . 580\n12.1.2 Shape from texture . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\n12.1.3 Shape from focus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584\n12.2 Active rangeﬁnding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585\n12.2.1 Range data merging\n. . . . . . . . . . . . . . . . . . . . . . . . . . 588\n12.2.2 Application: Digital heritage . . . . . . . . . . . . . . . . . . . . . . 590\n12.3 Surface representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591\n12.3.1 Surface interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 592\n12.3.2 Surface simpliﬁcation\n. . . . . . . . . . . . . . . . . . . . . . . . . 594\n12.3.3 Geometry images . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594\n12.4 Point-based representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 595\n12.5 Volumetric representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 596\n12.5.1 Implicit surfaces and level sets . . . . . . . . . . . . . . . . . . . . . 596\n12.6 Model-based reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . 598\n12.6.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598\n12.6.2 Heads and faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601\n12.6.3 Application: Facial animation\n. . . . . . . . . . . . . . . . . . . . . 603\n12.6.4 Whole body modeling and tracking\n. . . . . . . . . . . . . . . . . . 605\n12.7 Recovering texture maps and albedos\n. . . . . . . . . . . . . . . . . . . . . 610\n12.7.1 Estimating BRDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n12.7.2 Application: 3D photography\n. . . . . . . . . . . . . . . . . . . . . 613\n12.8 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614\n12.9 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616",
  "600": "578\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 12.1 3D shape acquisition and modeling techniques: (a) shaded image (Zhang, Tsai,\nCryer et al. 1999) c⃝1999 IEEE; (b) texture gradient (Garding 1992) c⃝1992 Springer; (c)\nreal-time depth from focus (Nayar, Watanabe, and Noguchi 1996) c⃝1996 IEEE; (d) scanning\na scene with a stick shadow (Bouguet and Perona 1999) c⃝1999 Springer; (e) merging range\nmaps into a 3D model (Curless and Levoy 1996) c⃝1996 ACM; (f) point-based surface\nmodeling (Pauly, Keiser, Kobbelt et al. 2003) c⃝2003 ACM; (g) automated modeling of a\n3D building using lines and planes (Werner and Zisserman 2002) c⃝2002 Springer; (h) 3D\nface model from spacetime stereo (Zhang, Snavely, Curless et al. 2004) c⃝2004 ACM; (i)\nperson tracking (Sigal, Bhatia, Roth et al. 2004) c⃝2004 IEEE.",
  "601": "12 3D reconstruction\n579\nAs we saw in the previous chapter, a variety of stereo matching techniques have been de-\nveloped to reconstruct high quality 3D models from two or more images. However, stereo\nis just one of the many potential cues that can be used to infer shape from images. In this\nchapter, we investigate a number of such techniques, which include not only visual cues such\nas shading and focus, but also techniques for merging multiple range or depth images into 3D\nmodels, as well as techniques for reconstructing specialized models, such as heads, bodies,\nor architecture.\nAmong the various cues that can be used to infer shape, the shading on a surface (Fig-\nure 12.1a) can provide a lot of information about local surface orientations and hence overall\nsurface shape (Section 12.1.1). This approach becomes even more powerful when lights\nshining from different directions can be turned on and off separately (photometric stereo).\nTexture gradients (Figure 12.1b), i.e., the foreshortening of regular patterns as the surface\nslants or bends away from the camera, can provide similar cues on local surface orientation\n(Section 12.1.2). Focus is another powerful cue to scene depth, especially when two or more\nimages with different focus settings are used (Section 12.1.3).\n3D shape can also be estimated using active illumination techniques such as light stripes\n(Figure 12.1d) or time of ﬂight range ﬁnders (Section 12.2). The partial surface models\nobtained using such techniques (or passive image-based stereo) can then be merged into more\ncoherent 3D surface models (Figure 12.1e), as discussed in Section 12.2.1. Such techniques\nhave been used to construct highly detailed and accurate models of cultural heritage such as\nhistoric sites (Section 12.2.2). The resulting surface models can then be simpliﬁed to support\nviewing at different resolutions and streaming across the Web (Section 12.3.2). An alternative\nto working with continuous surfaces is to represent 3D surfaces as dense collections of 3D\noriented points (Section 12.4) or as volumetric primitives (Section 12.5).\n3D modeling can be more efﬁcient and effective if we know something about the objects\nwe are trying to reconstruct. In Section 12.6, we look at three specialized but commonly\noccurring examples, namely architecture (Figure 12.1g), heads and faces (Figure 12.1h), and\nwhole bodies (Figure 12.1i). In addition to modeling people, we also discuss techniques for\ntracking them.\nThe last stage of shape and appearance modeling is to extract some textures to paint onto\nour 3D models (Section 12.7). Some techniques go beyond this and actually estimate full\nBRDFs (Section 12.7.1).\nBecause there exists such a large variety of techniques to perform 3D modeling, this\nchapter does not go into detail on any one of these. Readers are encouraged to ﬁnd more\ninformation in the cited references or more specialized publications and conferences de-\nvoted to these topics, e.g., the International Symposium on 3D Data Processing, Visualiza-\ntion, and Transmission (3DPVT), the International Conference on 3D Digital Imaging and\nModeling (3DIM), the International Conference on Automatic Face and Gesture Recognition",
  "602": "580\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(FG), the IEEE Workshop on Analysis and Modeling of Faces and Gestures, and the Interna-\ntional Workshop on Tracking Humans for the Evaluation of their Motion in Image Sequences\n(THEMIS).\n12.1 Shape from X\nIn addition to binocular disparity, shading, texture, and focus all play a role in how we per-\nceive shape. The study of how shape can be inferred from such cues is sometimes called\nshape from X, since the individual instances are called shape from shading, shape from tex-\nture, and shape from focus.1 In this section, we look at these three cues and how they can\nbe used to reconstruct 3D geometry. A good overview of all these topics can be found in the\ncollection of papers on physics-based shape inference edited by Wolff, Shafer, and Healey\n(1992b).\n12.1.1 Shape from shading and photometric stereo\nWhen you look at images of smooth shaded objects, such as the ones shown in Figure 12.2,\nyou can clearly see the shape of the object from just the shading variation. How is this\npossible? The answer is that as the surface normal changes across the object, the apparent\nbrightness changes as a function of the angle between the local surface orientation and the\nincident illumination, as shown in Figure 2.15 (Section 2.2.2).\nThe problem of recovering the shape of a surface from this intensity variation is known as\nshape from shading and is one of the classic problems in computer vision (Horn 1975). The\ncollection of papers edited by Horn and Brooks (1989) is a great source of information on\nthis topic, especially the chapter on variational approaches. The survey by Zhang, Tsai, Cryer\net al. (1999) not only reviews more recent techniques, but also provides some comparative\nresults.\nMost shape from shading algorithms assume that the surface under consideration is of a\nuniform albedo and reﬂectance, and that the light source directions are either known or can\nbe calibrated by the use of a reference object. Under the assumptions of distant light sources\nand observer, the variation in intensity (irradiance equation) become purely a function of the\nlocal surface orientation,\nI(x, y) = R(p(x, y), q(x, y)),\n(12.1)\nwhere (p, q) = (zx, zy) are the depth map derivatives and R(p, q) is called the reﬂectance\nmap. For example, a diffuse (Lambertian) surface has a reﬂectance map that is the (non-\n1 We have already seen examples of shape from stereo, shape from proﬁles, and shape from silhouettes in Chap-\nter 11.",
  "603": "12.1 Shape from X\n581\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 12.2 Synthetic shape from shading (Zhang, Tsai, Cryer et al. 1999) c⃝1999 IEEE:\nshaded images, (a–b) with light from in front (0, 0, 1) and (c–d) with light the front right\n(1, 0, 1); (e–f) corresponding shape from shading reconstructions using the technique of Tsai\nand Shah (1994).\nnegative) dot product (2.88) between the surface normal ˆn = (p, q, 1)/\np\n1 + p2 + q2 and\nthe light source direction v = (vx, vy, vz),\nR(p, q) = max\n \n0, ρpvx + qvy + vz\np\n1 + p2 + q2\n!\n,\n(12.2)\nwhere ρ is the surface reﬂectance factor (albedo).\nIn principle, Equations (12.1–12.2) can be used to estimate (p, q) using non-linear least\nsquares or some other method. Unfortunately, unless additional constraints are imposed, there\nare more unknowns per pixel (p, q) than there are measurements (I). One commonly used\nconstraint is the smoothness constraint,\nEs =\nZ\np2\nx + p2\ny + q2\nx + q2\ny dx dy =\nZ\n∥∇p∥2 + ∥∇q∥2 dx dy,\n(12.3)\nwhich we already saw in Section 3.7.1 (3.94). The other is the integrability constraint,\nEi =\nZ\n(py −qx)2 dx dy,\n(12.4)",
  "604": "582\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhich arises naturally, since for a valid depth map z(x, y) with (p, q) = (zx, zy), we have\npy = zxy = zyx = qx.\nInstead of ﬁrst recovering the orientation ﬁelds (p, q) and integrating them to obtain a\nsurface, it is also possible to directly minimize the discrepancy in the image formation equa-\ntion (12.1) while ﬁnding the optimal depth map z(x, y) (Horn 1990). Unfortunately, shape\nfrom shading is susceptible to local minima in the search space and, like other variational\nproblems that involve the simultaneous estimation of many variables, can also suffer from\nslow convergence. Using multi-resolution techniques (Szeliski 1991a) can help accelerate\nthe convergence, while using more sophisticated optimization techniques (Dupuis and Olien-\nsis 1994) can help avoid local minima.\nIn practice, surfaces other than plaster casts are rarely of a single uniform albedo. Shape\nfrom shading therefore needs to be combined with some other technique or extended in some\nway to make it useful. One way to do this is to combine it with stereo matching (Fua and\nLeclerc 1995) or known texture (surface patterns) (White and Forsyth 2006). The stereo and\ntexture components provide information in textured regions, while shape from shading helps\nﬁll in the information across uniformly colored regions and also provides ﬁner information\nabout surface shape.\nPhotometric stereo.\nAnother way to make shape from shading more reliable is to use mul-\ntiple light sources that can be selectively turned on and off. This technique is called photo-\nmetric stereo, since the light sources play a role analogous to the cameras located at different\nlocations in traditional stereo (Woodham 1981).2 For each light source, we have a differ-\nent reﬂectance map, R1(p, q), R2(p, q), etc. Given the corresponding intensities I1, I2, etc.\nat a pixel, we can in principle recover both an unknown albedo ρ and a surface orientation\nestimate (p, q).\nFor diffuse surfaces (12.2), if we parameterize the local orientation by ˆn, we get (for\nnon-shadowed pixels) a set of linear equations of the form\nIk = ρˆn · vk,\n(12.5)\nfrom which we can recover ρˆn using linear least squares. These equations are well condi-\ntioned as long as the (three or more) vectors vk are linearly independent, i.e., they are not\nalong the same azimuth (direction away from the viewer).\nOnce the surface normals or gradients have been recovered at each pixel, they can be\nintegrated into a depth map using a variant of regularized surface ﬁtting (3.100). (Nehab,\nRusinkiewicz, Davis et al. (2005) and Harker and O’Leary (2008) have produced some recent\nwork in this area.)\n2 An alternative to turning lights on-and-off is to use three colored lights (Woodham 1994; Hernandez, Vogiatzis,\nBrostow et al. 2007; Hernandez and Vogiatzis 2010).",
  "605": "12.1 Shape from X\n583\nCamera\nScene\nMirror surface \nc\nEstimate tangent planes\nScene Pattern\n(a)\n(b)\n(c)\n(d)\nFigure 12.3\nSynthetic shape from texture (Garding 1992) c⃝1992 Springer:\n(a) regular\ntexture wrapped onto a curved surface and (b) the corresponding surface normal estimates.\nShape from mirror reﬂections (Savarese, Chen, and Perona 2005) c⃝2005 Springer: (c) a\nregular pattern reﬂecting off a curved mirror gives rise to (d) curved lines, from which 3D\npoint locations and normals can be inferred.\nWhen surfaces are specular, more than three light directions may be required. In fact,\nthe irradiance equation given in (12.1) not only requires that the light sources and camera be\ndistant from the surface, it also neglects inter-reﬂections, which can be a signiﬁcant source\nof the shading observed on object surfaces, e.g., the darkening seen inside concave structures\nsuch as grooves and crevasses (Nayar, Ikeuchi, and Kanade 1991).\n12.1.2 Shape from texture\nThe variation in foreshortening observed in regular textures can also provide useful informa-\ntion about local surface orientation. Figure 12.3 shows an example of such a pattern, along\nwith the estimated local surface orientations. Shape from texture algorithms require a number\nof processing steps, including the extraction of repeated patterns or the measurement of local\nfrequencies in order to compute local afﬁne deformations, and a subsequent stage to infer lo-\ncal surface orientation. Details on these various stages can be found in the research literature\n(Witkin 1981; Ikeuchi 1981; Blostein and Ahuja 1987; Garding 1992; Malik and Rosenholtz\n1997; Lobay and Forsyth 2006).\nWhen the original pattern is regular, it is possible to ﬁt a regular but slightly deformed\ngrid to the image and use this grid for a variety of image replacement or analysis tasks (Liu,\nCollins, and Tsin 2004; Liu, Lin, and Hays 2004; Hays, Leordeanu, Efros et al. 2006; Lin,\nHays, Wu et al. 2006; Park, Brocklehurst, Collins et al. 2009). This process becomes even\neasier if specially printed textured cloth patterns are used (White and Forsyth 2006; White,\nCrane, and Forsyth 2007).\nThe deformations induced in a regular pattern when it is viewed in the reﬂection of a\ncurved mirror, as shown in Figure 12.3c–d, can be used to recover the shape of the surface",
  "606": "584\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 12.4 Real time depth from defocus (Nayar, Watanabe, and Noguchi 1996) c⃝1996\nIEEE: (a) the real-time focus range sensor, which includes a half-silvered mirror between the\ntwo telecentric lenses (lower right), a prism that splits the image into two CCD sensors (lower\nleft), and an edged checkerboard pattern illuminated by a Xenon lamp (top); (b–c) input video\nframes from the two cameras along with (d) the corresponding depth map; (e–f) two frames\n(you can see the texture if you zoom in) and (g) the corresponding 3D mesh model.\n(Savarese, Chen, and Perona 2005; Rozenfeld, Shimshoni, and Lindenbaum 2007). It is is\nalso possible to infer local shape information from specular ﬂow, i.e., the motion of specu-\nlarities when viewed from a moving camera (Oren and Nayar 1997; Zisserman, Giblin, and\nBlake 1989; Swaminathan, Kang, Szeliski et al. 2002).\n12.1.3 Shape from focus\nA strong cue for object depth is the amount of blur, which increases as the object’s surface\nmoves away from the camera’s focusing distance. As shown in Figure 2.19, moving the object\nsurface away from the focus plane increases the circle of confusion, according to a formula\nthat is easy to establish using similar triangles (Exercise 2.4).\nA number of techniques have been developed to estimate depth from the amount of de-\nfocus (depth from defocus) (Pentland 1987; Nayar and Nakagawa 1994; Nayar, Watanabe,\nand Noguchi 1996; Watanabe and Nayar 1998; Chaudhuri and Rajagopalan 1999; Favaro\nand Soatto 2006). In order to make such a technique practical, a number issues need to be\naddressed:\n• The amount of blur increase in both directions as you move away from the focus plane.\nTherefore, it is necessary to use two or more images captured with different focus",
  "607": "12.2 Active rangeﬁnding\n585\nSurface\nCCD\nLaser\n(a)\nDirection of travel\nObject\nCCD\nCCD image\n    plane\nLaser\nCylindrical lens\nLaser \nsheet\nσz\nσx\n(b)\n(c)\n(d)\nFigure 12.5 Range data scanning (Curless and Levoy 1996) c⃝1996 ACM: (a) a laser dot\non a surface is imaged by a CCD sensor; (b) a laser stripe (sheet) is imaged by the sensor (the\ndeformation of the stripe encodes the distance to the object); (c) the resulting set of 3D points\nare turned into (d) a triangulated mesh.\ndistance settings (Pentland 1987; Nayar, Watanabe, and Noguchi 1996) or to translate\nthe object in depth and look for the point of maximum sharpness (Nayar and Nakagawa\n1994).\n• The magniﬁcation of the object can vary as the focus distance is changed or the object is\nmoved. This can be modeled either explicitly (making correspondence more difﬁcult)\nor using telecentric optics, which approximate an orthographic camera and require an\naperture in front of the lens (Nayar, Watanabe, and Noguchi 1996).\n• The amount of defocus must be reliably estimated. A simple approach is to average the\nsquared gradient in a region but this suffers from several problems, including the image\nmagniﬁcation problem mentioned above. A better solution is to use carefully designed\nrational ﬁlters (Watanabe and Nayar 1998).\nFigure 12.4 shows an example of a real-time depth from defocus sensor, which employs\ntwo imaging chips at slightly different depths sharing a common optical path, as well as an\nactive illumination system that projects a checkerboard pattern from the same direction. As\nyou can see in Figure 12.4b–g, the system produces high-accuracy real-time depth maps for\nboth static and dynamic scenes.\n12.2 Active rangeﬁnding\nAs we have seen in the previous section, actively lighting a scene, whether for the purpose\nof estimating normals using photometric stereo or for adding artiﬁcial texture for shape from\ndefocus, can greatly improve the performance of vision systems. This kind of active illu-\nmination has been used from the earliest days of machine vision to construct highly reliable",
  "608": "586\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.6\nShape scanning using cast shadows (Bouguet and Perona 1999) c⃝1999\nSpringer: (a) camera setup with a point light source (a desk lamp without its reﬂector), a\nhand-held stick casting a shadow, and (b) the objects being scanned in front of two planar\nbackgrounds. (c) Real-time depth map using a pulsed illumination system (Iddan and Yahav\n2001) c⃝2001 SPIE.\nsensors for estimating 3D depth images using a variety of rangeﬁnding (or range sensing)\ntechniques (Besl 1989; Curless 1999; Hebert 2000).\nOne of the most popular active illumination sensors is a laser or light stripe sensor, which\nsweeps a plane of light across the scene or object while observing it from an offset viewpoint,\nas shown in Figure 12.5b (Rioux and Bird 1993; Curless and Levoy 1995). As the stripe falls\nacross the object, it deforms its shape according to the shape of the surface it is illuminating.\nIt is then a simple matter of using optical triangulation to estimate the 3D locations of all the\npoints seen in a particular stripe. In more detail, knowledge of the 3D plane equation of the\nlight stripe allows us to infer the 3D location corresponding to each illuminated pixel, as pre-\nviously discussed in (2.70–2.71). The accuracy of light striping techniques can be improved\nby ﬁnding the exact temporal peak in illumination for each pixel (Curless and Levoy 1995).\nThe ﬁnal accuracy of a scanner can be determined using slant edge modulation techniques,\ni.e., by imaging sharp creases in a calibration object (Goesele, Fuchs, and Seidel 2003).\nAn interesting variant on light stripe rangeﬁnding is presented by Bouguet and Perona\n(1999). Instead of projecting a light stripe, they simply wave a stick casting a shadow over a\nscene or object illuminated by a point light source such as a lamp or the sun (Figure 12.6a).\nAs the shadow falls across two background planes whose orientation relative to the cam-\nera is known (or inferred during pre-calibration), the plane equation for each stripe can be\ninferred from the two projected lines, whose 3D equations are known (Figure 12.6b). The\ndeformation of the shadow as it crosses the object being scanned then reveals its 3D shape,\nas with regular light stripe rangeﬁnding (Exercise 12.2). This technique can also be used to\nestimate the 3D geometry of a background scene and how its appearance varies as it moves\ninto shadow, in order to cast new shadows onto the scene (Chuang, Goldman, Curless et al.",
  "609": "12.2 Active rangeﬁnding\n587\n2003) (Section 10.4.3).\nThe time it takes to scan an object using a light stripe technique is proportional to the\nnumber of depth planes used, which is usually comparable to the number of pixels across\nan image. A much faster scanner can be constructed by turning different projector pixels on\nand off in a structured manner, e.g., using a binary or Gray code (Besl 1989). For example,\nlet us assume that the LCD projector we are using has 1024 columns of pixels. Taking the\n10-bit binary code corresponding to each column’s address (0 . . . 1023), we project the ﬁrst\nbit, then the second, etc. After 10 projections (e.g., a third of a second for a synchronized\n30Hz camera-projector system), each pixel in the camera knows which of the 1024 columns\nof projector light it is seeing. A similar approach can also be used to estimate the refractive\nproperties of an object by placing a monitor behind the object (Zongker, Werner, Curless et al.\n1999; Chuang, Zongker, Hindorff et al. 2000) (Section 13.4). Very fast scanners can also be\nconstructed with a single laser beam, i.e., a real-time ﬂying spot optical triangulation scanner\n(Rioux, Bechthold, Taylor et al. 1987).\nIf even faster, i.e., frame-rate, scanning is required, we can project a single textured pat-\ntern into the scene. Proesmans, Van Gool, and Defoort (1998) describe a system where a\ncheckerboard grid is projected onto an object (e.g., a person’s face) and the deformation of\nthe grid is used to infer 3D shape. Unfortunately, such a technique only works if the surface\nis continuous enough to link all of the grid points.\nA much better system can be constructed using high-speed custom illumination and sens-\ning hardware. Iddan and Yahav (2001) describe the construction of their 3DV Zcam video-\nrate depth sensing camera, which projects a pulsed plane of light onto the scene and then\nintegrates the returning light for a short interval, essentially obtaining time-of-ﬂight mea-\nsurement for the distance to individual pixels in the scene. A good description of earlier\ntime-of-ﬂight systems, including amplitude and frequency modulation schemes for LIDAR,\ncan be found in (Besl 1989).\nInstead of using a single camera, it is also possible to construct an active illumination\nrange sensor using stereo imaging setups. The simplest way to do this is to just project ran-\ndom stripe patterns onto the scene to create synthetic texture, which helps match textureless\nsurfaces (Kang, Webb, Zitnick et al. 1995). Projecting a known series of stripes, just as in\ncoded pattern single-camera rangeﬁnding, makes the correspondence between pixels unam-\nbiguous and allows for the recovery of depth estimates at pixels only seen in a single camera\n(Scharstein and Szeliski 2003). This technique has been used to produce large numbers of\nhighly accurate registered multi-image stereo pairs and depth maps for the purpose of eval-\nuating stereo correspondence algorithms (Scharstein and Szeliski 2002; Hirschm¨uller and\nScharstein 2009) and learning depth map priors and parameters (Scharstein and Pal 2007).\nWhile projecting multiple patterns usually requires the scene or object to remain still, ad-\nditional processing can enable the production of real-time depth maps for dynamic scenes.",
  "610": "588\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 12.7 Real-time dense 3D face capture using spacetime stereo (Zhang, Snavely, Cur-\nless et al. 2004) c⃝2004 ACM: (a) set of ﬁve consecutive video frames from one of two stereo\ncameras (every ﬁfth frame is free of stripe patterns, in order to extract texture); (b) resulting\nhigh-quality 3D surface model (depth map visualized as a shaded rendering).\nThe basic idea (Davis, Ramamoorthi, and Rusinkiewicz 2003; Zhang, Curless, and Seitz\n2003) is to assume that depth is nearly constant within a 3D space–time window around\neach pixel and to use the 3D window for matching and reconstruction. Depending on the\nsurface shape and motion, this assumption may be error-prone, as shown in (Davis, Nahab,\nRamamoorthi et al. 2005). To model shapes more accurately, Zhang, Curless, and Seitz\n(2003) model the linear disparity variation within the space–time window and show that bet-\nter results can be obtained by globally optimizing disparity and disparity gradient estimates\nover video volumes (Zhang, Snavely, Curless et al. 2004). Figure 12.7 shows the results of\napplying this system to a person’s face; the frame-rate 3D surface model can then be used for\nfurther model-based ﬁtting and computer graphics manipulation (Section 12.6.2).\n12.2.1 Range data merging\nWhile individual range images can be useful for applications such as real-time z-keying or fa-\ncial motion capture, they are often used as building blocks for more complete 3D object mod-\neling. In such applications, the next two steps in processing are the registration (alignment) of\npartial 3D surface models and their integration into coherent 3D surfaces (Curless 1999). If\ndesired, this can be followed by a model ﬁtting stage using either parametric representations\nsuch as generalized cylinders (Agin and Binford 1976; Nevatia and Binford 1977; Marr and\nNishihara 1978; Brooks 1981), superquadrics (Pentland 1986; Solina and Bajcsy 1990; Ter-\nzopoulos and Metaxas 1991), or non-parametric models such as triangular meshes (Boissonat\n1984) or physically-based models (Terzopoulos, Witkin, and Kass 1988; Delingette, Hebert,\nand Ikeuichi 1992; Terzopoulos and Metaxas 1991; McInerney and Terzopoulos 1993; Ter-\nzopoulos 1999). A number of techniques have also been developed for segmenting range\nimages into simpler constituent surfaces (Hoover, Jean-Baptiste, Jiang et al. 1996).\nThe most widely used 3D registration technique is the iterated closest point (ICP) algo-",
  "611": "12.2 Active rangeﬁnding\n589\nrithm, which alternates between ﬁnding the closest point matches between the two surfaces\nbeing aligned and then solving a 3D absolute orientation problem (Section 6.1.5, (6.31–6.32)\n(Besl and McKay 1992; Chen and Medioni 1992; Zhang 1994; Szeliski and Lavall´ee 1996;\nGold, Rangarajan, Lu et al. 1998; David, DeMenthon, Duraiswami et al. 2004; Li and Hart-\nley 2007; Enqvist, Josephson, and Kahl 2009).3 Since the two surfaces being aligned usually\nonly have partial overlap and may also have outliers, robust matching criteria (Section 6.1.4\nand Appendix B.3) are typically used. In order to speed up the determination of the closest\npoint, and also to make the distance-to-surface computation more accurate, one of the two\npoint sets (e.g., the current merged model) can be converted into a signed distance function,\noptionally represented using an octree spline for compactness (Lavall´ee and Szeliski 1995).\nVariants on the basic ICP algorithm can be used to register 3D point sets under non-rigid de-\nformations, e.g., for medical applications (Feldmar and Ayache 1996; Szeliski and Lavall´ee\n1996). Color values associated with the points or range measurements can also be used as\npart of the registration process to improve robustness (Johnson and Kang 1997; Pulli 1999).\nUnfortunately, the ICP algorithm and its variants can only ﬁnd a locally optimal alignment\nbetween 3D surfaces. If this is not known a priori, more global correspondence or search\ntechniques, based on local descriptors invariant to 3D rigid transformations, need to be used.\nAn example of such a descriptor is the spin image, which is a local circular projection of a\n3D surface patch around the local normal axis (Johnson and Hebert 1999). Another (earlier)\nexample is the splash representation introduced by Stein and Medioni (1992).\nOnce two or more 3D surfaces have been aligned, they can be merged into a single model.\nOne approach is to represent each surface using a triangulated mesh and combine these\nmeshes using a process that is sometimes called zippering (Soucy and Laurendeau 1992;\nTurk and Levoy 1994). Another, now more widely used, approach is to compute a signed\ndistance function that ﬁts all of the 3D data points (Hoppe, DeRose, Duchamp et al. 1992;\nCurless and Levoy 1996; Hilton, Stoddart, Illingworth et al. 1996; Wheeler, Sato, and Ikeuchi\n1998).\nFigure 12.8 shows one such approach, the volumetric range image processing (VRIP)\ntechnique developed by Curless and Levoy (1996), which ﬁrst computes a weighted signed\ndistance function from each range image and then merges them using a weighted averaging\nprocess. To make the representation more compact, run-length coding is used to encode\nthe empty, seen, and varying (signed distance) voxels, and only the signed distance values\nnear each surface are stored.4 Once the merged signed distance function has been computed,\na zero-crossing surface extraction algorithm, such as marching cubes (Lorensen and Cline\n1987), can be used to recover a meshed surface model. Figure 12.9 shows an example of the\n3 Some techniques, such as the one developed by Chen and Medioni (1992), use local surface tangent planes to\nmake this computation more accurate and to accelerate convergence.\n4 An alternative, even more compact, representation could be to use octrees (Lavall´ee and Szeliski 1995).",
  "612": "590\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 12.8 Range data merging (Curless and Levoy 1996) c⃝1996 ACM: (a) two signed\ndistance functions (top left) are merged with their (weights) bottom left to produce a com-\nbined set of functions (right column) from which an isosurface can be extracted (green dashed\nline); (b) the signed distance functions are combined with empty and unseen space labels to\nﬁll holes in the isosurface.\ncomplete range data merging and isosurface extraction pipeline.\nVolumetric range data merging techniques based on signed distance or characteristic\n(inside–outside) functions are also widely used to extract smooth well-behaved surfaces from\noriented or unoriented sets of points (Hoppe, DeRose, Duchamp et al. 1992; Ohtake, Belyaev,\nAlexa et al. 2003; Kazhdan, Bolitho, and Hoppe 2006; Lempitsky and Boykov 2007; Zach,\nPock, and Bischof 2007b; Zach 2008), as discussed in more detail in Section 12.5.1.\n12.2.2 Application: Digital heritage\nActive rangeﬁnding technologies, combined with surface modeling and appearance model-\ning techniques (Section 12.7), are widely used in the ﬁelds of archeological and historical\npreservation, which often also goes under the name digital heritage (MacDonald 2006). In\nsuch applications, detailed 3D models of cultural objects are acquired and later used for ap-\nplications such as analysis, preservation, restoration, and the production of duplicate artwork\n(Rioux and Bird 1993).\nA more recent example of such an endeavor is the Digital Michelangelo project of Levoy,\nPulli, Curless et al. (2000), which used Cyberware laser stripe scanners and high-quality\ndigital SLR cameras mounted on a large gantry to obtain detailed scans of Michelangelo’s\nDavid and other sculptures in Florence. The project also took scans of the Forma Urbis\nRomae, an ancient stone map of Rome that had shattered into pieces, for which new matches\nwere obtained using digital techniques. The whole process, from initial planning, to software",
  "613": "12.3 Surface representations\n591\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 12.9\nReconstruction and hardcopy of the “Happy Buddha” statuette (Curless and\nLevoy 1996) c⃝1996 ACM: (a) photograph of the original statue after spray painting with\nmatte gray; (b) partial range scan; (c) merged range scans; (d) colored rendering of the recon-\nstructed model; (e) hardcopy of the model constructed using stereolithography.\ndevelopment, acquisition, and post-processing, took several years (and many volunteers), and\nproduced a wealth of 3D shape and appearance modeling techniques as a result.\nEven larger-scale projects are now being attempted, for example, the scanning of com-\nplete temple sites such as Angkor-Thom (Ikeuchi and Sato 2001; Ikeuchi and Miyazaki 2007;\nBanno, Masuda, Oishi et al. 2008). Figure 12.10 shows details from this project, including a\nsample photograph, a detailed 3D (sculptural) head model scanned from ground level, and an\naerial overview of the ﬁnal merged 3D site model, which was acquired using a balloon.\n12.3 Surface representations\nIn previous sections, we have seen different representations being used to integrate 3D range\nscans. We now look at several of these representations in more detail. Explicit surface\nrepresentations, such as triangle meshes, splines (Farin 1992, 1996), and subdivision sur-\nfaces (Stollnitz, DeRose, and Salesin 1996; Zorin, Schr¨oder, and Sweldens 1996; Warren and\nWeimer 2001; Peters and Reif 2008), enable not only the creation of highly detailed models\nbut also processing operations, such as interpolation (Section 12.3.1), fairing or smoothing,\nand decimation and simpliﬁcation (Section 12.3.2). We also examine discrete point-based\nrepresentations (Section 12.4) and volumetric representations (Section 12.5).",
  "614": "592\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.10 Laser range modeling of the Bayon temple at Angkor-Thom (Banno, Masuda,\nOishi et al. 2008) c⃝2008 Springer: (a) sample photograph from the site; (b) a detailed head\nmodel scanned from the ground; (c) ﬁnal merged 3D model of the temple scanned using a\nlaser range sensor mounted on a balloon.\n12.3.1 Surface interpolation\nOne of the most common operations on surfaces is their reconstruction from a set of sparse\ndata constraints, i.e. scattered data interpolation. When formulating such problems, surfaces\nmay be parameterized as height ﬁelds f(x), as 3D parametric surfaces f(x), or as non-\nparametric models such as collections of triangles.\nIn the section on image processing, we saw how two-dimensional function interpolation\nand approximation problems {di} →f(x) could be cast as energy minimization problems\nusing regularization (Section 3.7.1 (3.94–3.98).5 Such problems can also specify the locations\nof discontinuities in the surface as well as local orientation constraints (Terzopoulos 1986b;\nZhang, Dugas-Phocion, Samson et al. 2002).\nOne approach to solving such problems is to discretize both the surface and the energy\non a discrete grid or mesh using ﬁnite element analysis (3.100–3.102) (Terzopoulos 1986b).\nSuch problems can then be solved using sparse system solving techniques, such as multigrid\n(Briggs, Henson, and McCormick 2000) or hierarchically preconditioned conjugate gradient\n(Szeliski 2006b). The surface can also be represented using a hierarchical combination of\nmultilevel B-splines (Lee, Wolberg, and Shin 1996).\nAn alternative approach is to use radial basis (or kernel) functions (Boult and Kender\n1986; Nielson 1993). To interpolate a ﬁeld f(x) through (or near) a number of data values\ndi located at xi, the radial basis function approach uses\nf(x) =\nP\ni wi(x)di\nP\ni wi(x) ,\n(12.6)\n5 The difference between interpolation and approximation is that the former requires the surface or function to\npass through the data while the latter allows the function to pass near the data, and can therefore be used for surface\nsmoothing as well.",
  "615": "12.3 Surface representations\n593\nwhere the weights,\nwi(x) = K(∥x −xi∥),\n(12.7)\nare computed using a radial basis (spherically symmetrical) function K(r).\nIf we want the function f(x) to exactly interpolate the data points, the kernel functions\nmust either be singular at the origin, limr→0 K(r) →∞(Nielson 1993), or a dense linear\nsystem must be solved to determine the magnitude associated with each basis function (Boult\nand Kender 1986). It turns out that, for certain regularized problems, e.g., (3.94–3.96), there\nexist radial basis functions (kernels) that give the same results as a full analytical solution\n(Boult and Kender 1986). Unfortunately, because the dense system solving is cubic in the\nnumber of data points, basis function approaches can only be used for small problems such\nas feature-based image morphing (Beier and Neely 1992).\nWhen a three-dimensional parametric surface is being modeled, the vector-valued func-\ntion f in (12.6) or (3.94–3.102) encodes 3D coordinates (x, y, z) on the surface and the\ndomain x = (s, t) encodes the surface parameterization. One example of such surfaces are\nsymmetry-seeking parametric models, which are elastically deformable versions of general-\nized cylinders6 (Terzopoulos, Witkin, and Kass 1987). In these models, s is the parameter\nalong the spine of the deformable tube and t is the parameter around the tube. A variety of\nsmoothness and radial symmetry forces are used to constrain the model while it is ﬁtted to\nimage-based silhouette curves.\nIt is also possible to deﬁne non-parametric surface models such as general triangulated\nmeshes and to equip such meshes (using ﬁnite element analysis) with both internal smooth-\nness metrics and external data ﬁtting metrics (Sander and Zucker 1990; Fua and Sander 1992;\nDelingette, Hebert, and Ikeuichi 1992; McInerney and Terzopoulos 1993). While most of\nthese approaches assume a standard elastic deformation model, which uses quadratic inter-\nnal smoothness terms, it is also possible to use sub-linear energy models in order to better\npreserve surface creases (Diebel, Thrun, and Br¨unig 2006). Triangle meshes can also be aug-\nmented with either spline elements (Sullivan and Ponce 1998) or subdivision surfaces (Stoll-\nnitz, DeRose, and Salesin 1996; Zorin, Schr¨oder, and Sweldens 1996; Warren and Weimer\n2001; Peters and Reif 2008) to produce surfaces with better smoothness control.\nBoth parametric and non-parametric surface models assume that the topology of the sur-\nface is known and ﬁxed ahead of time. For more ﬂexible surface modeling, we can either rep-\nresent the surface as a collection of oriented points (Section 12.4) or use 3D implicit functions\n(Section 12.5.1), which can also be combined with elastic 3D surface models (McInerney and\nTerzopoulos 1993).\n6 A generalized cylinder (Brooks 1981) is a solid of revolution, i.e., the result of rotating a (usually smooth) curve\naround an axis. It can also be generated by sweeping a slowly varying circular cross-section along the axis. (These\ntwo interpretations are equivalent.)",
  "616": "594\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 12.11 Progressive mesh representation of an airplane model (Hoppe 1996) c⃝1996\nACM: (a) base mesh M 0 (150 faces); (b) mesh M 175 (500 faces); (c) mesh M 425 (1000\nfaces); (d) original mesh M = M n (13,546 faces).\n12.3.2 Surface simpliﬁcation\nOnce a triangle mesh has been created from 3D data, it is often desirable to create a hierarchy\nof mesh models, for example, to control the displayed level of detail (LOD) in a computer\ngraphics application. (In essence, this is a 3D analog to image pyramids (Section 3.5).) One\napproach to doing this is to approximate a given mesh with one that has subdivision connec-\ntivity, over which a set of triangular wavelet coefﬁcients can then be computed (Eck, DeRose,\nDuchamp et al. 1995). A more continuous approach is to use sequential edge collapse opera-\ntions to go from the original ﬁne-resolution mesh to a coarse base-level mesh (Hoppe 1996).\nThe resulting progressive mesh (PM) representation can be used to render the 3D model at\narbitrary levels of detail, as shown in Figure 12.11.\n12.3.3 Geometry images\nWhile multi-resolution surface representations such as (Eck, DeRose, Duchamp et al. 1995;\nHoppe 1996) support level of detail operations, they still consist of an irregular collection of\ntriangles, which makes them more difﬁcult to compress and store in a cache-efﬁcient manner.7\nTo make the triangulation completely regular (uniform and gridded), Gu, Gortler, and\nHoppe (2002) describe how to create geometry images by cutting surface meshes along well-\nchosen lines and “ﬂattening” the resulting representation into a square. Figure 12.12a shows\nthe resulting (x, y, z) values of the surface mesh mapped over the unit square, while Fig-\nure 12.12b shows the associated (nx, ny, nz) normal map, i.e., the surface normals associ-\nated with each mesh vertex, which can be used to compensate for loss in visual ﬁdelity if the\noriginal geometry image is heavily compressed.\n7 Subdivision triangulations, such as those in (Eck, DeRose, Duchamp et al. 1995), are semi-regular, i.e., regular\n(ordered and nested) within each subdivided base triangle.",
  "617": "12.4 Point-based representations\n595\n(x, y, z)\n+\n(nx, ny, nz)\n=⇒\n(a)\n(b)\n(c)\nFigure 12.12 Geometry images (Gu, Gortler, and Hoppe 2002) c⃝2002 ACM: (a) the 257×\n257 geometry image deﬁnes a mesh over the surface; (b) the 512 × 512 normal map deﬁnes\nvertex normals; (c) ﬁnal lit 3D model.\n12.4 Point-based representations\nAs we mentioned previously, triangle-based surface models assume that the topology (and\noften the rough shape) of the 3D model is known ahead of time. While it is possible to\nre-mesh a model as it is being deformed or ﬁtted, a simpler solution is to dispense with an\nexplicit triangle mesh altogether and to have triangle vertices behave as oriented points, or\nparticles, or surface elements (surfels) (Szeliski and Tonnesen 1992).\nIn order to endow the resulting particle system with internal smoothness constraints, pair-\nwise interaction potentials can be deﬁned that approximate the equivalent elastic bending\nenergies that would be obtained using local ﬁnite-element analysis.8 Instead of deﬁning the\nﬁnite element neighborhood for each particle (vertex) ahead of time, a soft inﬂuence function\nis used to couple nearby particles. The resulting 3D model can change both topology and par-\nticle density as it evolves and can therefore be used to interpolate partial 3D data with holes\n(Szeliski, Tonnesen, and Terzopoulos 1993b). Discontinuities in both the surface orientation\nand crease curves can also be modeled (Szeliski, Tonnesen, and Terzopoulos 1993a).\nTo render the particle system as a continuous surface, local dynamic triangulation heuris-\ntics (Szeliski and Tonnesen 1992) or direct surface element splatting (Pﬁster, Zwicker, van\nBaar et al. 2000) can be used. Another alternative is to ﬁrst convert the point cloud into an\nimplicit signed distance or inside–outside function, using either minimum signed distances\nto the oriented points (Hoppe, DeRose, Duchamp et al. 1992) or by interpolating a charac-\nteristic (inside–outside) function using radial basis functions (Turk and O’Brien 2002; Dinh,\nTurk, and Slabaugh 2002). Even greater precision over the implicit function ﬁtting, including\nthe ability to handle irregular point densities, can be obtained by computing a moving least\n8 As mentioned before, an alternative is to use sub-linear interaction potentials, which encourage the preservation\nof surface creases (Diebel, Thrun, and Br¨unig 2006).",
  "618": "596\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 12.13 Point-based surface modeling with moving least squares (MLS) (Pauly, Keiser,\nKobbelt et al. 2003) c⃝2003 ACM: (a) a set of points (black dots) is turned into an implicit\ninside–outside function (black curve); (b) the signed distance to the nearest oriented point\ncan serve as an approximation to the inside–outside distance; (c) a set of oriented points\nwith variable sampling density representing a 3D surface (head model); (d) local estimate of\nsampling density, which is used in the moving least squares; (e) reconstructed continuous 3D\nsurface.\nsquares (MLS) estimate of the signed distance function (Alexa, Behr, Cohen-Or et al. 2003;\nPauly, Keiser, Kobbelt et al. 2003), as shown in Figure 12.13. Further improvements can\nbe obtained using local sphere ﬁtting (Guennebaud and Gross 2007), faster and more accu-\nrate re-sampling (Guennebaud, Germann, and Gross 2008), and kernel regression to better\ntolerate outliers (Oztireli, Guennebaud, and Gross 2008).\n12.5 Volumetric representations\nA third alternative for modeling 3D surfaces is to construct 3D volumetric inside–outside\nfunctions. We already saw examples of this in Section 11.6.1, where we looked at voxel color-\ning (Seitz and Dyer 1999), space carving (Kutulakos and Seitz 2000), and level set (Faugeras\nand Keriven 1998; Pons, Keriven, and Faugeras 2007) techniques for stereo matching, and\nSection 11.6.2, where we discussed using binary silhouette images to reconstruct volumes.\nIn this section, we look at continuous implicit (inside–outside) functions to represent 3D\nshape.\n12.5.1 Implicit surfaces and level sets\nWhile polyhedral and voxel-based representations can represent three-dimensional shapes\nto an arbitrary precision, they lack some of the intrinsic smoothness properties available\nwith continuous implicit surfaces, which use an indicator function (characteristic function)\nF(x, y, z) to indicate which 3D points are inside F(x, y, z) < 0 or outside F(x, y, z) > 0",
  "619": "12.5 Volumetric representations\n597\nthe object.\nAn early example of using implicit functions to model 3D objects in computer vision are\nsuperquadrics, which are a generalization of quadric (e.g., ellipsoidal) parametric volumetric\nmodels,\nF(x, y, z) =\n \u0012 x\na1\n\u00132/ϵ2\n+\n\u0012 y\na2\n\u00132/ϵ2!ϵ2/ϵ1\n+\n\u0012 x\na1\n\u00132/ϵ1\n−1 = 0\n(12.8)\n(Pentland 1986; Solina and Bajcsy 1990; Waithe and Ferrie 1991; Leonardis, Jakliˇc, and\nSolina 1997). The values of (a1, a2, a3) control the extent of model along each (x, y, z) axis,\nwhile the values of (ϵ1, ϵ2) control how “square” it is. To model a wider variety of shapes,\nsuperquadrics are usually combined with either rigid or non-rigid deformations (Terzopoulos\nand Metaxas 1991; Metaxas and Terzopoulos 2002). Superquadric models can either be ﬁt to\nrange data or used directly for stereo matching.\nA different kind of implicit shape model can be constructed by deﬁning a signed distance\nfunction over a regular three-dimensional grid, optionally using an octree spline to represent\nthis function more coarsely away from its surface (zero-set) (Lavall´ee and Szeliski 1995;\nSzeliski and Lavall´ee 1996; Frisken, Perry, Rockwood et al. 2000; Ohtake, Belyaev, Alexa\net al. 2003). We have already seen examples of signed distance functions being used to\nrepresent distance transforms (Section 3.3.3), level sets for 2D contour ﬁtting and tracking\n(Section 5.1.4), volumetric stereo (Section 11.6.1), range data merging (Section 12.2.1), and\npoint-based modeling (Section 12.4). The advantage of representing such functions directly\non a grid is that it is quick and easy to look up distance function values for any (x, y, z)\nlocation and also easy to extract the isosurface using the marching cubes algorithm (Lorensen\nand Cline 1987). The work of Ohtake, Belyaev, Alexa et al. (2003) is particularly notable\nsince it allows for several distance functions to be used simultaneously and then combined\nlocally to produce sharp features such as creases.\nPoisson surface reconstruction (Kazhdan, Bolitho, and Hoppe 2006) uses a closely related\nvolumetric function, namely a smoothed 0/1 inside–outside (characteristic) function, which\ncan be thought of as a clipped signed distance function. The gradients for this function are\nset to lie along oriented surface normals near known surface points and 0 elsewhere. The\nfunction itself is represented using a quadratic tensor-product B-spline over an octree, which\nprovides a compact representation with larger cells away from the surface or in regions of\nlower point density, and also admits the efﬁcient solution of the related Poisson equations\n(3.100–3.102), see Section 9.3.4 (P´erez, Gangnet, and Blake 2003).\nIt is also possible to replace the quadratic penalties used in the Poisson equations with\nL1 (total variation) constraints and still obtain a convex optimization problem, which can be\nsolved using either continuous (Zach, Pock, and Bischof 2007b; Zach 2008) or discrete graph\ncut (Lempitsky and Boykov 2007) techniques.",
  "620": "598\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSigned distance functions also play an integral role in level-set evolution equations ((Sec-\ntions 5.1.4 and 11.6.1), where the values of distance transforms on the mesh are updated as\nthe surface evolves to ﬁt multi-view stereo photoconsistency measures (Faugeras and Keriven\n1998).\n12.6 Model-based reconstruction\nWhen we know something ahead of time about the objects we are trying to model, we can\nconstruct more detailed and reliable 3D models using specialized techniques and representa-\ntions. For example, architecture is usually made up of large planar regions and other para-\nmetric forms (such as surfaces of revolution), usually oriented perpendicular to gravity and\nto each other (Section 12.6.1). Heads and faces can be represented using low-dimensional,\nnon-rigid shape models, since the variability in shape and appearance of human faces, while\nextremely large, is still bounded (Section 12.6.2). Human bodies or parts, such as hands, form\nhighly articulated structures, which can be represented using kinematic chains of piecewise\nrigid skeletal elements linked by joints (Section 12.6.4).\nIn this section, we highlight some of the main ideas, representations, and modeling algo-\nrithms used for these three cases. Additional details and references can be found in special-\nized conferences and workshops devoted to these topics, e.g., the International Symposium on\n3D Data Processing, Visualization, and Transmission (3DPVT), the International Conference\non 3D Digital Imaging and Modeling (3DIM), the International Conference on Automatic\nFace and Gesture Recognition (FG), the IEEE Workshop on Analysis and Modeling of Faces\nand Gestures, and the International Workshop on Tracking Humans for the Evaluation of their\nMotion in Image Sequences (THEMIS).\n12.6.1 Architecture\nArchitectural modeling, especially from aerial photography, has been one of the longest stud-\nied problems in both photogrammetry and computer vision (Walker and Herman 1988). Re-\ncently, the development of reliable image-based modeling techniques, as well as the preva-\nlence of digital cameras and 3D computer games, has spurred renewed interest in this area.\nThe work by Debevec, Taylor, and Malik (1996) was one of the earliest hybrid geometry-\nand image-based modeling and rendering systems. Their Fac¸ade system combines an inter-\nactive image-guided geometric modeling tool with model-based (local plane plus parallax)\nstereo matching and view-dependent texture mapping. During the interactive photogrammet-\nric modeling phase, the user selects block elements and aligns their edges with visible edges\nin the input images (Figure 12.14a). The system then automatically computes the dimensions\nand locations of the blocks along with the camera positions using constrained optimization",
  "621": "12.6 Model-based reconstruction\n599\nFigure 12.14 Interactive architectural modeling using the Fac¸ade system (Debevec, Taylor,\nand Malik 1996) c⃝1996 ACM: (a) input image with user-drawn edges shown in green;\n(b) shaded 3D solid model; (c) geometric primitives overlaid onto the input image; (d) ﬁnal\nview-dependent, texture-mapped 3D model.\n(Figure 12.14b–c). This approach is intrinsically more reliable than general feature-based\nstructure from motion, because it exploits the strong geometry available in the block primi-\ntives. Related work by Becker and Bove (1995), Horry, Anjyo, and Arai (1997), and Crimin-\nisi, Reid, and Zisserman (2000) exploits similar information available from vanishing points.\nIn the interactive, image-based modeling system of Sinha, Steedly, Szeliski et al. (2008),\nvanishing point directions are used to guide the user drawing of polygons, which are then\nautomatically ﬁtted to sparse 3D points recovered using structure from motion.\nOnce the rough geometry has been estimated, more detailed offset maps can be com-\nputed for each planar face using a local plane sweep, which Debevec, Taylor, and Malik\n(1996) call model-based stereo. Finally, during rendering, images from different viewpoints\nare warped and blended together as the camera moves around the scene, using a process (re-\nlated to light ﬁeld and Lumigraph rendering, see Section 13.3) called view-dependent texture\nmapping (Figure 12.14d).\nFor interior modeling, instead of working with single pictures, it is more useful to work\nwith panoramas, since you can see larger extents of walls and other structures. The 3D mod-\neling system developed by Shum, Han, and Szeliski (1998) ﬁrst constructs calibrated panora-\nmas from multiple images (Section 7.4) and then has the user draw vertical and horizontal\nlines in the image to demarcate the boundaries of planar regions. The lines are initially used\nto establish an absolute rotation for each panorama and are later used (along with the inferred\nvertices and planes) to optimize the 3D structure, which can be recovered up to scale from\none or more images (Figure 12.15). 360◦high dynamic range panoramas can also be used for\noutdoor modeling, since they provide highly reliable estimates of relative camera orientations\nas well as vanishing point directions (Antone and Teller 2002; Teller, Antone, Bodnar et al.",
  "622": "600\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 12.15\nInteractive 3D modeling from panoramas (Shum, Han, and Szeliski 1998)\nc⃝1998 IEEE: (a) wide-angle view of a panorama with user-drawn vertical and horizontal\n(axis-aligned) lines; (b) single-view reconstruction of the corridors.\n2003).\nWhile earlier image-based modeling systems required some user authoring, Werner and\nZisserman (2002) present a fully automated line-based reconstruction system. As described\nin Section 7.5.1, they ﬁrst detect lines and vanishing points and use them to calibrate the\ncamera; then they establish line correspondences using both appearance matching and tri-\nfocal tensors, which enables them to reconstruct families of 3D line segments, as shown in\nFigure 12.16a. They then generate plane hypotheses, using both co-planar 3D lines and a\nplane sweep (Section 11.1.2) based on cross-correlation scores evaluated at interest points.\nIntersections of planes are used to determine the extent of each plane, i.e., an initial coarse ge-\nometry, which is then reﬁned with the addition of rectangular or wedge-shaped indentations\nand extrusions (Figure 12.16c). Note that when top-down maps of the buildings being mod-\neled are available, these can be used to further constrain the 3D modeling process (Robertson\nand Cipolla 2002, 2009). The idea of using matched 3D lines for estimating vanishing point\ndirections and dominant planes continues to be used in a number of recent fully automated\nimage-based architectural modeling systems (Zebedin, Bauer, Karner et al. 2008; Miˇcuˇs´ık\nand Koˇseck´a 2009; Furukawa, Curless, Seitz et al. 2009b; Sinha, Steedly, and Szeliski 2009).\nAnother common characteristic of architecture is the repeated use of primitives such as\nwindows, doors, and colonnades. Architectural modeling systems can be designed to search\nfor such repeated elements and to use them as part of the structure inference process (Dick,\nTorr, and Cipolla 2004; Mueller, Zeng, Wonka et al. 2007; Schindler, Krishnamurthy, Lublin-\nerman et al. 2008; Sinha, Steedly, Szeliski et al. 2008).\nThe combination of all these techniques now makes it possible to reconstruct the structure\nof large 3D scenes (Zhu and Kanade 2008). For example, the Urbanscan system of Polle-\nfeys, Nist´er, Frahm et al. (2008) reconstructs texture-mapped 3D models of city streets from\nvideos acquired with a GPS-equipped vehicle. To obtain real-time performance, they use\nboth optimized on-line structure-from-motion algorithms, as well as GPU implementations",
  "623": "12.6 Model-based reconstruction\n601\n(a)\n(b)\n(c)\n(d)\nFigure 12.16\nAutomated architectural reconstruction using 3D lines and planes (Werner\nand Zisserman 2002) c⃝2002 Springer: (a) reconstructed 3D lines, color coded by their van-\nishing directions; (b) wire-frame model superimposed onto an input image; (c) triangulated\npiecewise-planar model with windows; (d) ﬁnal texture-mapped model.\nof plane-sweep stereo aligned to dominant planes and depth map fusion. Cornelis, Leibe,\nCornelis et al. (2008) present a related system that also uses plane-sweep stereo (aligned to\nvertical building fac¸ades) combined with object recognition and segmentation for vehicles.\nMiˇcuˇs´ık and Koˇseck´a (2009) build on these results using omni-directional images and super-\npixel-based stereo matching along dominant plane orientations. Reconstruction directly from\nactive range scanning data combined with color imagery that has been compensated for ex-\nposure and lighting variations is also possible (Chen and Chen 2008; Stamos, Liu, Chen et\nal. 2008; Troccoli and Allen 2008).\n12.6.2 Heads and faces\nAnother area in which specialized shape and appearance models are extremely helpful is in\nthe modeling of heads and faces. Even though the appearance of people seems at ﬁrst glance\nto be inﬁnitely variable, the actual shape of a person’s head and face can be described rea-\nsonably well using a few dozen parameters (Pighin, Hecker, Lischinski et al. 1998; Guenter,\nGrimm, Wood et al. 1998; DeCarlo, Metaxas, and Stone 1998; Blanz and Vetter 1999; Shan,\nLiu, and Zhang 2001).\nFigure 12.17 shows an example of an image-based modeling system, where user-speciﬁed\nkeypoints in several images are used to ﬁt a generic head model to a person’s face. As you\ncan see in Figure 12.17c, after specifying just over 100 keypoints, the shape of the face has\nbecome quite adapted and recognizable. Extracting a texture map from the original images\nand then applying it to the head model results in an animatable model with striking visual\nﬁdelity (Figure 12.18a).\nA more powerful system can be built by applying principal component analysis (PCA) to\na collection of 3D scanned faces, which is a topic we discuss in Section 12.6.3. As you can\nsee in Figure 12.19, it is then possible to ﬁt morphable 3D models to single images and to",
  "624": "602\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 12.17\n3D model ﬁtting to a collection of images: (Pighin, Hecker, Lischinski et\nal. 1998) c⃝1998 ACM: (a) set of ﬁve input images along with user-selected keypoints; (b)\nthe complete set of keypoints and curves; (c) three meshes—the original, adapted after 13\nkeypoints, and after an additional 99 keypoints; (d) the partition of the image into separately\nanimatable regions.\n(a)\n(b)\nFigure 12.18\nHead and expression tracking and re-animation using deformable 3D models.\n(a) Models ﬁt directly to ﬁve input video streams (Pighin, Szeliski, and Salesin 2002) c⃝\n2002 Springer: The bottom row shows the results of re-animating a synthetic texture-mapped\n3D model with pose and expression parameters ﬁtted to the input images in the top row. (b)\nModels ﬁt to frame-rate spacetime stereo surface models (Zhang, Snavely, Curless et al. 2004)\nc⃝2004 ACM: The top row shows the input images with synthetic green markers overlaid,\nwhile the bottom row shows the ﬁtted 3D surface model.",
  "625": "12.6 Model-based reconstruction\n603\nuse such models for a variety of animation and visual effects (Blanz and Vetter 1999). It is\nalso possible to design stereo matching algorithms that optimize directly for the head model\nparameters (Shan, Liu, and Zhang 2001; Kang and Jones 2002) or to use the output of real-\ntime stereo with active illumination (Zhang, Snavely, Curless et al. 2004) (Figures 12.7 and\n12.18b).\nAs the sophistication of 3D facial capture systems evolves, so does the detail and realism\nin the reconstructed models. Newer systems can capture (in real-time) not only surface details\nsuch as wrinkles and creases, but also accurate models of skin reﬂection, translucency, and\nsub-surface scattering (Weyrich, Matusik, Pﬁster et al. 2006; Golovinskiy, Matusik, ster et al.\n2006; Bickel, Botsch, Angst et al. 2007; Igarashi, Nishino, and Nayar 2007).\nOnce a 3D head model has been constructed, it can be used in a variety of applications,\nsuch as head tracking (Toyama 1998; Lepetit, Pilet, and Fua 2004; Matthews, Xiao, and Baker\n2007), as shown in Figures 4.29 and 14.24, and face transfer, i.e., replacing one person’s\nface with another in a video (Bregler, Covell, and Slaney 1997; Vlasic, Brand, Pﬁster et al.\n2005). Additional applications include face beautiﬁcation by warping face images toward a\nmore attractive “standard” (Leyvand, Cohen-Or, Dror et al. 2008), face de-identiﬁcation for\nprivacy protection (Gross, Sweeney, De la Torre et al. 2008), and face swapping (Bitouk,\nKumar, Dhillon et al. 2008).\n12.6.3 Application: Facial animation\nPerhaps the most widely used application of 3D head modeling is facial animation. Once\na parameterized 3D model of shape and appearance (surface texture) has been constructed,\nit can be used directly to track a person’s facial motions (Figure 12.18a) and to animate a\ndifferent character with these same motions and expressions (Pighin, Szeliski, and Salesin\n2002).\nAn improved version of such a system can be constructed by ﬁrst applying principal com-\nponent analysis (PCA) to the space of possible head shapes and facial appearances. Blanz\nand Vetter (1999) describe a system where they ﬁrst capture a set of 200 colored range scans\nof faces (Figure 12.19a), which can be represented as a large collection of (X, Y, Z, R, G, B)\nsamples (vertices).9 In order for 3D morphing to be meaningful, corresponding vertices in\ndifferent people’s scans must ﬁrst be put into correspondence (Pighin, Hecker, Lischinski et\nal. 1998). Once this is done, PCA can be applied to more naturally parameterize the 3D mor-\nphable model. The ﬂexibility of this model can be increased by performing separate analyses\nin different subregions, such as the eyes, nose, and mouth, just as in modular eigenspaces\n(Moghaddam and Pentland 1997).\n9 A cylindrical coordinate system provides a natural two-dimensional embedding for this collection, but such an\nembedding is not necessary to perform PCA.",
  "626": "604\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.19\n3D morphable face model (Blanz and Vetter 1999) c⃝1999 ACM: (a) orig-\ninal 3D face model with the addition of shape and texture variations in speciﬁc directions:\ndeviation from the mean (caricature), gender, expression, weight, and nose shape; (b) a 3D\nmorphable model is ﬁt to a single image, after which its weight or expression can be manip-\nulated; (c) another example of a 3D reconstruction along with a different set of 3D manipula-\ntions such as lighting and pose change.",
  "627": "12.6 Model-based reconstruction\n605\nAfter computing a subspace representation, different directions in this space can be as-\nsociated with different characteristics such as gender, facial expressions, or facial features\n(Figure 12.19a). As in the work of Rowland and Perrett (1995), faces can be turned into\ncaricatures by exaggerating their displacement from the mean image.\n3D morphable models can be ﬁtted to a single image using gradient descent on the error\nbetween the input image and the re-synthesized model image, after an initial manual place-\nment of the model in an approximately correct pose, scale, and location (Figures 12.19b–c).\nThe efﬁciency of this ﬁtting process can be increased using inverse compositional image\nalignment (8.64–8.65), as described by Romdhani and Vetter (2003).\nThe resulting texture-mapped 3D model can then be modiﬁed to produce a variety of vi-\nsual effects, including changing a person’s weight or expression, or three-dimensional effects\nsuch as re-lighting or 3D video-based animation (Section 13.5.1). Such models can also be\nused for video compression, e.g., by only transmitting a small number of facial expression\nand pose parameters to drive a synthetic avatar (Eisert, Wiegand, and Girod 2000; Gao, Chen,\nWang et al. 2003).\n3D facial animation is often matched to the performance of an actor, in what is known\nas performance-driven animation (Section 4.1.5) (Williams 1990). Traditional performance-\ndriven animation systems use marker-based motion capture (Ma, Jones, Chiang et al. 2008),\nwhile some newer systems use video footage to control the animation (Buck, Finkelstein,\nJacobs et al. 2000; Pighin, Szeliski, and Salesin 2002; Zhang, Snavely, Curless et al. 2004;\nVlasic, Brand, Pﬁster et al. 2005).\nAn example of the latter approach is the system developed for the ﬁlm Benjamin Button,\nin which Digital Domain used the CONTOUR system from Mova10 to capture actor Brad\nPitt’s facial motions and expressions (Roble and Zafar 2009). CONTOUR uses a combina-\ntion of phosphorescent paint and multiple high-resolution video cameras to capture real-time\n3D range scans of the actor. These 3D models were then translated into Facial Action Cod-\ning System (FACS) shape and expression parameters (Ekman and Friesen 1978) to drive a\ndifferent (older) synthetically animated computer-generated imagery (CGI) character.\n12.6.4 Whole body modeling and tracking\nThe topics of tracking humans, modeling their shape and appearance, and recognizing their\nactivities, are some of the most actively studied areas of computer vision. Annual confer-\nences11 and special journal issues (Hilton, Fua, and Ronfard 2006) are devoted to this sub-\nject, and two recent surveys (Forsyth, Arikan, Ikemoto et al. 2006; Moeslund, Hilton, and\n10 http://www.mova.com.\n11 International Conference on Automatic Face and Gesture Recognition (FG), IEEE Workshop on Analysis and\nModeling of Faces and Gestures, and International Workshop on Tracking Humans for the Evaluation of their Motion\nin Image Sequences (THEMIS).",
  "628": "606\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKr¨uger 2006) each list over 400 papers devoted to these topics.12 The HumanEva database\nof articulated human motions13 contains multi-view video sequences of human actions along\nwith corresponding motion capture data, evaluation code, and a reference 3D tracker based on\nparticle ﬁltering. The companion paper by Sigal, Balan, and Black (2010) not only describes\nthe database and evaluation but also has a nice survey of important work in this ﬁeld.\nGiven the breadth of this area, it is difﬁcult to categorize all of this research, especially\nsince different techniques usually build on each other. Moeslund, Hilton, and Kr¨uger (2006)\ndivide their survey into initialization, tracking (which includes background modeling and\nsegmentation), pose estimation, and action (activity) recognition. Forsyth, Arikan, Ikemoto et\nal. (2006) divide their survey into sections on tracking (background subtraction, deformable\ntemplates, ﬂow, and probabilistic models), recovering 3D pose from 2D observations, and\ndata association and body parts. They also include a section on motion synthesis, which is\nmore widely studied in computer graphics (Arikan and Forsyth 2002; Kovar, Gleicher, and\nPighin 2002; Lee, Chai, Reitsma et al. 2002; Li, Wang, and Shum 2002; Pullen and Bregler\n2002), see Section 13.5.2. Another potential taxonomy for work in this ﬁeld would be along\nthe lines of whether 2D or 3D (or multi-view) images are used as input and whether 2D or\n3D kinematic models are used.\nIn this section, we brieﬂy review some of the more seminal and widely cited papers in the\nareas of background subtraction, initialization and detection, tracking with ﬂow, 3D kinematic\nmodels, probabilistic models, adaptive shape modeling, and activity recognition. We refer the\nreader to the previously mentioned surveys for other topics and more details.\nBackground subtraction.\nOne of the ﬁrst steps in many (but certainly not all) human track-\ning systems is to model the background in order to extract the moving foreground objects\n(silhouettes) corresponding to people. Toyama, Krumm, Brumitt et al. (1999) review several\ndifference matting and background maintenance (modeling) techniques and provide a good\nintroduction to this topic. Stauffer and Grimson (1999) describe some techniques based on\nmixture models, while Sidenbladh and Black (2003) develop a more comprehensive treat-\nment, which models not only the background image statistics but also the appearance of the\nforeground objects, e.g., their edge and motion (frame difference) statistics.\nOnce silhouettes have been extracted from one or more cameras, they can then be mod-\neled using deformable templates or other contour models (Baumberg and Hogg 1996; Wren,\nAzarbayejani, Darrell et al. 1997). Tracking such silhouettes over time supports the analysis\nof multiple people moving around a scene, including building shape and appearance models\n12 Older surveys include those by Gavrila (1999) and Moeslund and Granum (2001). Some surveys on gesture\nrecognition, which we do not cover in this book, include those by Pavlovi´c, Sharma, and Huang (1997) and Yang,\nAhuja, and Tabb (2002).\n13 http://vision.cs.brown.edu/humaneva/.",
  "629": "12.6 Model-based reconstruction\n607\nand detecting if they are carrying objects (Haritaoglu, Harwood, and Davis 2000; Mittal and\nDavis 2003; Dimitrijevic, Lepetit, and Fua 2006).\nInitialization and detection.\nIn order to track people in a fully automated manner, it is\nnecessary to ﬁrst detect (or re-acquire) their presence in individual video frames. This topic\nis closely related to pedestrian detection, which is often considered as a kind of object recog-\nnition (Mori, Ren, Efros et al. 2004; Felzenszwalb and Huttenlocher 2005; Felzenszwalb,\nMcAllester, and Ramanan 2008), and is therefore treated in more depth in Section 14.1.2.\nAdditional techniques for initializing 3D trackers based on 2D images include those described\nby Howe, Leventon, and Freeman (2000), Rosales and Sclaroff (2000), Shakhnarovich, Viola,\nand Darrell (2003), Sminchisescu, Kanaujia, Li et al. (2005), Agarwal and Triggs (2006), Lee\nand Cohen (2006), Sigal and Black (2006), and Stenger, Thayananthan, Torr et al. (2006).\nSingle-frame human detection and pose estimation algorithms can sometimes be used by\nthemselves to perform tracking (Ramanan, Forsyth, and Zisserman 2005; Rogez, Rihan, Ra-\nmalingam et al. 2008; Bourdev and Malik 2009), as described in Section 4.1.4. More often,\nhowever, they are combined with frame-to-frame tracking techniques to provide better relia-\nbility (Fossati, Dimitrijevic, Lepetit et al. 2007; Andriluka, Roth, and Schiele 2008; Ferrari,\nMarin-Jimenez, and Zisserman 2008).\nTracking with ﬂow.\nThe tracking of people and their pose from frame to frame can be en-\nhanced by computing optic ﬂow or matching the appearance of their limbs from one frame\nto another. For example, the cardboard people model of Ju, Black, and Yacoob (1996) mod-\nels the appearance of each leg portion (upper and lower) as a moving rectangle, and uses\noptic ﬂow to estimate their location in each subsequent frame. Cham and Rehg (1999) and\nSidenbladh, Black, and Fleet (2000) track limbs using optical ﬂow and templates, along with\ntechniques for dealing with multiple hypotheses and uncertainty. Bregler, Malik, and Pullen\n(2004) use a full 3D model of limb and body motion, as described below. It is also possible to\nmatch the estimated motion ﬁeld itself to some prototypes in order to identify the particular\nphase of a running motion or to match two low-resolution video portions in order to perform\nvideo replacement (Efros, Berg, Mori et al. 2003).\n3D kinematic models.\nThe effectiveness of human modeling and tracking can be greatly\nenhanced using a more accurate 3D model of a person’s shape and motion. Underlying such\nrepresentations, which are ubiquitous in 3D computer animation in games and special effects,\nis a kinematic model or kinematic chain, which speciﬁes the length of each limb in a skeleton\nas well as the 2D or 3D rotation angles between the limbs or segments (Figure 12.20a–b).\nInferring the values of the joint angles from the locations of the visible surface points is\ncalled inverse kinematics (IK) and is widely studied in computer graphics.",
  "630": "608\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 12.20\nTracking 3D human motion: (a) kinematic chain model for a human hand\n(Rehg, Morris, and Kanade 2003) c⃝2003, reprinted by permission of SAGE; (b) tracking a\nkinematic chain blob model in a video sequence (Bregler, Malik, and Pullen 2004) c⃝2004\nSpringer; (c–d) probabilistic loose-limbed collection of body parts (Sigal, Bhatia, Roth et al.\n2004)\nFigure 12.20a shows the kinematic model for a human hand used by Rehg, Morris, and\nKanade (2003) to track hand motion in a video. As you can see, the attachment points between\nthe ﬁngers and the thumb have two degrees of freedom, while the ﬁnger joints themselves\nhave only one. Using this kind of model can greatly enhance the ability of an edge-based\ntracker to cope with rapid motion, ambiguities in 3D pose, and partial occlusions.\nKinematic chain models are even more widely used for whole body modeling and tracking\n(O’Rourke and Badler 1980; Hogg 1983; Rohr 1994). One popular approach is to associate\nan ellipsoid or superquadric with each rigid limb in the kinematic model, as shown in Fig-\nure 12.20b. This model can then be ﬁtted to each frame in one or more video streams either\nby matching silhouettes extracted from known backgrounds or by matching and tracking the\nlocations of occluding edges (Gavrila and Davis 1996; Kakadiaris and Metaxas 2000; Bre-\ngler, Malik, and Pullen 2004; Kehl and Van Gool 2006). Note that some techniques use 2D\nmodels coupled to 2D measurements, some use 3D measurements (range data or multi-view\nvideo) with 3D models, and some use monocular video to infer and track 3D models directly.\nIt is also possible to use temporal models to improve the tracking of periodic motions,\nsuch as walking, by analyzing the joint angles as functions of time (Polana and Nelson 1997;\nSeitz and Dyer 1997; Cutler and Davis 2000). The generality and applicability of such tech-\nniques can be improved by learning typical motion patterns using principal component anal-\nysis (Sidenbladh, Black, and Fleet 2000; Urtasun, Fleet, and Fua 2006).\nProbabilistic models.\nBecause tracking can be such a difﬁcult task, sophisticated proba-\nbilistic inference techniques are often used to estimate the likely states of the person being\ntracked. One popular approach, called particle ﬁltering (Isard and Blake 1998), was origi-\nnally developed for tracking the outlines of people and hands, as described in Section 5.1.2",
  "631": "12.6 Model-based reconstruction\n609\nFigure 12.21 Estimating human shape and pose from a single image using a parametric 3D\nmodel (Guan, Weiss, Bˇalan et al. 2009) c⃝2009 IEEE.\n(Figures 5.6–5.8). It was subsequently applied to whole-body tracking (Deutscher, Blake,\nand Reid 2000; Sidenbladh, Black, and Fleet 2000; Deutscher and Reid 2005) and continues\nto be used in modern trackers (Ong, Micilotta, Bowden et al. 2006). Alternative approaches\nto handling the uncertainty inherent in tracking include multiple hypothesis tracking (Cham\nand Rehg 1999) and inﬂated covariances (Sminchisescu and Triggs 2001).\nFigure 12.20c–d shows an example of a sophisticated spatio-temporal probabilistic graph-\nical model called loose-limbed people, which models not only the geometric relationship be-\ntween various limbs, but also their likely temporal dynamics (Sigal, Bhatia, Roth et al. 2004).\nThe conditional probabilities relating various limbs and time instances are learned from train-\ning data, and particle ﬁltering is used to perform the ﬁnal pose inference.\nAdaptive shape modeling.\nAnother essential component of whole body modeling and\ntracking is the ﬁtting of parameterized shape models to visual data. As we saw in Sec-\ntion 12.6.3 (Figure 12.19), the availability of large numbers of registered 3D range scans can\nbe used to create morphable models of shape and appearance (Allen, Curless, and Popovi´c\n2003). Building on this work, Anguelov, Srinivasan, Koller et al. (2005) develop a sophis-\nticated system called SCAPE (Shape Completion and Animation for PEople), which ﬁrst",
  "632": "610\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nacquires a large number of range scans of different people and of one person in different\nposes, and then registers these scans using semi-automated marker placement. The registered\ndatasets are used to model the variation in shape as a function of personal characteristics and\nskeletal pose, e.g., the bulging of muscles as certain joints are ﬂexed (Figure 12.21, top row).\nThe resulting system can then be used for shape completion, i.e., the recovery of a full 3D\nmesh model from a small number of captured markers, by ﬁnding the best model parameters\nin both shape and pose space that ﬁt the measured data.\nBecause it is constructed completely from scans of people in close-ﬁtting clothing and\nuses a parametric shape model, the SCAPE system cannot cope with people wearing loose-\nﬁtting clothing. B˘alan and Black (2008) overcome this limitation by estimating the body\nshape that ﬁts within the visual hull of the same person observed in multiple poses, while\nVlasic, Baran, Matusik et al. (2008) adapt an initial surface mesh ﬁtted with a parametric\nshape model to better match the visual hull.\nWhile the preceding body ﬁtting and pose estimation systems use multiple views to esti-\nmate body shape, even more recent work by Guan, Weiss, Bˇalan et al. (2009) can ﬁt a human\nshape and pose model to a single image of a person on a natural background. Manual ini-\ntialization is used to estimate a rough pose (skeleton) and height model, and this is then used\nto segment the person’s outline using the Grab Cut segmentation algorithm (Section 5.5).\nThe shape and pose estimate are then reﬁned using a combination of silhouette edge cues\nand shading information (Figure 12.21). The resulting 3D model can be used to create novel\nanimations.\nActivity recognition.\nThe ﬁnal widely studied topic in human modeling is motion, activity,\nand action recognition (Bobick 1997; Hu, Tan, Wang et al. 2004; Hilton, Fua, and Ronfard\n2006). Examples of actions that are commonly recognized include walking and running,\njumping, dancing, picking up objects, sitting down and standing up, and waving. Recent\nrepresentative papers on these topics have been written by Robertson and Reid (2006), Smin-\nchisescu, Kanaujia, and Metaxas (2006), Weinland, Ronfard, and Boyer (2006), Yilmaz and\nShah (2006), and Gorelick, Blank, Shechtman et al. (2007).\n12.7 Recovering texture maps and albedos\nAfter a 3D model of an object or person has been acquired, the ﬁnal step in modeling is\nusually to recover a texture map to describe the object’s surface appearance. This ﬁrst requires\nestablishing a parameterization for the (u, v) texture coordinates as a function of 3D surface\nposition. One simple way to do this is to associate a separate texture map with each triangle\n(or pair of triangles). More space-efﬁcient techniques involve unwrapping the surface onto",
  "633": "12.7 Recovering texture maps and albedos\n611\none or more maps, e.g., using a subdivision mesh (Section 12.3.2) (Eck, DeRose, Duchamp\net al. 1995) or a geometry image (Section 12.3.3) (Gu, Gortler, and Hoppe 2002).\nOnce the (u, v) coordinates for each triangle have been ﬁxed, the perspective projec-\ntion equations mapping from texture (u, v) to an image j’s pixel (uj, vj) coordinates can be\nobtained by concatenating the afﬁne (u, v) →(X, Y, Z) mapping with the perspective ho-\nmography (X, Y, Z) →(uj, vj) (Szeliski and Shum 1997). The color values for the (u, v)\ntexture map can then be re-sampled and stored, or the original image can itself be used as the\ntexture source using projective texture mapping (OpenGL-ARB 1997).\nThe situation becomes more involved when more than one source image is available for\nappearance recovery, which is the usual case. One possibility is to use a view-dependent\ntexture map (Section 13.1.1), in which a different source image (or combination of source\nimages) is used for each polygonal face based on the angles between the virtual camera, the\nsurface normals, and the source images (Debevec, Taylor, and Malik 1996; Pighin, Hecker,\nLischinski et al. 1998). An alternative approach is to estimate a complete Surface Light Field\nfor each surface point (Wood, Azuma, Aldinger et al. 2000), as described in Section 13.3.2.\nIn some situations, e.g., when using models in traditional 3D games, it is preferable to\nmerge all of the source images into a single coherent texture map during pre-processing.\nIdeally, each surface triangle should select the source image where it is seen most directly\n(perpendicular to its normal) and at the resolution best matching the texture map resolution.14\nThis can be posed as a graph cut optimization problem, where the smoothness term encour-\nages adjacent triangles to use similar source images, followed by blending to compensate\nfor exposure differences (Lempitsky and Ivanov 2007; Sinha, Steedly, Szeliski et al. 2008).\nEven better results can be obtained by explicitly modeling geometric and photometric mis-\nalignments between the source images (Shum and Szeliski 2000; Gal, Wexler, Ofek et al.\n2010).\nThese kinds of approaches produce good results when the lighting stays ﬁxed with respect\nto the object, i.e., when the camera moves around the object or space. When the lighting is\nstrongly directional, however, and the object is being moved relative to this lighting, strong\nshading effects or specularities may be present, which will interfere with the reliable recov-\nery of a texture (albedo) map. In this case, it is preferable to explicitly undo the shading\neffects (Section 12.1) by modeling the light source directions and estimating the surface re-\nﬂectance properties while recovering the texture map (Sato and Ikeuchi 1996; Sato, Wheeler,\nand Ikeuchi 1997; Yu and Malik 1998; Yu, Debevec, Malik et al. 1999). Figure 12.22 shows\nthe results of one such approach, where the specularities are ﬁrst removed while estimat-\ning the matte reﬂectance component (albedo) and then later re-introduced by estimating the\nspecular component ks in a Torrance–Sparrow reﬂection model (2.91).\n14 When surfaces are seen at oblique viewing angles, it may be necessary to blend different images together to\nobtain the best resolution (Wang, Kang, Szeliski et al. 2001).",
  "634": "612\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 12.22\nEstimating the diffuse albedo and reﬂectance parameters for a scanned 3D\nmodel (Sato, Wheeler, and Ikeuchi 1997) c⃝1997 ACM: (a) set of input images projected\nonto the model; (b) the complete diffuse reﬂection (albedo) model; (c) rendering from the\nreﬂectance model including the specular component.\n12.7.1 Estimating BRDFs\nA more ambitious approach to the problem of view-dependent appearance modeling is to\nestimate a general bidirectional reﬂectance distribution function (BRDF) for each point on an\nobject’s surface. Dana, van Ginneken, Nayar et al. (1999), Jensen, Marschner, Levoy et al.\n(2001), and Lensch, Kautz, Goesele et al. (2003) present different techniques for estimating\nsuch functions, while Dorsey, Rushmeier, and Sillion (2007) and Weyrich, Lawrence, Lensch\net al. (2008) present more recent surveys of the topics of BRDF modeling, recovery, and\nrendering.\nAs we saw in Section 2.2.2 (2.81), the BRDF can be written as\nfr(θi, φi, θr, φr; λ),\n(12.9)\nwhere (θi, φi) and (θr, φr) are the angles the incident ˆvi and reﬂected ˆvr light ray directions\nmake with the local surface coordinate frame ( ˆdx, ˆdy, ˆn) shown in Figure 2.15. When mod-\neling the appearance of an object, as opposed to the appearance of a patch of material, we\nneed to estimate this function at every point (x, y) on the object’s surface, which gives us the\nspatially varying BRDF, or SVBRDF (Weyrich, Lawrence, Lensch et al. 2008),\nfv(x, y, θi, φi, θr, φr; λ).\n(12.10)\nIf sub-surface scattering effects are being modeled, such as the long-range transmission\nof light through materials such as alabaster, the eight-dimensional bidirectional scattering-\nsurface reﬂectance-distribution function (BSSRDF) is used instead,\nfe(xi, yi, θi, φi, xe, ye, θe, φe; λ),\n(12.11)\nwhere the e subscript now represents the emitted rather than the reﬂected light directions.",
  "635": "12.7 Recovering texture maps and albedos\n613\n(a)\n(b)\nFigure 12.23\nImage-based reconstruction of appearance and detailed geometry (Lensch,\nKautz, Goesele et al. 2003) c⃝2003 ACM. (a) Appearance models (BRDFs) are re-estimated\nusing divisive clustering. (b) In order to model detailed spatially varying appearance, each\nlumitexel is projected onto the basis formed by the clustered materials.\nWeyrich, Lawrence, Lensch et al. (2008) provide a nice survey of these and related topics,\nincluding basic photometry, BRDF models, traditional BRDF acquisition using gonio reﬂec-\ntometry (the precise measurement of visual angles and reﬂectances), multiplexed illumination\n(Schechner, Nayar, and Belhumeur 2009), skin modeling (Debevec, Hawkins, Tchou et al.\n2000; Weyrich, Matusik, Pﬁster et al. 2006), and image-based acquisition techniques, which\nsimultaneously recover an object’s 3D shape and reﬂectometry from multiple photographs.\nA nice example of this latter approach is the system developed by Lensch, Kautz, Goesele\net al. (2003), who estimate locally varying BRDFs and reﬁne their shape models using local\nestimates of surface normals. To build up their models, they ﬁrst associate a lumitexels, which\ncontains a 3D position, a surface normal, and a set of sparse radiance samples, with each\nsurface point. Next, they cluster such lumitexels into materials that share common properties,\nusing a Lafortune reﬂectance model (Lafortune, Foo, Torrance et al. 1997) and a divisive\nclustering approach (Figure 12.23a). Finally, in order to model detailed spatially varying\nappearance, each lumitexel (surface point) is projected onto the basis of clustered appearance\nmodels (Figure 12.23b).\nWhile most of the techniques discussed in this section require large numbers of views\nto estimate surface properties, a challenging future direction will be to take these techniques\nout of the lab and into the real world, and to combine them with regular and Internet photo\nimage-based modeling approaches.\n12.7.2 Application: 3D photography\nThe techniques described in this chapter for building complete 3D models from multiple im-\nages and then recovering their surface appearance have opened up a whole new range of\napplications that often go under the name 3D photography. Pollefeys and Van Gool (2002)\nprovide a nice introduction to this ﬁeld, including the processing steps of feature matching,",
  "636": "614\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nstructure from motion recovery,15 dense depth map estimation, 3D model building, and tex-\nture map recovery. A complete Web-based system for automatically performing all of these\ntasks, called ARC3D, is described by Vergauwen and Van Gool (2006) and Moons, Van Gool,\nand Vergauwen (2010). The latter paper provides not only an in-depth survey of this whole\nﬁeld but also a detailed description of their complete end-to-end system.\nAn alternative to such fully automated systems is to put the user in the loop in what is\nsometimes called interactive computer vision. van den Hengel, Dick, Thormhlen et al. (2007)\ndescribe their VideoTrace system, which performs automated point tracking and 3D structure\nrecovery from video and then lets the user draw triangles and surfaces on top of the resulting\npoint cloud, as well as interactively adjusting the locations of model vertices. Sinha, Steedly,\nSzeliski et al. (2008) describe a related system that uses matched vanishing points in multiple\nimages (Figure 4.45) to infer 3D line orientations and plane normals. These are then used to\nguide the user drawing axis-aligned planes, which are automatically ﬁtted to the recovered\n3D point cloud. Fully automated variants on these ideas are described by Zebedin, Bauer,\nKarner et al. (2008), Furukawa, Curless, Seitz et al. (2009a), Furukawa, Curless, Seitz et al.\n(2009b), Miˇcuˇs´ık and Koˇseck´a (2009), and Sinha, Steedly, and Szeliski (2009).\nAs the sophistication and reliability of these techniques continues to improve, we can ex-\npect to see even more user-friendly applications for photorealistic 3D modeling from images\n(Exercise 12.8).\n12.8 Additional reading\nShape from shading is one of the classic problems in computer vision (Horn 1975). Some\nrepresentative papers in this area include those by Horn (1977), Ikeuchi and Horn (1981),\nPentland (1984), Horn and Brooks (1986), Horn (1990), Szeliski (1991a), Mancini and Wolff\n(1992), Dupuis and Oliensis (1994), and Fua and Leclerc (1995). The collection of papers\nedited by Horn and Brooks (1989) is a great source of information on this topic, especially\nthe chapter on variational approaches. The survey by Zhang, Tsai, Cryer et al. (1999) not\nonly reviews more recent techniques but also provides some comparative results.\nWoodham (1981) wrote the seminal paper of photometric stereo. Shape from texture\ntechniques include those by Witkin (1981), Ikeuchi (1981), Blostein and Ahuja (1987), Gard-\ning (1992), Malik and Rosenholtz (1997), Liu, Collins, and Tsin (2004), Liu, Lin, and Hays\n(2004), Hays, Leordeanu, Efros et al. (2006), Lin, Hays, Wu et al. (2006), Lobay and Forsyth\n(2006), White and Forsyth (2006), White, Crane, and Forsyth (2007), and Park, Brockle-\nhurst, Collins et al. (2009). Good papers and books on depth from defocus have been written\nby Pentland (1987), Nayar and Nakagawa (1994), Nayar, Watanabe, and Noguchi (1996),\n15 These earlier steps are also discussed in Section 7.4.4.",
  "637": "12.8 Additional reading\n615\nWatanabe and Nayar (1998), Chaudhuri and Rajagopalan (1999), and Favaro and Soatto\n(2006). Additional techniques for recovering shape from various kinds of illumination ef-\nfects, including inter-reﬂections (Nayar, Ikeuchi, and Kanade 1991), are discussed in the\nbook on shape recovery edited by Wolff, Shafer, and Healey (1992b).\nActive rangeﬁnding systems, which use laser or natural light illumination projected into\nthe scene, have been described by Besl (1989), Rioux and Bird (1993), Kang, Webb, Zit-\nnick et al. (1995), Curless and Levoy (1995), Curless and Levoy (1996), Proesmans, Van\nGool, and Defoort (1998), Bouguet and Perona (1999), Curless (1999), Hebert (2000), Id-\ndan and Yahav (2001), Goesele, Fuchs, and Seidel (2003), Scharstein and Szeliski (2003),\nDavis, Ramamoorthi, and Rusinkiewicz (2003), Zhang, Curless, and Seitz (2003), Zhang,\nSnavely, Curless et al. (2004), and Moons, Van Gool, and Vergauwen (2010). Individual\nrange scans can be aligned using 3D correspondence and distance optimization techniques\nsuch as iterated closest points and its variants (Besl and McKay 1992; Zhang 1994; Szeliski\nand Lavall´ee 1996; Johnson and Kang 1997; Gold, Rangarajan, Lu et al. 1998; Johnson\nand Hebert 1999; Pulli 1999; David, DeMenthon, Duraiswami et al. 2004; Li and Hartley\n2007; Enqvist, Josephson, and Kahl 2009). Once they have been aligned, range scans can\nbe merged using techniques that model the signed distance of surfaces to volumetric sam-\nple points (Hoppe, DeRose, Duchamp et al. 1992; Curless and Levoy 1996; Hilton, Stoddart,\nIllingworth et al. 1996; Wheeler, Sato, and Ikeuchi 1998; Kazhdan, Bolitho, and Hoppe 2006;\nLempitsky and Boykov 2007; Zach, Pock, and Bischof 2007b; Zach 2008).\nOnce constructed, 3D surfaces can be modeled and manipulated using a variety of three-\ndimensional representations, which include triangle meshes (Eck, DeRose, Duchamp et al.\n1995; Hoppe 1996), splines (Farin 1992, 1996; Lee, Wolberg, and Shin 1996), subdivision\nsurfaces (Stollnitz, DeRose, and Salesin 1996; Zorin, Schr¨oder, and Sweldens 1996; Warren\nand Weimer 2001; Peters and Reif 2008), and geometry images (Gu, Gortler, and Hoppe\n2002). Alternatively, they can be represented as collections of point samples with local ori-\nentation estimates (Hoppe, DeRose, Duchamp et al. 1992; Szeliski and Tonnesen 1992; Turk\nand O’Brien 2002; Pﬁster, Zwicker, van Baar et al. 2000; Alexa, Behr, Cohen-Or et al. 2003;\nPauly, Keiser, Kobbelt et al. 2003; Diebel, Thrun, and Br¨unig 2006; Guennebaud and Gross\n2007; Guennebaud, Germann, and Gross 2008; Oztireli, Guennebaud, and Gross 2008). They\ncan also be modeled using implicit inside–outside characteristic or signed distance functions\nsampled on regular or irregular (octree) volumetric grids (Lavall´ee and Szeliski 1995; Szeliski\nand Lavall´ee 1996; Frisken, Perry, Rockwood et al. 2000; Dinh, Turk, and Slabaugh 2002;\nKazhdan, Bolitho, and Hoppe 2006; Lempitsky and Boykov 2007; Zach, Pock, and Bischof\n2007b; Zach 2008).\nThe literature on model-based 3D reconstruction is extensive. For modeling architecture\nand urban scenes, both interactive and fully automated systems have been developed. A\nspecial journal issue devoted to the reconstruction of large-scale 3D scenes (Zhu and Kanade",
  "638": "616\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n2008) is a good source of references and Robertson and Cipolla (2009) give a nice description\nof a complete system. Lots of additional references can be found in Section 12.6.1.\nFace and whole body modeling and tracking is a very active sub-ﬁeld of computer vision,\nwith its own conferences and workshops, e.g., the International Conference on Automatic\nFace and Gesture Recognition (FG), the IEEE Workshop on Analysis and Modeling of Faces\nand Gestures, and the International Workshop on Tracking Humans for the Evaluation of their\nMotion in Image Sequences (THEMIS). Recent survey articles on the topic of whole body\nmodeling and tracking include those by Forsyth, Arikan, Ikemoto et al. (2006), Moeslund,\nHilton, and Kr¨uger (2006), and Sigal, Balan, and Black (2010).\n12.9 Exercises\nEx 12.1: Shape from focus\nGrab a series of focused images with a digital SLR set to man-\nual focus (or get one that allows for programmatic focus control) and recover the depth of an\nobject.\n1. Take some calibration images, e.g., of a checkerboard, so you can compute a mapping\nbetween the amount of defocus and the focus setting.\n2. Try both a fronto-parallel planar target and one which is slanted so that it covers the\nworking range of the sensor. Which one works better?\n3. Now put a real object in the scene and perform a similar focus sweep.\n4. For each pixel, compute the local sharpness and ﬁt a parabolic curve over focus settings\nto ﬁnd the most in-focus setting.\n5. Map these focus settings to depth and compare your result to ground truth. If you are\nusing a known simple object, such as sphere or cylinder (a ball or a soda can), it’s easy\nto measure its true shape.\n6. (Optional) See if you can recover the depth map from just two or three focus settings.\n7. (Optional) Use an LCD projector to project artiﬁcial texture onto the scene. Use a pair\nof cameras to compare the accuracy of your shape from focus and shape from stereo\ntechniques.\n8. (Optional) Create an all-in-focus image using the technique of Agarwala, Dontcheva,\nAgrawala et al. (2004).\nEx 12.2: Shadow striping\nImplement the handheld shadow striping system of Bouguet\nand Perona (1999). The basic steps include the following.",
  "639": "12.9 Exercises\n617\n1. Set up two background planes behind the object of interest and calculate their orienta-\ntion relative to the viewer, e.g., with ﬁducial marks.\n2. Cast a moving shadow with a stick across the scene; record the video or capture the\ndata with a webcam.\n3. Estimate each light plane equation from the projections of the cast shadow against the\ntwo backgrounds.\n4. Triangulate to the remaining points on each curve to get a 3D stripe and display the\nstripes using a 3D graphics engine.\n5. (Optional) remove the requirement for a known second (vertical) plane and infer its\nlocation (or that of the light source) using the techniques described by Bouguet and\nPerona (1999). The techniques from Exercise 10.9 may also be helpful here.\nEx 12.3: Range data registration\nRegister two or more 3D datasets using either iterated\nclosest points (ICP) (Besl and McKay 1992; Zhang 1994; Gold, Rangarajan, Lu et al. 1998)\nor octree signed distance ﬁelds (Szeliski and Lavall´ee 1996) (Section 12.2.1).\nApply your technique to narrow-baseline stereo pairs, e.g., obtained by moving a cam-\nera around an object, using structure from motion to recover the camera poses, and using a\nstandard stereo matching algorithm.\nEx 12.4: Range data merging\nMerge the datasets that you registered in the previous exer-\ncise using signed distance ﬁelds (Curless and Levoy 1996; Hilton, Stoddart, Illingworth et al.\n1996). You can optionally use an octree to represent and compress this ﬁeld if you already\nimplemented it in the previous registration step.\nExtract a meshed surface model from the signed distance ﬁeld using marching cubes and\ndisplay the resulting model.\nEx 12.5: Surface simpliﬁcation\nUse progressive meshes (Hoppe 1996) or some other tech-\nnique from Section 12.3.2 to create a hierarchical simpliﬁcation of your surface model.\nEx 12.6: Architectural modeler\nBuild a 3D interior or exterior model of some architec-\ntural structure, such as your house, from a series of handheld wide-angle photographs.\n1. Extract lines and vanishing points (Exercises 4.11–4.15) to estimate the dominant di-\nrections in each image.\n2. Use structure from motion to recover all of the camera poses and match up the vanish-\ning points.",
  "640": "618\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. Let the user sketch the locations of the walls by drawing lines corresponding to wall\nbottoms, tops, and horizontal extents onto the images (Sinha, Steedly, Szeliski et al.\n2008)—see also Exercise 6.9. Do something similar for openings (doors and windows)\nand simple furniture (tables and countertops).\n4. Convert the resulting polygonal meshes into a 3D model (e.g., VRML) and optionally\ntexture-map these surfaces from the images.\nEx 12.7: Body tracker\nDownload the video sequences from the HumanEva Web site.16\nEither implement a human motion tracker from scratch or extend the code on that Web site\n(Sigal, Balan, and Black 2010) in some interesting way.\nEx 12.8: 3D photography\nCombine all of your previously developed techniques to pro-\nduce a system that takes a series of photographs or a video and constructs a photorealistic\ntexture-mapped 3D model.\n16 http://vision.cs.brown.edu/humaneva/.",
  "641": "Chapter 13\nImage-based rendering\n13.1 View interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621\n13.1.1 View-dependent texture maps\n. . . . . . . . . . . . . . . . . . . . . 623\n13.1.2 Application: Photo Tourism\n. . . . . . . . . . . . . . . . . . . . . . 624\n13.2 Layered depth images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626\n13.2.1 Impostors, sprites, and layers . . . . . . . . . . . . . . . . . . . . . . 626\n13.3 Light ﬁelds and Lumigraphs\n. . . . . . . . . . . . . . . . . . . . . . . . . . 628\n13.3.1 Unstructured Lumigraph . . . . . . . . . . . . . . . . . . . . . . . . 632\n13.3.2 Surface light ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . 632\n13.3.3 Application: Concentric mosaics . . . . . . . . . . . . . . . . . . . . 634\n13.4 Environment mattes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634\n13.4.1 Higher-dimensional light ﬁelds . . . . . . . . . . . . . . . . . . . . . 636\n13.4.2 The modeling to rendering continuum . . . . . . . . . . . . . . . . . 637\n13.5 Video-based rendering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638\n13.5.1 Video-based animation . . . . . . . . . . . . . . . . . . . . . . . . . 639\n13.5.2 Video textures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640\n13.5.3 Application: Animating pictures . . . . . . . . . . . . . . . . . . . . 643\n13.5.4 3D Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\n13.5.5 Application: Video-based walkthroughs . . . . . . . . . . . . . . . . 645\n13.6 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648\n13.7 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650",
  "642": "620\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 13.1 Image-based and video-based rendering: (a) a 3D view of a Photo Tourism re-\nconstruction (Snavely, Seitz, and Szeliski 2006) c⃝2006 ACM; (b) a slice through a 4D light\nﬁeld (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996 ACM; (c) sprites with depth (Shade,\nGortler, He et al. 1998) c⃝1998 ACM; (d) surface light ﬁeld (Wood, Azuma, Aldinger et al.\n2000) c⃝2000 ACM; (e) environment matte in front of a novel background (Zongker, Werner,\nCurless et al. 1999) c⃝1999 ACM; (f) real-time video environment matte (Chuang, Zongker,\nHindorff et al. 2000) c⃝2000 ACM; (g) Video Rewrite used to re-animate old video (Bregler,\nCovell, and Slaney 1997) c⃝1997 ACM; (h) video texture of a candle ﬂame (Sch¨odl, Szeliski,\nSalesin et al. 2000) c⃝2000 ACM; (i) video view interpolation (Zitnick, Kang, Uyttendaele\net al. 2004) c⃝2004 ACM.",
  "643": "13.1 View interpolation\n621\nOver the last two decades, image-based rendering has emerged as one of the most exciting\napplications of computer vision (Kang, Li, Tong et al. 2006; Shum, Chan, and Kang 2007).\nIn image-based rendering, 3D reconstruction techniques from computer vision are combined\nwith computer graphics rendering techniques that use multiple views of a scene to create inter-\nactive photo-realistic experiences, such as the Photo Tourism system shown in Figure 13.1a.\nCommercial versions of such systems include immersive street-level navigation in on-line\nmapping systems1 and the creation of 3D Photosynths2 from large collections of casually\nacquired photographs.\nIn this chapter, we explore a variety of image-based rendering techniques, such as those\nillustrated in Figure 13.1. We begin with view interpolation (Section 13.1), which creates a\nseamless transition between a pair of reference images using one or more pre-computed depth\nmaps. Closely related to this idea are view-dependent texture maps (Section 13.1.1), which\nblend multiple texture maps on a 3D model’s surface. The representations used for both the\ncolor imagery and the 3D geometry in view interpolation include a number of clever variants\nsuch as layered depth images (Section 13.2) and sprites with depth (Section 13.2.1).\nWe continue our exploration of image-based rendering with the light ﬁeld and Lumigraph\nfour-dimensional representations of a scene’s appearance (Section 13.3), which can be used\nto render the scene from any arbitrary viewpoint. Variants on these representations include\nthe unstructured Lumigraph (Section 13.3.1), surface light ﬁelds (Section 13.3.2), concentric\nmosaics (Section 13.3.3), and environment mattes (Section 13.4).\nThe last part of this chapter explores the topic of video-based rendering, which uses one\nor more videos in order to create novel video-based experiences (Section 13.5). The topics\nwe cover include video-based facial animation (Section 13.5.1), as well as video textures\n(Section 13.5.2), in which short video clips can be seamlessly looped to create dynamic real-\ntime video-based renderings of a scene. We close with a discussion of 3D videos created from\nmultiple video streams (Section 13.5.4), as well as video-based walkthroughs of environments\n(Section 13.5.5), which have found widespread application in immersive outdoor mapping\nand driving direction systems.\n13.1 View interpolation\nWhile the term image-based rendering ﬁrst appeared in the papers by Chen (1995) and\nMcMillan and Bishop (1995), the work on view interpolation by Chen and Williams (1993)\nis considered as the seminal paper in the ﬁeld. In view interpolation, pairs of rendered color\nimages are combined with their pre-computed depth maps to generate interpolated views that\n1 http://maps.bing.com and http://maps.google.com.\n2 http://photosynth.net.",
  "644": "622\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 13.2\nView interpolation (Chen and Williams 1993) c⃝1993 ACM: (a) holes from\none source image (shown in blue); (b) holes after combining two widely spaced images; (c)\nholes after combining two closely spaced images; (d) after interpolation (hole ﬁlling).\nmimic what a virtual camera would see in between the two reference views.\nView interpolation combines two ideas that were previously used in computer vision and\ncomputer graphics. The ﬁrst is the idea of pairing a recovered depth map with the refer-\nence image used in its computation and then using the resulting texture-mapped 3D model\nto generate novel views (Figure 11.1). The second is the idea of morphing (Section 3.6.3)\n(Figure 3.53), where correspondences between pairs of images are used to warp each refer-\nence image to an in-between location while simultaneously cross-dissolving between the two\nwarped images.\nFigure 13.2 illustrates this process in more detail. First, both source images are warped\nto the novel view, using both the knowledge of the reference and virtual 3D camera pose\nalong with each image’s depth map (2.68–2.70). In the paper by Chen and Williams (1993),\na forward warping algorithm (Algorithm 3.1 and Figure 3.46) is used. The depth maps are\nrepresented as quadtrees for both space and rendering time efﬁciency (Samet 1989).\nDuring the forward warping process, multiple pixels (which occlude one another) may\nland on the same destination pixel. To resolve this conﬂict, either a z-buffer depth value can\nbe associated with each destination pixel or the images can be warped in back-to-front order,\nwhich can be computed based on the knowledge of epipolar geometry (Chen and Williams\n1993; Laveau and Faugeras 1994; McMillan and Bishop 1995).\nOnce the two reference images have been warped to the novel view (Figure 13.2a–b), they\ncan be merged to create a coherent composite (Figure 13.2c). Whenever one of the images\nhas a hole (illustrated as a cyan pixel), the other image is used as the ﬁnal value. When both\nimages have pixels to contribute, these can be blended as in usual morphing, i.e., according\nto the relative distances between the virtual and source cameras. Note that if the two images\nhave very different exposures, which can happen when performing view interpolation on real\nimages, the hole-ﬁlled regions and the blended regions will have different exposures, leading",
  "645": "13.1 View interpolation\n623\nto subtle artifacts.\nThe ﬁnal step in view interpolation (Figure 13.2d) is to ﬁll any remaining holes or cracks\ndue to the forward warping process or lack of source data (scene visibility). This can be done\nby copying pixels from the further pixels adjacent to the hole. (Otherwise, foreground objects\nare subject to a “fattening effect”.)\nThe above process works well for rigid scenes, although its visual quality (lack of alias-\ning) can be improved using a two-pass, forward–backward algorithm (Section 13.2.1) (Shade,\nGortler, He et al. 1998) or full 3D rendering (Zitnick, Kang, Uyttendaele et al. 2004). In the\ncase where the two reference images are views of a non-rigid scene, e.g., a person smiling\nin one image and frowning in the other, view morphing, which combines ideas from view\ninterpolation with regular morphing, can be used (Seitz and Dyer 1996).\nWhile the original view interpolation paper describes how to generate novel views based\non similar pre-computed (linear perspective) images, the plenoptic modeling paper of McMil-\nlan and Bishop (1995) argues that cylindrical images should be used to store the pre-computed\nrendering or real-world images. (Chen 1995) also propose using environment maps (cylin-\ndrical, cubic, or spherical) as source images for view interpolation.\n13.1.1 View-dependent texture maps\nView-dependent texture maps (Debevec, Taylor, and Malik 1996) are closely related to view\ninterpolation. Instead of associating a separate depth map with each input image, a single 3D\nmodel is created for the scene, but different images are used as texture map sources depending\non the virtual camera’s current position (Figure 13.3a).3\nIn more detail, given a new virtual camera position, the similarity of this camera’s view of\neach polygon (or pixel) is compared to that of potential source images. The images are then\nblended using a weighting that is inversely proportional to the angles αi between the virtual\nview and the source views (Figure 13.3a). Even though the geometric model can be fairly\ncoarse (Figure 13.3b), blending between different views gives a strong sense of more detailed\ngeometry because of the parallax (visual motion) between corresponding pixels. While the\noriginal paper performs the weighted blend computation separately at each pixel or coarsened\npolygon face, follow-on work by Debevec, Yu, and Borshukov (1998) presents a more efﬁ-\ncient implementation based on precomputing contributions for various portions of viewing\nspace and then using projective texture mapping (OpenGL-ARB 1997).\nThe idea of view-dependent texture mapping has been used in a large number of sub-\nsequent image-based rendering systems, including facial modeling and animation (Pighin,\n3 The term image-based modeling, which is now commonly used to describe the creation of texture-mapped 3D\nmodels from multiple images, appears to have ﬁrst been used by Debevec, Taylor, and Malik (1996), who also used\nthe term photogrammetric modeling to describe the same process.",
  "646": "624\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 13.3 View-dependent texture mapping (Debevec, Taylor, and Malik 1996) c⃝1996\nACM. (a) The weighting given to each input view depends on the relative angles between the\nnovel (virtual) view and the original views; (b) simpliﬁed 3D model geometry; (c) with view-\ndependent texture mapping, the geometry appears to have more detail (recessed windows).\nHecker, Lischinski et al. 1998) and 3D scanning and visualization (Pulli, Abi-Rached, Duchamp\net al. 1998). Closely related to view-dependent texture mapping is the idea of blending be-\ntween light rays in 4D space, which forms the basis of the Lumigraph and unstructured Lu-\nmigraph systems (Section 13.3) (Gortler, Grzeszczuk, Szeliski et al. 1996; Buehler, Bosse,\nMcMillan et al. 2001).\nIn order to provide even more realism in their Fac¸ade system, Debevec, Taylor, and Malik\n(1996) also include a model-based stereo component, which optionally computes an offset\n(parallax) map for each coarse planar facet of their 3D model. They call the resulting analysis\nand rendering system a hybrid geometry- and image-based approach, since it uses traditional\n3D geometric modeling to create the global 3D model, but then uses local depth offsets, along\nwith view interpolation, to add visual realism.\n13.1.2 Application: Photo Tourism\nWhile view interpolation was originally developed to accelerate the rendering of 3D scenes\non low-powered processors and systems without graphics acceleration, it turns out that it\ncan be applied directly to large collections of casually acquired photographs. The Photo\nTourism system developed by Snavely, Seitz, and Szeliski (2006) uses structure from motion\nto compute the 3D locations and poses of all the cameras taking the images, along with a\nsparse 3D point-cloud model of the scene (Section 7.4.4, Figure 7.11).\nTo perform an image-based exploration of the resulting sea of images (Aliaga, Funkhouser,\nYanovsky et al. 2003), Photo Tourism ﬁrst associates a 3D proxy with each image. While a\ntriangulated mesh obtained from the point cloud can sometimes form a suitable proxy, e.g.,\nfor outdoor terrain models, a simple dominant plane ﬁt to the 3D points visible in each image",
  "647": "13.1 View interpolation\n625\n(a)\n(b)\nFigure 13.4\nPhoto Tourism (Snavely, Seitz, and Szeliski 2006): c⃝2006 ACM: (a) a 3D\noverview of the scene, with translucent washes and lines painted onto the planar impostors;\n(b) once the user has selected a region of interest, a set of related thumbnails is displayed\nalong the bottom; (c) planar proxy selection for optimal stabilization (Snavely, Garg, Seitz et\nal. 2008) c⃝2008 ACM.\noften performs better, because it does not contain any erroneous segments or connections that\npop out as artifacts. As automated 3D modeling techniques continue to improve, however,\nthe pendulum may swing back to more detailed 3D geometry (Goesele, Snavely, Curless et\nal. 2007; Sinha, Steedly, and Szeliski 2009).\nThe resulting image-based navigation system lets users move from photo to photo, ei-\nther by selecting cameras from a top-down view of the scene (Figure 13.4a) or by selecting\nregions of interest in an image, navigating to nearby views, or selecting related thumbnails\n(Figure 13.4b). To create a background for the 3D scene, e.g., when being viewed from\nabove, non-photorealistic techniques (Section 10.5.2), such as translucent color washes or\nhighlighted 3D line segments, can be used (Figure 13.4a). The system can also be used to\nannotate regions of images and to automatically propagate such annotations to other pho-\ntographs.\nThe 3D planar proxies used in Photo Tourism and the related Photosynth system from\nMicrosoft result in non-photorealistic transitions reminiscent of visual effects such as “page\nﬂips”. Selecting a stable 3D axis for all the planes can reduce the amount of swimming and\nenhance the perception of 3D (Figure 13.4c) (Snavely, Garg, Seitz et al. 2008). It is also\npossible to automatically detect objects in the scene that are seen from multiple views and\ncreate “orbits” of viewpoints around such objects. Furthermore, nearby images in both 3D\nposition and viewing direction can be linked to create “virtual paths”, which can then be used\nto navigate between arbitrary pairs of images, such as those you might take yourself while\nwalking around a popular tourist site (Snavely, Garg, Seitz et al. 2008).\nThe spatial matching of image features and regions performed by Photo Tourism can\nalso be used to infer more information from large image collections. For example, Simon,\nSnavely, and Seitz (2007) show how the match graph between images of popular tourist sites",
  "648": "626\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ncan be used to ﬁnd the most iconic (commonly photographed) objects in the collection, along\nwith their related tags. In follow-on work, Simon and Seitz (2008) show how such tags can\nbe propagated to sub-regions of each image, using an analysis of which 3D points appear\nin the central portions of photographs. Extensions of these techniques to all of the world’s\nimages, including the use of GPS tags where available, have been investigated as well (Li,\nWu, Zach et al. 2008; Quack, Leibe, and Van Gool 2008; Crandall, Backstrom, Huttenlocher\net al. 2009; Li, Crandall, and Huttenlocher 2009; Zheng, Zhao, Song et al. 2009).\n13.2 Layered depth images\nTraditional view interpolation techniques associate a single depth map with each source or\nreference image. Unfortunately, when such a depth map is warped to a novel view, holes and\ncracks inevitably appear behind the foreground objects. One way to alleviate this problem is\nto keep several depth and color values (depth pixels) at every pixel in a reference image (or,\nat least for pixels near foreground–background transitions) (Figure 13.5). The resulting data\nstructure, which is called a layered depth image (LDI), can be used to render new views using\na back-to-front forward warping (splatting) algorithm (Shade, Gortler, He et al. 1998).\n13.2.1 Impostors, sprites, and layers\nAn alternative to keeping lists of color-depth values at each pixel, as is done in the LDI, is\nto organize objects into different layers or sprites. The term sprite originates in the computer\ngame industry, where it is used to designate ﬂat animated characters in games such as Pac-\nMan or Mario Bros. When put into a 3D setting, such objects are often called impostors,\nbecause they use a piece of ﬂat, alpha-matted geometry to represent simpliﬁed versions of 3D\nobjects that are far away from the camera (Shade, Lischinski, Salesin et al. 1996; Lengyel and\nSnyder 1997; Torborg and Kajiya 1996). In computer vision, such representations are usually\ncalled layers (Wang and Adelson 1994; Baker, Szeliski, and Anandan 1998; Torr, Szeliski,\nand Anandan 1999; Birchﬁeld, Natarajan, and Tomasi 2007). Section 8.5.2 discusses the\ntopics of transparent layers and reﬂections, which occur on specular and transparent surfaces\nsuch as glass.\nWhile ﬂat layers can often serve as an adequate representation of geometry and appear-\nance for far-away objects, better geometric ﬁdelity can be achieved by also modeling the\nper-pixel offsets relative to a base plane, as shown in Figures 13.5 and 13.6a–b. Such repre-\nsentations are called plane plus parallax in the computer vision literature (Kumar, Anandan,\nand Hanna 1994; Sawhney 1994; Szeliski and Coughlan 1997; Baker, Szeliski, and Anandan\n1998), as discussed in Section 8.5 (Figure 8.16). In addition to fully automated stereo tech-\nniques, it is also possible to paint in depth layers (Kang 1998; Oh, Chen, Dorsey et al. 2001;",
  "649": "13.2 Layered depth images\n627\nFigure 13.5\nA variety of image-based rendering primitives, which can be used depending\non the distance between the camera and the object of interest (Shade, Gortler, He et al. 1998)\nc⃝1998 ACM. Closer objects may require more detailed polygonal representations, while\nmid-level objects can use a layered depth image (LDI), and far-away objects can use sprites\n(potentially with depth) and environment maps.\n(a)\n(b)\n(c)\n(d)\nFigure 13.6\nSprites with depth (Shade, Gortler, He et al. 1998) c⃝1998 ACM: (a) alpha-\nmatted color sprite; (b) corresponding relative depth or parallax; (c) rendering without relative\ndepth; (d) rendering with depth (note the curved object boundaries).",
  "650": "628\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nShum, Sun, Yamazaki et al. 2004) or to infer their 3D structure from monocular image cues\n(Section 14.4.4) (Hoiem, Efros, and Hebert 2005b; Saxena, Sun, and Ng 2009).\nHow can we render a sprite with depth from a novel viewpoint? One possibility, as with\na regular depth map, is to just forward warp each pixel to its new location, which can cause\naliasing and cracks. A better way, which we already mentioned in Section 3.6.2, is to ﬁrst\nwarp the depth (or (u, v) displacement) map to the novel view, ﬁll in the cracks, and then use\nhigher-quality inverse warping to resample the color image (Shade, Gortler, He et al. 1998).\nFigure 13.6d shows the results of applying such a two-pass rendering algorithm. From this\nstill image, you can appreciate that the foreground sprites look more rounded; however, to\nfully appreciate the improvement in realism, you would have to look at the actual animated\nsequence.\nSprites with depth can also be rendered using conventional graphics hardware, as de-\nscribed in (Zitnick, Kang, Uyttendaele et al. 2004). Rogmans, Lu, Bekaert et al. (2009)\ndescribe GPU implementations of both real-time stereo matching and real-time forward and\ninverse rendering algorithms.\n13.3 Light ﬁelds and Lumigraphs\nWhile image-based rendering approaches can synthesize scene renderings from novel view-\npoints, they raise the following more general question:\nIs is possible to capture and render the appearance of a scene from all possible\nviewpoints and, if so, what is the complexity of the resulting structure?\nLet us assume that we are looking at a static scene, i.e., one where the objects and illu-\nminants are ﬁxed, and only the observer is moving around. Under these conditions, we can\ndescribe each image by the location and orientation of the virtual camera (6 dof) as well as\nits intrinsics (e.g., its focal length). However, if we capture a two-dimensional spherical im-\nage around each possible camera location, we can re-render any view from this information.4\nThus, taking the cross-product of the three-dimensional space of camera positions with the\n2D space of spherical images, we obtain the 5D plenoptic function of Adelson and Bergen\n(1991), which forms the basis of the image-based rendering system of McMillan and Bishop\n(1995).\nNotice, however, that when there is no light dispersion in the scene, i.e., no smoke or fog,\nall the coincident rays along a portion of free space (between solid or refractive objects) have\nthe same color value. Under these conditions, we can reduce the 5D plenoptic function to\n4 Since we are counting dimensions, we ignore for now any sampling or resolution issues.",
  "651": "13.3 Light ﬁelds and Lumigraphs\n629\ns\nt\nu\nv\n(s,t)\n(u,v)\nCamera center\nImage plane\n pixel\n(a)\n(b)\nFigure 13.7 The Lumigraph (Gortler, Grzeszczuk, Szeliski et al. 1996) c⃝1996 ACM: (a) a\nray is represented by its 4D two-plane parameters (s, t) and (u, v); (b) a slice through the 3D\nlight ﬁeld subset (u, v, s).\nthe 4D light ﬁeld of all possible rays (Gortler, Grzeszczuk, Szeliski et al. 1996; Levoy and\nHanrahan 1996; Levoy 2006).5\nTo make the parameterization of this 4D function simpler, let us put two planes in the\n3D scene roughly bounding the area of interest, as shown in Figure 13.7a. Any light ray\nterminating at a camera that lives in front of the st plane (assuming that this space is empty)\npasses through the two planes at (s, t) and (u, v) and can be described by its 4D coordinate\n(s, t, u, v). This diagram (and parameterization) can be interpreted as describing a family of\ncameras living on the st plane with their image planes being the uv plane. The uv plane\ncan be placed at inﬁnity, which corresponds to all the virtual cameras looking in the same\ndirection.\nIn practice, if the planes are of ﬁnite extent, the ﬁnite light slab L(s, t, u, v) can be used to\ngenerate any synthetic view that a camera would see through a (ﬁnite) viewport in the st plane\nwith a view frustum that wholly intersects the far uv plane. To enable the camera to move\nall the way around an object, the 3D space surrounding the object can be split into multiple\ndomains, each with its own light slab parameterization. Conversely, if the camera is moving\ninside a bounded volume of free space looking outward, multiple cube faces surrounding the\ncamera can be used as (s, t) planes.\n5 Levoy and Hanrahan (1996) borrowed the term light ﬁeld from a paper by Gershun (1939). Another name for\nthis representation is the photic ﬁeld (Moon and Spencer 1981).",
  "652": "630\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 13.8\nDepth compensation in the Lumigraph (Gortler, Grzeszczuk, Szeliski et al.\n1996) c⃝1996 ACM. To resample the (s, u) dashed light ray, the u parameter corresponding\nto each discrete si camera location is modiﬁed according to the out-of-plane depth z to yield\nnew coordinates u and u′; in (u, s) ray space, the original sample (△) is resampled from\nthe (si, u′) and (si+1, u′′) samples, which are themselves linear blends of their adjacent (◦)\nsamples.\nThinking about 4D spaces is difﬁcult, so let us drop our visualization by one dimension.\nIf we ﬁx the row value t and constrain our camera to move along the s axis while looking\nat the uv plane, we can stack all of the stabilized images the camera sees to get the (u, v, s)\nepipolar volume, which we discussed in Section 11.6. A “horizontal” cross-section through\nthis volume is the well-known epipolar plane image (Bolles, Baker, and Marimont 1987),\nwhich is the us slice shown in Figure 13.7b.\nAs you can see in this slice, each color pixel moves along a linear track whose slope\nis related to its depth (parallax) from the uv plane. (Pixels exactly on the uv plane appear\n“vertical”, i.e., they do not move as the camera moves along s.) Furthermore, pixel tracks\nocclude one another as their corresponding 3D surface elements occlude. Translucent pixels,\nhowever, composite over background pixels (Section 3.1.3, (3.8)) rather than occluding them.\nThus, we can think of adjacent pixels sharing a similar planar geometry as EPI strips or EPI\ntubes (Criminisi, Kang, Swaminathan et al. 2005).\nThe equations mapping from pixels (x, y) in a virtual camera and the corresponding\n(s, t, u, v) coordinates are relatively straightforward to derive and are sketched out in Ex-\nercise 13.7. It is also possible to show that the set of pixels corresponding to a regular ortho-\ngraphic or perspective camera, i.e., one that has a linear projective relationship between 3D\npoints and (x, y) pixels (2.63), lie along a two-dimensional hyperplane in the (s, t, u, v) light\nﬁeld (Exercise 13.7).",
  "653": "13.3 Light ﬁelds and Lumigraphs\n631\nWhile a light ﬁeld can be used to render a complex 3D scene from novel viewpoints, a\nmuch better rendering (with less ghosting) can be obtained if something is known about its\n3D geometry. The Lumigraph system of Gortler, Grzeszczuk, Szeliski et al. (1996) extends\nthe basic light ﬁeld rendering approach by taking into account the 3D location of surface\npoints corresponding to each 3D ray.\nConsider the ray (s, u) corresponding to the dashed line in Figure 13.8, which intersects\nthe object’s surface at a distance z from the uv plane. When we look up the pixel’s color in\ncamera si (assuming that the light ﬁeld is discretely sampled on a regular 4D (s, t, u, v) grid),\nthe actual pixel coordinate is u′, instead of the original u value speciﬁed by the (s, u) ray.\nSimilarly, for camera si+1 (where si ≤s ≤si+1), pixel address u′′ is used. Thus, instead of\nusing quadri-linear interpolation of the nearest sampled (s, t, u, v) values around a given ray\nto determine its color, the (u, v) values are modiﬁed for each discrete (si, ti) camera.\nFigure 13.8 also shows the same reasoning in ray space. Here, the original continuous-\nvalued (s, u) ray is represented by a triangle and the nearby sampled discrete values are\nshown as circles. Instead of just blending the four nearest samples, as would be indicated\nby the vertical and horizontal dashed lines, the modiﬁed (si, u′) and (si+1, u′′) values are\nsampled instead and their values are then blended.\nThe resulting rendering system produces images of much better quality than a proxy-free\nlight ﬁeld and is the method of choice whenever 3D geometry can be inferred. In subsequent\nwork, Isaksen, McMillan, and Gortler (2000) show how a planar proxy for the scene, which\nis a simpler 3D model, can be used to simplify the resampling equations. They also describe\nhow to create synthetic aperture photos, which mimic what might be seen by a wide-aperture\nlens, by blending more nearby samples (Levoy and Hanrahan 1996). A similar approach\ncan be used to re-focus images taken with a plenoptic (microlens array) camera (Ng, Levoy,\nBr´eedif et al. 2005; Ng 2005) or a light ﬁeld microscope (Levoy, Ng, Adams et al. 2006). It\ncan also be used to see through obstacles, using extremely large synthetic apertures focused\non a background that can blur out foreground objects and make them appear translucent\n(Wilburn, Joshi, Vaish et al. 2005; Vaish, Szeliski, Zitnick et al. 2006).\nNow that we understand how to render new images from a light ﬁeld, how do we go about\ncapturing such data sets? One answer is to move a calibrated camera with a motion control rig\nor gantry.6 Another approach is to take handheld photographs and to determine the pose and\nintrinsic calibration of each image using either a calibrated stage or structure from motion. In\nthis case, the images need to be rebinned into a regular 4D (s, t, u, v) space before they can\nbe used for rendering (Gortler, Grzeszczuk, Szeliski et al. 1996). Alternatively, the original\nimages can be used directly using a process called the unstructured Lumigraph, which we\n6 See http://lightﬁeld.stanford.edu/acq.html for a description of some of the gantries and camera arrays built at\nthe Stanford Computer Graphics Laboratory. This Web site also provides a number of light ﬁeld data sets that are a\ngreat source of research and project material.",
  "654": "632\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ndescribe below.\nBecause of the large number of images involved, light ﬁelds and Lumigraphs can be quite\nvoluminous to store and transmit. Fortunately, as you can tell from Figure 13.7b, there is\na tremendous amount of redundancy (coherence) in a light ﬁeld, which can be made even\nmore explicit by ﬁrst computing a 3D model, as in the Lumigraph. A number of techniques\nhave been developed to compress and progressively transmit such representations (Gortler,\nGrzeszczuk, Szeliski et al. 1996; Levoy and Hanrahan 1996; Rademacher and Bishop 1998;\nMagnor and Girod 2000; Wood, Azuma, Aldinger et al. 2000; Shum, Kang, and Chan 2003;\nMagnor, Ramanathan, and Girod 2003; Shum, Chan, and Kang 2007).\n13.3.1 Unstructured Lumigraph\nWhen the images in a Lumigraph are acquired in an unstructured (irregular) manner, it can be\ncounterproductive to resample the resulting light rays into a regularly binned (s, t, u, v) data\nstructure. This is both because resampling always introduces a certain amount of aliasing and\nbecause the resulting gridded light ﬁeld can be populated very sparsely or irregularly.\nThe alternative is to render directly from the acquired images, by ﬁnding for each light ray\nin a virtual camera the closest pixels in the original images. The unstructured Lumigraph ren-\ndering (ULR) system of Buehler, Bosse, McMillan et al. (2001) describes how to select such\npixels by combining a number of ﬁdelity criteria, including epipole consistency (distance of\nrays to a source camera’s center), angular deviation (similar incidence direction on the sur-\nface), resolution (similar sampling density along the surface), continuity (to nearby pixels),\nand consistency (along the ray). These criteria can all be combined to determine a weighting\nfunction between each virtual camera’s pixel and a number of candidate input cameras from\nwhich it can draw colors. To make the algorithm more efﬁcient, the computations are per-\nformed by discretizing the virtual camera’s image plane using a regular grid overlaid with the\npolyhedral object mesh model and the input camera centers of projection and interpolating\nthe weighting functions between vertices.\nThe unstructured Lumigraph generalizes previous work in both image-based rendering\nand light ﬁeld rendering. When the input cameras are gridded, the ULR behaves the same way\nas regular Lumigraph rendering. When fewer cameras are available but the geometry is accu-\nrate, the algorithm behaves similarly to view-dependent texture mapping (Section 13.1.1).\n13.3.2 Surface light ﬁelds\nOf course, using a two-plane parameterization for a light ﬁeld is not the only possible choice.\n(It is the one usually presented ﬁrst since the projection equations and visualizations are the\neasiest to draw and understand.) As we mentioned on the topic of light ﬁeld compression,",
  "655": "13.3 Light ﬁelds and Lumigraphs\n633\n(a)\n(b)\nFigure 13.9\nSurface light ﬁelds (Wood, Azuma, Aldinger et al. 2000) c⃝2000 ACM: (a)\nexample of a highly specular object with strong inter-reﬂections; (b) the surface light ﬁeld\nstores the light emanating from each surface point in all visible directions as a “Lumisphere”.\nif we know the 3D shape of the object or scene whose light ﬁeld is being modeled, we can\neffectively compress the ﬁeld because nearby rays emanating from nearby surface elements\nhave similar color values.\nIn fact, if the object is totally diffuse, ignoring occlusions, which can be handled using\n3D graphics algorithms or z-buffering, all rays passing through a given surface point will\nhave the same color value. Hence, the light ﬁeld “collapses” to the usual 2D texture-map\ndeﬁned over an object’s surface. Conversely, if the surface is totally specular (e.g., mirrored),\neach surface point reﬂects a miniature copy of the environment surrounding that point. In the\nabsence of inter-reﬂections (e.g., a convex object in a large open space), each surface point\nsimply reﬂects the far-ﬁeld environment map (Section 2.2.1), which again is two-dimensional.\nTherefore, is seems that re-parameterizing the 4D light ﬁeld to lie on the object’s surface can\nbe extremely beneﬁcial.\nThese observations underlie the surface light ﬁeld representation introduced by Wood,\nAzuma, Aldinger et al. (2000). In their system, an accurate 3D model is built of the object\nbeing represented. Then the Lumisphere of all rays emanating from each surface point is\nestimated or captured (Figure 13.9). Nearby Lumispheres will be highly correlated and hence\namenable to both compression and manipulation.\nTo estimate the diffuse component of each Lumisphere, a median ﬁltering over all visible\nexiting directions is ﬁrst performed for each channel. Once this has been subtracted from the\nLumisphere, the remaining values, which should consist mostly of the specular components,\nare reﬂected around the local surface normal (2.89), which turns each Lumisphere into a copy\nof the local environment around that point. Nearby Lumispheres can then be compressed\nusing predictive coding, vector quantization, or principal component analysis.",
  "656": "634\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe decomposition into a diffuse and specular component can also be used to perform\nediting or manipulation operations, such as re-painting the surface, changing the specular\ncomponent of the reﬂection (e.g., by blurring or sharpening the specular Lumispheres), or\neven geometrically deforming the object while preserving detailed surface appearance.\n13.3.3 Application: Concentric mosaics\nA useful and simple version of light ﬁeld rendering is a panoramic image with parallax, i.e., a\nvideo or series of photographs taken from a camera swinging in front of some rotation point.\nSuch panoramas can be captured by placing a camera on a boom on a tripod, or even more\nsimply, by holding a camera at arm’s length while rotating your body around a ﬁxed axis.\nThe resulting set of images can be thought of as a concentric mosaic (Shum and He 1999;\nShum, Wang, Chai et al. 2002) or a layered depth panorama (Zheng, Kang, Cohen et al.\n2007). The term “concentric mosaic” comes from a particular structure that can be used to\nre-bin all of the sampled rays, essentially associating each column of pixels with the “radius”\nof the concentric circle to which it is tangent (Shum and He 1999; Peleg, Ben-Ezra, and Pritch\n2001).\nRendering from such data structures is fast and straightforward. If we assume that the\nscene is far enough away, for any virtual camera location, we can associate each column of\npixels in the virtual camera with the nearest column of pixels in the input image set. (For\na regularly captured set of images, this computation can be performed analytically.) If we\nhave some rough knowledge of the depth of such pixels, columns can be stretched vertically\nto compensate for the change in depth between the two cameras. If we have an even more\ndetailed depth map (Peleg, Ben-Ezra, and Pritch 2001; Li, Shum, Tang et al. 2004; Zheng,\nKang, Cohen et al. 2007), we can perform pixel-by-pixel depth corrections.\nWhile the virtual camera’s motion is constrained to lie in the plane of the original cameras\nand within the radius of the original capture ring, the resulting experience can exhibit complex\nrendering phenomena, such as reﬂections and translucencies, which cannot be captured using\na texture-mapped 3D model of the world. Exercise 13.10 has you construct a concentric\nmosaic rendering system from a series of hand-held photos or video.\n13.4 Environment mattes\nSo far in this chapter, we have dealt with view interpolation and light ﬁelds, which are tech-\nniques for modeling and rendering complex static scenes seen from different viewpoints.\nWhat if instead of moving around a virtual camera, we take a complex, refractive object,\nsuch as the water goblet shown in Figure 13.10, and place it in front of a new background?",
  "657": "13.4 Environment mattes\n635\n(a)\n(b)\n(c)\n(d)\nFigure 13.10 Environment mattes: (a–b) a refractive object can be placed in front of a series\nof backgrounds and their light patterns will be correctly refracted (Zongker, Werner, Cur-\nless et al. 1999) (c) multiple refractions can be handled using a mixture of Gaussians model\nand (d) real-time mattes can be pulled using a single graded colored background (Chuang,\nZongker, Hindorff et al. 2000) c⃝2000 ACM.\nInstead of modeling the 4D space of rays emanating from a scene, we now need to model\nhow each pixel in our view of this object refracts incident light coming from its environment.\nWhat is the intrinsic dimensionality of such a representation and how do we go about\ncapturing it? Let us assume that if we trace a light ray from the camera at pixel (x, y) toward\nthe object, it is reﬂected or refracted back out toward its environment at an angle (φ, θ). If\nwe assume that other objects and illuminants are sufﬁciently distant (the same assumption we\nmade for surface light ﬁelds in Section 13.3.2), this 4D mapping (x, y) →(φ, θ) captures all\nthe information between a refractive object and its environment. Zongker, Werner, Curless et\nal. (1999) call such a representation an environment matte, since it generalizes the process of\nobject matting (Section 10.4) to not only cut and paste an object from one image into another\nbut also take into account the subtle refractive or reﬂective interplay between the object and\nits environment.\nRecall from Equations (3.8) and (10.30) that a foreground object can be represented by\nits premultiplied colors and opacities (αF, α). Such a matte can then be composited onto a\nnew background B using\nCi = αiFi + (1 −αi)Bi,\n(13.1)\nwhere i is the pixel under consideration. In environment matting, we augment this equation\nwith a reﬂective or refractive term to model indirect light paths between the environment\nand the camera. In the original work of Zongker, Werner, Curless et al. (1999), this indirect\ncomponent Ii is modeled as\nIi = Ri\nZ\nAi(x)B(x)dx,\n(13.2)\nwhere Ai is the rectangular area of support for that pixel, Ri is the colored reﬂectance or",
  "658": "636\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntransmittance (for colored glossy surfaces or glass), and B(x) is the background (environ-\nment) image, which is integrated over the area Ai(x). In follow-on work, Chuang, Zongker,\nHindorff et al. (2000) use a superposition of oriented Gaussians,\nIi =\nX\nj\nRij\nZ\nGij(x)B(x)dx,\n(13.3)\nwhere each 2D Gaussian\nGij(x) = G2D(x; cij, σij, θij)\n(13.4)\nis modeled by its center cij, unrotated widths σij = (σx\nij, σy\nij), and orientation θij.\nGiven a representation for an environment matte, how can we go about estimating it for a\nparticular object? The trick is to place the object in front of a monitor (or surrounded by a set\nof monitors), where we can change the illumination patterns B(x) and observe the value of\neach composite pixel Ci.7\nAs with traditional two-screen matting (Section 10.4.1), we can use a variety of solid\ncolored backgrounds to estimate each pixel’s foreground color αiFi and partial coverage\n(opacity) αi. To estimate the area of support Ai in (13.2), Zongker, Werner, Curless et al.\n(1999) use a series of periodic horizontal and vertical solid stripes at different frequencies and\nphases, which is reminiscent of the structured light patterns used in active rangeﬁnding (Sec-\ntion 12.2). For the more sophisticated mixture of Gaussian model (13.3), Chuang, Zongker,\nHindorff et al. (2000) sweep a series of narrow Gaussian stripes at four different orientations\n(horizontal, vertical, and two diagonals), which enables them to estimate multiple oriented\nGaussian responses at each pixel.\nOnce an environment matte has been “pulled”, it is then a simple matter to replace the\nbackground with a new image B(x) to obtain a novel composite of the object placed in a\ndifferent environment (Figure 13.10a–c). The use of multiple backgrounds during the matting\nprocess, however, precludes the use of this technique with dynamic scenes, e.g., water pouring\ninto a glass (Figure 13.10d). In this case, a single graded color background can be used to\nestimate a single 2D monochromatic displacement for each pixel (Chuang, Zongker, Hindorff\net al. 2000).\n13.4.1 Higher-dimensional light ﬁelds\nAs you can tell from the preceding discussion, an environment matte in principle maps every\npixel (x, y) into a 4D distribution over light rays and is, hence, a six-dimensional representa-\ntion. (In practice, each 2D pixel’s response is parameterized using a dozen or so parameters,\n7 If we relax the assumption that the environment is distant, the monitor can be placed at several depths to estimate\na depth-dependent mapping function (Zongker, Werner, Curless et al. 1999).",
  "659": "13.4 Environment mattes\n637\nFigure 13.11\nThe geometry-image continuum in image-based rendering (Kang, Szeliski,\nand Anandan 2000) c⃝2000 IEEE. Representations at the left of the spectrum use more\ndetailed geometry and simpler image representations, while representations and algorithms\non the right use more images and less geometry.\ne.g., {F, α, B, R, A}, instead of a full mapping.) What if we want to model an object’s re-\nfractive properties from every potential point of view? In this case, we need a mapping from\nevery incoming 4D light ray to every potential exiting 4D light ray, which is an 8D represen-\ntation. If we use the same trick as with surface light ﬁelds, we can parameterize each surface\npoint by its 4D BRDF to reduce this mapping back down to 6D but this loses the ability to\nhandle multiple refractive paths.\nIf we want to handle dynamic light ﬁelds, we need to add another temporal dimension.\n(Wenger, Gardner, Tchou et al. (2005) gives a nice example of a dynamic appearance and\nillumination acquisition system.) Similarly, if we want a continuous distribution over wave-\nlengths, this becomes another dimension.\nThese examples illustrate how modeling the full complexity of a visual scene through\nsampling can be extremely expensive. Fortunately, constructing specialized models, which\nexploit knowledge about the physics of light transport along with the natural coherence of\nreal-world objects, can make these problems more tractable.\n13.4.2 The modeling to rendering continuum\nThe image-based rendering representations and algorithms we have studied in this chapter\nspan a continuum ranging from classic 3D texture-mapped models all the way to pure sampled\nray-based representations such as light ﬁelds (Figure 13.11). Representations such as view-\ndependent texture maps and Lumigraphs still use a single global geometric model, but select\nthe colors to map onto these surfaces from nearby images. View-dependent geometry, e.g.,",
  "660": "638\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmultiple depth maps, sidestep the need for coherent 3D geometry, and can sometimes better\nmodel local non-rigid effects such as specular motion (Swaminathan, Kang, Szeliski et al.\n2002; Criminisi, Kang, Swaminathan et al. 2005). Sprites with depth and layered depth\nimages use image-based representations of both color and geometry and can be efﬁciently\nrendered using warping operations rather than 3D geometric rasterization.\nThe best choice of representation and rendering algorithm depends on both the quantity\nand quality of the input imagery as well as the intended application. When nearby views are\nbeing rendered, image-based representations capture more of the visual ﬁdelity of the real\nworld because they directly sample its appearance. On the other hand, if only a few input\nimages are available or the image-based models need to be manipulated, e.g., to change their\nshape or appearance, more abstract 3D representations such as geometric and local reﬂection\nmodels are a better ﬁt. As we continue to capture and manipulate increasingly larger quan-\ntities of visual data, research into these aspects of image-based modeling and rendering will\ncontinue to evolve.\n13.5 Video-based rendering\nSince multiple images can be used to render new images or interactive experiences, can some-\nthing similar be done with video? In fact, a fair amount of work has been done in the area\nof video-based rendering and video-based animation, two terms ﬁrst introduced by Sch¨odl,\nSzeliski, Salesin et al. (2000) to denote the process of generating new video sequences from\ncaptured video footage. An early example of such work is Video Rewrite (Bregler, Covell,\nand Slaney 1997), in which archival video footage is “re-animated” by having actors say new\nutterances (Figure 13.12). More recently, the term video-based rendering has been used by\nsome researchers to denote the creation of virtual camera moves from a set of synchronized\nvideo cameras placed in a studio (Magnor 2005). (The terms free-viewpoint video and 3D\nvideo are also sometimes used, see Section 13.5.4.)\nIn this section, we present a number of video-based rendering systems and applications.\nWe start with video-based animation (Section 13.5.1), in which video footage is re-arranged\nor modiﬁed, e.g., in the capture and re-rendering of facial expressions. A special case of this\nare video textures (Section 13.5.2), in which source video is automatically cut into segments\nand re-looped to create inﬁnitely long video animations. It is also possible to create such\nanimations from still pictures or paintings, by segmenting the image into separately moving\nregions and animating them using stochastic motion ﬁelds (Section 13.5.3).\nNext, we turn our attention to 3D video (Section 13.5.4), in which multiple synchronized\nvideo cameras are used to ﬁlm a scene from different directions. The source video frames can\nthen be re-combined using image-based rendering techniques, such as view interpolation, to",
  "661": "13.5 Video-based rendering\n639\nFigure 13.12\nVideo Rewrite (Bregler, Covell, and Slaney 1997) c⃝1997 ACM: the video\nframes are composed from bits and pieces of old video footage matched to a new audio track.\ncreate virtual camera paths between the source cameras as part of a real-time viewing expe-\nrience. Finally, we discuss capturing environments by driving or walking through them with\npanoramic video cameras in order to create interactive video-based walkthrough experiences\n(Section 13.5.5).\n13.5.1 Video-based animation\nAs we mentioned above, an early example of video-based animation is Video Rewrite, in\nwhich frames from original video footage are rearranged in order to match them to novel\nspoken utterances, e.g., for movie dubbing (Figure 13.12). This is similar in spirit to the way\nthat concatenative speech synthesis systems work (Taylor 2009).\nIn their system, Bregler, Covell, and Slaney (1997) ﬁrst use speech recognition to extract\nphonemes from both the source video material and the novel audio stream. Phonemes are\ngrouped into triphones (triplets of phonemes), since these better model the coarticulation\neffect present when people speak. Matching triphones are then found in the source footage\nand audio track. The mouth images corresponding to the selected video frames are then\ncut and pasted into the desired video footage being re-animated or dubbed, with appropriate\ngeometric transformations to account for head motion. During the analysis phase, features\ncorresponding to the lips, chin, and head are tracked using computer vision techniques. Dur-\ning synthesis, image morphing techniques are used to blend and stitch adjacent mouth shapes\ninto a more coherent whole. In more recent work, Ezzat, Geiger, and Poggio (2002) describe\nhow to use a multidimensional morphable model (Section 12.6.2) combined with regularized\ntrajectory synthesis to improve these results.\nA more sophisticated version of this system, called face transfer, uses a novel source\nvideo, instead of just an audio track, to drive the animation of a previously captured video, i.e.,\nto re-render a video of a talking head with the appropriate visual speech, expression, and head\npose elements (Vlasic, Brand, Pﬁster et al. 2005). This work is one of many performance-\ndriven animation systems (Section 4.1.5), which are often used to animate 3D facial models",
  "662": "640\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Figures 12.18–12.19). While traditional performance-driven animation systems use marker-\nbased motion capture (Williams 1990; Litwinowicz and Williams 1994; Ma, Jones, Chiang\net al. 2008), video footage can now often be used directly to control the animation (Buck,\nFinkelstein, Jacobs et al. 2000; Pighin, Szeliski, and Salesin 2002; Zhang, Snavely, Curless\net al. 2004; Vlasic, Brand, Pﬁster et al. 2005; Roble and Zafar 2009).\nIn addition to its most common application to facial animation, video-based animation\ncan also be applied to whole body motion (Section 12.6.4), e.g., by matching the ﬂow ﬁelds\nbetween two different source videos and using one to drive the other (Efros, Berg, Mori et al.\n2003). Another approach to video-based rendering is to use ﬂow or 3D modeling to unwrap\nsurface textures into stabilized images, which can then be manipulated and re-rendered onto\nthe original video (Pighin, Szeliski, and Salesin 2002; Rav-Acha, Kohli, Fitzgibbon et al.\n2008).\n13.5.2 Video textures\nVideo-based animation is a powerful means of creating photo-realistic videos by re-purposing\nexisting video footage to match some other desired activity or script. What if instead of\nconstructing a special animation or narrative, we simply want the video to continue playing\nin a plausible manner? For example, many Web sites use images or videos to highlight their\ndestinations, e.g., to portray attractive beaches with surf and palm trees waving in the wind.\nInstead of using a static image or a video clip that has a discontinuity when it loops, can we\ntransform the video clip into an inﬁnite-length animation that plays forever?\nThis idea is the basis of video textures, in which a short video clip can be arbitrarily\nextended by re-arranging video frames while preserving visual continuity (Sch¨odl, Szeliski,\nSalesin et al. 2000). The basic problem in creating video textures is how to perform this\nre-arrangement without introducing visual artifacts. Can you think of how you might do this?\nThe simplest approach is to match frames by visual similarity (e.g., L2 distance) and to\njump between frames that appear similar. Unfortunately, if the motions in the two frames\nare different, a dramatic visual artifact will occur (the video will appear to “stutter”). For\nexample, if we fail to match the motions of the clock pendulum in Figure 13.13a, it can\nsuddenly change direction in mid-swing.\nHow can we extend our basic frame matching to also match motion? In principle, we\ncould compute optic ﬂow at each frame and match this. However, ﬂow estimates are often\nunreliable (especially in textureless regions) and it is not clear how to weight the visual and\nmotion similarities relative to each other. As an alternative, Sch¨odl, Szeliski, Salesin et al.\n(2000) suggest matching triplets or larger neighborhoods of adjacent video frames, much\nin the same way as Video Rewrite matches triphones. Once we have constructed an n ×\nn similarity matrix between all video frames (where n is the number of frames), a simple",
  "663": "13.5 Video-based rendering\n641\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 13.13\nVideo textures (Sch¨odl, Szeliski, Salesin et al. 2000) c⃝2000 ACM: (a) a\nclock pendulum, with correctly matched direction of motion; (b) a candle ﬂame, showing\ntemporal transition arcs; (c) the ﬂag is generated using morphing at jumps; (d) a bonﬁre\nuses longer cross-dissolves; (e) a waterfall cross-dissolves several sequences at once; (f) a\nsmiling animated face; (g) two swinging children are animated separately; (h) the balloons\nare automatically segmented into separate moving regions; (i) a synthetic ﬁsh tank consisting\nof bubbles, plants, and ﬁsh. Videos corresponding to these images can be found at http:\n//www.cc.gatech.edu/gvu/perception/projects/videotexture/.",
  "664": "642\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nﬁnite impulse response (FIR) ﬁltering of each match sequence can be used to emphasize\nsubsequences that match well.\nThe results of this match computation gives us a jump table or, equivalently, a transition\nprobability between any two frames in the original video. This is shown schematically as\nred arcs in Figure 13.13b, where the red bar indicates which video frame is currently be-\ning displayed, and arcs light up as a forward or backward transition is taken. We can view\nthese transition probabilities as encoding the hidden Markov model (HMM) that underlies a\nstochastic video generation process.\nSometimes, it is not possible to ﬁnd exactly matching subsequences in the original video.\nIn this case, morphing, i.e., warping and blending frames during transitions (Section 3.6.3)\ncan be used to hide the visual differences (Figure 13.13c). If the motion is chaotic enough,\nas in a bonﬁre or a waterfall (Figures 13.13d–e), simple blending (extended cross-dissolves)\nmay be sufﬁcient. Improved transitions can also be obtained by performing 3D graph cuts on\nthe spatio-temporal volume around a transition (Kwatra, Sch¨odl, Essa et al. 2003).\nVideo textures need not be restricted to chaotic random phenomena such as ﬁre, wind,\nand water. Pleasing video textures can be created of people, e.g., a smiling face (as in Fig-\nure 13.13f) or someone running on a treadmill (Sch¨odl, Szeliski, Salesin et al. 2000). When\nmultiple people or objects are moving independently, as in Figures 13.13g–h, we must ﬁrst\nsegment the video into independently moving regions and animate each region separately.\nIt is also possible to create large panoramic video textures from a slowly panning camera\n(Agarwala, Zheng, Pal et al. 2005).\nInstead of just playing back the original frames in a stochastic (random) manner, video\ntextures can also be used to create scripted or interactive animations. If we extract individual\nelements, such as ﬁsh in a ﬁshtank (Figure 13.13i) into separate video sprites, we can animate\nthem along pre-speciﬁed paths (by matching the path direction with the original sprite motion)\nto make our video elements move in a desired fashion (Sch¨odl and Essa 2002). In fact, work\non video textures inspired research on systems that re-synthesize new motion sequences from\nmotion capture data, which some people refer to as “mocap soup” (Arikan and Forsyth 2002;\nKovar, Gleicher, and Pighin 2002; Lee, Chai, Reitsma et al. 2002; Li, Wang, and Shum 2002;\nPullen and Bregler 2002).\nWhile video textures primarily analyze the video as a sequence of frames (or regions) that\ncan be re-arranged in time, temporal textures (Szummer and Picard 1996; Bar-Joseph, El-\nYaniv, Lischinski et al. 2001) and dynamic textures (Doretto, Chiuso, Wu et al. 2003; Yuan,\nWen, Liu et al. 2004; Doretto and Soatto 2006) treat the video as a 3D spatio-temporal volume\nwith textural properties, which can be described using auto-regressive temporal models.",
  "665": "13.5 Video-based rendering\n643\n    displacement map\n...\n(a)\n(b)\n(c)\n(d)\n(e)\n...\n...\n=\n=\n=\n=\n=\n \nL1\nL2\nLl-2\nLl-1\nLl\nL  (t)\n1\nL  (t)\n2\nL    (t)\nl-2\nL    (t)\nl-1\nL (t)\nl\n    displacement map\n    displacement map\n    displacement map\n    displacement map\nd    (t)\n l-1\nd (t)\n l\nd    (t)\n l-2\nd  (t)\n 2\nd  (t)\n 1\ntype=“boat”\ntype=“still”\ntype=“tree”\ntype=“cloud”\ntype=“water”\nFigure 13.14 Animating still pictures (Chuang, Goldman, Zheng et al. 2005) c⃝2005 ACM.\n(a) The input still image is manually segmented into (b) several layers. (c) Each layer is\nthen animated with a different stochastic motion texture (d) The animated layers are then\ncomposited to produce (e) the ﬁnal animation\n13.5.3 Application: Animating pictures\nWhile video textures can turn a short video clip into an inﬁnitely long video, can the same\nthing be done with a single still image? The answer is yes, if you are willing to ﬁrst segment\nthe image into different layers and then animate each layer separately.\nChuang, Goldman, Zheng et al. (2005) describe how an image can be decomposed into\nseparate layers using interactive matting techniques. Each layer is then animated using a\nclass-speciﬁc synthetic motion. As shown in Figure 13.14, boats rock back and forth, trees\nsway in the wind, clouds move horizontally, and water ripples, using a shaped noise displace-\nment map. All of these effects can be tied to some global control parameters, such as the\nvelocity and direction of a virtual wind. After being individually animated, the layers can be\ncomposited to create a ﬁnal dynamic rendering.\n13.5.4 3D Video\nIn recent years, the popularity of 3D movies has grown dramatically, with recent releases\nranging from Hannah Montana, through U2’s 3D concert movie, to James Cameron’s Avatar.\nCurrently, such releases are ﬁlmed using stereoscopic camera rigs and displayed in theaters\n(or at home) to viewers wearing polarized glasses.8 In the future, however, home audiences\nmay wish to view such movies with multi-zone auto-stereoscopic displays, where each person\ngets his or her own customized stereo stream and can move around a scene to see it from\n8 http://www.3d-summit.com/.",
  "666": "644\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nRender\nbackground\nBi\nRender\nforeground\nFi\nOver\ncomposite\nCamera i\nRender\nbackground\nBi+1\nRender\nforeground\nFi+1\nOver\ncomposite\nBlend\nCamera i+1\n(a)\n(b)\ndi\nMi\nBi\nstrip\nwidth\nstrip\nwidth\ndepth\ndiscontinuity\nmatte\n(c)\n(d)\n(e)\n(f)\nFigure 13.15\nVideo view interpolation (Zitnick, Kang, Uyttendaele et al. 2004) c⃝2004\nACM: (a) the capture hardware consists of eight synchronized cameras; (b) the background\nand foreground images from each camera are rendered and composited before blending; (c)\nthe two-layer representation, before and after boundary matting; (d) background color esti-\nmates; (e) background depth estimates; (f) foreground color estimates.\ndifferent perspectives.9\nThe stereo matching techniques developed in the computer vision community along with\nimage-based rendering (view interpolation) techniques from graphics are both essential com-\nponents in such scenarios, which are sometimes called free-viewpoint video (Carranza, Theobalt,\nMagnor et al. 2003) or virtual viewpoint video (Zitnick, Kang, Uyttendaele et al. 2004). In\naddition to solving a series of per-frame reconstruction and view interpolation problems, the\ndepth maps or proxies produced by the analysis phase must be temporally consistent in order\nto avoid ﬂickering artifacts.\nShum, Chan, and Kang (2007) and Magnor (2005) present nice overviews of various\nvideo view interpolation techniques and systems. These include the Virtualized Reality sys-\ntem of Kanade, Rander, and Narayanan (1997) and Vedula, Baker, and Kanade (2005), Im-\nmersive Video (Moezzi, Katkere, Kuramura et al. 1996), Image-Based Visual Hulls (Matusik,\nBuehler, Raskar et al. 2000; Matusik, Buehler, and McMillan 2001), and Free-Viewpoint\nVideo (Carranza, Theobalt, Magnor et al. 2003), which all use global 3D geometric models\n(surface-based (Section 12.3) or volumetric (Section 12.5)) as their proxies for rendering.\nThe work of Vedula, Baker, and Kanade (2005) also computes scene ﬂow, i.e., the 3D motion\nbetween corresponding surface elements, which can then be used to perform spatio-temporal\ninterpolation of the multi-view video stream.\nThe Virtual Viewpoint Video system of Zitnick, Kang, Uyttendaele et al. (2004), on the\n9 http://www.siggraph.org/s2008/attendees/caf/3d/.",
  "667": "13.5 Video-based rendering\n645\nother hand, associates a two-layer depth map with each input image, which allows them to\naccurately model occlusion effects such as the mixed pixels that occur at object boundaries.\nTheir system, which consists of eight synchronized video cameras connected to a disk array\n(Figure 13.15a), ﬁrst uses segmentation-based stereo to extract a depth map for each input\nimage (Figure 13.15e). Near object boundaries (depth discontinuities), the background layer\nis extended along a strip behind the foreground object (Figure 13.15c) and its color is es-\ntimated from the neighboring images where it is not occluded (Figure 13.15d). Automated\nmatting techniques (Section 10.4) are then used to estimate the fractional opacity and color\nof boundary pixels in the foreground layer (Figure 13.15f).\nAt render time, given a new virtual camera that lies between two of the original cameras,\nthe layers in the neighboring cameras are rendered as texture-mapped triangles and the fore-\nground layer (which may have fractional opacities) is then composited over the background\nlayer (Figure 13.15b). The resulting two images are merged and blended by comparing their\nrespective z-buffer values. (Whenever the two z-values are sufﬁciently close, a linear blend of\nthe two colors is computed.) The interactive rendering system runs in real time using regular\ngraphics hardware. It can therefore be used to change the observer’s viewpoint while playing\nthe video or to freeze the scene and explore it in 3D. More recently, Rogmans, Lu, Bekaert\net al. (2009) have developed GPU implementations of both real-time stereo matching and\nreal-time rendering algorithms, which enable them to explore algorithmic alternatives in a\nreal-time setting.\nAt present, the depth maps computed from the eight stereo cameras using off-line stereo\nmatching have produced the highest quality depth maps associated with live video.10 They\nare therefore often used in studies of 3D video compression, which is an active area of re-\nsearch (Smolic and Kauff 2005; Gotchev and Rosenhahn 2009). Active video-rate depth\nsensing cameras, such as the 3DV Zcam (Iddan and Yahav 2001), which we discussed in\nSection 12.2.1, are another potential source of such data.\nWhen large numbers of closely spaced cameras are available, as in the Stanford Light\nField Camera (Wilburn, Joshi, Vaish et al. 2005), it may not always be necessary to compute\nexplicit depth maps to create video-based rendering effects, although the results are usually\nof higher quality if you do (Vaish, Szeliski, Zitnick et al. 2006).\n13.5.5 Application: Video-based walkthroughs\nVideo camera arrays enable the simultaneous capture of 3D dynamic scenes from multiple\nviewpoints, which can then enable the viewer to explore the scene from viewpoints near the\noriginal capture locations. What if instead we wish to capture an extended area, such as a\nhome, a movie set, or even an entire city?\n10 http://research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/.",
  "668": "646\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn this case, it makes more sense to move the camera through the environment and play\nback the video as an interactive video-based walkthrough. In order to allow the viewer to\nlook around in all directions, it is preferable to use a panoramic video camera (Uyttendaele,\nCriminisi, Kang et al. 2004).11\nOne way to structure the acquisition process is to capture these images in a 2D horizontal\nplane, e.g., over a grid superimposed inside a room. The resulting sea of images (Aliaga,\nFunkhouser, Yanovsky et al. 2003) can be used to enable continuous motion between the\ncaptured locations.12 However, extending this idea to larger settings, e.g., beyond a single\nroom, can become tedious and data-intensive.\nInstead, a natural way to explore a space is often to just walk through it along some pre-\nspeciﬁed paths, just as museums or home tours guide users along a particular path, say down\nthe middle of each room.13 Similarly, city-level exploration can be achieved by driving down\nthe middle of each street and allowing the user to branch at each intersection. This idea dates\nback to the Aspen MovieMap project (Lippman 1980), which recorded analog video taken\nfrom moving cars onto videodiscs for later interactive playback.\nRecent improvements in video technology now enable the capture of panoramic (spheri-\ncal) video using a small co-located array of cameras, such as the Point Grey Ladybug cam-\nera14 (Figure 13.16b) developed by Uyttendaele, Criminisi, Kang et al. (2004) for their inter-\nactive video-based walkthrough project. In their system, the synchronized video streams from\nthe six cameras (Figure 13.16a) are stitched together into 360◦panoramas using a variety of\ntechniques developed speciﬁcally for this project.\nBecause the cameras do not share the same center of projection, parallax between the\ncameras can lead to ghosting in the overlapping ﬁelds of view (Figure 13.16c). To remove\nthis, a multi-perspective plane sweep stereo algorithm is used to estimate per-pixel depths at\neach column in the overlap area. To calibrate the cameras relative to each other, the camera\nis spun in place and a constrained structure from motion algorithm (Figure 7.8) is used to\nestimate the relative camera poses and intrinsics. Feature tracking is then run on the walk-\nthrough video in order to stabilize the video sequence—Liu, Gleicher, Jin et al. (2009) have\ncarried out more recent work along these lines.\nIndoor environments with windows, as well as sunny outdoor environments with strong\nshadows, often have a dynamic range that exceeds the capabilities of video sensors. For\nthis reason, the Ladybug camera has a programmable exposure capability that enables the\nbracketing of exposures at subsequent video frames. In order to merge the resulting video\n11 See http://www.cis.upenn.edu/∼kostas/omni.html for descriptions of panoramic (omnidirectional) vision sys-\ntems and associated workshops.\n12 (The Photo Tourism system of Snavely, Seitz, and Szeliski (2006) applies this idea to less structured collections.\n13 In computer games, restricting a player to forward and backward motion along predetermined paths is called\nrail-based gaming.\n14 http://www.ptgrey.com/.",
  "669": "13.5 Video-based rendering\n647\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 13.16 Video-based walkthroughs (Uyttendaele, Criminisi, Kang et al. 2004) c⃝2004\nIEEE: (a) system diagram of video pre-processing; (b) the Point Grey Ladybug camera; (c)\nghost removal using multi-perspective plane sweep; (d) point tracking, used both for calibra-\ntion and stabilization; (e) interactive garden walkthrough with map below; (f) overhead map\nauthoring and sound placement; (g) interactive home walkthrough with navigation bar (top)\nand icons of interest (bottom).",
  "670": "648\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nframes into high dynamic range (HDR) video, pixels from adjacent frames need to be motion-\ncompensated before being merged (Kang, Uyttendaele, Winder et al. 2003).\nThe interactive walk-through experience becomes much richer and more navigable if an\noverview map is available as part of the experience. In Figure 13.16f, the map has annotations,\nwhich can show up during the tour, and localized sound sources, which play (with different\nvolumes) when the viewer is nearby. The process of aligning the video sequence with the\nmap can be automated using a process called map correlation (Levin and Szeliski 2004).\nAll of these elements combine to provide the user with a rich, interactive, and immersive\nexperience. Figure 13.16e shows a walk through the Bellevue Botanical Gardens, with an\noverview map in perspective below the live video window. Arrows on the ground are used to\nindicate potential directions of travel. The viewer simply orients his view towards one of the\narrows (the experience can be driven using a game controller) and “walks” forward along the\ndesired path.\nFigure 13.16g shows an indoor home tour experience. In addition to a schematic map\nin the lower left corner and adjacent room names along the top navigation bar, icons appear\nalong the bottom whenever items of interest, such as a homeowner’s art pieces, are visible\nin the main window. These icons can then be clicked to provide more information and 3D\nviews.\nThe development of interactive video tours spurred a renewed interest in 360◦video-based\nvirtual travel and mapping experiences, as evidenced by commercial sites such as Google’s\nStreet View and Bing Maps. The same videos can also be used to generate turn-by-turn driv-\ning directions, taking advantage of both expanded ﬁelds of view and image-based rendering\nto enhance the experience (Chen, Neubert, Ofek et al. 2009).\nAs we continue to capture more and more of our real world with large amounts of high-\nquality imagery and video, the interactive modeling, exploration, and rendering techniques\ndescribed in this chapter will play an even bigger role in bringing virtual experiences based\non remote areas of the world closer to everyone.\n13.6 Additional reading\nTwo good recent surveys of image-based rendering are by Kang, Li, Tong et al. (2006) and\nShum, Chan, and Kang (2007), with earlier surveys available from Kang (1999), McMillan\nand Gortler (1999), and Debevec (1999). The term image-based rendering was introduced by\nMcMillan and Bishop (1995), although the seminal paper in the ﬁeld is the view interpolation\npaper by Chen and Williams (1993). Debevec, Taylor, and Malik (1996) describe their Fac¸ade\nsystem, which not only created a variety of image-based modeling tools but also introduced\nthe widely used technique of view-dependent texture mapping.",
  "671": "13.6 Additional reading\n649\nEarly work on planar impostors and layers was carried out by Shade, Lischinski, Salesin\net al. (1996), Lengyel and Snyder (1997), and Torborg and Kajiya (1996), while newer work\nbased on sprites with depth is described by Shade, Gortler, He et al. (1998).\nThe two foundational papers in image-based rendering are Light ﬁeld rendering by Levoy\nand Hanrahan (1996) and The Lumigraph by Gortler, Grzeszczuk, Szeliski et al. (1996).\nBuehler, Bosse, McMillan et al. (2001) generalize the Lumigraph approach to irregularly\nspaced collections of images, while Levoy (2006) provides a survey and more gentle intro-\nduction to the topic of light ﬁeld and image-based rendering.\nSurface light ﬁelds (Wood, Azuma, Aldinger et al. 2000) provide an alternative param-\neterization for light ﬁelds with accurately known surface geometry and support both better\ncompression and the possibility of editing surface properties. Concentric mosaics (Shum and\nHe 1999; Shum, Wang, Chai et al. 2002) and panoramas with depth (Peleg, Ben-Ezra, and\nPritch 2001; Li, Shum, Tang et al. 2004; Zheng, Kang, Cohen et al. 2007), provide useful\nparameterizations for light ﬁelds captured with panning cameras. Multi-perspective images\n(Rademacher and Bishop 1998) and manifold projections (Peleg and Herman 1997), although\nnot true light ﬁelds, are also closely related to these ideas.\nAmong the possible extensions of light ﬁelds to higher-dimensional structures, environ-\nment mattes (Zongker, Werner, Curless et al. 1999; Chuang, Zongker, Hindorff et al. 2000)\nare the most useful, especially for placing captured objects into new scenes.\nVideo-based rendering, i.e., the re-use of video to create new animations or virtual ex-\nperiences, started with the seminal work of Szummer and Picard (1996), Bregler, Covell,\nand Slaney (1997), and Sch¨odl, Szeliski, Salesin et al. (2000). Important follow-on work\nto these basic re-targeting approaches was carried out by Sch¨odl and Essa (2002), Kwatra,\nSch¨odl, Essa et al. (2003), Doretto, Chiuso, Wu et al. (2003), Wang and Zhu (2003), Zhong\nand Sclaroff (2003), Yuan, Wen, Liu et al. (2004), Doretto and Soatto (2006), Zhao and\nPietik¨ainen (2007), and Chan and Vasconcelos (2009).\nSystems that allow users to change their 3D viewpoint based on multiple synchronized\nvideo streams include those by Moezzi, Katkere, Kuramura et al. (1996), Kanade, Ran-\nder, and Narayanan (1997), Matusik, Buehler, Raskar et al. (2000), Matusik, Buehler, and\nMcMillan (2001), Carranza, Theobalt, Magnor et al. (2003), Zitnick, Kang, Uyttendaele et\nal. (2004), Magnor (2005), and Vedula, Baker, and Kanade (2005). 3D (multiview) video\ncoding and compression is also an active area of research (Smolic and Kauff 2005; Gotchev\nand Rosenhahn 2009), with 3D Blu-Ray discs, encoded using the multiview video coding\n(MVC) extension to H.264/MPEG-4 AVC, expected by the end of 2010.",
  "672": "650\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n13.7 Exercises\nEx 13.1: Depth image rendering\nDevelop a “view extrapolation” algorithm to re-render a\npreviously computed stereo depth map coupled with its corresponding reference color image.\n1. Use a 3D graphics mesh rendering system such as OpenGL or Direct3D, with two\ntriangles per pixel quad and perspective (projective) texture mapping (Debevec, Yu,\nand Borshukov 1998).\n2. Alternatively, use the one- or two-pass forward warper you constructed in Exercise 3.24,\nextended using (2.68–2.70) to convert from disparities or depths into displacements.\n3. (Optional) Kinks in straight lines introduced during view interpolation or extrapola-\ntion are visually noticeable, which is one reason why image morphing systems let you\nspecify line correspondences (Beier and Neely 1992). Modify your depth estimation\nalgorithm to match and estimate the geometry of straight lines and incorporate it into\nyour image-based rendering algorithm.\nEx 13.2: View interpolation\nExtend the system you created in the previous exercise to ren-\nder two reference views and then blend the images using a combination of z-buffering, hole\nﬁling, and blending (morphing) to create the ﬁnal image (Section 13.1).\n1. (Optional) If the two source images have very different exposures, the hole-ﬁlled re-\ngions and the blended regions will have different exposures. Can you extend your\nalgorithm to mitigate this?\n2. (Optional) Extend your algorithm to perform three-way (trilinear) interpolation be-\ntween neighboring views. You can triangulate the reference camera poses and use\nbarycentric coordinates for the virtual camera in order to determine the blending weights.\nEx 13.3: View morphing\nModify your view interpolation algorithm to perform morphs be-\ntween views of a non-rigid object, such as a person changing expressions.\n1. Instead of using a pure stereo algorithm, use a general ﬂow algorithm to compute dis-\nplacements, but separate them into a rigid displacement due to camera motion and a\nnon-rigid deformation.\n2. At render time, use the rigid geometry to determine the new pixel location but then add\na fraction of the non-rigid displacement as well.\n3. Alternatively, compute a stereo depth map but let the user specify additional correspon-\ndences or use a feature-based matching algorithm to provide them automatically.",
  "673": "13.7 Exercises\n651\n4. (Optional) Take a single image, such as the Mona Lisa or a friend’s picture, and create\nan animated 3D view morph (Seitz and Dyer 1996).\n(a) Find the vertical axis of symmetry in the image and reﬂect your reference image\nto provide a virtual pair (assuming the person’s hairstyle is somewhat symmetric).\n(b) Use structure from motion to determine the relative camera pose of the pair.\n(c) Use dense stereo matching to estimate the 3D shape.\n(d) Use view morphing to create a 3D animation.\nEx 13.4: View dependent texture mapping\nUse a 3D model you created along with the\noriginal images to implement a view-dependent texture mapping system.\n1. Use one of the 3D reconstruction techniques you developed in Exercises 7.3, 11.9,\n11.10, or 12.8 to build a triangulated 3D image-based model from multiple photographs.\n2. Extract textures for each model face from your photographs, either by performing the\nappropriate resampling or by ﬁguring out how to use the texture mapping software to\ndirectly access the source images.\n3. At run time, for each new camera view, select the best source image for each visible\nmodel face.\n4. Extend this to blend between the top two or three textures. This is trickier, since it\ninvolves the use of texture blending or pixel shading (Debevec, Taylor, and Malik 1996;\nDebevec, Yu, and Borshukov 1998; Pighin, Hecker, Lischinski et al. 1998).\nEx 13.5: Layered depth images\nExtend your view interpolation algorithm (Exercise 13.2)\nto store more than one depth or color value per pixel (Shade, Gortler, He et al. 1998), i.e., a\nlayered depth image (LDI). Modify your rendering algorithm accordingly. For your data, you\ncan use synthetic ray tracing, a layered reconstructed model, or a volumetric reconstruction.\nEx 13.6: Rendering from sprites or layers\nExtend your view interpolation algorithm to\nhandle multiple planes or sprites (Section 13.2.1) (Shade, Gortler, He et al. 1998).\n1. Extract your layers using the technique you developed in Exercise 8.9.\n2. Alternatively, use an interactive painting and 3D placement system to extract your lay-\ners (Kang 1998; Oh, Chen, Dorsey et al. 2001; Shum, Sun, Yamazaki et al. 2004).\n3. Determine a back-to-front order based on expected visibility or add a z-buffer to your\nrendering algorithm to handle occlusions.",
  "674": "652\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n4. Render and composite all of the resulting layers, with optional alpha matting to handle\nthe edges of layers and sprites.\nEx 13.7: Light ﬁeld transformations\nDerive the equations relating regular images to 4D\nlight ﬁeld coordinates.\n1. Determine the mapping between the far plane (u, v) coordinates and a virtual camera’s\n(x, y) coordinates.\n(a) Start by parameterizing a 3D point on the uv plane in terms of its (u, v) coordi-\nnates.\n(b) Project the resulting 3D point to the camera pixels (x, y, 1) using the usual 3 × 4\ncamera matrix P (2.63).\n(c) Derive the 2D homography relating (u, v) and (x, y) coordinates.\n2. Write down a similar transformation for (s, t) to (x, y) coordinates.\n3. Prove that if the virtual camera is actually on the (s, t) plane, the (s, t) value depends\nonly on the camera’s optical center and is independent of (x, y).\n4. Prove that an image taken by a regular orthographic or perspective camera, i.e., one that\nhas a linear projective relationship between 3D points and (x, y) pixels (2.63), samples\nthe (s, t, u, v) light ﬁeld along a two-dimensional hyperplane.\nEx 13.8: Light ﬁeld and Lumigraph rendering\nImplement a light ﬁeld or Lumigraph ren-\ndering system:\n1. Download one of the light ﬁeld data sets from http://lightﬁeld.stanford.edu/.\n2. Write an algorithm to synthesize a new view from this light ﬁeld, using quadri-linear\ninterpolation of (s, t, u, v) ray samples.\n3. Try varying the focal plane corresponding to your desired view (Isaksen, McMillan,\nand Gortler 2000) and see if the resulting image looks sharper.\n4. Determine a 3D proxy for the objects in your scene. You can do this by running multi-\nview stereo over one of your light ﬁelds to obtain a depth map per image.\n5. Implement the Lumigraph rendering algorithm, which modiﬁes the sampling of rays\naccording to the 3D location of each surface element.\n6. Collect a set of images yourself and determine their pose using structure from motion.",
  "675": "13.7 Exercises\n653\n7. Implement the unstructured Lumigraph rendering algorithm from Buehler, Bosse, McMil-\nlan et al. (2001).\nEx 13.9: Surface light ﬁelds\nConstruct a surface light ﬁeld (Wood, Azuma, Aldinger et al.\n2000) and see how well you can compress it.\n1. Acquire an interesting light ﬁeld of a specular scene or object, or download one from\nhttp://lightﬁeld.stanford.edu/.\n2. Build a 3D model of the object using a multi-view stereo algorithm that is robust to\noutliers due to specularities.\n3. Estimate the Lumisphere for each surface point on the object.\n4. Estimate its diffuse components. Is the median the best way to do this? Why not use\nthe minimum color value? What happens if there is Lambertian shading on the diffuse\ncomponent?\n5. Model and compress the remaining portion of the Lumisphere using one of the tech-\nniques suggested by Wood, Azuma, Aldinger et al. (2000) or invent one of your own.\n6. Study how well your compression algorithm works and what artifacts it produces.\n7. (Optional) Develop a system to edit and manipulate your surface light ﬁeld.\nEx 13.10: Handheld concentric mosaics\nDevelop a system to navigate a handheld con-\ncentric mosaic.\n1. Stand in the middle of a room with a camcorder held at arm’s length in front of you and\nspin in a circle.\n2. Use a structure from motion system to determine the camera pose and sparse 3D struc-\nture for each input frame.\n3. (Optional) Re-bin your image pixels into a more regular concentric mosaic structure.\n4. At view time, determine from the new camera’s view (which should be near the plane\nof your original capture) which source pixels to display. You can simplify your com-\nputations to determine a source column (and scaling) for each output column.\n5. (Optional) Use your sparse 3D structure, interpolated to a dense depth map, to improve\nyour rendering (Zheng, Kang, Cohen et al. 2007).",
  "676": "654\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEx 13.11: Video textures\nCapture some videos of natural phenomena, such as a water\nfountain, ﬁre, or smiling face, and loop the video seamlessly into an inﬁnite length video\n(Sch¨odl, Szeliski, Salesin et al. 2000).\n1. Compare all the frames in the original clip using an L2 (sum of square difference)\nmetric. (This assumes the videos were shot on a tripod or have already been stabilized.)\n2. Filter the comparison table temporally to accentuate temporal sub-sequences that match\nwell together.\n3. Convert your similarity table into a jump probability table through some exponential\ndistribution. Be sure to modify transitions near the end so you do not get “stuck” in the\nlast frame.\n4. Starting with the ﬁrst frame, use your transition table to decide whether to jump for-\nward, backward, or continue to the next frame.\n5. (Optional) Add any of the other extensions to the original video textures idea, such\nas multiple moving regions, interactive control, or graph cut spatio-temporal texture\nseaming.",
  "677": "Chapter 14\nRecognition\n14.1 Object detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658\n14.1.1 Face detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658\n14.1.2 Pedestrian detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . 666\n14.2 Face recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668\n14.2.1 Eigenfaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671\n14.2.2 Active appearance and 3D shape models . . . . . . . . . . . . . . . . 679\n14.2.3 Application: Personal photo collections . . . . . . . . . . . . . . . . 684\n14.3 Instance recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685\n14.3.1 Geometric alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 686\n14.3.2 Large databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687\n14.3.3 Application: Location recognition . . . . . . . . . . . . . . . . . . . 693\n14.4 Category recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 696\n14.4.1 Bag of words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 697\n14.4.2 Part-based models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 701\n14.4.3 Recognition with segmentation . . . . . . . . . . . . . . . . . . . . . 704\n14.4.4 Application: Intelligent photo editing\n. . . . . . . . . . . . . . . . . 709\n14.5 Context and scene understanding . . . . . . . . . . . . . . . . . . . . . . . . 712\n14.5.1 Learning and large image collections\n. . . . . . . . . . . . . . . . . 714\n14.5.2 Application: Image search . . . . . . . . . . . . . . . . . . . . . . . 717\n14.6 Recognition databases and test sets . . . . . . . . . . . . . . . . . . . . . . . 718\n14.7 Additional reading\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722\n14.8 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 725",
  "678": "656\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 14.1\nRecognition:\nface recognition with (a) pictorial structures (Fischler and\nElschlager 1973) c⃝1973 IEEE and (b) eigenfaces (Turk and Pentland 1991b);\n(c) real-\ntime face detection (Viola and Jones 2004) c⃝2004 Springer; (d) instance (known object)\nrecognition (Lowe 1999) c⃝1999 IEEE; (e) feature-based recognition (Fergus, Perona, and\nZisserman 2007); (f) region-based recognition (Mori, Ren, Efros et al. 2004) c⃝2004 IEEE;\n(g) simultaneous recognition and segmentation (Shotton, Winn, Rother et al. 2009) c⃝2009\nSpringer; (h) location recognition (Philbin, Chum, Isard et al. 2007) c⃝2007 IEEE; (i) using\ncontext (Russell, Torralba, Liu et al. 2007).",
  "679": "14 Recognition\n657\nOf all the visual tasks we might ask a computer to perform, analyzing a scene and recog-\nnizing all of the constituent objects remains the most challenging. While computers excel at\naccurately reconstructing the 3D shape of a scene from images taken from different views,\nthey cannot name all the objects and animals present in a picture, even at the level of a two-\nyear-old child. There is not even any consensus among researchers on when this level of\nperformance might be achieved.\nWhy is recognition so hard? The real world is made of a jumble of objects, which all oc-\nclude one another and appear in different poses. Furthermore, the variability intrinsic within\na class (e.g., dogs), due to complex non-rigid articulation and extreme variations in shape and\nappearance (e.g., between different breeds), makes it unlikely that we can simply perform\nexhaustive matching against a database of exemplars.1\nThe recognition problem can be broken down along several axes. For example, if we\nknow what we are looking for, the problem is one of object detection (Section 14.1), which\ninvolves quickly scanning an image to determine where a match may occur (Figure 14.1c). If\nwe have a speciﬁc rigid object we are trying to recognize (instance recognition, Section 14.3),\nwe can search for characteristic feature points (Section 4.1) and verify that they align in a\ngeometrically plausible way (Section 14.3.1) (Figure 14.1d).\nThe most challenging version of recognition is general category (or class) recognition\n(Section 14.4), which may involve recognizing instances of extremely varied classes such\nas animals or furniture. Some techniques rely purely on the presence of features (known\nas a “bag of words” model—see Section 14.4.1), their relative positions (part-based models\n(Section 14.4.2)), Figure 14.1e, while others involve segmenting the image into semantically\nmeaningful regions (Section 14.4.3) (Figure 14.1f). In many instances, recognition depends\nheavily on the context of surrounding objects and scene elements (Section 14.5). Woven into\nall of these techniques is the topic of learning (Section 14.5.1), since hand-crafting speciﬁc\nobject recognizers seems like a futile approach given the complexity of the problem.\nGiven the extremely rich and complex nature of this topic, this chapter is structured to\nbuild from simpler concepts to more complex ones. We begin with a discussion of face and\nobject detection (Section 14.1), where we introduce a number of machine-learning techniques\nsuch as boosting, neural networks, and support vector machines. Next, we study face recogni-\ntion (Section 14.2), which is one of the more widely known applications of recognition. This\ntopic serves as an introduction to subspace (PCA) models and Bayesian approaches to recog-\nnition and classiﬁcation. We then present techniques for instance recognition (Section 14.3),\nbuilding upon earlier topics in this book, such as feature detection, matching, and geomet-\nric alignment (Section 14.3.1). We introduce topics from the information and document re-\ntrieval communities, such as frequency vectors, feature quantization, and inverted indices\n1 However, some recent research suggests that direct image matching may be feasible for large enough databases\n(Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros 2008; Torralba, Freeman, and Fergus 2008).",
  "680": "658\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Section 14.3.2). We also present applications of location recognition (Section 14.3.3).\nIn the second half of the chapter, we address the most challenging variant of recognition,\nnamely the problem of category recognition (Section 14.4). This includes approaches that use\nbags of features (Section 14.4.1), parts (Section 14.4.2), and segmentation (Section 14.4.3).\nWe show how such techniques can be used to automate photo editing tasks, such as 3D mod-\neling, scene completion, and creating collages (Section 14.4.4). Next, we discuss the role\nthat context can play in both individual object recognition and more holistic scene under-\nstanding (Section 14.5). We close this chapter with a discussion of databases and test sets for\nconstructing and evaluating recognition systems (Section 14.6).\nWhile there is no comprehensive reference on object recognition, an excellent set of notes\ncan be found in the ICCV 2009 short course (Fei-Fei, Fergus, and Torralba 2009), Antonio\nTorralba’s more comprehensive MIT course (Torralba 2008), and two recent collections of\npapers (Ponce, Hebert, Schmid et al. 2006; Dickinson, Leonardis, Schiele et al. 2007) and a\nsurvey on object categorization (Pinz 2005). An evaluation of some of the best performing\nrecognition algorithms can be found on the PASCAL Visual Object Classes (VOC) Challenge\nWeb site at http://pascallin.ecs.soton.ac.uk/challenges/VOC/.\n14.1 Object detection\nIf we are given an image to analyze, such as the group portrait in Figure 14.2, we could try to\napply a recognition algorithm to every possible sub-window in this image. Such algorithms\nare likely to be both slow and error-prone. Instead, it is more effective to construct special-\npurpose detectors, whose job it is to rapidly ﬁnd likely regions where particular objects might\noccur.\nWe begin this section with face detectors, which are some of the more successful examples\nof recognition. For example, such algorithms are built into most of today’s digital cameras to\nenhance auto-focus and into video conferencing systems to control pan-tilt heads. We then\nlook at pedestrian detectors, as an example of more general methods for object detection.\nSuch detectors can be used in automotive safety applications, e.g., detecting pedestrians and\nother cars from moving vehicles (Leibe, Cornelis, Cornelis et al. 2007).\n14.1.1 Face detection\nBefore face recognition can be applied to a general image, the locations and sizes of any faces\nmust ﬁrst be found (Figures 14.1c and 14.2). In principle, we could apply a face recognition\nalgorithm at every pixel and scale (Moghaddam and Pentland 1997) but such a process would\nbe too slow in practice.",
  "681": "14.1 Object detection\n659\nFigure 14.2\nFace detection results produced by Rowley, Baluja, and Kanade (1998a) c⃝\n1998 IEEE. Can you ﬁnd the one false positive (a box around a non-face) among the 57 true\npositive results?\nOver the years, a wide variety of fast face detection algorithms have been developed.\nYang, Kriegman, and Ahuja (2002) provide a comprehensive survey of earlier work in this\nﬁeld; Yang’s ICPR 2004 tutorial2 and the Torralba (2007) short course provide more recent\nreviews.3\nAccording to the taxonomy of Yang, Kriegman, and Ahuja (2002), face detection tech-\nniques can be classiﬁed as feature-based, template-based, or appearance-based. Feature-\nbased techniques attempt to ﬁnd the locations of distinctive image features such as the eyes,\nnose, and mouth, and then verify whether these features are in a plausible geometrical ar-\nrangement. These techniques include some of the early approaches to face recognition (Fis-\nchler and Elschlager 1973; Kanade 1977; Yuille 1991), as well as more recent approaches\nbased on modular eigenspaces (Moghaddam and Pentland 1997), local ﬁlter jets (Leung,\nBurl, and Perona 1995; Penev and Atick 1996; Wiskott, Fellous, Kr¨uger et al. 1997), support\nvector machines (Heisele, Ho, Wu et al. 2003; Heisele, Serre, and Poggio 2007), and boosting\n(Schneiderman and Kanade 2004).\nTemplate-based approaches, such as active appearance models (AAMs) (Section 14.2.2),\ncan deal with a wide range of pose and expression variability. Typically, they require good\ninitialization near a real face and are therefore not suitable as fast face detectors.\n2 http://vision.ai.uiuc.edu/mhyang/face-detection-survey.html.\n3 An alternative approach to detecting faces is to look for regions of skin color in the image (Forsyth and Fleck\n1999; Jones and Rehg 2001). See Exercise 2.8 for some additional discussion and references.",
  "682": "660\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 14.3\nPre-processing stages for face detector training (Rowley, Baluja, and Kanade\n1998a) c⃝1998 IEEE: (a) artiﬁcially mirroring, rotating, scaling, and translating training\nimages for greater variability; (b) using images without faces (looking up at a tree) to generate\nnon-face examples; (c) pre-processing the patches by subtracting a best ﬁt linear function\n(constant gradient) and histogram equalizing.\nAppearance-based approaches scan over small overlapping rectangular patches of the im-\nage searching for likely face candidates, which can then be reﬁned using a cascade of more\nexpensive but selective detection algorithms (Sung and Poggio 1998; Rowley, Baluja, and\nKanade 1998a; Romdhani, Torr, Sch¨olkopf et al. 2001; Fleuret and Geman 2001; Viola and\nJones 2004). In order to deal with scale variation, the image is usually converted into a\nsub-octave pyramid and a separate scan is performed on each level. Most appearance-based\napproaches today rely heavily on training classiﬁers using sets of labeled face and non-face\npatches.\nSung and Poggio (1998) and Rowley, Baluja, and Kanade (1998a) present two of the ear-\nliest appearance-based face detectors and introduce a number of innovations that are widely\nused in later work by others.\nTo start with, both systems collect a set of labeled face patches (Figure 14.2) as well as a\nset of patches taken from images that are known not to contain faces, such as aerial images or\nvegetation (Figure 14.3b). The collected face images are augmented by artiﬁcially mirroring,\nrotating, scaling, and translating the images by small amounts to make the face detectors less\nsensitive to such effects (Figure 14.3a).\nAfter an initial set of training images has been collected, some optional pre-processing\ncan be performed, such as subtracting an average gradient (linear function) from the image\nto compensate for global shading effects and using histogram equalization to compensate for\nvarying camera contrast (Figure 14.3c).",
  "683": "14.1 Object detection\n661\nFigure 14.4\nLearning a mixture of Gaussians model for face detection (Sung and Poggio\n1998) c⃝1998 IEEE. The face and non-face images (192-long vectors) are ﬁrst clustered into\nsix separate clusters (each) using k-means and then analyzed using PCA. The cluster centers\nare shown in the right-hand columns.\nClustering and PCA.\nOnce the face and non-face patterns have been pre-processed, Sung\nand Poggio (1998) cluster each of these datasets into six separate clusters using k-means\nand then ﬁt PCA subspaces to each of the resulting 12 clusters (Figure 14.4). At detection\ntime, the DIFS and DFFS metrics ﬁrst developed by Moghaddam and Pentland (1997) (see\nFigure 14.14 and (14.14)) are used to produce 24 Mahalanobis distance measurements (two\nper cluster). The resulting 24 measurements are input to a multi-layer perceptron (MLP),\nwhich is a neural network with alternating layers of weighted summations and sigmoidal non-\nlinearities trained using the “backpropagation” algorithm (Rumelhart, Hinton, and Williams\n1986).\nNeural networks.\nInstead of ﬁrst clustering the data and computing Mahalanobis distances\nto the cluster centers, Rowley, Baluja, and Kanade (1998a) apply a neural network (MLP) di-\nrectly to the 20×20 pixel patches of gray-level intensities, using a variety of differently sized\nhand-crafted “receptive ﬁelds” to capture both large-scale and smaller scale structure (Fig-\nure 14.5). The resulting neural network directly outputs the likelihood of a face at the center",
  "684": "662\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.5\nA neural network for face detection (Rowley, Baluja, and Kanade 1998a) c⃝\n1998 IEEE. Overlapping patches are extracted from different levels of a pyramid and then\npre-processed as shown in Figure 14.3b. A three-layer neural network is then used to detect\nlikely face locations.\nof every overlapping patch in a multi-resolution pyramid. Since several overlapping patches\n(in both space and resolution) may ﬁre near a face, an additional merging network is used\nto merge overlapping detections. The authors also experiment with training several networks\nand merging their outputs. Figure 14.2 shows a sample result from their face detector.\nTo make the detector run faster, a separate network operating on 30×30 patches is trained\nto detect both faces and faces shifted by ±5 pixels. This network is evaluated at every 10th\npixel in the image (horizontally and vertically) and the results of this “coarse” or “sloppy”\ndetector are used to select regions on which to run the slower single-pixel overlap technique.\nTo deal with in-plane rotations of faces, Rowley, Baluja, and Kanade (1998b) train a router\nnetwork to estimate likely rotation angles from input patches and then apply the estimated\nrotation to each patch before running the result through their upright face detector.\nSupport vector machines.\nInstead of using a neural network to classify patches, Osuna,\nFreund, and Girosi (1997) use a support vector machine (SVM) (Hastie, Tibshirani, and\nFriedman 2001; Sch¨olkopf and Smola 2002; Bishop 2006; Lampert 2008) to classify the same\npreprocessed patches as Sung and Poggio (1998). An SVM searches for a series of maximum\nmargin separating planes in feature space between different classes (in this case, face and\nnon-face patches). In those cases where linear classiﬁcation boundaries are insufﬁcient, the\nfeature space can be lifted into higher-dimensional features using kernels (Hastie, Tibshirani,\nand Friedman 2001; Sch¨olkopf and Smola 2002; Bishop 2006). SVMs have been used by\nother researchers for both face detection and face recognition (Heisele, Ho, Wu et al. 2003;",
  "685": "14.1 Object detection\n663\n(a)\n(b)\nFigure 14.6\nSimple features used in boosting-based face detector (Viola and Jones 2004)\nc⃝2004 Springer: (a) difference of rectangle feature composed of 2–4 different rectangles\n(pixels inside the white rectangles are subtracted from the gray ones); (b) the ﬁrst and second\nfeatures selected by AdaBoost. The ﬁrst feature measures the differences in intensity between\nthe eyes and the cheeks, the second one between the eyes and the bridge of the nose.\nHeisele, Serre, and Poggio 2007) and are a widely used tool in object recognition in general.\nBoosting.\nOf all the face detectors currently in use, the one introduced by Viola and Jones\n(2004) is probably the best known and most widely used. Their technique was the ﬁrst to\nintroduce the concept of boosting to the computer vision community, which involves train-\ning a series of increasingly discriminating simple classiﬁers and then blending their outputs\n(Hastie, Tibshirani, and Friedman 2001; Bishop 2006).\nIn more detail, boosting involves constructing a classiﬁer h(x) as a sum of simple weak\nlearners,\nh(x) = sign\n\n\nm−1\nX\nj=0\nαjhj(x)\n\n,\n(14.1)\nwhere each of the weak learners hj(x) is an extremely simple function of the input, and hence\nis not expected to contribute much (in isolation) to the classiﬁcation performance.\nIn most variants of boosting, the weak learners are threshold functions,\nhj(x) = aj[fj < θj] + bj[fj ≥θj] =\n(\naj\nif fj < θj\nbj\notherwise,\n(14.2)\nwhich are also known as decision stumps (basically, the simplest possible version of decision\ntrees). In most cases, it is also traditional (and simpler) to set aj and bj to ±1, i.e., aj = −sj,\nbj = +sj, so that only the feature fj, the threshold value θj, and the polarity of the threshold\nsj ∈±1 need to be selected.4\n4Some variants, such as that of Viola and Jones (2004), use (aj, bj) ∈[0, 1] and adjust the learning algorithm",
  "686": "664\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWeak classifier 1\nWeights increased\nWeak classifier 2\nWeights increased\nWeak classifier 3\nFinal classifier\nFigure 14.7 Schematic illustration of boosting, courtesy of Svetlana Lazebnik, after origi-\nnal illustrations from Paul Viola and David Lowe. After each weak classiﬁer (decision stump\nor hyperplane) is selected, data points that are erroneously classiﬁed have their weights in-\ncreased. The ﬁnal classiﬁer is a linear combination of the simple weak classiﬁers.\nIn many applications of boosting, the features are simply coordinate axes xk, i.e., the\nboosting algorithm selects one of the input vector components as the best one to threshold. In\nViola and Jones’ face detector, the features are differences of rectangular regions in the input\npatch, as shown in Figure 14.6. The advantage of using these features is that, while they are\nmore discriminating than single pixels, they are extremely fast to compute once a summed\narea table has been pre-computed, as described in Section 3.2.3 (3.31–3.32). Essentially, for\nthe cost of an O(N) pre-computation phase (where N is the number of pixels in the image),\nsubsequent differences of rectangles can be computed in 4r additions or subtractions, where\nr ∈{2, 3, 4} is the number of rectangles in the feature.\nThe key to the success of boosting is the method for incrementally selecting the weak\nlearners and for re-weighting the training examples after each stage (Figure 14.7). The Ad-\naBoost (Adaptive Boosting) algorithm (Hastie, Tibshirani, and Friedman 2001; Bishop 2006)\ndoes this by re-weighting each sample as a function of whether it is correctly classiﬁed at each\nstage, and using the stage-wise average classiﬁcation error to determine the ﬁnal weightings\nαj among the weak classiﬁers, as described in Algorithm 14.1. While the resulting classi-\nﬁer is extremely fast in practice, the training time can be quite slow (in the order of weeks),\nbecause of the large number of feature (difference of rectangle) hypotheses that need to be\nexamined at each stage.\nTo further increase the speed of the detector, it is possible to create a cascade of classiﬁers,\nwhere each classiﬁer uses a small number of tests (say, a two-term AdaBoost classiﬁer) to\nreject a large fraction of non-faces while trying to pass through all potential face candidates\n(Fleuret and Geman 2001; Viola and Jones 2004). An even faster algorithm for performing\ncascade learning has recently been developed by Brubaker, Wu, Sun et al. (2008).\naccordingly.",
  "687": "14.1 Object detection\n665\n1. Input the positive and negative training examples along with their labels {(xi, yi)},\nwhere yi = 1 for positive (face) examples and yi = −1 for negative examples.\n2. Initialize all the weights to wi,1 ←\n1\nN , where N is the number of training exam-\nples. (Viola and Jones (2004) use a separate N1 and N2 for positive and negative\nexamples.)\n3. For each training stage j = 1 . . . M:\n(a) Renormalize the weights so that they sum up to 1 (divide them by their sum).\n(b) Select the best classiﬁer hj(x; fj, θj, sj) by ﬁnding the one that minimizes\nthe weighted classiﬁcation error\nej\n=\nN−1\nX\ni=0\nwi,jei,j,\n(14.3)\nei,j\n=\n1 −δ(yi, hj(xi; fj, θj, sj)).\n(14.4)\nFor any given fj function, the optimal values of (θj, sj) can be found in\nlinear time using a variant of weighted median computation (Exercise 14.2).\n(c) Compute the modiﬁed error rate βj and classiﬁer weight αj,\nβj =\nej\n1 −ej\nand\nαj = −log βj.\n(14.5)\n(d) Update the weights according to the classiﬁcation errors ei,j\nwi,j+1 ←wi,jβ1−ei,j\nj\n,\n(14.6)\ni.e., downweight the training samples that were correctly classiﬁed in pro-\nportion to the overall classiﬁcation error.\n4. Set the ﬁnal classiﬁer to\nh(x) = sign\n\n\nm−1\nX\nj=0\nαjhj(x)\n\n.\n(14.7)\nAlgorithm 14.1\nThe AdaBoost training algorithm, adapted from Hastie, Tibshirani, and\nFriedman (2001), Viola and Jones (2004), and Bishop (2006).",
  "688": "666\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 14.8 Pedestrian detection using histograms of oriented gradients (Dalal and Triggs\n2005) c⃝2005 IEEE: (a) the average gradient image over the training examples; (b) each\n“pixel” shows the maximum positive SVM weight in the block centered on the pixel; (c) like-\nwise, for the negative SVM weights; (d) a test image; (e) the computed R-HOG (rectangular\nhistogram of gradients) descriptor; (f) the R-HOG descriptor weighted by the positive SVM\nweights; (g) the R-HOG descriptor weighted by the negative SVM weights.\n14.1.2 Pedestrian detection\nWhile a lot of the research on object detection has focused on faces, the detection of other\nobjects, such as pedestrians and cars, has also received widespread attention (Gavrila and\nPhilomin 1999; Gavrila 1999; Papageorgiou and Poggio 2000; Mohan, Papageorgiou, and\nPoggio 2001; Schneiderman and Kanade 2004). Some of these techniques maintain the same\nfocus as face detection on speed and efﬁciency. Others, however, focus instead on accuracy,\nviewing detection as a more challenging variant of generic class recognition (Section 14.4)\nin which the locations and extents of objects are to be determined as accurately as possible.\n(See, for example, the PASCAL VOC detection challenge, http://pascallin.ecs.soton.ac.uk/\nchallenges/VOC/.)\nAn example of a well-known pedestrian detector is the algorithm developed by Dalal\nand Triggs (2005), who use a set of overlapping histogram of oriented gradients (HOG) de-\nscriptors fed into a support vector machine (Figure 14.8). Each HOG has cells to accumulate\nmagnitude-weighted votes for gradients at particular orientations, just as in the scale invariant\nfeature transform (SIFT) developed by Lowe (2004), which we discussed in Section 4.1.2 and\nFigure 4.18. Unlike SIFT, however, which is only evaluated at interest point locations, HOGs\nare evaluated on a regular overlapping grid and their descriptor magnitudes are normalized\nusing an even coarser grid; they are only computed at a single scale and a ﬁxed orientation. In\norder to capture the subtle variations in orientation around a person’s outline, a large number\nof orientation bins is used and no smoothing is performed in the central difference gradi-\nent computation—see the work of Dalal and Triggs (2005) for more implementation details.",
  "689": "14.1 Object detection\n667\nFigure 14.8d shows a sample input image, while Figure 14.8e shows the associated HOG\ndescriptors.\nOnce the descriptors have been computed, a support vector machine (SVM) is trained\non the resulting high-dimensional continuous descriptor vectors. Figures 14.8b–c show a\ndiagram of the (most) positive and negative SVM weights in each block, while Figures 14.8f–\ng show the corresponding weighted HOG responses for the central input image. As you can\nsee, there are a fair number of positive responses around the head, torso, and feet of the\nperson, and relatively few negative responses (mainly around the middle and the neck of the\nsweater).\nThe ﬁelds of pedestrian and general object detection have continued to evolve rapidly\nover the last decade (Belongie, Malik, and Puzicha 2002; Mikolajczyk, Schmid, and Zis-\nserman 2004; Leibe, Seemann, and Schiele 2005; Opelt, Pinz, and Zisserman 2006; Tor-\nralba 2007; Andriluka, Roth, and Schiele 2009, 2010; Doll`ar, Belongie, and Perona 2010).\nMunder and Gavrila (2006) compare a number of pedestrian detectors and conclude that\nthose based on local receptive ﬁelds and SVMs perform the best, with a boosting-based ap-\nproach coming close. Maji, Berg, and Malik (2008) improve on the best of these results using\nnon-overlapping multi-resolution HOG descriptors and a histogram intersection kernel SVM\nbased on a spatial pyramid match kernel from Lazebnik, Schmid, and Ponce (2006).\nWhen detectors for several different classes are being constructed simultaneously, Tor-\nralba, Murphy, and Freeman (2007) show that sharing features and weak learners between\ndetectors yields better performance, both in terms of faster computation times and fewer\ntraining examples. To ﬁnd the features and decision stumps that work best in a shared man-\nner, they introduce a novel joint boosting algorithm that optimizes, at each stage, a summed\nexpected exponential loss function using the “gentleboost” algorithm of Friedman, Hastie,\nand Tibshirani (2000).\nIn more recent work, Felzenszwalb, McAllester, and Ramanan (2008) extend the his-\ntogram of oriented gradients person detector to incorporate ﬂexible parts models (Section 14.4.2).\nEach part is trained and detected on HOGs evaluated at two pyramid levels below the overall\nobject model and the locations of the parts relative to the parent node (the overall bounding\nbox) are also learned and used during recognition (Figure 14.9b). To compensate for inac-\ncuracies or inconsistencies in the training example bounding boxes (dashed white lines in\nFigure 14.9c), the “true” location of the parent (blue) bounding box is considered a latent\n(hidden) variable and is inferred during both training and recognition. Since the locations\nof the parts are also latent, the system can be trained in a semi-supervised fashion, without\nneeding part labels in the training data. An extension to this system (Felzenszwalb, Girshick,\nMcAllester et al. 2010), which includes among its improvements a simple contextual model,\nwas among the two best object detection systems in the 2008 Visual Object Classes detection\nchallenge. Other recent improvements to part-based person detection and pose estimation in-",
  "690": "668\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.9\nPart-based object detection (Felzenszwalb, McAllester, and Ramanan 2008)\nc⃝2008 IEEE: (a) An input photograph and its associated person (blue) and part (yellow)\ndetection results. (b) The detection model is deﬁned by a coarse template, several higher\nresolution part templates, and a spatial model for the location of each part. (c) True positive\ndetection of a skier and (d) false positive detection of a cow (labeled as a person).\nclude the work by Andriluka, Roth, and Schiele (2009) and Kumar, Zisserman, and H.S.Torr\n(2009).\nAn even more accurate estimate of a person’s pose and location is presented by Rogez,\nRihan, Ramalingam et al. (2008), who compute both the phase of a person in a walk cycle and\nthe locations of individual joints, using random forests built on top of HOGs (Figure 14.11).\nSince their system produces full 3D pose information, it is closer in its application domain to\n3D person trackers (Sidenbladh, Black, and Fleet 2000; Andriluka, Roth, and Schiele 2010),\nwhich we discussed in Section 12.6.4.\nOne ﬁnal note on person and object detection. When video sequences are available, the\nadditional information present in the optic ﬂow and motion discontinuities can greatly aid in\nthe detection task, as discussed by Efros, Berg, Mori et al. (2003), Viola, Jones, and Snow\n(2003), and Dalal, Triggs, and Schmid (2006).\n14.2 Face recognition\nAmong the various recognition tasks that computers might be asked to perform, face recog-\nnition is the one where they have arguably had the most success.5 While computers cannot\npick out suspects from thousands of people streaming in front of video cameras (even people\ncannot readily distinguish between similar people with whom they are not familiar (O’Toole,\nJiang, Roark et al. 2006; O’Toole, Phillips, Jiang et al. 2009)), their ability to distinguish\n5Instance recognition, i.e., the re-recognition of known objects such as locations or planar objects, is the other\nmost successful application of general image recognition. In the general domain of biometrics, i.e., identity recogni-\ntion, specialized images such as irises and ﬁngerprints perform even better (Jain, Bolle, and Pankanti 1999; Pankanti,\nBolle, and Jain 2000; Daugman 2004).",
  "691": "14.2 Face recognition\n669\nFigure 14.10\nPart-based object detection results for people, bicycles, and horses (Felzen-\nszwalb, McAllester, and Ramanan 2008) c⃝2008 IEEE. The ﬁrst three columns show correct\ndetections, while the rightmost column shows false positives.\nFigure 14.11 Pose detection using random forests (Rogez, Rihan, Ramalingam et al. 2008)\nc⃝2008 IEEE. The estimated pose (state of the kinematic model) is drawn over each input\nframe.",
  "692": "670\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.12 Humans can recognize low-resolution faces of familiar people (Sinha, Balas,\nOstrovsky et al. 2006) c⃝2006 IEEE.\namong a small number of family members and friends has found its way into consumer-level\nphoto applications, such as Picasa and iPhoto. Face recognition can also be used in a variety\nof additional applications, including human–computer interaction (HCI), identity veriﬁcation\n(Kirovski, Jojic, and Jancke 2004), desktop login, parental controls, and patient monitoring\n(Zhao, Chellappa, Phillips et al. 2003).\nToday’s face recognizers work best when they are given full frontal images of faces under\nrelatively uniform illumination conditions, although databases that include large amounts\nof pose and lighting variation have been collected (Phillips, Moon, Rizvi et al. 2000; Sim,\nBaker, and Bsat 2003; Gross, Shi, and Cohn 2005; Huang, Ramesh, Berg et al. 2007; Phillips,\nScruggs, O’Toole et al. 2010). (See Table 14.1 in Section 14.6 for more details.)\nSome of the earliest approaches to face recognition involved ﬁnding the locations of\ndistinctive image features, such as the eyes, nose, and mouth, and measuring the distances\nbetween these feature locations (Fischler and Elschlager 1973; Kanade 1977; Yuille 1991).\nMore recent approaches rely on comparing gray-level images projected onto lower dimen-\nsional subspaces called eigenfaces (Section 14.2.1) and jointly modeling shape and appear-\nance variations (while discounting pose variations) using active appearance models (Sec-\ntion 14.2.2).\nDescriptions of additional face recognition techniques can be found in a number of sur-\nveys and books on this topic (Chellappa, Wilson, and Sirohey 1995; Zhao, Chellappa, Phillips\net al. 2003; Li and Jain 2005) as well as the Face Recognition Web site.6 The survey on face\nrecognition by humans by Sinha, Balas, Ostrovsky et al. (2006) is also well worth reading; it\nincludes a number of surprising results, such as humans’ ability to recognize low-resolution\nimages of familiar faces (Figure 14.12) and the importance of eyebrows in recognition.\n6 http://www.face-rec.org/.",
  "693": "14.2 Face recognition\n671\n(a)\n(b)\n(c)\n(d)\nFigure 14.13 Face modeling and compression using eigenfaces (Moghaddam and Pentland\n1997) c⃝1997 IEEE: (a) input image; (b) the ﬁrst eight eigenfaces; (c) image reconstructed\nby projecting onto this basis and compressing the image to 85 bytes; (d) image reconstructed\nusing JPEG (530 bytes).\n14.2.1 Eigenfaces\nEigenfaces rely on the observation ﬁrst made by Kirby and Sirovich (1990) that an arbitrary\nface image x can be compressed and reconstructed by starting with a mean image m (Fig-\nure 14.1b) and adding a small number of scaled signed images ui,7\n˜x = m +\nM−1\nX\ni=0\naiui,\n(14.8)\nwhere the signed basis images (Figure 14.13b) can be derived from an ensemble of train-\ning images using principal component analysis (also known as eigenvalue analysis or the\nKarhunen–Lo`eve transform). Turk and Pentland (1991a) recognized that the coefﬁcients ai\nin the eigenface expansion could themselves be used to construct a fast image matching algo-\nrithm.\nIn more detail, let us start with a collection of training images {xj}, from which we can\ncompute the mean image m and a scatter or covariance matrix\nC = 1\nN\nN−1\nX\nj=0\n(xj −m)(xj −m)T .\n(14.9)\nWe can apply the eigenvalue decomposition (A.6) to represent this matrix as\nC = UΛU T =\nN−1\nX\ni=0\nλiuiuT\ni ,\n(14.10)\nwhere the λi are the eigenvalues of C and the ui are the eigenvectors. For general im-\nages, Kirby and Sirovich (1990) call these vectors eigenpictures; for faces, Turk and Pentland\n7 In previous chapters, we used I to indicate images; in this chapter, we use the more abstract quantities x and u\nto indicate collections of pixels in an image turned into a vector.",
  "694": "672\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx\nDFFS\nDIFS\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nF\nF\n_\nm\n~x\nFigure 14.14 Projection onto the linear subspace spanned by the eigenface images (Moghad-\ndam and Pentland 1997) c⃝1997 IEEE. The distance from face space (DFFS) is the orthog-\nonal distance to the plane, while the distance in face space (DIFS) is the distance along the\nplane from the mean image. Both distances can be turned into Mahalanobis distances and\ngiven probabilistic interpretations.\n(1991a) call them eigenfaces (Figure 14.13b).8\nTwo important properties of the eigenvalue decomposition are that the optimal (best ap-\nproximation) coefﬁcients ai for any new image x can be computed as\nai = (x −m) · ui,\n(14.11)\nand that, assuming the eigenvalues {λi} are sorted in decreasing order, truncating the ap-\nproximation given in (14.8) at any point M gives the best possible approximation (least er-\nror) between ˜x and x. Figure 14.13c shows the resulting approximation corresponding to\nFigure 14.13a and shows how much better it is at compressing a face image than JPEG.\nTruncating the eigenface decomposition of a face image (14.8) after M components is\nequivalent to projecting the image onto a linear subspace F, which we can call the face space\n(Figure 14.14). Because the eigenvectors (eigenfaces) are orthogonal and of unit norm, the\ndistance of a projected face ˜x to the mean face m can be written as\nDIFS = ∥˜x −m∥=\nv\nu\nu\nt\nM−1\nX\ni=0\na2\ni ,\n(14.12)\nwhere DIFS stands for distance in face space (Moghaddam and Pentland 1997). The re-\nmaining distance between the original image x and its projection onto face space ˜x, i.e., the\n8 In actual practice, the full P ×P scatter matrix (14.9) is never computed. Instead, a smaller N ×N matrix con-\nsisting of the inner products between all the signed deviations (xi−m) is accumulated instead. See Appendix A.1.2\n(A.13–A.14) for details.",
  "695": "14.2 Face recognition\n673\ndistance from face space (DFFS), can be computed directly in pixel space and represents the\n“faceness” of a particular image.9 It is also possible to measure the distance between two\ndifferent faces in face space as\nDIFS(x, y) = ∥˜x −˜y∥=\nv\nu\nu\nt\nM−1\nX\ni=0\n(ai −bi)2,\n(14.13)\nwhere the bi = (y −m) · ui are the eigenface coefﬁcients corresponding to y.\nComputing such distances in Euclidean vector space, however, does not exploit the ad-\nditional information that the eigenvalue decomposition of our covariance matrix (14.10) pro-\nvides. If we interpret the covariance matrix C as the covariance of a multi-variate Gaussian\n(Appendix B.1.1),10 we can turn the DIFS into a log likelihood by computing the Maha-\nlanobis distance\nDIFS′ = ∥˜x −m∥C\n−1 =\nv\nu\nu\nt\nM−1\nX\ni=0\na2\ni /λ2\ni .\n(14.14)\nInstead of measuring the squared distance along each principal component in face space F,\nthe Mahalanobis distance measures the ratio between the squared distance and the corre-\nsponding variance σ2\ni = λi and then sums these squared ratios (per-component log-likelihoods).\nAn alternative way to implement this is to pre-scale each eigenvector by the inverse square\nroot of its corresponding eigenvalue,\nˆU = UΛ−1/2.\n(14.15)\nThis whitening transformation then means that Euclidean distances in feature (face) space\nnow correspond directly to log likelihoods (Moghaddam, Jebara, and Pentland 2000). (This\nsame whitening approach can also be used in feature-based matching algorithms, as discussed\nin Section 4.1.3.)\nIf the distribution in eigenface space is very elongated, the Mahalanobis distance properly\nscales the components to come up with a sensible (probabilistic) distance from the mean.\nA similar analysis can be performed for computing a sensible difference from face space\n(DFFS) (Moghaddam and Pentland 1997) and the two terms can be combined to produce an\nestimate of the likelihood of being a true face, which can be useful in doing face detection\n(Section 14.1.1). More detailed explanations of probabilistic and Bayesian PCA can be found\nin textbooks on statistical learning (Hastie, Tibshirani, and Friedman 2001; Bishop 2006),\nwhich also discuss techniques for selecting the optimum number of components M to use in\nmodeling a distribution.\n9 This can be used to form a simple face detector, as mentioned in Section 14.1.1.\n10 The ellipse shown in Figure 14.14 denotes an equi-probability contour of this multi-variate Gaussian.",
  "696": "674\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.15 Images from the Harvard database used by Belhumeur, Hespanha, and Krieg-\nman (1997) c⃝1997 IEEE. Note the wide range of illumination variation, which can be more\ndramatic than inter-personal variations.\nOne of the biggest advantages of using eigenfaces is that they reduce the comparison\nof a new face image x to a prototype (training) face image xk (one of the colored xs in\nFigure 14.14) from a P-dimensional difference in pixel space to an M-dimensional difference\nin face space,\n∥x −xk∥= ∥a −ak∥,\n(14.16)\nwhere a = U T (x −m) (14.11) involves computing a dot product between the signed\ndifference-from-mean image (x −m) and each of the eigenfaces ui. Once again, however,\nthis Euclidean distance ignores the fact that we have more information about face likelihoods\navailable in the distribution of training images.\nConsider the set of images of one person taken under a wide range of illuminations shown\nin Figure 14.15. As you can see, the intrapersonal variability within these images is much\ngreater than the typical extrapersonal variability between any two people taken under the\nsame illumination. Regular PCA analysis fails to distinguish between these two sources of\nvariability and may, in fact, devote most of its principal components to modeling the intrap-\nersonal variability.\nIf we are going to approximate faces by a linear subspace, it is more useful to have a\nspace that discriminates between different classes (people) and is less sensitive to within-class\nvariations (Belhumeur, Hespanha, and Kriegman 1997). Consider the three classes shown as\ndifferent colors in Figure 14.16. As you can see, the distributions within a class (indicated\nby the tilted colored axes) are elongated and tilted with respect to the main face space PCA,",
  "697": "14.2 Face recognition\n675\n-2\n-1\n0\n1\n2\n-3\n-2\n-1\n0\n1\n2\n3\nFigure 14.16\nSimple example of Fisher linear discriminant analysis. The samples come\nfrom three different classes, shown in different colors along with their principal axes, which\nare scaled to 2σi. (The intersections of the tilted axes are the class means mk.) The dashed\nline is the (dominant) Fisher linear discriminant direction and the dotted lines are the linear\ndiscriminants between the classes. Note how the discriminant direction is a blend between\nthe principal directions of the between-class and within-class scatter matrices.\nwhich is aligned with the black x and y axes. We can compute the total within-class scatter\nmatrix as\nSW =\nK−1\nX\nk=0\nSk =\nK−1\nX\nk=0\nX\ni∈Ck\n(xi −mk)(xi −mk)T ,\n(14.17)\nwhere mk is the mean of class k and Sk is its within-class scatter matrix.11 Similarly, we\ncan compute the between-class scatter as\nSB =\nK−1\nX\nk=0\nNk(mk −m)(mk −m)T ,\n(14.18)\nwhere Nk are the number of exemplars in each class and m is the overall mean. For the three\ndistributions shown in Figure 14.16, we have\nSW = 3N\n\"\n0.246\n0.183\n0.183\n0.457\n#\nand\nSB = N\n\"\n6.125\n0\n0\n0.375\n#\n,\n(14.19)\n11 To be consistent with Belhumeur, Hespanha, and Kriegman (1997), we use SW and SB to denote the scatter\nmatrices, even though we use C elsewhere (14.9).",
  "698": "676\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwhere N = Nk = 13 is the number of samples in each class.\nTo compute the most discriminating direction, Fisher’s linear discriminant (FLD) (Bel-\nhumeur, Hespanha, and Kriegman 1997; Hastie, Tibshirani, and Friedman 2001; Bishop\n2006), which is also known as linear discriminant analysis (LDA), selects the direction u\nthat results in the largest ratio between the projected between-class and within-class varia-\ntions\nu∗= arg max\nu\nuT SBu\nuT SWu,\n(14.20)\nwhich is equivalent to ﬁnding the eigenvector corresponding to the largest eigenvalue of the\ngeneralized eigenvalue problem\nSBu = λSWu\nor\nλu = S−1\nW SBu.\n(14.21)\nFor the problem shown in Figure 14.16,\nS−1\nW SB =\n\"\n11.796\n−0.289\n−4.715\n0.3889\n#\nand\nu =\n\"\n0.926\n−0.379\n#\n(14.22)\nAs you can see, using this direction results in a better separation between the classes than\nusing the dominant PCA direction, which is the horizontal axis. In their paper, Belhumeur,\nHespanha, and Kriegman (1997) show that Fisherfaces signiﬁcantly outperform the original\neigenfaces algorithm, especially when faces have large amounts of illumination variation, as\nin Figure 14.15.\nAn alternative for modeling within-class (intrapersonal) and between-class (extraper-\nsonal) variations is to model each distribution separately and then use Bayesian techniques\nto ﬁnd the closest exemplar (Moghaddam, Jebara, and Pentland 2000). Instead of computing\nthe mean for each class and then the within-class and between-class distributions, consider\nevaluating the difference images\n∆ij = xi −xj\n(14.23)\nbetween all pairs of training images (xi, xj). The differences between pairs that are in the\nsame class (the same person) are used to estimate the intrapersonal covariance matrix ΣI,\nwhile differences between different people are used to estimate the extrapersonal covariance\nΣE.12 The principal components (eigenfaces) corresponding to these two classes are shown\nin Figure 14.17.\nAt recognition time, we can compute the distance ∆i between a new face x and a stored\ntraining image xi and evaluate its intrapersonal likelihood as\npI(∆i) = pN (∆i; ΣI) =\n1\n|2πΣI|1/2 exp −∥∆i∥2\nΣ\n−1\nI ,\n(14.24)\n12 Note that the difference distributions are zero mean because for every ∆ij there corresponds a negative ∆ji.",
  "699": "14.2 Face recognition\n677\n(a)\n(b)\nFigure 14.17 “Dual” eigenfaces (Moghaddam, Jebara, and Pentland 2000) c⃝2000 Elsevier:\n(a) intrapersonal and (b) extrapersonal.\nwhere pN is a normal (Gaussian) distribution with covariance ΣI and\n|2πΣI|1/2 = (2π)M/2\nM\nY\nj=1\nλ1/2\nj\n(14.25)\nis its volume. The Mahalanobis distance\n∥∆i∥2\nΣ\n−1\nI\n= ∆T\ni Σ−1\nI ∆i = ∥aI −aI\ni ∥2\n(14.26)\ncan be computed more efﬁciently by ﬁrst projecting the new image x into the whitened in-\ntrapersonal face space (14.15)\naI = ˆ\nU Ix\n(14.27)\nand then computing a Euclidean distance to the training image vector aI\ni , which can be pre-\ncomputed ofﬂine. The extrapersonal likelihood pE(∆i) can be computed in a similar fashion.\nOnce the intrapersonal and extrapersonal likelihoods have been computed, we can com-\npute the Bayesian likelihood of a new image x matching a training image xi as\np(∆i) =\npI(∆i)lI\npI(∆i)lI + pE(∆i)lE\n,\n(14.28)\nwhere lI and lE are the prior probabilities of two images being in the same or in different\nclasses (Moghaddam, Jebara, and Pentland 2000). A simpler approach, which does not re-\nquire the evaluation of extrapersonal probabilities, is to simply choose the training image with",
  "700": "678\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.18\nModular eigenspace for face recognition (Moghaddam and Pentland 1997)\nc⃝1997 IEEE. (a) By detecting separate features in the faces (eyes, nose, mouth), separate\neigenspaces can be estimated for each one. (b) The relative positions of each feature can be\ndetected at recognition time, thus allowing for more ﬂexibility in viewpoint and expression.\nthe highest likelihood pI(∆i). In this case, nearest neighbor search techniques in the space\nspanned by the precomputed {aI\ni } vectors could be used to speed up ﬁnding the best match.13\nAnother way to improve the performance of eigenface-based approaches is to break up\nthe image into separate regions such as the eyes, nose, and mouth (Figure 14.18) and to match\neach of these modular eigenspaces independently (Moghaddam and Pentland 1997; Heisele,\nHo, Wu et al. 2003; Heisele, Serre, and Poggio 2007). The advantage of such a modular\napproach is that it can tolerate a wider range of viewpoints, because each part can move\nrelative to the others. It also supports a larger variety of combinations, e.g., we can model one\nperson as having a narrow nose and bushy eyebrows, without requiring the eigenfaces to span\nall possible combinations of nose, mouth, and eyebrows. (If you remember the cardboard\nchildren’s books where you can select different top and bottom faces, or Mr. Potato Head,\nyou get the idea.)\nAnother approach to dealing with large variability in appearance is to create view-based\n(view-speciﬁc) eigenspaces, as shown in Figure 14.19 (Moghaddam and Pentland 1997). We\ncan think of these view-based eigenspaces as local descriptors that select different axes de-\npending on which part of the face space you are in. Note that such approaches, however,\npotentially require large amounts of training data, i.e., pictures of every person in every pos-\nsible pose or expression. This is in contrast to the shape and appearance models we study in\n13 Note that while the covariance matrices ΣI and ΣE are computed by looking at differences between all pairs of\nimages, the run-time evaluation selects the nearest image to determine the facial identity. Whether this is statistically\ncorrect is explored in Exercise 14.4.",
  "701": "14.2 Face recognition\n679\n(a)\n(b)\nFigure 14.19 View-based eigenspace (Moghaddam and Pentland 1997) c⃝1997 IEEE. (a)\nComparison between a regular (parametric) eigenspace reconstruction (middle column) and\na view-based eigenspace reconstruction (right column) corresponding to the input image (left\ncolumn). The top row is from a training image, the bottom row is from the test set. (b) A\nschematic representation of the two approaches, showing how each view computes its own\nlocal basis representation.\nSection 14.2.2, which can learn deformations across all individuals.\nIt is also possible to generalize the bilinear factorization implicit in PCA and SVD ap-\nproaches to multilinear (tensor) formulations that can model several interacting factors si-\nmultaneously (Vasilescu and Terzopoulos 2007). These ideas are related to currently active\ntopics in machine learning such as subspace learning (Cai, He, Hu et al. 2007), local distance\nfunctions (Frome, Singer, Sha et al. 2007), and metric learning (Ramanan and Baker 2009).\nLearning approaches play an increasingly important role in face recognition, e.g., in the work\nof Sivic, Everingham, and Zisserman (2009) and Guillaumin, Verbeek, and Schmid (2009).\n14.2.2 Active appearance and 3D shape models\nThe need to use modular or view-based eigenspaces for face recognition is symptomatic of\na more general observation, i.e., that facial appearance and identiﬁability depend as much\non shape as they do on color or texture (which is what eigenfaces capture). Furthermore,\nwhen dealing with 3D head rotations, the pose of a person’s head should be discounted when\nperforming recognition.\nIn fact, the earliest face recognition systems, such as those by Fischler and Elschlager\n(1973), Kanade (1977), and Yuille (1991), found distinctive feature points on facial images\nand performed recognition on the basis of their relative positions or distances. Newer tech-\nniques such as local feature analysis (Penev and Atick 1996) and elastic bunch graph match-\ning (Wiskott, Fellous, Kr¨uger et al. 1997) combine local ﬁlter responses (jets) at distinctive",
  "702": "680\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.20 Manipulating facial appearance through shape and color (Rowland and Perrett\n1995) c⃝1995 IEEE. By adding or subtracting gender-speciﬁc shape and color characteristics\nto (b) an input image, different amounts of gender variation can be induced. The amounts\nadded (from the mean) are: (a) +50% (gender enhancement), (c) -50% (near “androgyny”),\n(d) -100% (gender switched), and (e) -150% (opposite gender attributes enhanced).\nfeature locations together with shape models to perform recognition.\nA visually compelling example of why both shape and texture are important is the work\nof Rowland and Perrett (1995), who manually traced the contours of facial features and then\nused these contours to normalize (warp) each image to a canonical shape. After analyzing\nboth the shape and color images for deviations from the mean, they were able to associate\ncertain shape and color deformations with personal characteristics such as age and gender\n(Figure 14.20). Their work demonstrates that both shape and color have an important inﬂu-\nence on the perception of such characteristics.\nAround the same time, researchers in computer vision were beginning to use simultane-\nous shape deformations and texture interpolation to model the variability in facial appearance\ncaused by identity or expression (Beymer 1996; Vetter and Poggio 1997), developing tech-\nniques such as Active Shape Models (Lanitis, Taylor, and Cootes 1997), 3D Morphable Mod-\nels (Blanz and Vetter 1999), and Elastic Bunch Graph Matching (Wiskott, Fellous, Kr¨uger et\nal. 1997).14\nOf all these techniques, the active appearance models (AAMs) of Cootes, Edwards, and\nTaylor (2001) are among the most widely used for face recognition and tracking. Like other\nshape and texture models, an AAM models both the variation in the shape of an image s,\nwhich is normally encoded by the location of key feature points on the image (Figure 14.21b),\n14 We have already seen the application of PCA to 3D head and face modeling and animation in Section 12.6.3.",
  "703": "14.2 Face recognition\n681\n(a)\n(b)\n(c)\nFigure 14.21\nActive Appearance Models (Cootes, Edwards, and Taylor 2001) c⃝2001\nIEEE: (a) input image with registered feature points; (b) the feature points (shape vector\ns); (c) the shape-free appearance image (texture vector t).\nas well as the variation in texture t, which is normalized to a canonical shape before being\nanalyzed (Figure 14.21c).15\nBoth shape and texture are represented as deviations from a mean shape ¯s and texture ¯t,\ns\n=\n¯s + U sa\n(14.29)\nt\n=\n¯t + U ta,\n(14.30)\nwhere the eigenvectors in U s and U t have been pre-scaled (whitened) so that unit vectors in\na represent one standard deviation of variation observed in the training data. In addition to\nthese principal deformations, the shape parameters are transformed by a global similarity to\nmatch the location, size, and orientation of a given face. Similarly, the texture image contains\na scale and offset to best match novel illumination conditions.\nAs you can see, the same appearance parameters a in (14.29–14.30) simultaneously con-\ntrol both the shape and texture deformations from the mean, which makes sense if we believe\nthem to be correlated. Figure 14.22 shows how moving three standard deviations along each\nof the ﬁrst four principal directions ends up changing several correlated factors in a person’s\nappearance, including expression, gender, age, and identity.\nIn order to ﬁt an active appearance model to a novel image, Cootes, Edwards, and Taylor\n(2001) pre-compute a set of “difference decomposition” images, using an approach related to\nother fast techniques for incremental tracking, such as those we discussed in Sections 4.1.4,\n8.1.3, and 8.2 (Gleicher 1997; Hager and Belhumeur 1998), which often learn a discrimi-\nnative mapping between matching errors and incremental displacements (Avidan 2001; Jurie\nand Dhome 2002; Liu, Chen, and Kumar 2003; Sclaroff and Isidoro 2003; Romdhani and\nVetter 2003; Williams, Blake, and Cipolla 2003).\n15 When only the shape variation is being captured, such models are called active shape models (ASMs) (Cootes,\nCooper, Taylor et al. 1995; Davies, Twining, and Taylor 2008). These were already discussed in Section 5.1.1\n(5.13–5.17).",
  "704": "682\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.22 Principal modes of variation in active appearance models (Cootes, Edwards,\nand Taylor 2001) c⃝2001 IEEE. The four images show the effects of simultaneously changing\nthe ﬁrst four modes of variation in both shape and texture by ±σ from the mean. You can\nclearly see how the shape of the face and the shading are simultaneously affected.\nIn more detail, Cootes, Edwards, and Taylor (2001) compute the derivatives of a set of\ntraining images with respect to each of the parameters in a using ﬁnite differences and then\ncompute a set of displacement weight images\nW =\n\u0014∂xT\n∂a\n∂x\n∂a\n\u0015−1 ∂xT\n∂a ,\n(14.31)\nwhich can be multiplied by the current error residual to produce an update step in the pa-\nrameters, δa = −W r. Matthews and Baker (2004) use their inverse compositional method,\nwhich they ﬁrst developed for parametric optical ﬂow (8.64–8.65), to further speed up active\nappearance model ﬁtting and tracking. Examples of AAMs being ﬁtted to two input images\nare shown in Figure 14.23.\nAlthough active appearance models are primarily designed to accurately capture the vari-\nability in appearance and deformation that are characteristic of faces, they can be adapted to\nface recognition by computing an identity subspace that separates variation in identity from\nother sources of variability such as lighting, pose, and expression (Costen, Cootes, Edwards\net al. 1999). The basic idea, which is modeled after similar work in eigenfaces (Belhumeur,\nHespanha, and Kriegman 1997; Moghaddam, Jebara, and Pentland 2000), is to compute sep-\narate statistics for intrapersonal and extrapersonal variation and then ﬁnd discriminating di-\nrections in these subspaces. While AAMs have sometimes been used directly for recognition\n(Blanz and Vetter 2003), their main use in the context of recognition is to align faces into\na canonical pose (Liang, Xiao, Wen et al. 2008) so that more traditional methods of face",
  "705": "14.2 Face recognition\n683\nFigure 14.23\nMultiresolution model ﬁtting (search) in active appearance models (Cootes,\nEdwards, and Taylor 2001) c⃝2001 IEEE. The columns show the initial model, the results\nafter 3, 8, and 11 iterations, and the ﬁnal convergence. The rightmost column shows the input\nimage.\nrecognition (Penev and Atick 1996; Wiskott, Fellous, Kr¨uger et al. 1997; Ahonen, Hadid,\nand Pietik¨ainen 2006; Zhao and Pietik¨ainen 2007; Cao, Yin, Tang et al. 2010) can be used.\nAAMs (or, actually, their simpler version, Active Shape Models (ASMs)) can also be used to\nalign face images to perform automated morphing (Zanella and Fuentes 2004).\nActive appearance models continue to be an active research area, with enhancements to\ndeal with illumination and viewpoint variation (Gross, Baker, Matthews et al. 2005) as well\nas occlusions (Gross, Matthews, and Baker 2006). One of the most signiﬁcant extensions is\nto construct 3D models of shape (Matthews, Xiao, and Baker 2007), which are much better at\ncapturing and explaining the full variability of facial appearance across wide changes in pose.\nFigure 14.24\nHead tracking with 3D AAMs (Matthews, Xiao, and Baker 2007) c⃝2007\nSpringer. Each image shows a video frame along with the estimate yaw, pitch, and roll\nparameters and the ﬁtted 3D deformable mesh.",
  "706": "684\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.25\nPerson detection and re-recognition using a combined face, hair, and torso\nmodel (Sivic, Zitnick, and Szeliski 2006) c⃝2006 Springer. (a) Using face detection alone,\nseveral of the heads are missed. (b) The combined face and clothing model successfully\nre-ﬁnds all the people.\nSuch models can be constructed either from monocular video sequences (Matthews, Xiao,\nand Baker 2007), as shown in Figure 14.24, or from multi-view video sequences (Ramnath,\nKoterba, Xiao et al. 2008), which provide even greater reliability and accuracy in reconstruc-\ntion and tracking. (For a recent review of progress in head pose estimation, please see the\nsurvey paper by Murphy-Chutorian and Trivedi (2009).)\n14.2.3 Application: Personal photo collections\nIn addition to digital cameras automatically ﬁnding faces to aid in auto-focusing and video\ncameras ﬁnding faces in video conferencing to center on the speaker (either mechanically\nor digitally), face detection has found its way into most consumer-level photo organization\npackages, such as iPhoto, Picasa, and Windows Live Photo Gallery. Finding faces and al-\nlowing users to tag them makes it easier to ﬁnd photos of selected people at a later date or to\nautomatically share them with friends. In fact, the ability to tag friends in photos is one of the\nmore popular features on Facebook.\nSometimes, however, faces can be hard to ﬁnd and recognize, especially if they are small,",
  "707": "14.3 Instance recognition\n685\nFigure 14.26 Recognizing objects in a cluttered scene (Lowe 2004) c⃝2004 Springer. Two\nof the training images in the database are shown on the left. They are matched to the cluttered\nscene in the middle using SIFT features, shown as small squares in the right image. The afﬁne\nwarp of each recognized database image onto the scene is shown as a larger parallelogram in\nthe right image.\nturned away from the camera, or otherwise occluded. In such cases, combining face recog-\nnition with person detection and clothes recognition can be very effective, as illustrated in\nFigure 14.25 (Sivic, Zitnick, and Szeliski 2006). Combining person recognition with other\nkinds of context, such as location recognition (Section 14.3.3) or activity or event recognition,\ncan also help boost performance (Lin, Kapoor, Hua et al. 2010).\n14.3 Instance recognition\nGeneral object recognition falls into two broad categories, namely instance recognition and\nclass recognition. The former involves re-recognizing a known 2D or 3D rigid object, poten-\ntially being viewed from a novel viewpoint, against a cluttered background, and with partial\nocclusions. The latter, which is also known as category-level or generic object recognition\n(Ponce, Hebert, Schmid et al. 2006), is the much more challenging problem of recognizing\nany instance of a particular general class such as “cat”, “car”, or “bicycle”.\nOver the years, many different algorithms have been developed for instance recognition.\nMundy (2006) surveys earlier approaches, which focused on extracting lines, contours, or\n3D surfaces from images and matching them to known 3D object models. Another popu-\nlar approach was to acquire images from a large set of viewpoints and illuminations and to\nrepresent them using an eigenspace decomposition (Murase and Nayar 1995). More recent\napproaches (Lowe 2004; Rothganger, Lazebnik, Schmid et al. 2006; Ferrari, Tuytelaars, and\nVan Gool 2006b; Gordon and Lowe 2006; Obdrˇz´alek and Matas 2006; Sivic and Zisserman\n2009) tend to use viewpoint-invariant 2D features, such as those we saw in Section 4.1.2. Af-",
  "708": "686\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nter extracting informative sparse 2D features from both the new image and the images in the\ndatabase, image features are matched against the object database, using one of the sparse fea-\nture matching strategies described in Section 4.1.3. Whenever a sufﬁcient number of matches\nhave been found, they are veriﬁed by ﬁnding a geometric transformation that aligns the two\nsets of features (Figure 14.26).\nBelow, we describe some of the techniques that have been proposed for representing the\ngeometric relationships between such features (Section 14.3.1). We also discuss how to make\nthe feature matching process more efﬁcient using ideas from text and information retrieval\n(Section 14.3.2).\n14.3.1 Geometric alignment\nTo recognize one or more instances of some known objects, such as those shown in the left\ncolumn of Figure 14.26, the recognition system ﬁrst extracts a set of interest points in each\ndatabase image and stores the associated descriptors (and original positions) in an indexing\nstructure such as a search tree (Section 4.1.3). At recognition time, features are extracted\nfrom the new image and compared against the stored object features. Whenever a sufﬁcient\nnumber of matching features (say, three or more) are found for a given object, the system then\ninvokes a match veriﬁcation stage, whose job is to determine whether the spatial arrangement\nof matching features is consistent with those in the database image.\nBecause images can be highly cluttered and similar features may belong to several objects,\nthe original set of feature matches can have a large number of outliers. For this reason, Lowe\n(2004) suggests using a Hough transform (Section 4.3.2) to accumulate votes for likely geo-\nmetric transformations. In his system, he uses an afﬁne transformation between the database\nobject and the collection of scene features, which works well for objects that are mostly pla-\nnar, or where at least several corresponding features share a quasi-planar geometry.16\nSince SIFT features carry with them their own location, scale, and orientation, Lowe uses\na four-dimensional similarity transformation as the original Hough binning structure, i.e.,\neach bin denotes a particular location for the object center, scale, and in-plane rotation. Each\nmatching feature votes for the nearest 24 bins and peaks in the transform are then selected for\na more careful afﬁne motion ﬁt. Figure 14.26 (right image) shows three instances of the two\nobjects on the left that were recognized by the system. Obdrˇz´alek and Matas (2006) general-\nize Lowe’s approach to use feature descriptors with full local afﬁne frames and evaluate their\napproach on a number of object recognition databases.\nAnother system that uses local afﬁne frames is the one developed by Rothganger, Lazeb-\n16 When a larger number of features is available, a full fundamental matrix can be used (Brown and Lowe 2002;\nGordon and Lowe 2006). When image stitching is being performed (Brown and Lowe 2007), the motion models\ndiscussed in Section 9.1 can be used instead.",
  "709": "14.3 Instance recognition\n687\n(a)\n(b)\n(c)\n(d)\nFigure 14.27\n3D object recognition with afﬁne regions (Rothganger, Lazebnik, Schmid et\nal. 2006) c⃝2006 Springer: (a) sample input image; (b) ﬁve of the recognized (reprojected)\nobjects along with their bounding boxes; (c) a few of the local afﬁne regions; (d) local afﬁne\nregion (patch) reprojected into a canonical (square) frame, along with its geometric afﬁne\ntransformations.\nnik, Schmid et al. (2006). In their system, the afﬁne region detector of Mikolajczyk and\nSchmid (2004) is used to rectify local image patches (Figure 14.27d), from which both a\nSIFT descriptor and a 10 × 10 UV color histogram are computed and used for matching\nand recognition. Corresponding patches in different views of the same object, along with\ntheir local afﬁne deformations, are used to compute a 3D afﬁne model for the object using\nan extension of the factorization algorithm of Section 7.3, which can then be upgraded to a\nEuclidean reconstruction (Tomasi and Kanade 1992).\nAt recognition time, local Euclidean neighborhood constraints are used to ﬁlter potential\nmatches, in a manner analogous to the afﬁne geometric constraints used by Lowe (2004) and\nObdrˇz´alek and Matas (2006). Figure 14.27 shows the results of recognizing ﬁve objects in a\ncluttered scene using this approach.\nWhile feature-based approaches are normally used to detect and localize known objects in\nscenes, it is also possible to get pixel-level segmentations of the scene based on such matches.\nFerrari, Tuytelaars, and Van Gool (2006b) describe such a system for simultaneously recog-\nnizing objects and segmenting scenes, while Kannala, Rahtu, Brandt et al. (2008) extend this\napproach to non-rigid deformations. Section 14.4.3 re-visits this topic of joint recognition\nand segmentation in the context of generic class (category) recognition.\n14.3.2 Large databases\nAs the number of objects in the database starts to grow large (say, millions of objects or video\nframes being searched), the time it takes to match a new image against each database image\ncan become prohibitive. Instead of comparing the images one at a time, techniques are needed\nto quickly narrow down the search to a few likely images, which can then be compared using\na more detailed and conservative veriﬁcation stage.",
  "710": "688\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.28\nVisual words obtained from elliptical normalized afﬁne regions (Sivic and\nZisserman 2009) c⃝2009 IEEE. (a) Afﬁne covariant regions are extracted from each frame\nand clustered into visual words using k-means clustering on SIFT descriptors with a learned\nMahalanobis distance. (b) The central patch in each grid shows the query and the surrounding\npatches show the nearest neighbors.\nThe problem of quickly ﬁnding partial matches between documents is one of the cen-\ntral problems in information retrieval (IR) (Baeza-Yates and Ribeiro-Neto 1999; Manning,\nRaghavan, and Sch¨utze 2008). The basic approach in fast document retrieval algorithms is to\npre-compute an inverted index between individual words and the documents (or Web pages\nor news stories) where they occur. More precisely, the frequency of occurrence of particular\nwords in a document is used to quickly ﬁnd documents that match a particular query.\nSivic and Zisserman (2009) were the ﬁrst to adapt IR techniques to visual search. In their\nVideo Google system, afﬁne invariant features are ﬁrst detected in all the video frames they\nare indexing using both shape adapted regions around Harris feature points (Schaffalitzky\nand Zisserman 2002; Mikolajczyk and Schmid 2004) and maximally stable extremal regions\n(Matas, Chum, Urban et al. 2004), (Section 4.1.1), as shown in Figure 14.28a. Next, 128-\ndimensional SIFT descriptors are computed from each normalized region (i.e., the patches\nshown in Figure 14.28b). Then, an average covariance matrix for these descriptors is es-\ntimated by accumulating statistics for features tracked from frame to frame. The feature\ndescriptor covariance Σ is then used to deﬁne a Mahalanobis distance between feature de-\nscriptors,\nd(x0, x1) = ∥x0 −x1∥Σ\n−1 =\nq\n(x0 −x1)T Σ−1(x0 −x1).\n(14.32)\nIn practice, feature descriptors are whitened by pre-multiplying them by Σ−1/2 so that Eu-\nclidean distances can be used.17\nIn order to apply fast information retrieval techniques to images, the high-dimensional\nfeature descriptors that occur in each image must ﬁrst be mapped into discrete visual words.\n17 Note that the computation of feature covariances from matched feature points is much more sensible than simply\nperforming a PCA on the descriptor space (Winder and Brown 2007). This corresponds roughly to the within-class\nscatter matrix (14.17) we studied in Section 14.2.1.",
  "711": "14.3 Instance recognition\n689\n(a)\n(b)\nFigure 14.29 Matching based on visual words (Sivic and Zisserman 2009) c⃝2009 IEEE.\n(a) Features in the query region on the left are matched to corresponding features in a highly\nranked video frame. (b) Results after removing the stop words and ﬁltering the results using\nspatial consistency.\nSivic and Zisserman (2003) perform this mapping using k-means clustering, while some of\nnewer methods discussed below (Nist´er and Stew´enius 2006; Philbin, Chum, Isard et al.\n2007) use alternative techniques, such as vocabulary trees or randomized forests. To keep the\nclustering time manageable, only a few hundred video frames are used to learn the cluster\ncenters, which still involves estimating several thousand clusters from about 300,000 descrip-\ntors. At visual query time, each feature in a new query region (e.g., Figure 14.28a, which is\na cropped region from a larger video frame) is mapped to its corresponding visual word. To\nkeep very common patterns from contaminating the results, a stop list of the most common\nvisual words is created and such words are dropped from further consideration.\nOnce a query image or region has been mapped into its constituent visual words, likely\nmatching images or video frames must then be retrieved from the database. Information\nretrieval systems do this by matching word distributions (term frequencies) nid/nd between\nthe query and target documents, where nid is how many times word i occurs in document d,\nand nd is the total number of words in document d. In order to downweight words that occur\nfrequently and to focus the search on rarer (and hence, more informative) terms, an inverse\ndocument frequency weighting log N/Ni is applied, where Ni is the number of documents\ncontaining word i, and N is the total number of documents in the database. The combination\nof these two factors results in the term frequency-inverse document frequency (tf-idf) measure,\nti = nid\nnd\nlog N\nNi\n.\n(14.33)\nAt match time, each document (or query region) is represented by its tf-idf vector,\nt = (t1, . . . , ti, . . . tm).\n(14.34)\nThe similarity between two documents is measured by the dot product between their corre-\nsponding normalized vectors ˆt = t/∥t∥, which means that their dissimilarity is proportional\nto their Euclidean distance. In their journal paper, Sivic and Zisserman (2009) compare this",
  "712": "690\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n1. Vocabulary construction (off-line)\n(a) Extract afﬁne covariant regions from each database image.\n(b) Compute descriptors and optionally whiten them to make Euclidean dis-\ntances meaningful (Sivic and Zisserman 2009).\n(c) Cluster the descriptors into visual words, either using k-means (Sivic and\nZisserman 2009), hierarchical clustering (Nist´er and Stew´enius 2006), or\nrandomized k-d trees (Philbin, Chum, Isard et al. 2007).\n(d) Decide which words are too common and put them in the stop list.\n2. Database construction (off-line)\n(a) Compute term frequencies for the visual word in each image, document fre-\nquencies for each word, and normalized tf-idf vectors for each document.\n(b) Compute inverted indices from visual words to images (with word counts).\n3. Image retrieval (on-line)\n(a) Extract regions, descriptors, and visual words, and compute a tf-idf vector\nfor the query image or region.\n(b) Retrieve the top image candidates, either by exhaustively comparing sparse\ntf-idf vectors (Sivic and Zisserman 2009) or by using inverted indices to ex-\namine only a subset of the images (Nist´er and Stew´enius 2006).\n(c) Optionally re-rank or verify all the candidate matches, using either spatial\nconsistency (Sivic and Zisserman 2009) or an afﬁne (or simpler) transforma-\ntion model (Philbin, Chum, Isard et al. 2007).\n(d) Optionally expand the answer set by re-submitting highly ranked matches as\nnew queries (Chum, Philbin, Sivic et al. 2007).\nAlgorithm 14.2 Image retrieval using visual words (Sivic and Zisserman 2009; Nist´er and\nStew´enius 2006; Philbin, Chum, Isard et al. 2007; Chum, Philbin, Sivic et al. 2007; Philbin,\nChum, Sivic et al. 2008).",
  "713": "14.3 Instance recognition\n691\nsimple metric to a dozen other metrics and conclude that it performs just about as well as\nmore complicated metrics. Because the number of non-zero ti terms in a typical query or\ndocument is small (M ≈200) compared to the number of visual words (V ≈20, 000), the\ndistance between pairs of (sparse) tf-idf vectors can be computed quite quickly.\nAfter retrieving the top Ns = 500 documents based on word frequencies, Sivic and Zis-\nserman (2009) re-rank these results using spatial consistency. This step involves taking every\nmatching feature and counting the number of k = 15 nearest adjacent features that also match\nbetween the two documents. (This latter process is accelerated using inverted ﬁles, which we\ndiscuss in more detail below.) As shown in Figure 14.29, this step helps remove spurious false\npositive matches and produces a better estimate of which frames and regions in the video are\nactually true matches. Algorithm 14.2 summarizes the processing steps involved in image\nretrieval using visual words.\nWhile this approach works well for tens of thousand of visual words and thousands of\nkeyframes, as the size of the database continues to increase, both the time to quantize each\nfeature and to ﬁnd potential matching frames or images can become prohibitive. Nist´er and\nStew´enius (2006) address this problem by constructing a hierarchical vocabulary tree, where\nfeature vectors are hierarchically clustered into a k-way tree of prototypes. (This technique is\nalso known as tree-structured vector quantization (Gersho and Gray 1991).) At both database\nconstruction time and query time, each descriptor vector is compared to several prototypes\nat a given level in the vocabulary tree and the branch with the closest prototype is selected\nfor further reﬁnement (Figure 14.30). In this way, vocabularies with millions (106) of words\ncan be supported, which enables individual words to be far more discriminative, while only\nrequiring 10 · 6 comparisons for quantizing each descriptor.\nAt query time, each node in the vocabulary tree keeps its own inverted ﬁle index, so that\nfeatures that match a particular node in the tree can be rapidly mapped to potential matching\nimages. (Interior leaf nodes just use the inverted indices of their corresponding leaf-node\ndescendants.) To score a particular query tf-idf vector tq against all document vectors {tj}\nusing an Lp metric,18 the non-zero tiq entries in tq are used to fetch corresponding non-zero\ntij entries, and the Lp norm is efﬁciently computed as\n∥tq −tj∥p\np = 2 +\nX\ni|tiq>0∧tij>0\n(|tiq −tij|p −|tiq|p −|tij|p).\n(14.35)\nIn order to mitigate quantization errors due to noise in the descriptor vectors, Nist´er and\nStew´enius (2006) not only score leaf nodes in the vocabulary tree (corresponding to visual\nwords), but also score interior nodes in the tree, which correspond to clusters of similar visual\nwords.\n18 In their actual implementation, Nist´er and Stew´enius (2006) use an L1 metric.",
  "714": "692\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\nFigure 14.30 Scalable recognition using a vocabulary tree (Nist´er and Stew´enius 2006) c⃝\n2006 IEEE. (a) Each MSER elliptical region is converted into a SIFT descriptor, which is\nthen quantized by comparing it hierarchically to some prototype descriptors in a vocabulary\ntree. Each leaf node stores its own inverted index (sparse list of non-zero tf-idf counts) into\nimages that contain that feature. (b) A recognition result, showing a query image (top row)\nbeing indexed into a database of 6000 test images and correctly ﬁnding the corresponding\nfour images.\nBecause of the high efﬁciency in both quantizing and scoring features, their vocabulary-\ntree-based recognition system is able to process incoming images in real time against a\ndatabase of 40,000 CD covers and at 1Hz when matching a database of one million frames\ntaken from six feature-length movies. Figure 14.30b shows some typical images from the\ndatabase of objects taken under varying viewpoints and illumination that was used to train\nand test the vocabulary tree recognition system.\nThe state of the art in instance recognition continues to improve rapidly. Philbin, Chum,\nIsard et al. (2007) have shown that randomized forest of k-d trees perform better than vocabu-\nlary trees on a large location recognition task (Figure 14.31). They also compare the effects of\nusing different 2D motion models (Section 2.1.2) in the veriﬁcation stage. In follow-on work,\nChum, Philbin, Sivic et al. (2007) apply another idea from information retrieval, namely",
  "715": "14.3 Instance recognition\n693\nFigure 14.31 Location or building recognition using randomized trees (Philbin, Chum, Isard\net al. 2007) c⃝2007 IEEE. The left image is the query, the other images are the highest-ranked\nresults.\nquery expansion, which involves re-submitting top-ranked images from the initial query as\nadditional queries to generate additional candidate results, to further improve recognition\nrates for difﬁcult (occluded or oblique) examples. Philbin, Chum, Sivic et al. (2008) show\nhow to mitigate quantization problems in visual words selection using soft assignment, where\neach feature descriptor is mapped to a number of visual words based on its distance from the\ncluster prototypes. The soft weights derived from these distances are used, in turn, to weight\nthe counts used in the tf-idf vectors and to retrieve additional images for later veriﬁcation.\nTaken together, these recent advances hold the promise of extending current instance recog-\nnition algorithms to performing Web-scale retrieval and matching tasks (Agarwal, Snavely,\nSimon et al. 2009; Agarwal, Furukawa, Snavely et al. 2010; Snavely, Simon, Goesele et al.\n2010).\n14.3.3 Application: Location recognition\nOne of the most exciting applications of instance recognition today is in the area of location\nrecognition, which can be used both in desktop applications (where did I take this holiday\nsnap?) and in mobile (cell-phone) applications. The latter case includes not only ﬁnding out\nyour current location based on a cell-phone image but also providing you with navigation\ndirections or annotating your images with useful information, such as building names and\nrestaurant reviews (i.e., a portable form of augmented reality).\nSome approaches to location recognition assume that the photos consist of architectural\nscenes for which vanishing directions can be used to pre-rectify the images for easier match-\ning (Robertson and Cipolla 2004). Other approaches use general afﬁne covariant interest\npoints to perform wide baseline matching (Schaffalitzky and Zisserman 2002). The Photo\nTourism system of Snavely, Seitz, and Szeliski (2006) (Section 13.1.2) was the ﬁrst to apply\nthese kinds of ideas to large-scale image matching and (implicit) location recognition from",
  "716": "694\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\nFigure 14.32 Feature-based location recognition (Schindler, Brown, and Szeliski 2007) c⃝\n2007 IEEE: (a) three typical series of overlapping street photos; (b) handheld camera shots\nand (c) their corresponding database photos.\nInternet photo collections taken under a wide variety of viewing conditions.\nThe main difﬁculty in location recognition is in dealing with the extremely large commu-\nnity (user-generated) photo collections on Web sites such as Flickr (Philbin, Chum, Isard et\nal. 2007; Chum, Philbin, Sivic et al. 2007; Philbin, Chum, Sivic et al. 2008; Turcot and Lowe\n2009) or commercially captured databases (Schindler, Brown, and Szeliski 2007). The preva-\nlence of commonly appearing elements such as foliage, signs, and common architectural ele-\nments further complicates the task. Figure 14.31 shows some results on location recognition\nfrom community photo collections, while Figure 14.32 shows sample results from denser\ncommercially acquired datasets. In the latter case, the overlap between adjacent database\nimages can be used to verify and prune potential matches using “temporal” ﬁltering, i.e., re-\nquiring the query image to match nearby overlapping database images before accepting the\nmatch.\nAnother variant on location recognition is the automatic discovery of landmarks, i.e.,\nfrequently photographed objects and locations. Simon, Snavely, and Seitz (2007) show how\nthese kinds of objects can be discovered simply by analyzing the matching graph constructed\nas part of the 3D modeling process in Photo Tourism. More recent work has extended this\napproach to larger data sets using efﬁcient clustering techniques (Philbin and Zisserman 2008;\nLi, Wu, Zach et al. 2008; Chum, Philbin, and Zisserman 2008; Chum and Matas 2010) as well\nas combining meta-data such as GPS and textual tags with visual search (Quack, Leibe, and\nVan Gool 2008; Crandall, Backstrom, Huttenlocher et al. 2009), as shown in Figure 14.33.\nIt is now even possible to automatically associate object tags with images based on their co-\noccurrence in multiple loosely tagged images (Simon and Seitz 2008; Gammeter, Bossard,",
  "717": "14.4 Category recognition\n695\nFigure 14.33\nAutomatic mining, annotation, and localization of community photo collec-\ntions (Quack, Leibe, and Van Gool 2008) c⃝2008 ACM. This ﬁgure does not show the textual\nannotations or corresponding Wikipedia entries, which are also discovered.\nA\nB\nC\nD\n(a)\n(b)\nFigure 14.34 Locating star ﬁelds using astrometry, http://astrometry.net/. (a) Input star ﬁeld\nand some selected star quads. (b) The 2D coordinates of stars C and D are encoded relative\nto the unit square deﬁned by A and B.\nQuack et al. 2009).\nThe concept of organizing the world’s photo collections by location has even been re-\ncently extended to organizing all of the universe’s (astronomical) photos in an application\ncalled astrometry, http://astrometry.net/. The technique used to match any two star ﬁelds is\nto take quadruplets of nearby stars (a pair of stars and another pair inside their diameter) to\nform a 30-bit geometric hash by encoding the relative positions of the second pair of points\nusing the inscribed square as the reference frame, as shown in Figure 14.34. Traditional in-\nformation retrieval techniques (k-d trees built for different parts of a sky atlas) are then used\nto ﬁnd matching quads as potential star ﬁeld location hypotheses, which can then be veriﬁed\nusing a similarity transform.",
  "718": "696\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.35 Sample images from the Xerox 10 class dataset (Csurka, Dance, Perronnin et\nal. 2006) c⃝2007 Springer. Imagine trying to write a program to distinguish such images\nfrom other photographs.\n14.4 Category recognition\nWhile instance recognition techniques are relatively mature and are used in commercial ap-\nplications, such as Photosynth (Section 13.1.2), generic category (class) recognition is still\na largely unsolved problem. Consider for example the set of photographs in Figure 14.35,\nwhich shows objects taken from 10 different visual categories. (I’ll leave it up to you to name\neach of the categories.) How would you go about writing a program to categorize each of\nthese images into the appropriate class, especially if you were also given the choice “none of\nthe above”?\nAs you can tell from this example, visual category recognition is an extremely challenging\nproblem; no one has yet constructed a system that approaches the performance level of a two-\nyear-old child. However, the progress in the ﬁeld has been quite dramatic, if judged by how\nmuch better today’s algorithms are compared to those of a decade ago.\nFigure 14.54 shows a sample image from each of the 20 categories used in the 2008\nPASCAL Visual Object Classes Challenge. The yellow boxes represent the extent of each of\nthe objects found in a given image. On such closed world collections where the task is to\ndecide among 20 categories, today’s classiﬁcation algorithms can do remarkably well.",
  "719": "14.4 Category recognition\n697\nFigure 14.36 A typical processing pipeline for a bag-of-words category recognition system\n(Csurka, Dance, Perronnin et al. 2006) c⃝2007 Springer. Features are ﬁrst extracted at\nkeypoints and then quantized to get a distribution (histogram) over the learned visual words\n(feature cluster centers). The feature distribution histogram is used to learn a decision surface\nusing a classiﬁcation algorithm, such as a support vector machine.\nIn this section, we look at a number of approaches to solving category recognition. While\nhistorically, part-based representations and recognition algorithms (Section 14.4.2) were the\npreferred approach (Fischler and Elschlager 1973; Felzenszwalb and Huttenlocher 2005;\nFergus, Perona, and Zisserman 2007), we begin by describing simpler bag-of-features ap-\nproaches (Section 14.4.1) that represent objects and images as unordered collections of fea-\nture descriptors. We then look at the problem of simultaneously segmenting images while\nrecognizing objects (Section 14.4.3) and also present some applications of such techniques to\nphoto manipulation (Section 14.4.4). In Section 14.5, we look at how context and scene un-\nderstanding, as well as machine learning, can improve overall recognition results. Additional\ndetails on the techniques presented in this section can be found in (Pinz 2005; Ponce, Hebert,\nSchmid et al. 2006; Dickinson, Leonardis, Schiele et al. 2007; Fei-Fei, Fergus, and Torralba\n2009).\n14.4.1 Bag of words\nOne of the simplest algorithms for category recognition is the bag of words (also known as\nbag of features or bag of keypoints) approach (Csurka, Dance, Fan et al. 2004; Lazebnik,\nSchmid, and Ponce 2006; Csurka, Dance, Perronnin et al. 2006; Zhang, Marszalek, Lazeb-\nnik et al. 2007). As shown in Figure 14.36, this algorithm simply computes the distribu-\ntion (histogram) of visual words found in the query image and compares this distribution\nto those found in the training images. We have already seen elements of this approach in\nSection 14.3.2, Equations (14.33–14.35) and Algorithm 14.2. The biggest difference from\ninstance recognition is the absence of a geometric veriﬁcation stage (Section 14.3.1), since\nindividual instances of generic visual categories, such as those shown in Figure 14.35, have\nrelatively little spatial coherence to their features (but see the work by Lazebnik, Schmid, and",
  "720": "698\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPonce (2006)).\nCsurka, Dance, Fan et al. (2004) were the ﬁrst to use the term bag of keypoints to describe\nsuch approaches and among the ﬁrst to demonstrate the utility of frequency-based techniques\nfor category recognition. Their original system used afﬁne covariant regions and SIFT de-\nscriptors, k-means visual vocabulary construction, and both a na¨ıve Bayesian classiﬁer and\nsupport vector machines for classiﬁcation. (The latter was found to perform better.) Their\nnewer system (Csurka, Dance, Perronnin et al. 2006) uses regular (non-afﬁne) SIFT patches,\nboosting instead of SVMs, and incorporates a small amount of geometric consistency infor-\nmation.\nZhang, Marszalek, Lazebnik et al. (2007) perform a more detailed study of such bag of\nfeatures systems. They compare a number of feature detectors (Harris–Laplace (Mikolajczyk\nand Schmid 2004) and Laplacian (Lindeberg 1998b)), descriptors (SIFT, RIFT, and SPIN\n(Lazebnik, Schmid, and Ponce 2005)), and SVM kernel functions. To estimate distances for\nthe kernel function, they form an image signature\nS = ((t1, m1), . . . , (tm, mm)),\n(14.36)\nanalogous to the tf-idf vector t in (14.34), where the cluster centers mi are made explicit.\nThey then investigate two different kernels for comparing such image signatures. The ﬁrst is\nthe earth mover’s distance (EMD) (Rubner, Tomasi, and Guibas 2000),\nEMD(S, S′) =\nP\ni\nP\nj fijd(mi, m′\nj)\nP\ni\nP\nj fij\n,\n(14.37)\nwhere fij is a ﬂow value that can be computed using a linear program and d(mi, m′\nj) is the\nground distance (Euclidean distance) between mi and m′\nj. Note that the EMD can be used\nto compare two signatures of different lengths, where the entries do not need to correspond.\nThe second is a χ2 distance\nχ2(S, S′) = 1\n2\nX\ni\n(ti −t′\ni)2\nti + t′\ni\n,\n(14.38)\nwhich measures the likelihood that the two signatures were generated from consistent random\nprocesses. These distance metrics are then converted into SVM kernels using a generalized\nGaussian kernel\nK(S, S′) = exp\n\u0012\n−1\nAD(S, S′)\n\u0013\n,\n(14.39)\nwhere A is a scaling parameter set to the mean distance between training images. In their\nexperiments, they ﬁnd that the EMD works best for visual category recognition and the χ2\nmeasure is best for texture recognition.",
  "721": "14.4 Category recognition\n699\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nlevel 2\nlevel 1\nlevel 0\n\u0001 1/4\n\u0001 1/4\n\u0001 1/2\n+\n+\n+\n(a)\n(b)\nFigure 14.37\nComparing collections of feature vectors using pyramid matching. (a) The\nfeature-space pyramid match kernel (Grauman and Darrell 2007b) constructs a pyramid in\nhigh-dimensional feature space and uses it to compute distances (and implicit correspon-\ndences) between sets of feature vectors. (b) Spatial pyramid matching (Lazebnik, Schmid,\nand Ponce 2006) c⃝2006 IEEE divides the image into a pyramid of pooling regions and\ncomputes separate visual word histograms (distributions) inside each spatial bin.\nInstead of quantizing feature vectors to visual words, Grauman and Darrell (2007b) de-\nvelop a technique for directly computing an approximate distance between two variably sized\ncollections of feature vectors. Their approach is to bin the feature vectors into a multi-\nresolution pyramid deﬁned in feature space (Figure 14.37a) and count the number of features\nthat land in corresponding bins Bil and B′\nil (Figure 14.38a–c). The distance between the two\nsets of feature vectors (which can be thought of as points in a high-dimensional space) is\ncomputed using histogram intersection between corresponding bins\nCl =\nX\ni\nmin(Bil, B′\nil)\n(14.40)\n(Figure 14.38d). These per-level counts are then summed up in a weighted fashion\nD∆=\nX\nl\nwlNl\nwith\nNl = Cl −Cl−1\nand\nwl =\n1\nd2l\n(14.41)\n(Figure 14.38e), which discounts matches already found at ﬁner levels while weighting ﬁner\nmatches more heavily. (d is the dimension of the embedding space, i.e., the length of the\nfeature vectors.) In follow-on work, Grauman and Darrell (2007a) show how an explicit\nconstruction of the pyramid can be avoided using hashing techniques.\nInspired by this work, Lazebnik, Schmid, and Ponce (2006) show how a similar idea\ncan be employed to augment bags of keypoints with loose notions of 2D spatial location",
  "722": "700\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.38\nA one-dimensional illustration of comparing collections of feature vectors\nusing the pyramid match kernel (Grauman and Darrell 2007b): (a) distribution of feature\nvectors (point sets) into the pyramidal bins; (b–c) histogram of point counts in bins Bil and\nB′\nil for the two images; (d) histogram intersections (minimum values); (e) per-level similarity\nscores, which are weighted and summed to form the ﬁnal distance/similarity metric.\nanalogous to the pooling performed by SIFT (Lowe 2004) and “gist” (Torralba, Murphy,\nFreeman et al. 2003). In their work, they extract afﬁne region descriptors (Lazebnik, Schmid,\nand Ponce 2005) and quantize them into visual words. (Based on previous results by Fei-Fei\nand Perona (2005), the feature descriptors are extracted densely (on a regular grid) over the\nimage, which can be helpful in describing textureless regions such as the sky.) They then form\na spatial pyramid of bins containing word counts (histograms), as shown in Figure 14.37b, and\nuse a similar pyramid match kernel to combine histogram intersection counts in a hierarchical\nfashion.\nThe debate about whether to use quantized feature descriptors or continuous descriptors\nand also whether to use sparse or dense features continues to this day. Boiman, Shechtman,\nand Irani (2008) show that if query images are compared to all the features representing a\ngiven class, rather than just each class image individually, nearest-neighbor matching fol-\nlowed by a na¨ıve Bayes classiﬁer outperforms quantized visual words (Figure 14.39). In-\nstead of using generic feature detectors and descriptors, some authors have been investigat-\ning learning class-speciﬁc features (Ferencz, Learned-Miller, and Malik 2008), often using\nrandomized forests (Philbin, Chum, Isard et al. 2007; Moosmann, Nowak, and Jurie 2008;\nShotton, Johnson, and Cipolla 2008) or combining the feature generation and image classi-",
  "723": "14.4 Category recognition\n701\nFigure 14.39\n“Image-to-Image” vs. “Image-to-Class” distance comparison (Boiman,\nShechtman, and Irani 2008) c⃝2008 IEEE. The query image on the upper left may not match\nthe feature distribution of any of the database images in the bottom row. However, if each\nfeature in the query is matched to its closest analog in all the class images, a good match can\nbe found.\nﬁcation stages (Yang, Jin, Sukthankar et al. 2008). Others, such as Serre, Wolf, and Poggio\n(2005) and Mutch and Lowe (2008) use hierarchies of dense feature transforms inspired by\nbiological (visual cortical) processing combined with SVMs for ﬁnal classiﬁcation.\n14.4.2 Part-based models\nRecognizing an object by ﬁnding its constituent parts and measuring their geometric rela-\ntionships is one of the oldest approaches to object recognition (Fischler and Elschlager 1973;\nKanade 1977; Yuille 1991). We have already seen examples of part-based approaches being\nused for face recognition (Figure 14.18) (Moghaddam and Pentland 1997; Heisele, Ho, Wu\net al. 2003; Heisele, Serre, and Poggio 2007) and pedestrian detection (Figure 14.9) (Felzen-\nszwalb, McAllester, and Ramanan 2008).\nIn this section, we look more closely at some of the central issues in part-based recog-\nnition, namely, the representation of geometric relationships, the representation of individ-\nual parts, and algorithms for learning such descriptions and recognizing them at run time.\nMore details on part-based models for recognition can be found in the course notes of Fergus\n(2007b, 2009).\nThe earliest approaches to representing geometric relationships were dubbed pictorial\nstructures by Fischler and Elschlager (1973) and consisted of spring-like connections between\ndifferent feature locations (Figure 14.1a). To ﬁt a pictorial structure to an image, an energy",
  "724": "702\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.40 Using pictorial structures to locate and track a person (Felzenszwalb and Hut-\ntenlocher 2005) c⃝2005 Springer. The structure consists of articulated rectangular body parts\n(torso, head, and limbs) connected in a tree topology that encodes relative part positions and\norientations. To ﬁt a pictorial structure model, a binary silhouette image is ﬁrst computed\nusing background subtraction.\nfunction of the form\nE =\nX\ni\nVi(li) +\nX\nij∈E\nVij(li, lj)\n(14.42)\nis minimized over all potential part locations or poses {li} and pairs of parts (i, j) for which\nan edge (geometric relationship) exists in E. Note how this energy is closely related to\nthat used with Markov random ﬁelds (3.108–3.109), which can be used to embed pictorial\nstructures in a probabilistic framework that makes parameter learning easier (Felzenszwalb\nand Huttenlocher 2005).\nPart-based models can have different topologies for the geometric connections between\nthe parts (Figure 14.41). For example, Felzenszwalb and Huttenlocher (2005) restrict the\nconnections to a tree (Figure 14.41d), which makes learning and inference more tractable. A\ntree topology enables the use of a recursive Viterbi (dynamic programming) algorithm (Pearl\n1988; Bishop 2006), in which leaf nodes are ﬁrst optimized as a function of their parents, and\nthe resulting values are then plugged in and eliminated from the energy function—see Ap-\npendix B.5.2. The Viterbi algorithm computes an optimal match in O(N 2|E| + NP) time,\nwhere N is the number of potential locations or poses for each part, |E| is the number of\nedges (pairwise constraints), and P = |V | is the number of parts (vertices in the graphical\nmodel, which is equal to |E| + 1 in a tree). To further increase the efﬁciency of the infer-\nence algorithm, Felzenszwalb and Huttenlocher (2005) restrict the pairwise energy functions\nVij(li, lj) to be Mahalanobis distances on functions of location variables and then use fast\ndistance transform algorithms to minimize each pairwise interaction in time that is closer to\nlinear in N.\nFigure 14.40 shows the results of using their pictorial structures algorithm to ﬁt an articu-",
  "725": "14.4 Category recognition\n703\nX1\nX2\nX3\nX4\nX5\nX6\nX1\nX2\nX3\nX4\nX5\nX6\nX4\nX5\nX3\nX6\nX2\nX1\nX1\nX2\nX3\nX4\nX5\nX6\n(a)\n(b)\n(c)\n(d)\nX2\nX3\nX4\nX5\nX6\nX1\ng\nh1\nhg\nl1\nl2\nlK\nX1\nX3\nX2\nX5\nX6\nX7\n. . .\n. . .\nCenter\nPart\nSubpart\n. . .\nX1\nX2\nX3\nX4\nX5\nX6\nX1\nX2\nX3\nX4\nX5\nX6\nk=1\nk=2\n(e)\n(f)\n(g)\nFigure 14.41\nGraphical models for geometric spatial priors (Carneiro and Lowe 2006) c⃝\n2006 Springer: (a) constellation (Fergus, Perona, and Zisserman 2007); (b) star (Crandall,\nFelzenszwalb, and Huttenlocher 2005; Fergus, Perona, and Zisserman 2005); (c) k-fan (k =\n2) (Crandall, Felzenszwalb, and Huttenlocher 2005); (d) tree (Felzenszwalb and Huttenlocher\n2005); (e) bag of features (Csurka, Dance, Fan et al. 2004); (f) hierarchy (Bouchard and\nTriggs 2005); (g) sparse ﬂexible model (Carneiro and Lowe 2006).\nlated body model to a binary image obtained by background segmentation. In this application\nof pictorial structures, parts are parameterized by the locations, sizes, and orientations of their\napproximating rectangles. Unary matching potentials Vi(li) are determined by counting the\npercentage of foreground and background pixels inside and just outside the tilted rectangle\nrepresenting each part.\nOver the last decade, a large number of different graphical models have been proposed\nfor part-based recognition, as shown in Figure 14.41. Carneiro and Lowe (2006) discuss\na number of these models and propose one of their own, which they call a sparse ﬂexible\nmodel; it involves ordering the parts and having each part’s location depend on at most k of\nits ancestor locations.\nThe simplest models, which we saw in Section 14.4.1, are bags of words, where there are\nno geometric relationships between different parts or features. While such models can be very\nefﬁcient, they have a very limited capacity to express the spatial arrangement of parts. Trees\nand stars (a special case of trees where all leaf nodes are directly connected to a common root)\nare the most efﬁcient in terms of inference and hence also learning (Felzenszwalb and Hutten-\nlocher 2005; Fergus, Perona, and Zisserman 2005; Felzenszwalb, McAllester, and Ramanan\n2008). Directed acyclic graphs (Figure 14.41f–g) come next in terms of complexity and can\nstill support efﬁcient inference, although at the cost of imposing a causal structure on the",
  "726": "704\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\npart model (Bouchard and Triggs 2005; Carneiro and Lowe 2006). k-fans, in which a clique\nof size k forms the root of a star-shaped model (Figure 14.41c) have inference complexity\nO(N k+1), although with distance transforms and Gaussian priors, this can be lowered to\nO(N k) (Crandall, Felzenszwalb, and Huttenlocher 2005; Crandall and Huttenlocher 2006).\nFinally, fully connected constellation models (Figure 14.41a) are the most general, but the\nassignment of features to parts becomes intractable for moderate numbers of parts P, since\nthe complexity of such an assignment is O(N P ) (Fergus, Perona, and Zisserman 2007).\nThe original constellation model was developed by Burl, Weber, and Perona (1998) and\nconsists of a number of parts whose relative positions are encoded by their mean locations\nand a full covariance matrix, which is used to denote not only positional uncertainty but also\npotential correlations (covariance) between different parts (Figure 14.42a). Weber, Welling,\nand Perona (2000) extended this technique to a weakly supervised setting, where both the\nappearance of each part and its locations are automatically learned given only whole image\nlabels. Fergus, Perona, and Zisserman (2007) further extend this approach to simultaneous\nlearning of appearance and shape models from scale-invariant keypoint detections.\nFigure 14.42a shows the shape model learned for the motorcycle class. The top ﬁgure\nshows the mean relative locations for each part along with their position covariances (inter-\npart covariances are not shown) and likelihood of occurrence. The bottom curve shows the\nGaussian PDFs for the relative log-scale of each part with respect to the “landmark” feature.\nFigure 14.42b shows the appearance model learned for each part, visualized as the patches\naround detected features in the training database that best match the appearance model. Fig-\nure 14.42c shows the features detected in the test database (pink dots) along with the corre-\nsponding parts that they were assigned to (colored circles). As you can see, the system has\nsuccessfully learned and then used a fairly complex model of motorcycle appearance.\nThe part-based approach to recognition has also been extended to learning new categories\nfrom small numbers of examples, building on recognition components developed for other\nclasses (Fei-Fei, Fergus, and Perona 2006). More complex hierarchical part-based models can\nbe developed using the concept of grammars (Bouchard and Triggs 2005; Zhu and Mumford\n2006). A simpler way to use parts is to have keypoints that are recognized as being part of\na class vote for the estimated part locations, as shown in the top row of Figure 14.43 (Leibe,\nLeonardis, and Schiele 2008). (Implicitly, this corresponds to having a star-shaped geometric\nmodel.)\n14.4.3 Recognition with segmentation\nThe most challenging version of generic object recognition is to simultaneously perform\nrecognition with accurate boundary segmentation (Fergus 2007a). For instance recognition\n(Section 14.3.1), this can sometimes be achieved by backprojecting the object model into",
  "727": "14.4 Category recognition\n705\n(a)\n(b)\nCorrect\nCorrect\nINCORRECT\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nCorrect\nINCORRECT\nINCORRECT\nCorrect\nINCORRECT\nCorrect\nCorrect\nCorrect\n(c)\nFigure 14.42\nPart-based recognition (Fergus, Perona, and Zisserman 2007)\nc⃝2007\nSpringer: (a) locations and covariance ellipses for each part, along with their occurrence\nprobabilities (top) and relative log-scale densities (bottom); (b) part examples drawn from\nthe training images that best match the average appearance; (c) recognition results for the\nmotorcycle class, showing detected features (pink dots) and parts (colored circles).",
  "728": "706\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.43\nInterleaved recognition and segmentation (Leibe, Leonardis, and Schiele\n2008) c⃝2008 Springer. The process starts by re-recognizing visual words (codebook en-\ntries) in a new image (scene) and having each part vote for likely locations and size in a\n3D (x, y, s) voting space (top row). Once a maximum has been found, the parts (features)\ncorresponding to this instance are determined by backprojecting the contributing votes. The\nforeground–background segmentation for each object can be found by backprojecting proba-\nbilistic masks associated with each codebook entry. The whole recognition and segmentation\nprocess can then be repeated.\nthe scene (Lowe 2004), as shown in Figure 14.1d, or matching portions of the new scene to\npre-learned (segmented) object models (Ferrari, Tuytelaars, and Van Gool 2006b; Kannala,\nRahtu, Brandt et al. 2008).\nFor more complex (ﬂexible) object models, such as those for humans Figure 14.1f, a\ndifferent approach is to pre-segment the image into larger or smaller pieces (Chapter 5) and\nthen match such pieces to portions of the model (Mori, Ren, Efros et al. 2004; Mori 2005;\nHe, Zemel, and Ray 2006; Gu, Lim, Arbelaez et al. 2009).\nAn alternative approach by Leibe, Leonardis, and Schiele (2008), which we introduced\nin the previous section, votes for potential object locations and scales based on the detec-\ntion of features corresponding to pre-clustered visual codebook entries (Figure 14.43). To\nsupport segmentation, each codebook entry has an associated foreground–background mask,\nwhich is learned as part of the codebook clustering process from pre-labeled object segmen-\ntation masks. During recognition, once a maximum in the voting space is found, the masks\nassociated with the entries that voted for this instance are combined to obtain an object seg-\nmentation, as shown on the left side of Figure 14.43.\nA more holistic approach to recognition and segmentation is to formulate the problem as\none of labeling every pixel in an image with its class membership, and to solve this prob-",
  "729": "14.4 Category recognition\n707\n(a)\n(b)\nFigure 14.44\nSimultaneous recognition and segmentation using TextonBoost (Shotton,\nWinn, Rother et al. 2009) c⃝2009 Springer: (a) successful recognition results; (b) less suc-\ncessful results.",
  "730": "708\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.45 Layout consistent random ﬁeld (Winn and Shotton 2006) c⃝2006 IEEE. The\nnumbers indicate the kind of neighborhood relations that can exist between pixels assigned\nto the same or different classes. Each pairwise relationship carries its own likelihood (energy\npenalty).\nlem using energy minimization or Bayesian inference techniques, i.e., conditional random\nﬁelds (Section 3.7.2, (3.118)) (Kumar and Hebert 2006; He, Zemel, and Carreira-Perpi˜n´an\n2004). The TextonBoost system of Shotton, Winn, Rother et al. (2009) uses unary (pixel-\nwise) potentials based on image-speciﬁc color distributions (Section 5.5) (Boykov and Jolly\n2001; Rother, Kolmogorov, and Blake 2004), location information (e.g., foreground objects\nare more likely to be in the middle of the image, sky is likely to be higher, and road is likely\nto be lower), and novel texture-layout classiﬁers trained using shared boosting. It also uses\ntraditional pairwise potentials that look at image color gradients (Veksler 2001; Boykov and\nJolly 2001; Rother, Kolmogorov, and Blake 2004). The texton-layout features ﬁrst ﬁlter the\nimage with a series of 17 oriented ﬁlter banks and then cluster the responses to classify each\npixel into 30 different texton classes (Malik, Belongie, Leung et al. 2001). The responses\nare then ﬁltered using offset rectangular regions trained with joint boosting (Viola and Jones\n2004) to produce the texton-layout features used as unary potentials.\nFigure 14.44a shows some examples of images successfully labeled and segmented using\nTextonBoost, while Figure 14.44b shows examples where it does not do as well. As you can\nsee, this kind of semantic labeling can be extremely challenging.\nThe TextonBoost conditional random ﬁeld framework has been extended to LayoutCRFs\nby Winn and Shotton (2006), who incorporate additional constraints to recognize multiple\nobject instances and deal with occlusions (Figure 14.45), and even more recently by Hoiem,\nRother, and Winn (2007) to incorporate full 3D models.\nConditional random ﬁelds continue to be widely used and extended for simultaneous\nrecognition and segmentation applications (Kumar and Hebert 2006; He, Zemel, and Ray\n2006; Levin and Weiss 2006; Verbeek and Triggs 2007; Yang, Meer, and Foran 2007; Rabi-\nnovich, Vedaldi, Galleguillos et al. 2007; Batra, Sukthankar, and Chen 2008; Larlus and Jurie",
  "731": "14.4 Category recognition\n709\n(a)\n(b)\n(c)\n(d)\nFigure 14.46\nScene completion using millions of photographs (Hays and Efros 2007) c⃝\n2007 ACM: (a) original image; (b) after unwanted foreground removal; (c) plausible scene\nmatches, with the one the user selected highlighted in red; (d) output image after replacement\nand blending.\n2008; He and Zemel 2008; Kumar, Torr, and Zisserman 2010), producing some of the best\nresults on the difﬁcult PASCAL VOC segmentation challenge (Shotton, Johnson, and Cipolla\n2008; Kohli, Ladick´y, and Torr 2009). Approaches that ﬁrst segment the image into unique\nor multiple segmentations (Borenstein and Ullman 2008; He, Zemel, and Ray 2006; Russell,\nEfros, Sivic et al. 2006) (potentially combined with CRF models) also do quite well: Csurka\nand Perronnin (2008) have one of the top algorithms in the VOC segmentation challenge.\nHierarchical (multi-scale) and grammar (parsing) models are also sometimes used (Tu, Chen,\nYuille et al. 2005; Zhu, Chen, Lin et al. 2008).\n14.4.4 Application: Intelligent photo editing\nRecent advances in object recognition and scene understanding have greatly increased the\npower of intelligent (semi-automated) photo editing applications. One example is the Photo\nClip Art system of Lalonde, Hoiem, Efros et al. (2007), which recognizes and segments\nobjects of interest, such as pedestrians, in Internet photo collections and then allows users to\npaste them into their own photos. Another is the scene completion system of Hays and Efros\n(2007), which tackles the same inpainting problem we studied in Section 10.5. Given an\nimage in which we wish to erase and ﬁll in a large section (Figure 14.46a–b), where do you\nget the pixels to ﬁll in the gaps in the edited image? Traditional approaches either use smooth\ncontinuation (Bertalmio, Sapiro, Caselles et al. 2000) or borrowing pixels from other parts of\nthe image (Efros and Leung 1999; Criminisi, P´erez, and Toyama 2004; Efros and Freeman\n2001). With the advent of huge repositories of images on the Web (a topic we return to in\nSection 14.5.1), it often makes more sense to ﬁnd a different image to serve as the source of\nthe missing pixels.\nIn their system, Hays and Efros (2007) compute the gist of each image (Oliva and Tor-",
  "732": "710\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.47\nAutomatic photo pop-up (Hoiem, Efros, and Hebert 2005a) c⃝2005 ACM:\n(a) input image; (b) superpixels are grouped into (c) multiple regions; (d) labelings indicating\nground (green), vertical (red), and sky (blue); (e) novel view of resulting piecewise-planar 3D\nmodel.\nralba 2001; Torralba, Murphy, Freeman et al. 2003) to ﬁnd images with similar colors and\ncomposition. They then run a graph cut algorithm that minimizes image gradient differences\nand composite the new replacement piece into the original image using Poisson image blend-\ning (Section 9.3.4) (P´erez, Gangnet, and Blake 2003). Figure 14.46d shows the resulting\nimage with the erased foreground rooftops region replaced with sailboats.\nA different application of image recognition and segmentation is to infer 3D structure\nfrom a single photo by recognizing certain scene structures. For example, Criminisi, Reid,\nand Zisserman (2000) detect vanishing points and have the user draw basic structures, such\nas walls, in order infer the 3D geometry (Section 6.3.3). Hoiem, Efros, and Hebert (2005a)\non the other hand, work with more “organic” scenes such as the one shown in Figure 14.47.\nTheir system uses a variety of classiﬁers and statistics learned from labeled images to classify\neach pixel as either ground, vertical, or sky (Figure 14.47d). To do this, they begin by com-\nputing superpixels (Figure 14.47b) and then group them into plausible regions that are likely\nto share similar geometric labels (Figure 14.47c). After all the pixels have been labeled, the\nboundaries between the vertical and ground pixels can be used to infer 3D lines along which\nthe image can be folded into a “pop-up” (after removing the sky pixels), as shown in Fig-\nure 14.47e. In related work, Saxena, Sun, and Ng (2009) develop a system that directly infers\nthe depth and orientation of each pixel instead of using just three geometric class labels.\nFace detection and localization can also be used in a variety of photo editing applications\n(in addition to being used in-camera to provide better focus, exposure, and ﬂash settings).\nZanella and Fuentes (2004) use active shape models (Section 14.2.2) to register facial features\nfor creating automated morphs. Rother, Bordeaux, Hamadi et al. (2006) use face and sky\ndetection to determine regions of interest in order to decide which pieces from a collection\nof images to stitch into a collage. Bitouk, Kumar, Dhillon et al. (2008) describe a system\nthat matches a given face image to a large collection of Internet face images, which can\nthen be used (with careful relighting algorithms) to replace the face in the original image.\nApplications they describe include de-identiﬁcation and getting the best possible smile from",
  "733": "14.4 Category recognition\n711\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 14.48\nThe importance of context (images courtesy of Antonio Torralba). Can you\nname all of the objects in images (a–b), especially those that are circled in (c–d). Look\ncarefully at the circled objects. Did you notice that they all have the same shape (after being\nrotated), as shown in column (e)?\neveryone in a “burst mode” group shot. Leyvand, Cohen-Or, Dror et al. (2008) show how\naccurately locating facial features using an active shape model (Cootes, Edwards, and Taylor\n2001; Zhou, Gu, and Zhang 2003) can be used to warp such features (and hence the image)\ntowards conﬁgurations resembling those found in images whose facial attractiveness was\nhighly rated, thereby “beautifying” the image without completely losing a person’s identity.\nMost of these techniques rely either on a set of labeled training images, which is an\nessential component of all learning techniques, or the even more recent explosion in images\navailable on the Internet. The assumption in some of this work (and in recognition systems\nbased on such very large databases (Section 14.5.1)) is that as the collection of accessible (and\npotentially partially labeled) images gets larger, ﬁnding a close match gets easier. As Hays\nand Efros (2007) state in their abstract “Our chief insight is that while the space of images is\neffectively inﬁnite, the space of semantically differentiable scenes is actually not that large.”\nIn an interesting commentary on their paper, Levoy (2008) disputes this assertion, claiming\nthat “features in natural scenes form a heavy-tailed distribution, meaning that while some\nfeatures in photographs are more common than others, the relative occurrence of less common\nfeatures drops slowly. In other words, there are many unusual photographs in the world.” He\ndoes, however agree that in computational photography, as in many other applications such\nas speech recognition, synthesis, and translation, “simple machine learning algorithms often\noutperform more sophisticated ones if trained on large enough databases.” He also goes on\nto point out both the potential advantages of such systems, such as better automatic color\nbalancing, and potential issues and pitfalls with the kind of image fakery that these new\napproaches enable.\nFor additional examples of photo editing and computational photography applications",
  "734": "712\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFigure 14.49 More examples of context: read the letters in the ﬁrst group, the numbers in\nthe second, and the letters and numbers in the third. (Images courtesy of Antonio Torralba.)\nenabled by Internet computer vision, please see recent workshops on this topic,19 as well as\nthe special journal issue (Avidan, Baker, and Shan 2010), and the course on Internet Vision\nby Tamara Berg (2008).\n14.5 Context and scene understanding\nThus far, we have mostly considered the task of recognizing and localizing objects in isolation\nfrom that of understanding the scene (context) in which the object occur. This is a severe\nlimitation, as context plays a very important role in human object recognition (Oliva and\nTorralba 2007). As we will see in this section, it can greatly improve the performance of\nobject recognition algorithms (Divvala, Hoiem, Hays et al. 2009), as well as providing useful\nsemantic clues for general scene understanding (Torralba 2008).\nConsider the two photographs in Figure 14.48a–b. Can you name all of the objects,\nespecially those circled in images (c–d)? Now have a closer look at the circled objects.\nDo see any similarity in their shapes? In fact, if you rotate them by 90◦, they are all the\nsame as the “blob” shown in Figure 14.48e. So much for our ability to recognize object by\ntheir shape! Another (perhaps more artiﬁcial) example of recognition in context is shown in\nFigure 14.49. Try to name all of the letters and numbers, and then see if you guessed right.\nEven though we have not addressed context explicitly earlier in this chapter, we have\nalready seen several instances of this general idea being used. A simple way to incorporate\nspatial information into a recognition algorithm is to compute feature statistics over different\nregions, as in the spatial pyramid system of Lazebnik, Schmid, and Ponce (2006). Part-based\nmodels (Section 14.4.2, Figures 14.40–14.43), use a kind of local context, where various parts\nneed to be arranged in a proper geometric relationship to constitute an object.\nThe biggest difference between part-based and context models is that the latter combine\nobjects into scenes and the number of constituent objects from each class is not known in\nadvance. In fact, it is possible to combine part-based and context models into the same\n19 http://www.internetvisioner.org/.",
  "735": "14.5 Context and scene understanding\n713\n(a)\n(b)\n(c)\nFigure 14.50 Contextual scene models for object recognition (Sudderth, Torralba, Freeman\net al. 2008) c⃝2008 Springer: (a) some street scenes and their corresponding labels (magenta\n= buildings, red = cars, green = trees, blue = road); (b) some ofﬁce scenes (red = computer\nscreen, green = keyboard, blue = mouse); (c) learned contextual models built from these\nlabeled scenes. The top row shows a sample label image and the distribution of the objects\nrelative to the center red (car or screen) object. The bottom rows show the distributions of\nparts that make up each object.\nrecognition architecture (Murphy, Torralba, and Freeman 2003; Sudderth, Torralba, Freeman\net al. 2008; Crandall and Huttenlocher 2007).\nConsider the street and ofﬁce scenes shown in Figure 14.50a–b. If we have enough train-\ning images with labeled regions, such as buildings, cars, and roads or monitors, keyboards,\nand mice, we can develop a geometric model for describing their relative positions. Sud-\nderth, Torralba, Freeman et al. (2008) develop such a model, which can be thought of as a\ntwo-level constellation model. At the top level, the distributions of objects relative to each\nother (say, buildings with respect to cars) is modeled as a Gaussian (Figure 14.50c, upper\nright corners). At the bottom level, the distribution of parts (afﬁne covariant features) with\nrespect to the object center is modeled using a mixture of Gaussians (Figure 14.50c, lower\ntwo rows). However, since the number of objects in the scene and parts in each object is\nunknown, a latent Dirichlet process (LDP) is used to model object and part creation in a gen-\nerative framework. The distributions for all of the objects and parts are learned from a large\nlabeled database and then later used during inference (recognition) to label the elements of a\nscene.\nAnother example of context is in simultaneous segmentation and recognition (Section 14.4.3)",
  "736": "714\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(Figures 14.44–14.45), where the arrangements of various objects in a scene are used as part\nof the labeling process. Torralba, Murphy, and Freeman (2004) describe a conditional random\nﬁeld where the estimated locations of building and roads inﬂuence the detection of cars, and\nwhere boosting is used to learn the structure of the CRF. Rabinovich, Vedaldi, Galleguillos\net al. (2007) use context to improve the results of CRF segmentation by noting that certain\nadjacencies (relationships) are more likely than others, e.g., a person is more likely to be on\na horse than on a dog.\nContext also plays an important role in 3D inference from single images (Figure 14.47),\nusing computer vision techniques for labeling pixels as belonging to the ground, vertical\nsurfaces, or sky (Hoiem, Efros, and Hebert 2005a,b). This line of work has been extended to\na more holistic approach that simultaneously reasons about object identity, location, surface\norientations, occlusions, and camera viewing parameters (Hoiem, Efros, and Hebert 2008a,b).\nA number of approaches use the gist of a scene (Torralba 2003; Torralba, Murphy, Free-\nman et al. 2003) to determine where instances of particular objects are likely to occur. For\nexample, Murphy, Torralba, and Freeman (2003) train a regressor to predict the vertical loca-\ntions of objects such as pedestrians, cars, and buildings (or screens and keyboard for indoor\nofﬁce scenes) based on the gist of an image. These location distributions are then used with\nclassic object detectors to improve the performance of the detectors. Gists can also be used to\ndirectly match complete images, as we saw in the scene completion work of Hays and Efros\n(2007).\nFinally, some of the most recent work in scene understanding exploits the existence of\nlarge numbers of labeled (or even unlabeled) images to perform matching directly against\nwhole images, where the images themselves implicitly encode the expected relationships\nbetween objects (Figure 14.51) (Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros\n2008). We discuss such techniques in the next section, where we look at the inﬂuence that\nlarge image databases have had on object recognition and scene understanding.\n14.5.1 Learning and large image collections\nGiven how learning techniques are widely used in recognition algorithms, you may wonder\nwhether the topic of learning deserves its own section (or even chapter), or whether it is just\npart of the basic fabric of all recognition tasks. In fact, trying to build a recognition system\nwithout lots of training data for anything other than a basic pattern such as a UPC code has\nproven to be a dismal failure.\nIn this chapter, we have already seen lots of techniques borrowed from the machine learn-\ning, statistics, and pattern recognition communities. These include principal component, sub-\nspace, and discriminant analysis (Section 14.2.1) and more sophisticated discriminative clas-\nsiﬁcation algorithms such as neural networks, support vector machines, and boosting (Sec-",
  "737": "14.5 Context and scene understanding\n715\n(a)\n(b)\n(c)\nFigure 14.51 Recognition by scene alignment (Russell, Torralba, Liu et al. 2007): (a) input\nimage; (b) matched images with similar scene conﬁgurations; (c) ﬁnal labeling of the input\nimage.\ntion 14.1.1). Some of the best-performing techniques on challenging recognition benchmarks\n(Varma and Ray 2007; Felzenszwalb, McAllester, and Ramanan 2008; Fritz and Schiele 2008;\nVedaldi, Gulshan, Varma et al. 2009) rely heavily on the latest machine learning techniques,\nwhose development is often being driven by challenging vision problems (Freeman, Perona,\nand Sch¨olkopf 2008).\nA distinction sometimes made in the recognition community is between problems where\nmost of the variables of interest (say, parts) are already (partially) labeled and systems that\nlearn more of the problem structure with less supervision (Fergus, Perona, and Zisserman\n2007; Fei-Fei, Fergus, and Perona 2006). In fact, recent work by Sivic, Russell, Zisserman et\nal. (2008) has demonstrated the ability to learn visual hierarchies (hierarchies of object parts\nwith related visual appearance) and scene segmentations in a totally unsupervised framework.\nPerhaps the most dramatic change in the recognition community has been the appearance\nof very large databases of training images.20 Early learning-based algorithms, such as those\nfor face and pedestrian detection (Section 14.1), used relatively few (in the hundreds) labeled\nexamples to train recognition algorithm parameters (say, the thresholds used in boosting). To-\nday, some recognition algorithms use databases such as LabelMe (Russell, Torralba, Murphy\net al. 2008), which contain tens of thousands of labeled examples.\nThe existence of such large databases opens up the possibility of matching directly against\nthe training images rather than using them to learn the parameters of recognition algorithms.\nRussell, Torralba, Liu et al. (2007) describe a system where a new image is matched against\neach of the training images, from which a consensus labeling for the unknown objects in\nthe scene can be inferred, as shown in Figure 14.51. Malisiewicz and Efros (2008) start\nby over-segmenting each image and then use the LabelMe database to search for similar\nimages and conﬁgurations in order to obtain per-pixel category labelings. It is also possible\nto combine feature-based correspondence algorithms with large labeled databases to perform\n20 We have already seen some computational photography applications of such databases in Section 14.4.4.",
  "738": "716\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(a)\n(b)\n(c)\n(d)\nFigure 14.52 Recognition using tiny images (Torralba, Freeman, and Fergus 2008) c⃝2008\nIEEE: columns (a) and (c) show sample input images and columns (b) and (d) show the\ncorresponding 16 nearest neighbors in the database of 80 million tiny images.\nsimultaneous recognition and segmentation (Liu, Yuen, and Torralba 2009).\nWhen the database of images becomes large enough, it is even possible to directly match\ncomplete images with the expectation of ﬁnding a good match. Torralba, Freeman, and Fergus\n(2008) start with a database of 80 million tiny (32 × 32) images and compensate for the poor\naccuracy in their image labels, which are collected automatically from the Internet, by using\na semantic taxonomy (Wordnet) to infer the most likely labels for a new image. Somewhere\nin the 80 million images, there are enough examples to associate some set of images with\neach of the 75,000 non-abstract nouns in Wordnet that they use in their system. Some sample\nrecognition results are shown in Figure 14.52.\nAnother example of a large labeled database of images is ImageNet (Deng, Dong, Socher\net al. 2009), which is collecting images for the 80,000 nouns (synonym sets) in WordNet\n(Fellbaum 1998). As of April 2010, about 500–1000 carefully vetted examples for 14841",
  "739": "14.5 Context and scene understanding\n717\nFigure 14.53\nImageNet (Deng, Dong, Socher et al. 2009) c⃝2009 IEEE. This database\ncontains over 500 carefully vetted images for each of 14,841 (as of April, 2010) nouns from\nthe WordNet hierarchy.\nsynsets have been collected (Figure 14.53). The paper by Deng, Dong, Socher et al. (2009)\nalso has a nice review of related databases.\nAs we mentioned in Section 14.4.3, the existence of large databases of partially labeled\nInternet imagery has given rise to a new sub-ﬁeld of Internet computer vision, with its own\nworkshops21 and a special journal issue (Avidan, Baker, and Shan 2010).\n14.5.2 Application: Image search\nEven though visual recognition algorithms are by some measures still in their infancy, they\nare already starting to have some impact on image search, i.e., the retrieval of images from the\nWeb using combinations of keywords and visual similarity. Today, most image search engines\nrely mostly on textual keywords found in captions, nearby text, and ﬁlenames, augmented by\nuser click-through data (Craswell and Szummer 2007). As recognition algorithms continue\nto improve, however, visual features and visual similarity will start being used to recognize\nimages with missing or erroneous keywords.\nThe topic of searching by visual similarity has a long history and goes by a variety of\nnames, including content-based image retrieval (CBIR) (Smeulders, Worring, Santini et al.\n2000; Lew, Sebe, Djeraba et al. 2006; Vasconcelos 2007; Datta, Joshi, Li et al. 2008) and\nquery by image content (QBIC) (Flickner, Sawhney, Niblack et al. 1995). Original publica-\ntions in these ﬁelds were based primarily on simple whole-image similarity metrics, such as\ncolor and texture (Swain and Ballard 1991; Jacobs, Finkelstein, and Salesin 1995; Manjunathi\nand Ma 1996).\n21 http://www.internetvisioner.org/.",
  "740": "718\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn more recent work, Fergus, Perona, and Zisserman (2004) use a feature-based learning\nand recognition algorithm to re-rank the outputs from a traditional keyword-based image\nsearch engine. In follow-on work, Fergus, Fei-Fei, Perona et al. (2005) cluster the results\nreturned by image search using an extension of probabilistic latest semantic analysis (PLSA)\n(Hofmann 1999) and then select the clusters associated with the highest ranked results as the\nrepresentative images for that category.\nEven more recent work relies on carefully annotated image databases such as LabelMe\n(Russell, Torralba, Murphy et al. 2008). For example, Malisiewicz and Efros (2008) describe\na system that, given a query image, can ﬁnd similar LabelMe images, whereas Liu, Yuen, and\nTorralba (2009) combine feature-based correspondence algorithms with the labeled database\nto perform simultaneous recognition and segmentation.\n14.6 Recognition databases and test sets\nIn addition to rapid advances in machine learning and statistical modeling techniques, one\nof the key ingredients in the continued improvement of recognition algorithms has been the\nincreased availability and quality of image recognition databases.\nTables 14.1 and 14.2, which are based on similar tables in Fei-Fei, Fergus, and Torralba\n(2009), updated with more recent entries and URLs, show some of the mostly widely used\nrecognition databases. Some of these databases, such as the ones for face recognition and\nlocalization, date back over a decade. The most recent ones, such as the PASCAL database,\nare refreshed annually with ever more challenging problems. Table 14.1 shows examples of\ndatabases used primarily for (whole image) recognition while Table 14.2 shows databases\nwhere more accurate localization or segmentation information is available and expected.\nPonce, Berg, Everingham et al. (2006) discuss some of the problems with earlier datasets\nand describe how the latest PASCAL Visual Object Classes Challenge aims to overcome\nthese. Some examples of the 20 visual classes in the 2008 challenge are shown in Fig-\nure 14.54. The slides from the VOC workshops,22 are a great source for pointers to the\nbest recognition techniques currently available.\nTwo of the most recent trends in recognition databases are the emergence of Web-based\nannotation and data collection tools, and the use of search and recognition algorithms to build\nup databases (Ponce, Berg, Everingham et al. 2006). Some of the most interesting work in\nhuman annotation of images comes from a series of interactive multi-person games such as\nESP (von Ahn and Dabbish 2004) and Peekaboom (von Ahn, Liu, and Blum 2006). In these\ngames, people help each other guess the identity of a hidden image by giving textual clues\nas to its contents, which implicitly labels either the whole image or just regions. A more\n22 http://pascallin.ecs.soton.ac.uk/challenges/VOC/.",
  "741": "14.6 Recognition databases and test sets\n719\nName / URL\nExtents\nContents / Reference\nFace and person recognition\nYale face database\nCentered face images\nFrontal faces\nhttp://www1.cs.columbia.edu/∼belhumeur/\nBelhumeur, Hespanha, and Kriegman (1997)\nResources for face detection\nVarious databases\nFaces in various poses\nhttp://vision.ai.uiuc.edu/mhyang/face-detection-survey.html\nYang, Kriegman, and Ahuja (2002)\nFERET\nCentered face images\nFrontal faces\nhttp://www.frvt.org/FERET\nPhillips, Moon, Rizvi et al. (2000)\nFRVT\nCentered face images\nFaces in various poses\nhttp://www.frvt.org/\nPhillips, Scruggs, O’Toole et al. (2010)\nCMU PIE database\nCentered face image\nFaces in various poses\nhttp://www.ri.cmu.edu/projects/project 418.html\nSim, Baker, and Bsat (2003)\nCMU Multi-PIE database\nCentered face image\nFaces in various poses\nhttp://multipie.org\nGross, Matthews, Cohn et al. (2010)\nFaces in the Wild\nInternet images\nFaces in various poses\nhttp://vis-www.cs.umass.edu/lfw/\nHuang, Ramesh, Berg et al. (2007)\nConsumer image person DB\nComplete images\nPeople\nhttp://chenlab.ece.cornell.edu/people/Andy/GallagherDataset.html\nGallagher and Chen (2008)\nObject recognition\nCaltech 101\nSegmentation masks\n101 categories\nhttp://www.vision.caltech.edu/Image Datasets/Caltech101/\nFei-Fei, Fergus, and Perona (2006)\nCaltech 256\nCentered objects\n256 categories and clutter\nhttp://www.vision.caltech.edu/Image Datasets/Caltech256/\nGrifﬁn, Holub, and Perona (2007)\nCOIL-100\nCentered objects\n100 instances\nhttp://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php\nNene, Nayar, and Murase (1996)\nETH-80\nCentered objects\n8 instances, 10 views\nhttp://www.mis.tu-darmstadt.de/datasets\nLeibe and Schiele (2003)\nInstance recognition benchmark\nObjects in various poses\n2550 objects\nhttp://vis.uky.edu/∼stewe/ukbench/\nNist´er and Stew´enius (2006)\nOxford buildings dataset\nPictures of buildings\n5062 images\nhttp://www.robots.ox.ac.uk/∼vgg/data/oxbuildings/\nPhilbin, Chum, Isard et al. (2007)\nNORB\nBounding box\n50 toys\nhttp://www.cs.nyu.edu/∼ylclab/data/norb-v1.0/\nLeCun, Huang, and Bottou (2004)\nTiny images\nComplete images\n75,000 (Wordnet) things\nhttp://people.csail.mit.edu/torralba/tinyimages/\nTorralba, Freeman, and Fergus (2008)\nImageNet\nComplete images\n14,000 (Wordnet) things\nhttp://www.image-net.org/\nDeng, Dong, Socher et al. (2009)\nTable 14.1\nImage databases for recognition, adapted and expanded from Fei-Fei, Fergus,\nand Torralba (2009).",
  "742": "720\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nName / URL\nExtents\nContents / Reference\nObject detection / localization\nCMU frontal faces\nPatches\nFrontal faces\nhttp://vasc.ri.cmu.edu/idb/html/face/frontal images\nRowley, Baluja, and Kanade (1998a)\nMIT frontal faces\nPatches\nFrontal faces\nhttp://cbcl.mit.edu/software-datasets/FaceData2.html\nSung and Poggio (1998)\nCMU face detection databases\nMultiple faces\nFaces in various poses\nhttp://www.ri.cmu.edu/research project detail.html?project id=419\nSchneiderman and Kanade (2004)\nUIUC Image DB\nBounding boxes\nCars\nhttp://l2r.cs.uiuc.edu/∼cogcomp/Data/Car/\nAgarwal and Roth (2002)\nCaltech Pedestrian Dataset\nBounding boxes\nPedestrians\nhttp://www.vision.caltech.edu/Image Datasets/CaltechPedestrians/\nDoll`ar, Wojek, Schiele et al. (2009)\nGraz-02 Database\nSegmentation masks\nBikes, cars, people\nhttp://www.emt.tugraz.at/∼pinz/data/GRAZ 02/\nOpelt, Pinz, Fussenegger et al. (2006)\nETHZ Toys\nCluttered images\nToys, boxes, magazines\nhttp://www.vision.ee.ethz.ch/∼calvin/datasets.html\nFerrari, Tuytelaars, and Van Gool (2006b)\nTU Darmstadt DB\nSegmentation masks\nMotorbikes, cars, cows\nhttp://www.vision.ee.ethz.ch/∼bleibe/data/datasets.html\nLeibe, Leonardis, and Schiele (2008)\nMSR Cambridge\nSegmentation masks\n23 classes\nhttp://research.microsoft.com/en-us/projects/objectclassrecognition/\nShotton, Winn, Rother et al. (2009)\nLabelMe dataset\nPolygonal boundary\n>500 categories\nhttp://labelme.csail.mit.edu/\nRussell, Torralba, Murphy et al. (2008)\nLotus Hill\nSegmentation masks\nScenes and hierarchies\nhttp://www.imageparsing.com/\nYao, Yang, Lin et al. (2010)\nOn-line annotation tools\nESP game\nImage descriptions\nWeb images\nhttp://www.gwap.com/gwap/\nvon Ahn and Dabbish (2004)\nPeekaboom\nLabeled regions\nWeb images\nhttp://www.gwap.com/gwap/\nvon Ahn, Liu, and Blum (2006)\nLabelMe\nPolygonal boundary\nHigh-resolution images\nhttp://labelme.csail.mit.edu/\nRussell, Torralba, Murphy et al. (2008)\nCollections of challenges\nPASCAL\nSegmentation, boxes\nVarious\nhttp://pascallin.ecs.soton.ac.uk/challenges/VOC/\nEveringham, Van Gool, Williams et al. (2010)\nTable 14.2 Image databases for detection and localization, adapted and expanded from Fei-\nFei, Fergus, and Torralba (2009).",
  "743": "14.6 Recognition databases and test sets\n721\nairplane\nbicycle\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ndiningtable\ndog\nhorse\nmotorbike\nperson\npottedplant\nsheep\nsofa\ntrain\ntvmonitor\nFigure 14.54\nSample images from the PASCAL Visual Object Classes Challenge 2008\n(VOC2008) database (Everingham, Van Gool, Williams et al. 2008). The original images\nwere obtained from ﬂickr (http://www.ﬂickr.com/) and the database rights are explained on\nhttp://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2008/.\n“serious” volunteer effort is the LabelMe database, in which vision researchers contribute\nmanual polygonal region annotations in return for gaining access to the database (Russell,\nTorralba, Murphy et al. 2008).\nThe use of computer vision algorithms for collecting recognition databases dates back to\nthe work of Fergus, Fei-Fei, Perona et al. (2005), who cluster the results returned by Google\nimage search using an extension of PLSA and then select the clusters associated with the\nhighest ranked results. More recent examples of related techniques include the work of Berg\nand Forsyth (2006) and Li and Fei-Fei (2010).\nWhatever methods are used to collect and validate recognition databases, they will con-\ntinue to grow in size, utility, and difﬁculty from year to year. They will also continue to be\nan essential component of research into the recognition and scene understanding problems,\nwhich remain, as always, the grand challenges of computer vision.",
  "744": "722\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n14.7 Additional reading\nAlthough there are currently no specialized textbooks on image recognition and scene un-\nderstanding, some surveys (Pinz 2005) and collections of papers (Ponce, Hebert, Schmid et\nal. 2006; Dickinson, Leonardis, Schiele et al. 2007) can be found that describe the latest ap-\nproaches. Other good sources of recent research are courses on this topic, such as the ICCV\n2009 short course (Fei-Fei, Fergus, and Torralba 2009) and Antonio Torralba’s more com-\nprehensive MIT course (Torralba 2008). The PASCAL VOC Challenge Web site contains\nworkshop slides that summarize today’s best performing algorithms.\nThe literature on face, pedestrian, car, and other object detection is quite extensive. Sem-\ninal papers in face detection include those by Osuna, Freund, and Girosi (1997), Sung and\nPoggio (1998), Rowley, Baluja, and Kanade (1998a), and Viola and Jones (2004), with Yang,\nKriegman, and Ahuja (2002) providing a comprehensive survey of early work in this ﬁeld.\nMore recent examples include (Heisele, Ho, Wu et al. 2003; Heisele, Serre, and Poggio 2007).\nEarly work in pedestrian and car detection was carried out by Gavrila and Philomin\n(1999), Gavrila (1999), Papageorgiou and Poggio (2000), Mohan, Papageorgiou, and Pog-\ngio (2001), and Schneiderman and Kanade (2004). More recent examples include the work\nof Belongie, Malik, and Puzicha (2002), Mikolajczyk, Schmid, and Zisserman (2004), Dalal\nand Triggs (2005), Leibe, Seemann, and Schiele (2005), Dalal, Triggs, and Schmid (2006),\nOpelt, Pinz, and Zisserman (2006), Torralba (2007), Andriluka, Roth, and Schiele (2008),\nFelzenszwalb, McAllester, and Ramanan (2008), Rogez, Rihan, Ramalingam et al. (2008),\nAndriluka, Roth, and Schiele (2009), Kumar, Zisserman, and H.S.Torr (2009), Doll`ar, Be-\nlongie, and Perona (2010). and Felzenszwalb, Girshick, McAllester et al. (2010).\nWhile some of the earliest approaches to face recognition involved ﬁnding the distinc-\ntive image features and measuring the distances between them (Fischler and Elschlager 1973;\nKanade 1977; Yuille 1991), more recent approaches rely on comparing gray-level images,\noften projected onto lower dimensional subspaces (Turk and Pentland 1991a; Belhumeur,\nHespanha, and Kriegman 1997; Moghaddam and Pentland 1997; Moghaddam, Jebara, and\nPentland 2000; Heisele, Ho, Wu et al. 2003; Heisele, Serre, and Poggio 2007). Additional\ndetails on principal component analysis (PCA) and its Bayesian counterparts can be found in\nAppendix B.1.1 and books and articles on this topic (Hastie, Tibshirani, and Friedman 2001;\nBishop 2006; Roweis 1998; Tipping and Bishop 1999; Leonardis and Bischof 2000; Vidal,\nMa, and Sastry 2010). The topics of subspace learning, local distance functions, and metric\nlearning are covered by Cai, He, Hu et al. (2007), Frome, Singer, Sha et al. (2007), Guil-\nlaumin, Verbeek, and Schmid (2009), Ramanan and Baker (2009), and Sivic, Everingham,\nand Zisserman (2009). An alternative to directly matching gray-level images or patches is to\nuse non-linear local transforms such as local binary patterns (Ahonen, Hadid, and Pietik¨ainen\n2006; Zhao and Pietik¨ainen 2007; Cao, Yin, Tang et al. 2010).",
  "745": "14.7 Additional reading\n723\nIn order to boost the performance of what are essentially 2D appearance-based models,\na variety of shape and pose deformation models have been developed (Beymer 1996; Vet-\nter and Poggio 1997), including Active Shape Models (Lanitis, Taylor, and Cootes 1997;\nCootes, Cooper, Taylor et al. 1995; Davies, Twining, and Taylor 2008), Elastic Bunch Graph\nMatching (Wiskott, Fellous, Kr¨uger et al. 1997), 3D Morphable Models (Blanz and Vetter\n1999), and Active Appearance Models (Costen, Cootes, Edwards et al. 1999; Cootes, Ed-\nwards, and Taylor 2001; Gross, Baker, Matthews et al. 2005; Gross, Matthews, and Baker\n2006; Matthews, Xiao, and Baker 2007; Liang, Xiao, Wen et al. 2008; Ramnath, Koterba,\nXiao et al. 2008). The topic of head pose estimation, in particular, is covered in a recent\nsurvey by Murphy-Chutorian and Trivedi (2009).\nAdditional information about face recognition can be found in a number of surveys and\nbooks on this topic (Chellappa, Wilson, and Sirohey 1995; Zhao, Chellappa, Phillips et al.\n2003; Li and Jain 2005) as well as on the Face Recognition Web site.23 Databases for face\nrecognition are discussed by Phillips, Moon, Rizvi et al. (2000), Sim, Baker, and Bsat (2003),\nGross, Shi, and Cohn (2005), Huang, Ramesh, Berg et al. (2007), and Phillips, Scruggs,\nO’Toole et al. (2010).\nAlgorithms for instance recognition, i.e., the detection of static man-made objects that\nonly vary slightly in appearance but may vary in 3D pose, are mostly based on detecting\n2D points of interest and describing them using viewpoint-invariant descriptors (Lowe 2004;\nRothganger, Lazebnik, Schmid et al. 2006; Ferrari, Tuytelaars, and Van Gool 2006b; Gordon\nand Lowe 2006; Obdrˇz´alek and Matas 2006; Kannala, Rahtu, Brandt et al. 2008; Sivic and\nZisserman 2009).\nAs the size of the database being matched increases, it becomes more efﬁcient to quantize\nthe visual descriptors into words (Sivic and Zisserman 2003; Schindler, Brown, and Szeliski\n2007; Sivic and Zisserman 2009; Turcot and Lowe 2009), and to then use information-\nretrieval techniques, such as inverted indices (Nist´er and Stew´enius 2006; Philbin, Chum,\nIsard et al. 2007; Philbin, Chum, Sivic et al. 2008), query expansion (Chum, Philbin, Sivic\net al. 2007; Agarwal, Snavely, Simon et al. 2009), and min hashing (Philbin and Zisserman\n2008; Li, Wu, Zach et al. 2008; Chum, Philbin, and Zisserman 2008; Chum and Matas 2010)\nto perform efﬁcient retrieval and clustering.\nA number of surveys, collections of papers, and course notes have been written on the\ntopic of category recognition (Pinz 2005; Ponce, Hebert, Schmid et al. 2006; Dickinson,\nLeonardis, Schiele et al. 2007; Fei-Fei, Fergus, and Torralba 2009). Some of the seminal\npapers on the bag of words (bag of keypoints) approach to whole-image category recognition\nhave been written by Csurka, Dance, Fan et al. (2004), Lazebnik, Schmid, and Ponce (2006),\nCsurka, Dance, Perronnin et al. (2006), Grauman and Darrell (2007b), and Zhang, Marszalek,\n23 http://www.face-rec.org/.",
  "746": "724\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLazebnik et al. (2007). Additional and more recent papers in this area include Sivic, Russell,\nEfros et al. (2005), Serre, Wolf, and Poggio (2005), Opelt, Pinz, Fussenegger et al. (2006),\nGrauman and Darrell (2007a), Torralba, Murphy, and Freeman (2007), Boiman, Shechtman,\nand Irani (2008), Ferencz, Learned-Miller, and Malik (2008), and Mutch and Lowe (2008).\nIt is also possible to recognize objects based on their contours, e.g., using shape contexts\n(Belongie, Malik, and Puzicha 2002) or other techniques (Jurie and Schmid 2004; Shotton,\nBlake, and Cipolla 2005; Opelt, Pinz, and Zisserman 2006; Ferrari, Tuytelaars, and Van Gool\n2006a).\nMany object recognition algorithms use part-based decompositions to provide greater in-\nvariance to articulation and pose. Early algorithms focused on the relative positions of the\nparts (Fischler and Elschlager 1973; Kanade 1977; Yuille 1991) while newer algorithms use\nmore sophisticated models of appearance (Felzenszwalb and Huttenlocher 2005; Fergus, Per-\nona, and Zisserman 2007; Felzenszwalb, McAllester, and Ramanan 2008). Good overviews\non part-based models for recognition can be found in the course notes of Fergus 2007b; 2009.\nCarneiro and Lowe (2006) discuss a number of graphical models used for part-based\nrecognition, which include trees and stars (Felzenszwalb and Huttenlocher 2005; Fergus, Per-\nona, and Zisserman 2005; Felzenszwalb, McAllester, and Ramanan 2008), k-fans (Crandall,\nFelzenszwalb, and Huttenlocher 2005; Crandall and Huttenlocher 2006), and constellations\n(Burl, Weber, and Perona 1998; Weber, Welling, and Perona 2000; Fergus, Perona, and Zis-\nserman 2007). Other techniques that use part-based recognition include those developed by\nDork´o and Schmid (2003) and Bar-Hillel, Hertz, and Weinshall (2005).\nCombining object recognition with scene segmentation can yield strong beneﬁts. One\napproach is to pre-segment the image into pieces and then match the pieces to portions of\nthe model (Mori, Ren, Efros et al. 2004; Mori 2005; He, Zemel, and Ray 2006; Russell,\nEfros, Sivic et al. 2006; Borenstein and Ullman 2008; Csurka and Perronnin 2008; Gu, Lim,\nArbelaez et al. 2009). Another is to vote for potential object locations and scales based on\nobject detection (Leibe, Leonardis, and Schiele 2008). One of the currently most popular\napproaches is to use conditional random ﬁelds (Kumar and Hebert 2006; He, Zemel, and\nCarreira-Perpi˜n´an 2004; He, Zemel, and Ray 2006; Levin and Weiss 2006; Winn and Shotton\n2006; Hoiem, Rother, and Winn 2007; Rabinovich, Vedaldi, Galleguillos et al. 2007; Verbeek\nand Triggs 2007; Yang, Meer, and Foran 2007; Batra, Sukthankar, and Chen 2008; Larlus\nand Jurie 2008; He and Zemel 2008; Shotton, Winn, Rother et al. 2009; Kumar, Torr, and\nZisserman 2010), which produce some of the best results on the difﬁcult PASCAL VOC seg-\nmentation challenge (Shotton, Johnson, and Cipolla 2008; Kohli, Ladick´y, and Torr 2009).\nMore and more recognition algorithms are starting to use scene context as part of their\nrecognition strategy. Representative papers in this area include those by Torralba (2003),\nTorralba, Murphy, Freeman et al. (2003), Murphy, Torralba, and Freeman (2003), Torralba,\nMurphy, and Freeman (2004), Crandall and Huttenlocher (2007), Rabinovich, Vedaldi, Gal-",
  "747": "14.8 Exercises\n725\nleguillos et al. (2007), Russell, Torralba, Liu et al. (2007), Hoiem, Efros, and Hebert (2008a),\nHoiem, Efros, and Hebert (2008b), Sudderth, Torralba, Freeman et al. (2008), and Divvala,\nHoiem, Hays et al. (2009).\nSophisticated machine learning techniques are also becoming a key component of suc-\ncessful object detection and recognition algorithms (Varma and Ray 2007; Felzenszwalb,\nMcAllester, and Ramanan 2008; Fritz and Schiele 2008; Sivic, Russell, Zisserman et al.\n2008; Vedaldi, Gulshan, Varma et al. 2009), as is exploiting large human-labeled databases\n(Russell, Torralba, Liu et al. 2007; Malisiewicz and Efros 2008; Torralba, Freeman, and Fer-\ngus 2008; Liu, Yuen, and Torralba 2009). Rough three-dimensional models are also making\na comeback for recognition, as evidenced in some recent papers (Savarese and Fei-Fei 2007,\n2008; Sun, Su, Savarese et al. 2009; Su, Sun, Fei-Fei et al. 2009). As always, the latest con-\nferences on computer vision are your best reference for the newest algorithms in this rapidly\nevolving ﬁeld.\n14.8 Exercises\nEx 14.1: Face detection\nBuild and test one of the face detectors presented in Section 14.1.1.\n1. Download one or more of the labeled face detection databases in Table 14.2.\n2. Generate your own negative examples by ﬁnding photographs that do not contain any\npeople.\n3. Implement one of the following face detectors (or devise one of your own):\n• boosting (Algorithm 14.1) based on simple area features, with an optional cascade\nof detectors (Viola and Jones 2004);\n• PCA face subspace (Moghaddam and Pentland 1997);\n• distances to clustered face and non-face prototypes, followed by a neural network\n(Sung and Poggio 1998) or SVM (Osuna, Freund, and Girosi 1997) classiﬁer;\n• a multi-resolution neural network trained directly on normalized gray-level patches\n(Rowley, Baluja, and Kanade 1998a).\n4. Test the performance of your detector on the database by evaluating the detector at ev-\nery location in a sub-octave pyramid. Optionally retrain your detector on false positive\nexamples you get on non-face images.\nEx 14.2: Determining the threshold for AdaBoost\nGiven a set of function evaluations on\nthe training examples xi, fi = f(xi) ∈±1, training labels yi ∈±1, and weights wi ∈(0, 1),",
  "748": "726\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nas explained in Algorithm 14.1, devise an efﬁcient algorithm to ﬁnd values of θ and s = ±1\nthat maximize\nX\ni\nwiyih(sfi, θ),\n(14.43)\nwhere h(x, θ) = sign(x −θ).\nEx 14.3: Face recognition using eigenfaces\nCollect a set of facial photographs and then\nbuild a recognition system to re-recognize the same people.\n1. Take several photos of each of your classmates and store them.\n2. Align the images by automatically or manually detecting the corners of the eyes and\nusing a similarity transform to stretch and rotate each image to a canonical position.\n3. Compute the average image and a PCA subspace for the face images\n4. Take a new set of photographs a week later and use them as your test set.\n5. Compare each new image to each database image and select the nearest one as the\nrecognized identity. Verify that the distance in PCA space is close to the distance\ncomputed with a full SSD (sum of squared difference) measure.\n6. (Optional) Compute different principal components for identity and expression, and\nuse them to improve your recognition results.\nEx 14.4: Bayesian face recognition\nMoghaddam, Jebara, and Pentland (2000) compute\nseparate covariance matrices ΣI and ΣE by looking at differences between all pairs of im-\nages. At run time, they select the nearest image to determine the facial identity. Does it make\nsense to estimate statistics for all pairs of images and use them for testing the distance to the\nnearest exemplar? Discuss whether this is statistically correct.\nHow is the all-pair intrapersonal covariance matrix ΣI related to the within-class scatter\nmatrix SW? Does a similar relationship hold between ΣE and SB?\nEx 14.5: Modular eigenfaces\nExtend your face recognition system to separately match the\neye, nose, and mouth regions, as shown in Figure 14.18.\n1. After normalizing face images to a canonical scale and location, manually segment out\nsome of the eye, nose, and face regions.\n2. Build separate detectors for these three (or four) kinds of region, either using a subspace\n(PCA) approach or one of the techniques presented in Section 14.1.1.\n3. For each new image to be recognized, ﬁrst detect the locations of the facial features.",
  "749": "14.8 Exercises\n727\n4. Then, match the individual features against your database and note the locations of\nthese features.\n5. Train and test a classiﬁer that uses the individual feature matching IDs as well as (op-\ntionally) the feature locations to perform face recognition.\nEx 14.6: Recognition-based color balancing\nBuild a system that recognizes the most im-\nportant color areas in common photographs (sky, grass, skin) and color balances the image\naccordingly. Some references and ideas for skin detection are given in Exercise 2.8 and\nby Forsyth and Fleck (1999), Jones and Rehg (2001), Vezhnevets, Sazonov, and Andreeva\n(2003), and Kakumanu, Makrogiannis, and Bourbakis (2007). These may give you ideas\nfor how to detect other regions or you can try more sophisticated MRF-based approaches\n(Shotton, Winn, Rother et al. 2009).\nEx 14.7: Pedestrian detection\nBuild and test one of the pedestrian detectors presented in\nSection 14.1.2.\nEx 14.8: Simple instance recognition\nUse the feature detection, matching, and alignment\nalgorithms you developed in Exercises 4.1–4.4 and 9.2 to ﬁnd matching images given a query\nimage or region (Figure 14.26).\nEvaluate several feature detectors, descriptors, and robust geometric veriﬁcation strate-\ngies, either on your own or by comparing your results with those of classmates.\nEx 14.9: Large databases and location recognition\nExtend the previous exercise to larger\ndatabases using quantized visual words and information retrieval techniques, as described in\nAlgorithm 14.2.\nTest your algorithm on a large database, such as the one used by Nist´er and Stew´enius\n(2006) or Philbin, Chum, Sivic et al. (2008), which are listed in Table 14.1. Alternatively,\nuse keyword search on the Web or in a photo sharing site (e.g., for a city) to create your own\ndatabase.\nEx 14.10: Bag of words\nAdapt the feature extraction and matching pipeline developed in\nExercise 14.8 to category (class) recognition, using some of the techniques described in Sec-\ntion 14.4.1.\n1. Download the training and test images from one or more of the databases listed in\nTables 14.1 and 14.2, e.g., Caltech 101, Caltech 256, or PASCAL VOC.\n2. Extract features from each of the training images, quantize them, and compute the tf-idf\nvectors (bag of words histograms).",
  "750": "728\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n3. As an option, consider not quantizing the features and using pyramid matching (14.40–\n14.41) (Grauman and Darrell 2007b) or using a spatial pyramid for greater selectivity\n(Lazebnik, Schmid, and Ponce 2006).\n4. Choose a classiﬁcation algorithm (e.g., nearest neighbor classiﬁcation or support vector\nmachine) and “train” your recognizer, i.e., build up the appropriate data structures (e.g.,\nk-d trees) or set the appropriate classiﬁer parameters.\n5. Test your algorithm on the test data set using the same pipeline you developed in steps\n2–4 and compare your results to the best reported results.\n6. Explain why your results differ from the previously reported ones and give some ideas\nfor how you could improve your system.\nYou can ﬁnd a good synopsis of the best-performing classiﬁcation algorithms and their ap-\nproaches in the report of the PASCAL Visual Object Classes Challenge found on their Web\nsite (http://pascallin.ecs.soton.ac.uk/challenges/VOC/).\nEx 14.11: Object detection and localization\nExtend the classiﬁcation algorithm developed\nin the previous exercise to localize the objects in an image by reporting a bounding box around\neach detected object. The easiest way to do this is to use a sliding window approach. Some\npointers to recent techniques in this area can be found in the workshop associated with the\nPASCAL VOC 2008 Challenge.\nEx 14.12: Part-based recognition\nChoose one or more of the techniques described in Sec-\ntion 14.4.2 and implement a part-based recognition system. Since these techniques are fairly\ninvolved, you will need to read several of the research papers in this area, select which gen-\neral approach you want to follow, and then implement your algorithm. A good starting point\ncould be the paper by Felzenszwalb, McAllester, and Ramanan (2008), since it performed\nwell in the PASCAL VOC 2008 detection challenge.\nEx 14.13: Recognition and segmentation\nChoose one or more of the techniques described\nin Section 14.4.3 and implement a simultaneous recognition and segmentation system. Since\nthese techniques are fairly involved, you will need to read several of the research papers in this\narea, select which general approach you want to follow, and then implement your algorithm.\nTest your algorithm on one or more of the segmentation databases in Table 14.2.\nEx 14.14: Context\nImplement one or more of the context and scene understanding sys-\ntems described in Section 14.5 and report on your experience. Does context or whole scene\nunderstanding perform better at naming objects than stand-alone systems?",
  "751": "14.8 Exercises\n729\nEx 14.15: Tiny images\nDownload the tiny images database from http://people.csail.mit.\nedu/torralba/tinyimages/ and build a classiﬁer based on comparing your test images directly\nagainst all of the labeled training images. Does this seem like a promising approach?",
  "752": "730\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "753": "Chapter 15\nConclusion\nIn this book, we have covered a broad range of computer vision topics. Starting with image\nformation, we have seen how images can be pre-processed to remove noise or blur, segmented\ninto regions, or converted into feature descriptors. Multiple images can be matched and\nregistered, with the results used to estimate motion, track people, reconstruct 3D models,\nor merge images into more attractive and interesting composites and renderings. Images can\nalso be analyzed to produce semantic descriptions of their content. However, the gap between\ncomputer and human performance in this area is still large and is likely to remain so for many\nyears.\nOur study has also exposed us to a wide range of mathematical techniques. These include\ncontinuous mathematics, such as signal processing, variational approaches, three-dimensional\nand projective geometry, linear algebra, and least squares. We have also studied topics in\ndiscrete mathematics and computer science, such as graph algorithms, combinatorial opti-\nmization, and even database techniques for information retrieval. Since many problems in\ncomputer vision are inverse problems that involve estimating unknown quantities from noisy\ninput data, we have also looked at Bayesian statistical inference techniques, as well as ma-\nchine learning techniques to learn probabilistic models from large amounts of training data.\nAs the availability of partially labeled visual imagery on the Internet continues to increase\nexponentially, this latter approach will continue to have a major impact on our ﬁeld.\nYou may ask: why is our ﬁeld so broad and aren’t there any unifying principles that can\nbe used to simplify our study? Part of the answer lies in the expansive deﬁnition of com-\nputer vision, which is the analysis of images and video, as well as the incredible complexity\ninherent in the formation of visual imagery. In some ways, our ﬁeld is as complex as the\nstudy of automotive engineering, which requires an understanding of internal combustion,\nmechanics, aerodynamics, ergonomics, electrical circuitry, and control systems, among other",
  "754": "732\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ntopics. Computer vision similarly draws on a wide variety of sub-disciplines, which makes it\nchallenging to cover in a one-semester course, let alone to achieve mastery during a course\nof graduate studies. Conversely, the incredible breadth and technical complexity of computer\nvision problems is what draws many people to this research ﬁeld.\nBecause of this richness and the difﬁculty in making and measuring progress, I have at-\ntempted to instill in my students and in readers of this book a discipline founded on principles\nfrom engineering, science, and statistics.\nThe engineering approach to problem solving is to ﬁrst carefully deﬁne the overall prob-\nlem being tackled and to question the basic assumptions and goals inherent in this process.\nOnce this has been done, a number of alternative solutions or approaches are implemented\nand carefully tested, paying attention to issues such as reliability and computational cost.\nFinally, one or more solutions are deployed and evaluated in real-world settings. For this\nreason, this book contains many different alternatives for solving vision problems, many of\nwhich are sketched out in the exercises for students to implement and test on their own.\nThe scientiﬁc approach builds upon a basic understanding of physical principles. In the\ncase of computer vision, this includes the physics of man-made and natural structures, image\nformation, including lighting and atmospheric effects, optics, and noisy sensors. The task is to\nthen invert this formation using stable and efﬁcient algorithms to obtain reliable descriptions\nof the scene and other quantities of interest. The scientiﬁc approach also encourages us to\nformulate and test hypotheses, which is similar to the extensive testing and evaluation inherent\nin engineering disciplines.\nLastly, because so much about the image formation process is inherently uncertain and\nambiguous, a statistical approach that models both uncertainty in the world (e.g., the number\nand types of animals in a picture) and noise in the image formation process, is often essential.\nBayesian inference techniques can then be used to combine prior and measurement models\nto estimate the unknowns and to model their uncertainty. Machine learning techniques can\nbe used to create the probabilistic models in the ﬁrst place. Efﬁcient learning and inference\nalgorithms, such as dynamic programming, graph cuts, and belief propagation, often play a\ncrucial role in this process.\nGiven the breadth of material we have covered in this book, what new developments are\nwe likely to see in the future? As I have mentioned before, one of the recent trends in com-\nputer vision is using the massive amounts of partially labeled visual data on the Internet as\nsources for learning visual models of scenes and objects. We have already seen data-driven\napproaches succeed in related ﬁelds such as speech recognition, machine translation, speech\nand music synthesis, and even computer graphics (both in image-based rendering and anima-\ntion from motion capture). A similar process has been occurring in computer vision, with\nsome of the most exciting new work occurring at the intersection of the object recognition\nand machine learning ﬁelds.",
  "755": "15 Conclusion\n733\nMore traditional quantitative techniques in computer vision such as motion estimation,\nstereo correspondence, and image enhancement, all beneﬁt from better prior models for im-\nages, motions, and disparities, as well as efﬁcient statistical inference techniques such as\nthose for inhomogeneous and higher-order Markov random ﬁelds. Some techniques, such as\nfeature matching and structure from motion, have matured to where they can be applied to\nalmost arbitrary collections of images of static scenes. This has resulted in an explosion of\nwork in 3D modeling from Internet datasets, which again is related to visual recognition from\nmassive amounts of data.\nWhile these are all encouraging developments, the gap between human and machine per-\nformance in semantic scene understanding remains large. It may be many years before com-\nputers can name and outline all of the objects in a photograph with the same skill as a two-\nyear-old child. However, we have to remember that human performance is often the result of\nmany years of training and familiarity and often works best in special ecologically important\nsituations. For example, while humans appear to be experts at face recognition, our actual\nperformance when shown people we do not know well is not that good. Combining vision\nalgorithms with general inference techniques that reason about the real world will likely lead\nto more breakthroughs, although some of the problems may turn out to be “AI-complete”, in\nthe sense that a full emulation of human experience and intelligence may be necessary.\nWhatever the outcome of these research endeavors, computer vision is already having\na tremendous impact in many areas, including digital photography, visual effects, medical\nimaging, safety and surveillance, and Web-based search. The breadth of the problems and\ntechniques inherent in this ﬁeld, combined with the richness of the mathematics and the\nutility of the resulting algorithms, will ensure that this remains an exciting area of study for\nyears to come.",
  "756": "734\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "757": "Appendix A\nLinear algebra and numerical\ntechniques\nA.1\nMatrix decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 736\nA.1.1\nSingular value decomposition\n. . . . . . . . . . . . . . . . . . . . . 736\nA.1.2\nEigenvalue decomposition . . . . . . . . . . . . . . . . . . . . . . . 737\nA.1.3\nQR factorization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 740\nA.1.4\nCholesky factorization . . . . . . . . . . . . . . . . . . . . . . . . . 741\nA.2\nLinear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742\nA.2.1\nTotal least squares\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 744\nA.3\nNon-linear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 746\nA.4\nDirect sparse matrix techniques . . . . . . . . . . . . . . . . . . . . . . . . . 747\nA.4.1\nVariable reordering . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\nA.5\nIterative techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\nA.5.1\nConjugate gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . 749\nA.5.2\nPreconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 751\nA.5.3\nMultigrid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753",
  "758": "736\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn this appendix, we introduce some elements of linear algebra and numerical techniques that\nare used elsewhere in the book. We start with some basic decompositions in matrix algebra,\nincluding the singular value decomposition (SVD), eigenvalue decompositions, and other\nmatrix decompositions (factorizations). Next, we look at the problem of linear least squares,\nwhich can be solved using either the QR decomposition or normal equations. This is followed\nby non-linear least squares, which arise when the measurement equations are not linear in the\nunknowns or when robust error functions are used. Such problems require iteration to ﬁnd\na solution. Next, we look at direct solution (factorization) techniques for sparse problems,\nwhere the ordering of the variables can have a large inﬂuence on the computation and memory\nrequirements. Finally, we discuss iterative techniques for solving large linear (or linearized)\nleast squares problems. Good general references for much of this material include the work\nby Bj¨orck (1996), Golub and Van Loan (1996), Trefethen and Bau (1997), Meyer (2000),\nNocedal and Wright (2006), and Bj¨orck and Dahlquist (2010).\nA note on vector and matrix indexing.\nTo be consistent with the rest of the book and\nwith the general usage in the computer science and computer vision communities, I adopt\na 0-based indexing scheme for vector and matrix element indexing. Please note that most\nmathematical textbooks and papers use 1-based indexing, so you need to be aware of the\ndifferences when you read this book.\nSoftware implementations.\nHighly optimized and tested libraries corresponding to the al-\ngorithms described in this appendix are readily available and are listed in Appendix C.2.\nA.1 Matrix decompositions\nIn order to better understand the structure of matrices and more stably perform operations\nsuch as inversion and system solving, a number of decompositions (or factorizations) can be\nused. In this section, we review singular value decomposition (SVD), eigenvalue decomposi-\ntion, QR factorization, and Cholesky factorization.\nA.1.1 Singular value decomposition\nOne of the most useful decompositions in matrix algebra is the singular value decomposition\n(SVD), which states that any real-valued M × N matrix A can be written as\nAM×N\n=\nU M×P ΣP ×P V T\nP ×N\n(A.1)",
  "759": "A.1 Matrix decompositions\n737\n=\n\nu0\n· · ·\nup−1\n\n\n\n\nσ0\n...\nσp−1\n\n\n\n\nvT\n0\n· · ·\nvT\np−1\n\n,\nwhere P = min(M, N). The matrices U and V are orthonormal, i.e., U T U = I and\nV T V = I, and so are their column vectors,\nui · uj = vi · vj = δij.\n(A.2)\nThe singular values are all non-negative and can be ordered in decreasing order\nσ0 ≥σ1 ≥· · · ≥σp−1 ≥0.\n(A.3)\nA geometric intuition for the SVD of a matrix A can be obtained by re-writing A =\nUΣV T in (A.2) as\nAV = UΣ\nor\nAvj = σjuj.\n(A.4)\nThis formula says that the matrix A takes any basis vector vj and maps it to a direction uj\nwith length σj, as shown in Figure A.1\nIf only the ﬁrst r singular values are positive, the matrix A is of rank r and the index p\nin the SVD decomposition (A.2) can be replaced by r. (In other words, we can drop the last\np −r columns of U and V .)\nAn important property of the singular value decomposition of a matrix (also true for\nthe eigenvalue decomposition of a real symmetric non-negative deﬁnite matrix) is that if we\ntruncate the expansion\nA =\nt\nX\nj=0\nσjujvT\nj ,\n(A.5)\nwe obtain the best possible least squares approximation to the original matrix A. This is\nused both in eigenface-based face recognition systems (Section 14.2.1) and in the separable\napproximation of convolution kernels (3.21).\nA.1.2 Eigenvalue decomposition\nIf the matrix C is symmetric (m = n),1 it can be written as an eigenvalue decomposition,\nC\n=\nUΛU T =\n\nu0\n· · ·\nun−1\n\n\n\n\nλ0\n...\nλn−1\n\n\n\n\nuT\n0\n· · ·\nuT\nn−1\n\n\n=\nn−1\nX\ni=0\nλiuiuT\ni .\n(A.6)\n1 In this appendix, we denote symmetric matrices using C and general rectangular matrices using A.",
  "760": "738\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nv0\nv1\nu0\nu1\nσ0\nσ1\nA\nFigure A.1 The action of a matrix A can be visualized by thinking of the domain as being\nspanned by a set of orthonormal vectors vj, each of which is transformed to a new orthogonal\nvector uj with a length σj. When A is interpreted as a covariance matrix and its eigenvalue\ndecomposition is performed, each of the uj axes denote a principal direction (component)\nand each σj denotes one standard deviation along that direction.\n(The eigenvector matrix U is sometimes written as Φ and the eigenvectors u as φ.) In this\ncase, the eigenvalues\nλ0 ≥λ1 ≥· · · ≥λn−1\n(A.7)\ncan be both positive and negative.2\nA special case of the symmetric matrix C occurs when it is constructed as the sum of a\nnumber of outer products\nC =\nX\ni\naiaT\ni = AAT ,\n(A.8)\nwhich often occurs when solving least squares problems (Appendix A.2), where the matrix A\nconsists of all the ai column vectors stacked side-by-side. In this case, we are guaranteed that\nall of the eigenvalues λi are non-negative. The associated matrix C is positive semi-deﬁnite\nxT Cx ≥0, ∀x.\n(A.9)\nIf the matrix C is of full rank, the eigenvalues are all positive and the matrix is called sym-\nmetric positive deﬁnite (SPD).\nSymmetric positive semi-deﬁnite matrices also arise in the statistical analysis of data,\nsince they represent the covariance of a set of {xi} points around their mean ¯x,\nC = 1\nn\nX\ni\n(xi −¯x)(xi −¯x)T .\n(A.10)\nIn this case, performing the eigenvalue decomposition is known as principal component anal-\nysis (PCA), since it models the principal directions (and magnitudes) of variation of the point\n2 Eigenvalue decompositions can be computed for non-symmetric matrices but the eigenvalues and eigenvectors\ncan have complex entries in that case.",
  "761": "A.1 Matrix decompositions\n739\ndistribution around their mean, as shown in Section 5.1.1 (5.13–5.15), Section 14.2.1 (14.9),\nand Appendix B.1.1 (B.10). Figure A.1 shows how the principal components of the covari-\nance matrix C denote the principal axes uj of the uncertainty ellipsoid corresponding to this\npoint distribution and how the σj =\np\nλj denote the standard deviations along each axis.\nThe eigenvalues and eigenvectors of C and the singular values and singular vectors of A\nare closely related. Given\nA = UΣV T ,\n(A.11)\nwe get\nC = AAT = UΣV T V ΣU T = UΛU T .\n(A.12)\nFrom this, we see that λi = σ2\ni and that the left singular vectors of A are the eigenvectors of\nC.\nThis relationship gives us an efﬁcient method for computing the eigenvalue decomposi-\ntion of large matrices that are rank deﬁcient, such as the scatter matrices observed in comput-\ning eigenfaces (Section 14.2.1). Observe that the covariance matrix C in (14.9) is exactly the\nsame as C in (A.8). Note also that the individual difference-from-mean images ai = xi −¯x\nare long vectors of length P (the number of pixels in the image), while the total number of ex-\nemplars N (the number of faces in the training database) is much smaller. Instead of forming\nC = AAT , which is P × P, we form the matrix\nˆ\nC = AT A,\n(A.13)\nwhich is N × N. (This involves taking the dot product between every pair of difference\nimages ai and aj.) The eigenvalues of ˆ\nC are the squared singular values of A, namely Σ2,\nand are hence also the eigenvalues of C. The eigenvectors of ˆ\nC are the right singular vectors\nV of A, from which the desired eigenfaces U, which are the left singular vectors of A, can\nbe computed as\nU = AV Σ−1.\n(A.14)\nThis ﬁnal step is essentially computing the eigenfaces as linear combinations of the difference\nimages (Turk and Pentland 1991a). If you have access to a high-quality linear algebra pack-\nage such as LAPACK, routines for efﬁciently computing a small number of the left singular\nvectors and singular values of rectangular matrices such as A are usually provided (Ap-\npendix C.2). However, if storing all of the images in memory is prohibitive, the construction\nof ˆ\nC in (A.13) can be used instead.\nHow can eigenvalue and singular value decompositions actually be computed? Notice\nthat an eigenvector is deﬁned by the equation\nλiui = Cui\nor\n(λiI −C)ui = 0.\n(A.15)",
  "762": "740\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(This can be derived from (A.6) by post-multiplying both sides by ui.) Since the latter equa-\ntion is homogeneous, i.e., it has a zero right-hand-side, it can only have a non-zero (non-\ntrivial) solution for ui if the system is rank deﬁcient, i.e.,\n|(λI −C)| = 0.\n(A.16)\nEvaluating this determinant yields a characteristic polynomial equation in λ, which can be\nsolved for small problems, e.g., 2 × 2 or 3 × 3 matrices, in closed form.\nFor larger matrices, iterative algorithms that ﬁrst reduce the matrix C to a real symmetric\ntridiagonal form using orthogonal transforms and then perform QR iterations are normally\nused (Golub and Van Loan 1996; Trefethen and Bau 1997; Bj¨orck and Dahlquist 2010). Since\nthese techniques are rather involved, it is best to use a linear algebra package such as LAPACK\n(Anderson, Bai, Bischof et al. 1999)—see Appendix C.2.\nFactorization with missing data requires different kinds of iterative algorithms, which of-\nten involve either hallucinating the missing terms or minimizing some weighted reconstruc-\ntion metric, which is intrinsically much more challenging than regular factorization. This\narea has been widely studied in computer vision (Shum, Ikeuchi, and Reddy 1995; De la\nTorre and Black 2003; Huynh, Hartley, and Heyden 2003; Buchanan and Fitzgibbon 2005;\nGross, Matthews, and Baker 2006; Torresani, Hertzmann, and Bregler 2008) and is some-\ntimes called generalized PCA. However, this term is also sometimes used to denote algebraic\nsubspace clustering techniques, which is the subject of a forthcoming monograph by Vidal,\nMa, and Sastry (2010).\nA.1.3 QR factorization\nA widely used technique for stably solving poorly conditioned least squares problems (Bj¨orck\n1996) and as the basis of more complex algorithms, such as computing the SVD and eigen-\nvalue decompositions, is the QR factorization,\nA = QR,\n(A.17)\nwhere Q is an orthonormal (or unitary) matrix QQT = I and R is upper triangular.3 In\ncomputer vision, QR can be used to convert a camera matrix into a rotation matrix and\nan upper-triangular calibration matrix (6.35) and also in various self-calibration algorithms\n(Section 7.2.2). The most common algorithms for computing QR decompositions, modiﬁed\nGram–Schmidt, Householder transformations, and Givens rotations, are described by Golub\nand Van Loan (1996), Trefethen and Bau (1997), and Bj¨orck and Dahlquist (2010) and are\n3 The term “R” comes from the German name for the lower–upper (LU) decomposition, which is LR for “links”\nand “rechts” (left and right of the diagonal).",
  "763": "A.1 Matrix decompositions\n741\nprocedure Cholesky(C, R):\nR = C\nfor i = 0 . . . n −1\nfor j = i + 1 . . . n −1\nRj,j:n−1 = Rj,j:n−1 −rijr−1\nii Ri,j:n−1\nRi,i:n−1 = r−1/2\nii\nRi,i:n−1\nAlgorithm A.1 Cholesky decomposition of the matrix C into its upper triangular form R.\nalso found in LAPACK. Unlike the SVD and eigenvalue decompositions, QR factorization\ndoes not require iteration and can be computed exactly in O(MN 2 + N 3) operations, where\nM is the number of rows and N is the number of columns (for a tall matrix).\nA.1.4 Cholesky factorization\nCholesky factorization can be applied to any symmetric positive deﬁnite matrix C to convert\nit into a product of symmetric lower and upper triangular matrices,\nC = LLT = RT R,\n(A.18)\nwhere L is a lower-triangular matrix and R is an upper-triangular matrix. Unlike Gaussian\nelimination, which may require pivoting (row and column reordering) or may become un-\nstable (sensitive to roundoff errors or reordering), Cholesky factorization remains stable for\npositive deﬁnite matrices, such as those that arise from normal equations in least squares prob-\nlems (Appendix A.2). Because of the form of (A.18), the matrices L and R are sometimes\ncalled matrix square roots.4\nThe algorithm to compute an upper triangular Cholesky decomposition of C is a straight-\nforward symmetric generalization of Gaussian elimination and is based on the decomposition\n(Bj¨orck 1996; Golub and Van Loan 1996)\nC\n=\n\"\nγ\ncT\nc\nC11\n#\n(A.19)\n=\n\"\nγ1/2\n0T\ncγ−1/2\nI\n# \"\n1\n0T\n0\nC11 −cγ−1cT\n# \"\nγ1/2\nγ−1/2cT\n0\nI\n#\n(A.20)\n4 In fact, there exists a whole family of matrix square roots. Any matrix of the form LQ or QR, where Q is a\nunitary matrix, is a square root of C.",
  "764": "742\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n=\nRT\n0 C1R0,\n(A.21)\nwhich, through recursion, can be turned into\nC = RT\n0 . . . RT\nn−1Rn−1 . . . R0 = RT R.\n(A.22)\nAlgorithm A.1 provides a more procedural deﬁnition, which can store the upper-triangular\nmatrix R in the same space as C, if desired. The total operation count for Cholesky factor-\nization is O(N 3) for a dense matrix but can be signiﬁcantly lower for sparse matrices with\nlow ﬁll-in (Appendix A.4).\nNote that Cholesky decomposition can also be applied to block-structured matrices, where\nthe term γ in (A.19) is now a square block sub-matrix and c is a rectangular matrix (Golub\nand Van Loan 1996). The computation of square roots can be avoided by leaving the γ on\nthe diagonal of the middle factor in (A.20), which results in the C = LDLT factorization,\nwhere D is a diagonal matrix. However, since square roots are relatively fast on modern\ncomputers, this is not worth the bother and Cholesky factorization is usually preferred.\nA.2 Linear least squares\nLeast squares ﬁtting problems are pervasive in computer vision. For example, the alignment\nof images based on matching feature points involves the minimization of a squared distance\nobjective function (6.2),\nELS =\nX\ni\n∥ri∥2 =\nX\ni\n∥f(xi; p) −x′\ni∥2,\n(A.23)\nwhere\nri = f(xi; p) −x′\ni = ˆx′\ni −˜x′\ni\n(A.24)\nis the residual between the measured location ˆx′\ni and its corresponding current predicted lo-\ncation ˜x′\ni = f(xi; p). More complex versions of least squares problems, such as large-scale\nstructure from motion (Section 7.4), may involve the minimization of functions of thousands\nof variables. Even problems such as image ﬁltering (Section 3.4.3) and regularization (Sec-\ntion 3.7.1) may involve the minimization of sums of squared errors.\nFigure A.2a shows an example of a simple least squares line ﬁtting problem, where the\nquantities being estimated are the line equation parameters (m, b). When the sampled vertical\nvalues yi are assumed to be noisy versions of points on the line y = mx + b, the optimal\nestimates for (m, b) can be found by minimizing the squared vertical residuals\nEVLS =\nX\ni\n|yi −(mxi + b)|2.\n(A.25)",
  "765": "A.2 Linear least squares\n743\nNote that the function being ﬁtted need not itself be linear to use linear least squares. All that\nis required is that the function be linear in the unknown parameters. For example, polynomial\nﬁtting can be written as\nEPLS =\nX\ni\n|yi −(\np\nX\nj=0\najxj\ni)|2,\n(A.26)\nwhile sinusoid ﬁtting with unknown amplitude A and phase φ (but known frequency f) can\nbe written as\nESLS =\nX\ni\n|yi −A sin(2πfxi +φ)|2 =\nX\ni\n|yi −(B sin 2πfxi +C cos 2πfxi)|2, (A.27)\nwhich is linear in (B, C).\nIn general, it is more common to denote the unknown parameters using x and to write the\ngeneral form of linear least squares as5\nELLS =\nX\ni\n|aix −bi|2 = ∥Ax −b∥2.\n(A.28)\nExpanding the above equation gives us\nELLS = xT (AT A)x −2xT (AT b) + ∥b∥2,\n(A.29)\nwhose minimum value for x can be found by solving the associated normal equations (Bj¨orck\n1996; Golub and Van Loan 1996)\n(AT A)x = AT b.\n(A.30)\nThe preferred way to solve the normal equations is to use Cholesky factorization. Let\nC = AT A = RT R,\n(A.31)\nwhere R is the upper-triangular Cholesky factor of the Hessian C, and\nd = AT b.\n(A.32)\nAfter factorization, the solution for x can be obtained as\nRT z = d,\nRx = z,\n(A.33)\nwhich involves the solution of two triangular systems, i.e., forward and backward substitution\n(Bj¨orck 1996).\n5 Be extra careful in interpreting the variable names here. In the 2D line-ﬁtting example, x is used to denote the\nhorizontal axis, but in the general least squares problem, x = (m, b) denotes the unknown parameter vector.",
  "766": "744\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nx\ny\nb\nm\ny=mx+b\n×\n×\n×\n×\nx\ny\nax+by+c=0\n×\n×\n×\n×\n(a)\n(b)\nFigure A.2 Least squares regression. (a) The line y = mx + b is ﬁt to the four noisy data\npoints, {(xi, yi)}, denoted by × by minimizing the squared vertical residuals between the\ndata points and the line, P\ni ∥yi −(mxi + b)∥2. (b) When the measurements {(xi, yi)} are\nassumed to have noise in all directions, the sum of orthogonal squared distances to the line\nP\ni ∥axi + byi + c∥2 is minimized using total least squares.\nIn cases where the least squares problem is numerically poorly conditioned (which should\ngenerally be avoided by adding sufﬁcient regularization or prior knowledge about the param-\neters, (Appendix A.3)), it is possible to use QR factorization or SVD directly on the matrix\nA (Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Nocedal and Wright\n2006; Bj¨orck and Dahlquist 2010), e.g.,\nAx = QRx = b\n−→\nRx = QT b.\n(A.34)\nNote that the upper triangular matrices R produced by the Cholesky factorization of C =\nAT A and the QR factorization of A are the same, but that solving (A.34) is generally more\nstable (less sensitive to roundoff error) but slower (by a constant factor).\nA.2.1 Total least squares\nIn some problems, e.g., when performing geometric line ﬁtting in 2D images or 3D plane\nﬁtting to point cloud data, instead of having measurement error along one particular axis, the\nmeasured points have uncertainty in all directions, which is known as the errors-in-variables\nmodel (Van Huffel and Lemmerling 2002; Matei and Meer 2006). In this case, it makes more\nsense to minimize a set of homogeneous squared errors of the form\nETLS =\nX\ni\n(aix)2 = ∥Ax∥2,\n(A.35)\nwhich is known as total least squares (TLS) (Van Huffel and Vandewalle 1991; Bj¨orck 1996;\nGolub and Van Loan 1996; Van Huffel and Lemmerling 2002).",
  "767": "A.2 Linear least squares\n745\nThe above error metric has a trivial minimum solution at x = 0 and is, in fact, homoge-\nneous in x. For this reason, we augment this minimization problem with the requirement that\n∥x∥2 = 1. which results in the eigenvalue problem\nx = arg min\nx xT (AT A)x\nsuch that\n∥x∥2 = 1.\n(A.36)\nThe value of x that minimizes this constrained problem is the eigenvector associated with the\nsmallest eigenvalue of AT A. This is the same as the last right singular vector of A, since\nA\n=\nUΣV ,\n(A.37)\nAT A\n=\nV Σ2V ,\n(A.38)\nAT Avk\n=\nσ2\nk,\n(A.39)\nwhich is minimized by selecting the smallest σk value.\nFigure A.2b shows a line ﬁtting problem where, in this case, the measurement errors are\nassumed to be isotropic in (x, y). The solution for the best line equation ax + by + c = 0 is\nfound by minimizing\nETLS−2D =\nX\ni\n(axi + byi + c)2,\n(A.40)\ni.e., ﬁnding the eigenvector associated with the smallest eigenvalue of6\nC = AT A =\nX\ni\n\n\nxi\nyi\n1\n\n\nh\nxi\nyi\n1\ni\n.\n(A.41)\nNotice, however, that minimizing P\ni(aix)2 in (A.35) is only statistically optimal (Ap-\npendix B.1.1) if all of the measured terms in the ai, e.g., the (xi, yi, 1) measurements, have\nequal noise. This is deﬁnitely not the case in the line-ﬁtting example of Figure A.2b (A.40),\nsince the 1 values are noise-free. To mitigate this, we ﬁrst subtract the mean x and y values\nfrom all the measured points\nˆxi\n=\nxi −¯x\n(A.42)\nˆyi\n=\nyi −¯y\n(A.43)\nand then ﬁt the 2D line equation a(x −¯x) + b(y −¯y) = 0 by minimizing\nETLS−2Dm =\nX\ni\n(aˆxi + bˆyi)2.\n(A.44)\n6 Again, be careful with the variable names here. The measurement equation is ai = (xi, yi, 1) and the unknown\nparameters are x = (a, b, c).",
  "768": "746\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe more general case where each individual measurement component can have different\nnoise level, as is the case in estimating essential and fundamental matrices (Section 7.2), is\ncalled the heteroscedastic errors-in-variable (HEIV) model and is discussed by Matei and\nMeer (2006).\nA.3 Non-linear least squares\nIn many vision problems, such as structure from motion, the least squares problem formulated\nin (A.23) involves functions f(xi; p) that are not linear in the unknown parameters p. This\nproblem is known as non-linear least squares or non-linear regression (Bj¨orck 1996; Madsen,\nNielsen, and Tingleff 2004; Nocedal and Wright 2006). It is usually solved by iteratively re-\nlinearizing (A.23) around the current estimate of p using the gradient derivative (Jacobian)\nJ = ∂f/∂p and computing an incremental improvement ∆p.\nAs shown in Equations (6.13–6.17), this results in\nENLS(∆p)\n=\nX\ni\n∥f(xi; p + ∆p) −x′\ni∥2\n(A.45)\n≈\nX\ni\n∥J(xi; p)∆p −ri∥2,\n(A.46)\nwhere the Jacobians J(xi; p) and residual vectors ri play the same role in forming the normal\nequations as ai and bi in (A.28).\nBecause the above approximation only holds near a local minimum or for small values\nof ∆p, the update p ←p + ∆p may not always decrease the summed square residual error\n(A.45). One way to mitigate this problem is to take a smaller step,\np ←p + α∆p,\n0 < α ≤1.\n(A.47)\nA simple way to determine a reasonable value of α is to start with 1 and successively halve\nthe value, which is a simple form of line search (Al-Baali and Fletcher. 1986; Bj¨orck 1996;\nNocedal and Wright 2006).\nAnother approach to ensuring a downhill step in error is to add a diagonal damping term\nto the approximate Hessian\nC =\nX\ni\nJT (xi)J(xi),\n(A.48)\ni.e., to solve\n[C + λ diag(C)]∆p = d,\n(A.49)\nwhere\nd =\nX\ni\nJT (xi)ri,\n(A.50)",
  "769": "A.4 Direct sparse matrix techniques\n747\nwhich is called a damped Gauss–Newton method. The damping parameter λ is increased if\nthe squared residual is not decreasing as fast as expected, i.e., as predicted by (A.46), and\nis decreased if the expected decrease is obtained (Madsen, Nielsen, and Tingleff 2004). The\ncombination of the Newton (ﬁrst-order Taylor series) approximation (A.46) and the adaptive\ndamping parameter λ is commonly known as the Levenberg–Marquardt algorithm (Leven-\nberg 1944; Marquardt 1963) and is an example of more general trust region methods, which\nare discussed in more detail in (Bj¨orck 1996; Conn, Gould, and Toint 2000; Madsen, Nielsen,\nand Tingleff 2004; Nocedal and Wright 2006).\nWhen the initial solution is far away from its quadratic region of convergence around a\nlocal minimum, large residual methods, e.g., Newton-type methods, which add a second-order\nterm to the Taylor series expansion in (A.46), may converge faster. Quasi-Newton methods\nsuch as BFGS, which require only gradient evaluations, can also be useful if memory size is\nan issue. Such techniques are discussed in textbooks and papers on numerical optimization\n(Toint 1987; Bj¨orck 1996; Conn, Gould, and Toint 2000; Nocedal and Wright 2006).\nA.4 Direct sparse matrix techniques\nMany optimization problems in computer vision, such as bundle adjustment (Szeliski and\nKang 1994; Triggs, McLauchlan, Hartley et al. 1999; Hartley and Zisserman 2004; Snavely,\nSeitz, and Szeliski 2008b; Agarwal, Snavely, Simon et al. 2009) have Jacobian and (approx-\nimate) Hessian matrices that are extremely sparse (Section 7.4.1). For example, Figure 7.9a\nshows the bipartite model typical of structure from motion problems, in which most points\nare only observed by a subset of the cameras, which results in the sparsity patterns for the\nJacobian and Hessian shown in Figure 7.9b–c.\nWhenever the Hessian matrix is sparse enough, it is more efﬁcient to use sparse Cholesky\nfactorization instead of regular Cholesky factorization. In such sparse direct techniques, the\nHessian matrix C and its associated Cholesky factor R are stored in compressed form, in\nwhich the amount of storage is proportional to the number of (potentially) non-zero entries\n(Bj¨orck 1996; Davis 2006).7 Algorithms for computing the non-zero elements in C and R\nfrom the sparsity pattern of the Jacobian matrix J are given by Bj¨orck (1996, Section 6.4),\nand algorithms for computing the numerical Cholesky and QR decompositions (once the\nsparsity pattern has been computed and storage allocated) are discussed by Bj¨orck (1996,\nSection 6.5).\n7 For example, you can store a list of (i, j, cij) triples. One example of such a scheme is compressed sparse\nrow (CSR) storage. An alternative storage method called skyline, which stores adjacent vertical spans of non-zero\nelements (Bathe 2007), is sometimes used in ﬁnite element analysis. Banded systems such as snakes (5.3) can store\njust the non-zero band elements (Bj¨orck 1996, Section 6.2) and can be solved in O(nb2), where n is the number of\nvariables and b is the bandwidth.",
  "770": "748\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nA.4.1 Variable reordering\nThe key to efﬁciently solving sparse problems using direct (non-iterative) techniques is to\ndetermine an efﬁcient ordering for the variables, which reduces the amount of ﬁll-in, i.e., the\nnumber of non-zero entries in R that were zero in the original C matrix. We already saw in\nSection 7.4.1 how storing the more numerous 3D point parameters before the camera param-\neters and using the Schur complement (7.56) results in a more efﬁcient algorithm. Similarly,\nsorting parameters by time in video-based reconstruction problems usually results in lower\nﬁll-in. Furthermore, any problem whose adjacency graph (the graph corresponding to the\nsparsity pattern) is a tree can be solved in linear time with an appropriate reordering of the\nvariables (putting all the children before their parents). All of these are examples of good\nreordering techniques.\nIn the general case of unstructured data, there are many heuristics available to ﬁnd good\nreorderings (Bj¨orck 1996; Davis 2006).8 For general adjacency (sparsity) graphs, minimum\ndegree orderings generally produce good results. For planar graphs, which often arise on im-\nage or spline grids (Section 8.3), nested dissection, which recursively splits the graph into two\nequal halves along a frontier (or boundary) of small size, generally works well. Such domain\ndecomposition (or multi-frontal) techniques also enable the use of parallel processing, since\nindependent sub-graphs can be processed in parallel on separate processors (Davis 2008).\nThe overall set of steps used to perform the direct solution of sparse least squares problems\nare summarized in Algorithm A.2, which is a modiﬁed version of Algorithm 6.6.1 by Bj¨orck\n(1996, Section 6.6)). If a series of related least squares problems is being solved, as is the\ncase in iterative non-linear least squares (Appendix A.3), steps 1–3 can be performed ahead of\ntime and reused for each new invocation with different C and d values. When the problem is\nblock-structured, as is the case in structure from motion where point (structure) variables have\ndense 3×3 sub-entries in C and cameras have 6×6 (or larger) entries, the cost of performing\nthe reordering computation is small compared to the actual numerical factorization, which\ncan beneﬁt from block-structured matrix operations (Golub and Van Loan 1996). It is also\npossible to apply sparse reordering and multifrontal techniques to QR factorization (Davis\n2008), which may be preferable when the least squares problems are poorly conditioned.\nA.5 Iterative techniques\nWhen problems become large, the amount of memory required to store the Hessian matrix\nC and its factor R, and the amount of time it takes to compute the factorization, can be-\ncome prohibitively large, especially when there are large amounts of ﬁll-in. This is often\n8Finding the optimal reordering with minimal ﬁll-in is provably NP-hard.",
  "771": "A.5 Iterative techniques\n749\nprocedure SparseCholeskySolve(C, d):\n1. Determine symbolically the structure of C, i.e., the adjacency graph.\n2. (Optional) Compute a reordering for the variables, taking into ac-\ncount any block structure inherent in the problem.\n3. Determine the ﬁll-in pattern for R and allocate the compressed stor-\nage for R as well as storage for the permuted right hand side ˆd.\n4. Copy the elements of C and d into R and ˆd, permuting the values\naccording to the computed ordering.\n5. Perform the numerical factorization of R using Algorithm A.1.\n6. Solve the factored system (A.33), i.e.,\nRT z = ˆd,\nRx = z.\n7. Return the solution x, after undoing the permutation.\nAlgorithm A.2\nSparse least squares using a sparse Cholesky decomposition of the matrix\nC.\nthe case with image processing problems deﬁned on pixel grids, since, even with the optimal\nreordering (nested dissection) the amount of ﬁll can still be large.\nA preferable approach to solving such linear systems is to use iterative techniques, which\ncompute a series of estimates that converge to the ﬁnal solution, e.g., by taking a series of\ndownhill steps in an energy function such as (A.29).\nA large number of iterative techniques have been developed over the years, including such\nwell-known algorithms as successive overrelaxation and multi-grid. These are described in\nspecialized textbooks on iterative solution techniques (Axelsson 1996; Saad 2003) as well as\nin more general books on numerical linear algebra and least squares techniques (Bj¨orck 1996;\nGolub and Van Loan 1996; Trefethen and Bau 1997; Nocedal and Wright 2006; Bj¨orck and\nDahlquist 2010).\nA.5.1 Conjugate gradient\nThe iterative solution technique that often performs best is conjugate gradient descent, which\ntakes a series of downhill steps that are conjugate to each other with respect to the C matrix,",
  "772": "750\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ni.e., if the u and v descent directions satisfy uT Cv = 0. In practice, conjugate gradient\ndescent outperforms other kinds of gradient descent algorithm because its convergence rate\nis proportional to the square root of the condition number of C instead of the condition\nnumber itself.9 Shewchuk (1994) provides a nice introduction to this topic, with clear intuitive\nexplanations of the reasoning behind the conjugate gradient algorithm and its performance.\nAlgorithm A.3 describes the conjugate gradient algorithm and its related least squares\ncounterpart, which can be used when the original set of least squares linear equations are\navailable in the form of Ax = b (A.28). While it is easy to convince yourself that the two\nforms are mathematically equivalent, the least squares form is preferable if rounding errors\nstart to affect the results because of poor conditioning. It may also be preferable if, due to\nthe sparsity structure of A, multiplies with the original A matrix are faster or more space\nefﬁcient than multiplies with C.\nThe conjugate gradient algorithm starts by computing the current residual r0 = d−Cx0,\nwhich is the direction of steepest descent of the energy function (A.28). It sets the original\ndescent direction p0 = r0. Next, it multiplies the descent direction by the quadratic form\n(Hessian) matrix C and combines this with the residual to estimate the optimal step size αk.\nThe solution vector xk and the residual vector rk are then updated using this step size. (No-\ntice how the least squares variant of the conjugate gradient algorithm splits the multiplication\nby the C = AT A matrix across steps 4 and 8.) Finally, a new search direction is calculated\nby ﬁrst computing a factor β as the ratio of current to previous residual magnitudes. The\nnew search direction pk+1 is then set to the residual plus β times the old search direction pk,\nwhich keeps the directions conjugate with respect to C.\nIt turns out that conjugate gradient descent can also be directly applied to non-quadratic\nenergy functions, e.g., those arising from non-linear least squares (Appendix A.3). Instead\nof explicitly forming a local quadratic approximation C and then computing residuals rk,\nnon-linear conjugate gradient descent computes the gradient of the energy function E (A.45)\ndirectly inside each iteration and uses it to set the search direction (Nocedal and Wright 2006).\nSince the quadratic approximation to the energy function may not exist or may be inaccurate,\nline search is often used to determine the step size αk. Furthermore, to compensate for errors\nin ﬁnding the true function minimum, alternative formulas for βk+1 such as Polak–Ribi`ere,\nβk+1 = ∇E(xk+1)[∇E(xk+1) −∇E(xk)]\n∥∇E(xk)∥2\n(A.51)\nare often used (Nocedal and Wright 2006).\n9 The condition number κ(C) is the ratio of the largest and smallest eigenvalues of C. The actual convergence\nrate depends on the clustering of the eigenvalues, as discussed in the references cited in this section.",
  "773": "A.5 Iterative techniques\n751\nConjugateGradient(C, d, x0)\n1. r0 = d −Cx0\n2. p0 = r0\n3. for k = 0 . . .\n4.\nwk = Cpk\n5.\nαk = ∥rk∥2/(pk · wk)\n6.\nxk+1 = xk + αkpk\n7.\nrk+1 = rk −αkwk\n8.\n9.\nβk+1 = ∥rk+1∥2/∥rk∥2\n10.\npk+1 = rk+1 + βkpk\nConjugateGradientLS(A, b, x0)\n1. q0 = b −Ax0, r0 = AT q0\n2. p0 = r0\n3. for k = 0 . . .\n4.\nvk = Apk\n5.\nαk = ∥rk∥2/∥vk∥2\n6.\nxk+1 = xk + αkpk\n7.\nqk+1 = qk −αkvk\n8.\nrk+1 = AT qk+1\n9.\nβk+1 = ∥rk+1∥2/∥rk∥2\n10.\npk+1 = rk+1 + βkpk\nAlgorithm A.3\nConjugate gradient and conjugate gradient least squares algorithms. The\nalgorithm is described in more detail in the text, but in brief, they choose descent directions\npk that are conjugate to each other with respect to C by computing a factor β by which to\ndiscount the previous search direction pk−1. They then ﬁnd the optimal step size α and take\na downhill step by an amount αkpk.\nA.5.2 Preconditioning\nAs we mentioned previously, the rate of convergence of the conjugate gradient algorithm\nis governed in large part by the condition number κ(C). Its effectiveness can therefore be\nincreased dramatically by reducing this number, e.g., by rescaling elements in x, which cor-\nresponds to rescaling rows and columns in C.\nIn general, preconditioning is usually thought of as a change of basis from the vector x to\na new vector\nˆx = Sx.\n(A.52)\nThe corresponding linear system being solved then becomes\nAS−1ˆx = S−1b\nor\nˆ\nAˆx = ˆb,\n(A.53)",
  "774": "752\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nwith a corresponding least squares energy (A.29) of the form\nEPLS = ˆxT (S−T CS−1)ˆx −2ˆxT (S−T d) + ∥ˆb∥2.\n(A.54)\nThe actual preconditioned matrix ˆC = S−T CS−1 is usually not explicitly computed. In-\nstead, Algorithm A.3 is extended to insert S−T and ST operations at the appropriate places\n(Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Saad 2003; Nocedal and\nWright 2006).\nA good preconditioner S is easy and cheap to compute, but is also a decent approximation\nto a square root of C, so that κ(S−T CS−1) is closer to 1. The simplest such choice is the\nsquare root of the diagonal matrix S = D1/2, with D = diag(C). This has the advantage\nthat any scalar change in variables (e.g., using radians instead of degrees for angular measure-\nments) has no effect on the range of convergence of the iterative technique. For problems that\nare naturally block-structured, e.g., for structure from motion, where 3D point positions or\n6D camera poses are being estimated, a block diagonal preconditioner is often a good choice.\nA wide variety of more sophisticated preconditioners have been developed over the years\n(Bj¨orck 1996; Golub and Van Loan 1996; Trefethen and Bau 1997; Saad 2003; Nocedal and\nWright 2006), many of which can be directly applied to problems in computer vision (Byr¨od\nand øAstr¨om 2009; Jeong, Nist´er, Steedly et al. 2010; Agarwal, Snavely, Seitz et al. 2010).\nSome of these are based on an incomplete Cholesky factorization of C, i.e., one in which the\namount of ﬁll-in in R is strictly limited, e.g., to just the original non-zero elements in C.10\nOther preconditioners are based on a sparsiﬁed, e.g., tree-based or clustered, approximation\nto C (Koutis 2007; Koutis and Miller 2008; Grady 2008; Koutis, Miller, and Tolliver 2009),\nsince these are known to have efﬁcient inversion properties.\nFor grid-based image-processing applications, parallel or hierarchical preconditioners\noften perform extremely well (Yserentant 1986; Szeliski 1990b; Pentland 1994; Saad 2003;\nSzeliski 2006b). These approaches use a change of basis transformation S that resembles\nthe pyramidal or wavelet representations discussed in Section 3.5, and are hence amenable\nto parallel and GPU-based implementations. Coarser elements in the new representation\nquickly converge to the low-frequency components in the solution, while ﬁner-level elements\nencode the higher-frequency components. Some of the relationships between hierarchical\npreconditioners, incomplete Cholesky factorization, and multigrid techniques are explored\nby Saad (2003) and Szeliski (2006b).\n10 If a complete Cholesky factorization C = RT R is used, we get ˆC = R−T CR−1 = I and all iterative\nalgorithms converge in a single step, thereby obviating the need to use them, but the complete factorization is often\ntoo expensive. Note that incomplete factorization can also beneﬁt from reordering.",
  "775": "A.5 Iterative techniques\n753\nA.5.3 Multigrid\nOne other class of iterative techniques widely used in computer vision is multigrid techniques\n(Briggs, Henson, and McCormick 2000; Trottenberg, Oosterlee, and Schuller 2000), which\nhave been applied to problems such as surface interpolation (Terzopoulos 1986a), optical\nﬂow (Terzopoulos 1986a; Bruhn, Weickert, Kohlberger et al. 2006), high dynamic range tone\nmapping (Fattal, Lischinski, and Werman 2002), colorization (Levin, Lischinski, and Weiss\n2004), natural image matting (Levin, Lischinski, and Weiss 2008), and segmentation (Grady\n2008).\nThe main idea behind multigrid is to form coarser (lower-resolution) versions of the prob-\nlems and use them to compute the low-frequency components of the solution. However,\nunlike simple coarse-to-ﬁne techniques, which use the coarse solutions to initialize the ﬁne\nsolution, multigrid techniques only correct the low-frequency component of the current solu-\ntion and use multiple rounds of coarsening and reﬁnement (in what are often called “V” and\n“W” patterns of motion across the pyramid) to obtain rapid convergence.\nOn certain simple homogeneous problems (such as solving Poisson equations), multigrid\ntechniques can achieve optimal performance, i.e., computation times linear in the number\nof variables. However, for more inhomogeneous problems or problems on irregular grids,\nvariants on these techniques, such as algebraic multigrid (AMG) approaches, which look at\nthe structure of C to derive coarse level problems, may be preferable. Saad (2003) has a\nnice discussion of the relationship between multigrid and parallel preconditioners and on the\nrelative merits of using multigrid or conjugate gradient approaches.",
  "776": "754\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "777": "Appendix B\nBayesian modeling and inference\nB.1\nEstimation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757\nB.1.1\nLikelihood for multivariate Gaussian noise\n. . . . . . . . . . . . . . 757\nB.2\nMaximum likelihood estimation and least squares . . . . . . . . . . . . . . . 759\nB.3\nRobust statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 760\nB.4\nPrior models and Bayesian inference . . . . . . . . . . . . . . . . . . . . . . 762\nB.5\nMarkov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763\nB.5.1\nGradient descent and simulated annealing . . . . . . . . . . . . . . . 765\nB.5.2\nDynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 766\nB.5.3\nBelief propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\nB.5.4\nGraph cuts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770\nB.5.5\nLinear programming . . . . . . . . . . . . . . . . . . . . . . . . . . 773\nB.6\nUncertainty estimation (error analysis) . . . . . . . . . . . . . . . . . . . . . 775",
  "778": "756\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThe following problem commonly recurs in this book: Given a number of measurements\n(images, feature positions, etc.), estimate the values of some unknown structure or parameter\n(camera positions, object shape, etc.). These kinds of problems are in general called inverse\nproblems because they involve estimating unknown model parameters instead of simulating\nthe forward formation equations.1 Computer graphics is a classic forward modeling problem\n(given some objects, cameras, and lighting, simulate the images that would result), while\ncomputer vision problems are usually of the inverse kind (given one or more images, recover\nthe scene that gave rise to these images).\nGiven an instance of an inverse problem, there are, in general, several ways to proceed.\nFor instance, through clever (or sometimes straightforward) algebraic manipulation, a closed\nform solution for the unknowns can sometimes be derived. Consider, for example, the camera\nmatrix calibration problem (Section 6.2.1): given an image of a calibration pattern consisting\nof known 3D point positions, compute the 3×4 camera matrix P that maps these points onto\nthe image plane.\nIn more detail, we can write this problem as (6.33–6.34)\nxi\n=\np00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(B.1)\nyi\n=\np10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n,\n(B.2)\nwhere (xi, yi) is the feature position of the ith point measured in the image plane, (Xi, Yi, Zi)\nis the corresponding 3D point position, and the pij are the unknown entries of the camera\nmatrix P . Moving the denominator over to the left hand side, we end up with a set of\nsimultaneous linear equations,\nxi(p20Xi + p21Yi + p22Zi + p23)\n=\np00Xi + p01Yi + p02Zi + p03,\n(B.3)\nyi(p20Xi + p21Yi + p22Zi + p23)\n=\np10Xi + p11Yi + p12Zi + p13,\n(B.4)\nwhich we can solve using linear least squares (Appendix A.2) to obtain an estimate of P .\nThe question then arises: is this set of equations the right ones to be solving? If the\nmeasurements are totally noise-free or we do not care about getting the best possible answer,\nthen the answer is yes. However, in general, we cannot be sure that we have a reasonable\nalgorithm unless we make a model of the likely sources of error and devise an algorithm that\nperforms as well as possible given these potential errors.\n1 In machine learning, these problems are called regression problems, because we are trying to estimate a contin-\nuous quantity from noisy inputs, as opposed to a discrete classiﬁcation task (Bishop 2006).",
  "779": "B.1 Estimation theory\n757\nB.1 Estimation theory\nThe study of such inference problems from noisy data is often called estimation theory (Gelb\n1974), and its extension to problems where we explicitly choose a loss function is called sta-\ntistical decision theory (Berger 1993; Hastie, Tibshirani, and Friedman 2001; Bishop 2006;\nRobert 2007). We ﬁrst start by writing down the forward process that leads from our un-\nknowns (and knowns) to a set of noise-corrupted measurements. We then devise an algorithm\nthat will give us an estimate (or set of estimates) that are both insensitive to the noise (as best\nthey can be) and also quantify the reliability of these estimates.\nThe speciﬁc equations above (B.1) are just a particular instance of a more general set of\nmeasurement equations,\nyi = f i(x) + ni.\n(B.5)\nHere, the yi are the noise-corrupted measurements, e.g., (xi, yi) in Equation (B.1), and x is\nthe unknown state vector.2\nEach measurement comes with its associated measurement model f i(x), which maps the\nunknown into that particular measurement. An alternative formulation would be to have one\ngeneral function f(x, pi) and to use a per-measurement parameter vector pi to distinguish\nbetween different measurements, e.g., (Xi, Yi, Zi) in Equation (B.1). Note that the use of the\nf i(x) form makes it straightforward to have measurements of different dimensions, which\nbecomes useful when we start adding in prior information (Appendix B.4).\nEach measurement is also contaminated with some noise ni. In Equation (B.5), we have\nindicated that ni is a zero-mean normal (Gaussian) random variable with a covariance matrix\nΣi. In general, the noise need not be Gaussian and, in fact, it is usually prudent to assume\nthat some measurements may be outliers. However, we defer this discussion to Appendix B.3,\nafter we have explored the simpler Gaussian noise case more fully. We also assume that the\nnoise vectors ni are independent. In the case where they are not (e.g., when some constant\ngain or offset contaminates all of the pixels in a given image), we can add this effect as a\nnuisance parameter to our state vector x and later estimate its value (and discard it, if so\ndesired).\nB.1.1 Likelihood for multivariate Gaussian noise\nGiven all of the noisy measurements y = {yi}, we would like to infer a probability distribu-\ntion on the unknown x vector. We can write the likelihood of having observed the {yi} given\na particular value of x as\nL = p(y|x) =\nY\ni\np(yi|x) =\nY\ni\np(yi|f i(x)) =\nY\ni\np(ni).\n(B.6)\n2 In the Kalman ﬁltering literature (Gelb 1974), it is more common to use z instead of y to denote measurements.",
  "780": "758\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWhen each noise vector ni is a multivariate Gaussian with covariance Σi,\nni ∼N(0, Σi),\n(B.7)\nwe can write this likelihood as\nL\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2(yi −f i(x))T Σ−1\ni (yi −f i(x))\n\u0013\n(B.8)\n=\nY\ni\n|2πΣi|−1/2 exp\n\u0012\n−1\n2∥yi −f i(x)∥2\nΣ\n−1\ni\n\u0013\n,\nwhere the matrix norm ∥x∥2\nA is a shorthand notation for xT Ax.\nThe norm ∥yi −yi∥Σ\n−1\ni\nis often called the Mahalanobis distance (5.26 and 14.14) and is\nused to measure the distance between a measurement and the mean of a multivariate Gaussian\ndistribution. Contours of equal Mahalanobis distance are equi-probability contours. Note\nthat when the measurement covariance is isotropic (the same in all directions), i.e., when\nΣi = σ2\ni I, the likelihood can be written as\nL =\nY\ni\n(2πσ2\ni )−Ni/2 exp\n\u0012\n−1\n2σ2\ni\n∥yi −f i(x)∥2\n\u0013\n,\n(B.9)\nwhere Ni is the length of the ith measurement vector yi.\nWe can more easily visualize the structure of the covariance matrix and the correspond-\ning Mahalanobis distance if we ﬁrst perform an eigenvalue or principal component analysis\n(PCA) of the covariance matrix (A.6),\nΣ = Φ diag(λ0 . . . λN−1) ΦT .\n(B.10)\nEqual-probability contours of the corresponding multi-variate Gaussian, which are also equi-\ndistance contours in the Mahalanobis distance (Figure 14.14), are multi-dimensional ellip-\nsoids whose axis directions are given by the columns of Φ (the eigenvectors) and whose\nlengths are given by the σj =\np\nλj (Figure A.1).\nIt is usually more convenient to work with the negative log likelihood, which we can think\nof as a cost or energy\nE = −log L\n=\n1\n2\nX\ni\n(yi −f i(x))T Σ−1\ni (yi −f i(x)) + k\n(B.11)\n=\n1\n2\nX\ni\n∥yi −f i(x)∥2\nΣ\n−1\ni\n+ k,\n(B.12)\nwhere k = P\ni log |2πΣi| is a constant that depends on the measurement variances, but is\nindependent of x.",
  "781": "B.2 Maximum likelihood estimation and least squares\n759\nNotice that the inverse covariance Ci = Σ−1\ni\nplays the role of a weight on each of the\nmeasurement error residuals, i.e., the difference between the contaminated measurement yi\nand its uncontaminated (predicted) value f i(x). In fact, the inverse covariance is often called\nthe (Fisher) information matrix (Bishop 2006), since it tells us how much information is\ncontained in a given measurement, i.e., how well it constrains the ﬁnal estimate. We can also\nthink of this matrix as denoting the amount of conﬁdence to associate with each measurement\n(hence the letter C).\nIn this formulation, it is quite acceptable for some information matrices to be singular\n(of degenerate rank) or even zero (if the measurement is missing altogether). Rank-deﬁcient\nmeasurements often occur, for example, when using a line feature or edge to measure a 3D\nedge-like feature, since its exact position along the edge is unknown (of inﬁnite or extremely\nlarge variance) §8.1.3.\nIn order to make the distinction between the noise contaminated measurement and its\nexpected value for a particular setting of x more explicit, we adopt the notation ˜y for the\nformer (think of the tilde as the approximate or noisy value) and ˆy = f i(x) for the latter\n(think of the hat as the predicted or expected value). We can then write the negative log\nlikelihood as\nE = −log L =\nX\ni\n∥˜yi −ˆyi∥Σ\n−1\ni\n+ k.\n(B.13)\nB.2 Maximum likelihood estimation and least squares\nNow that we have presented the likelihood and log likelihood functions, how can we ﬁnd the\noptimal value for our state estimate x? One plausible choice might be to select the value of x\nthat maximizes L = p(y|x). In fact, in the absence of any prior model for x (Appendix B.4),\nwe have\nL = p(y|x) = p(y, x) = p(x|y).\nTherefore, choosing the value of x that maximizes the likelihood is equivalent to choosing\nthe maximum of our probability density estimate for x.\nWhen might this be a good idea? If the data (measurements) constrain the possible values\nof x so that they all cluster tightly around one value (e.g., if the distribution p(x|y) is a\nunimodal Gaussian), the maximum likelihood estimate is the optimal one in that it is both\nunbiased and has the least possible variance. In many other cases, e.g., if a single estimate\nis all that is required, it is still often the best estimate.3 However, if the probability is multi-\nmodal, i.e., it has several local minima in the log likelihood (Figure 5.7), much more care\n3 According to the Gauss-Markov theorem, least squares produces the best linear unbiased estimator (BLUE) for\na linear measurement model regardless of the actual noise distribution, assuming that the noise is zero mean and\nuncorrelated.",
  "782": "760\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nmay be required. In particular, it might be necessary to defer certain decisions (such as the\nultimate position of an object being tracked) until more measurements have been taken. The\nCONDENSATION algorithm presented in Section 5.1.2 is one possible method for modeling\nand updating such multi-modal distributions but is just one example of more general particle\nﬁltering and Markov Chain Monte Carlo (MCMC) techniques (Andrieu, de Freitas, Doucet\net al. 2003; Bishop 2006; Koller and Friedman 2009).\nAnother possible way to choose the best estimate is to maximize the expected utility\n(or, conversely, to minimize the expected risk or loss) associated with obtaining the correct\nestimate, i.e., by minimizing\nEloss(x, y) =\nZ\nl(x −z)p(z|y)dz.\n(B.14)\nFor example, if a robot wants to avoid hitting a wall at all costs, the loss function will be\nhigh whenever the estimate underestimates the true distance to the wall. When l(x −y) =\nδ(x −y), we obtain the maximum likelihood estimate, whereas when l(x −y) = ∥x −y∥2,\nwe obtain the mean square error (MSE) or expected value estimate. The explicit modeling of\na utility or loss function is what characterizes statistical decision theory (Berger 1993; Hastie,\nTibshirani, and Friedman 2001; Bishop 2006; Robert 2007).\nHow do we ﬁnd the maximum likelihood estimate? If the measurement noise is Gaussian,\nwe can minimize the quadratic objective function (B.13). This becomes even simpler if the\nmeasurement equations are linear, i.e.,\nf i(x) = Hix,\n(B.15)\nwhere H is the measurement matrix relating unknown state variables x to measurements ˜y.\nIn this case, (B.13) becomes\nE =\nX\ni\n∥˜yi −Hix∥Σ\n−1\ni\n=\nX\ni\n(˜yi −Hix)T Ci(˜yi −Hix),\n(B.16)\nwhich is a simple quadratic form in x, which can be solved using linear least squares (Ap-\npendix A.2). When the measurements are non-linear, the system must be solved iteratively\nusing non-linear least squares (Appendix A.3).\nB.3 Robust statistics\nIn Appendix B.1.1, we assumed that the noise being added to each measurement (B.5) was\nmultivariate Gaussian (B.7). This is an appropriate model if the noise is the result of lots of\ntiny errors being added together, e.g., from thermal noise in a silicon imager. In most cases,\nhowever, measurements can be contaminated with larger outliers, i.e., gross failures in the",
  "783": "B.3 Robust statistics\n761\nmeasurement process. Examples of such outliers include bad feature matches (Section 6.1.4),\nocclusions in stereo matching (Chapter 11), and discontinuities in an otherwise smooth image,\ndepth map, or label image (Sections 3.7.1 and 3.7.2).\nIn such cases, it makes more sense to model the measurement noise with a long-tailed\ncontaminated noise model such as a Laplacian. The negative log likelihood in this case,\nrather than being quadratic in the measurement residuals (B.12–B.16), has a slower growth\nin the penalty function to account for the increased likelihood of large errors.\nThis formulation of the inference problem is called an M-estimator in the robust statistics\nliterature (Huber 1981; Hampel, Ronchetti, Rousseeuw et al. 1986; Black and Rangarajan\n1996; Stewart 1999) and involves applying a robust penalty function ρ(r) to the residuals\nERLS(∆p) =\nX\ni\nρ(∥ri∥)\n(B.17)\ninstead of squaring them.\nAs we mentioned in Section 6.1.4, we can take the derivative of this function with respect\nto p and set it to 0,\nX\ni\nψ(∥ri∥)∂∥ri∥\n∂p\n=\nX\ni\nψ(∥ri∥)\n∥ri∥\nrT\ni\n∂ri\n∂p = 0,\n(B.18)\nwhere ψ(r) = ρ′(r) is the derivative of ρ and is called the inﬂuence function. If we introduce a\nweight function, w(r) = Ψ(r)/r, we observe that ﬁnding the stationary point of (B.17) using\n(B.18) is equivalent to minimizing the iteratively re-weighted least squares (IRLS) problem\nEIRLS =\nX\ni\nw(∥ri∥)∥ri∥2,\n(B.19)\nwhere the w(∥ri∥) play the same local weighting role as Ci = Σ−1\ni\nin (B.12). Black and\nAnandan (1996) describe a variety of robust penalty functions and their corresponding inﬂu-\nence and weighting function.\nThe IRLS algorithm alternates between computing the inﬂuence functions w(∥ri∥) and\nsolving the resulting weighted least squares problem (with ﬁxed w values). Alternative in-\ncremental robust least squares algorithms can be found in the work of Sawhney and Ayer\n(1996); Black and Anandan (1996); Black and Rangarajan (1996); Baker, Gross, Ishikawa et\nal. (2003) and textbooks and tutorials on robust statistics (Huber 1981; Hampel, Ronchetti,\nRousseeuw et al. 1986; Rousseeuw and Leroy 1987; Stewart 1999). It is also possible to ap-\nply general optimization techniques (Appendix A.3) directly to the non-linear cost function\ngiven in Equation (B.19), which may sometimes have better convergence properties.\nMost robust penalty functions involve a scale parameter, which should typically be set to\nthe variance (or standard deviation, depending on the formulation) of the non-contaminated",
  "784": "762\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\n(inlier) noise. Estimating such noise levels directly from the measurements or their residuals,\nhowever, can be problematic, as such estimates themselves become contaminated by outliers.\nThe robust statistics literature contains a variety of techniques to estimate such parameters.\nOne of the simplest and most effective is the median absolute deviation (MAD),\nMAD = medi∥ri∥,\n(B.20)\nwhich, when multiplied by 1.4, provides a robust estimate of the standard deviation of the\ninlier noise process.\nAs mentioned in Section 6.1.4, it is often better to start iterative non-linear minimiza-\ntion techniques, such as IRLS, in the vicinity of a good solution by ﬁrst randomly selecting\nsmall subsets of measurements until a good set of inliers is found. The best known of these\ntechniques is RANdom SAmple Consensus (RANSAC) (Fischler and Bolles 1981), although\neven better variants such as Preemptive RANSAC (Nist´er 2003) and PROgressive SAmple\nConsensus (PROSAC) (Chum and Matas 2005) have since been developed.\nB.4 Prior models and Bayesian inference\nWhile maximum likelihood estimation can often lead to good solutions, in some cases the\nrange of possible solutions consistent with the measurements is too large to be useful. For\nexample, consider the problem of image denoising (Sections 3.4.4 and 3.7.3). If we esti-\nmate each pixel separately based on just its noisy version, we cannot make any progress,\nas there are a large number of values that could lead to each noisy measurement.4 Instead,\nwe need to rely on typical properties of images, e.g., that they tend to be piecewise smooth\n(Section 3.7.1).\nThe propensity of images to be piecewise smooth can be encoded in a prior distribution\np(x), which measures the likelihood of an image being a natural image. For example, to\nencode piecewise smoothness, we can use a Markov random ﬁeld model (3.109 and B.24)\nwhose negative log likelihood is proportional to a robustiﬁed measure of image smoothness\n(gradient magnitudes).\nPrior models need not be restricted to image processing applications. For example, we\nmay have some external knowledge about the rough dimensions of an object being scanned,\nthe focal length of a lens being calibrated, or the likelihood that a particular object might\nappear in an image. All of these are examples of prior distributions or probabilities and they\ncan be used to produce more reliable estimates.\nAs we have already seen in (3.68) and (3.106), Bayes’ Rule states that a posterior distribu-\ntion p(x|y) over the unknowns x given the measurements y can be obtained by multiplying\n4 In fact, the maximum likelihood estimate is just the noisy image itself.",
  "785": "B.5 Markov random ﬁelds\n763\nthe measurement likelihood p(y|x) by the prior distribution p(x),\np(x|y) = p(y|x)p(x)\np(y)\n,\n(B.21)\nwhere p(y) =\nR\nx p(y|x)p(x) is a normalizing constant used to make the p(x|y) distribution\nproper (integrate to 1). Taking the negative logarithm of both sides of Equation (B.21), we\nget\n−log p(x|y) = −log p(y|x) −log p(x) + log p(y),\n(B.22)\nwhich is the negative posterior log likelihood. It is common to drop the constant log p(y) be-\ncause its value does not matter during energy minimization. However, if the prior distribution\np(x) depends on some unknown parameters, we may wish to keep log p(y) in order to com-\npute the most likely value of these parameters using Occam’s razor, i.e., by maximizing the\nlikelihood of the observations, or to select the correct number of free parameters using model\nselection (Hastie, Tibshirani, and Friedman 2001; Torr 2002; Bishop 2006; Robert 2007).\nTo ﬁnd the most likely (maximum a posteriori or MAP) solution x given some measure-\nments y, we simply minimize this negative log likelihood, which can also be thought of as an\nenergy,\nE(x, y) = Ed(x, y) + Ep(x).\n(B.23)\nThe ﬁrst term Ed(x, y) is the data energy or data penalty and measures the negative log\nlikelihood that the measurements y were observed given the unknown state x. The second\nterm Ep(x) is the prior energy and it plays a role analogous to the smoothness energy in\nregularization. Note that the MAP estimate may not always be desirable, since it selects the\n“peak” in the posterior distribution rather than some more stable statistic such as MSE—see\nthe discussion in Appendix B.2 about loss functions and decision theory.\nB.5 Markov random ﬁelds\nMarkov random ﬁelds (Blake, Kohli, and Rother 2010) are the most popular types of prior\nmodel for gridded image-like data,5 which include not only regular natural images (Sec-\ntion 3.7.2) but also two-dimensional ﬁelds such as optic ﬂow (Chapter 8) or depth maps\n(Chapter 11), as well as binary ﬁelds, such as segmentations (Section 5.5).\nAs we discussed in Section 3.7.2, the prior probability p(x) for a Markov random ﬁeld is\na Gibbs or Boltzmann distribution, whose negative log likelihood (according to the Hammer-\n5 Alternative formulations include power spectra (Section 3.4.3) and non-local means (Buades, Coll, and Morel\n2008).",
  "786": "764\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nf (i, j)\nsx(i, j)\nf (i, j+1)\nsy(i, j)\nw(i, j)\nd (i, j)\nf (i+1, j)\nf (i+1, j+1)\nFigure B.1\nGraphical model for an N4 neighborhood Markov random ﬁeld. The white\ncircles are the unknowns f(i, j), while the dark circles are the input data d(i, j). The sx(i, j)\nand sy(i, j) black boxes denote arbitrary interaction potentials between adjacent nodes in the\nrandom ﬁeld, and the w(i, j) denote the data penalty functions. They are all examples of the\ngeneral potentials Vi,j,k,l(f(i, j), f(k, l)) used in Equation (B.24).\nsley–Clifford Theorem) can be written as a sum of pairwise interaction potentials,\nEp(x) =\nX\n{(i,j),(k,l)}∈N\nVi,j,k,l(f(i, j), f(k, l)),\n(B.24)\nwhere N(i, j) denotes the neighbors of pixel (i, j). In the more general case, MRFs can also\ncontain unary potentials, as well as higher-order potentials deﬁned over larger cardinality\ncliques (Kindermann and Snell 1980; Geman and Geman 1984; Bishop 2006; Potetz and Lee\n2008; Kohli, Kumar, and Torr 2009; Kohli, Ladick´y, and Torr 2009; Rother, Kohli, Feng et\nal. 2009; Alahari, Kohli, and Torr 2011). They can also contain line processes, i.e., additional\nbinary variables that mediate discontinuities between adjacent elements (Geman and Geman\n1984). Black and Rangarajan (1996) show how independent line process variables can be\neliminated and incorporated into regular MRFs using robust pairwise penalty functions.\nThe most commonly used neighborhood in Markov random ﬁeld modeling is the N4\nneighborhood, where each pixel in the ﬁeld f(i, j) interacts only with its immediate neighbors—\nFigure B.1 shows such an N4 MRF. The sx(i, j) and sy(i, j) black boxes denote arbitrary\ninteraction potentials between adjacent nodes in the random ﬁeld and the w(i, j) denote the\nelemental data penalty terms in Ed (B.23). These square nodes can also be interpreted as fac-\ntors in a factor graph version of the undirected graphical model (Bishop 2006; Wainwright\nand Jordan 2008; Koller and Friedman 2009), which is another name for interaction poten-\ntials. (Strictly speaking, the factors are improper probability functions whose product is the\nun-normalized posterior distribution.)\nMore complex and higher-dimensional interaction models and neighborhoods are also",
  "787": "B.5 Markov random ﬁelds\n765\npossible. For example, 2D grids can be enhanced with the addition of diagonal connections\n(an N8 neighborhood) or even larger numbers of pairwise terms (Boykov and Kolmogorov\n2003; Rother, Kolmogorov, Lempitsky et al. 2007). 3D grids can be used to compute glob-\nally optimal segmentations in 3D volumetric medical images (Boykov and Funka-Lea 2006)\n(Section 5.5.1). Higher-order cliques can also be used to develop more sophisticated models\n(Potetz and Lee 2008; Kohli, Ladick´y, and Torr 2009; Kohli, Kumar, and Torr 2009).\nOne of the biggest challenges in using MRF models is to develop efﬁcient inference algo-\nrithms that will ﬁnd low-energy solutions (Veksler 1999; Boykov, Veksler, and Zabih 2001;\nKohli 2007; Kumar 2008). Over the years, a large variety of such algorithms have been de-\nveloped, including simulated annealing, graph cuts, and loopy belief propagation. The choice\nof inference technique can greatly affect the overall performance of a vision system. For\nexample, most of the top-performing algorithms on the Middlebury Stereo Evaluation page\neither use belief propagation or graph cuts.\nIn the next few subsections, we review some of the more widely used MRF inference\ntechniques. More in-depth descriptions of most of these algorithms can be found in a re-\ncently published book on advances in MRF techniques (Blake, Kohli, and Rother 2010).\nExperimental comparisons, along with test datasets and reference software, are provided by\nSzeliski, Zabih, Scharstein et al. (2008).6\nB.5.1 Gradient descent and simulated annealing\nThe simplest optimization technique is gradient descent, which minimizes the energy by\nchanging independent subsets of nodes to take on lower-energy conﬁgurations. Such tech-\nniques go under a variety of names, including contextual classiﬁcation (Kittler and F¨oglein\n1984) and iterated conditional modes (ICM) (Besag 1986).7 Variables can either be updated\nsequentially, e.g., in raster scan, or in parallel, e.g., using red–black coloring on a checker-\nboard. Chou and Brown (1990) suggests using highest conﬁdence ﬁrst (HCF), i.e., choosing\nvariables based on how large a difference they make in reducing the energy.\nThe problem with gradient descent is that it is prone to getting stuck in local minima,\nwhich is almost always the case with MRF problems. One way around this is to use stochastic\ngradient descent or Markov chain Monte Carlo (MCMC) (Metropolis, Rosenbluth, Rosen-\nbluth et al. 1953), i.e., to randomly take occasional uphill steps in order to get out of such\nminima. One popular update rule is the Gibbs sampler (Geman and Geman 1984); rather\nthan choosing the lowest energy state for a variable being updated, it chooses the state with\n6 http://vision.middlebury.edu/MRF/.\n7 The name comes from iteratively setting variables to the mode (most likely, i.e., lowest energy) state conditioned\non its currently ﬁxed neighbors.",
  "788": "766\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nprobability\np(x) ∝e−E(x)/T ,\n(B.25)\nwhere T is called the temperature and controls how likely the system is to choose a more\nrandom update. Stochastic gradient descent is usually combined with simulated annealing\n(Kirkpatrick, Gelatt, and Vecchi 1983), which starts at a relatively high temperature, thereby\nrandomly exploring a large part of the state space, and gradually cools (anneals) the tem-\nperature to ﬁnd a good local minimum. During the late 1980s, simulated annealing was the\nmethod of choice for solving MRF inference problems (Szeliski 1986; Marroquin, Mitter,\nand Poggio 1985; Barnard 1989).\nAnother variant on simulated annealing is the Swendsen–Wang algorithm (Swendsen and\nWang 1987; Barbu and Zhu 2003, 2005). Here, instead of “ﬂipping” (changing) single vari-\nables, a connected subset of variables, chosen using a random walk based on MRF connec-\ntively strengths, is selected as the basic update unit. This can sometimes help make larger\nstate changes, and hence ﬁnd better-quality solutions in less time.\nWhile simulated annealing has largely been superseded by the newer graph cuts and loopy\nbelief propagation techniques, it still occasionally ﬁnds use, especially in highly connected\nand highly non-submodular graphs (Rother, Kolmogorov, Lempitsky et al. 2007).\nB.5.2 Dynamic programming\nDynamic programming (DP) is an efﬁcient inference procedure that works for any tree-\nstructured graphical model, i.e., one that does not have any cycles. Given such a tree, pick\nany node as the root r and ﬁguratively pick up the tree by its root. The depth or distance of all\nthe other nodes from this root induces a partial ordering over the vertices, from which a total\nordering can be obtained by arbitrarily breaking ties. Let us now lay out this graph as a tree\nwith the root on the right and indices increasing from left to right, as shown in Figure B.2a.\nBefore describing the DP algorithm, let us re-write the potential function of Equation (B.24)\nin a more general but succinct form,\nE(x) =\nX\n(i,j)∈N\nVi,j(xi, xj) +\nX\ni\nVi(xi),\n(B.26)\nwhere instead of using pixel indices (i, j) and (k, l), we just use scalar index variables i\nand j. We also replace the function value f(i, j) with the more succinct notation xi, with\nthe {xi} variables making up the state vector x. We can simplify this function even further\nby adding dummy nodes (vertices) i−for every node that has a non-zero Vi(xi) and setting\nVi,i−(xi, xi−) = Vi(xi), which lets us drop the Vi terms from (B.26).\nDynamic programming proceeds by computing partial sums in a left-to-right fashion, i.e.,\nin order of increasing variable index. Let Ck be the children of k, i.e., i < k, (i, k) ∈N).",
  "789": "B.5 Markov random ﬁelds\n767\nxk\nVik\nxj\nxi\nVjk\n...\n...\n...\nxr\nVij\nxk\nVijk\nxj\nxi\n...\n...\n...\n(a)\n(b)\nFigure B.2\nDynamic programming over a tree drawn as a factor graph. (a) To compute\nthe lowest energy solution ˆEk(xk) at node xk conditioned on the best solutions to the left\nof this node, we enumerate all possible values of ˆEi(xi) + Vik(xi, xk) and pick the smallest\none (and similarly for j). (b) For higher-order cliques, we need to try all combinations of\n(xi, xj) in order to select the best possible conﬁguration. The arrows show the basic ﬂow\nof the computation. The lightly shaded factor Vij in (a) shows an additional connection that\nturns the tree into a cyclic graph, for which exact inference cannot be efﬁciently computed.\nThen, deﬁne\n˜Ek(x) =\nX\ni<k, j≤k\nVi,j(xi, xj) =\nX\ni∈Ck\nh\nVi,k(xi, xk) + ˜Ei(x)\ni\n,\n(B.27)\nas a partial sum of (B.26) over all variables up to and including k, i.e., over all parts of the\ngraph shown in Figure B.2a to the left of xk. This sum depends on the state of all the unknown\nvariables in x with i ≤k.\nNow suppose we wish to ﬁnd the setting for all variables i < k that minimizes this sum.\nIt turns out that we can use a simple recursive formula\nˆEk(xk) =\nmin\n{xi, i<k}\n˜Ek(x) =\nX\ni∈Ck\nmin\nxi\nh\nVi,k(xi, xk) + ˆEi(xi)\ni\n(B.28)\nto ﬁnd this minimum. Visually, this is easy to understand. Looking at Figure B.2a, associate\nan energy ˆEk(xk) with each node k and each possible setting of its value xk that is based on\nthe best possible setting of variables to the left of that node. It is easy to convince yourself\nthat in this ﬁgure, you only need to know ˆEi(xi) and ˆEj(xj) in order to compute this value.\nOnce the ﬂow of information in the tree has been processed from left to right, the min-\nimum value of ˆEr(xr) at the root gives the MAP (lowest-energy) solution for E(x). The\nroot node is set to the choice of xr that minimizes this function, and other nodes are set in a\nbackward chaining pass by selecting the values of child nodes i ∈Ck that were minimal in\nthe original recursion (B.28).",
  "790": "768\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nDynamic programming is not restricted to trees with pairwise potentials. Figure B.2b\nshows an example of a three-way potential Vijk(xi, xj, xk) inside a tree. To compute the\noptimum value of ˆEk(xk), the recursion formula in (B.28) now has to evaluate the mini-\nmum over all combinations of possible state values leading into a factor node (gray box).\nFor this reason, dynamic programming is normally exponential in complexity in the order\nof the clique size, i.e., a clique of size n with l labels at each node requires the evaluation\nof ln−1 possible states (Potetz and Lee 2008; Kohli, Kumar, and Torr 2009). However, for\ncertain kinds of potential functions Vi,k(xi, xk), including the Potts model (delta function),\nabsolute values (total variation), and quadratic (Gaussian MRF), Felzenszwalb and Hutten-\nlocher (2006) show how to reduce the complexity of the min-ﬁnding step (B.28) from O(l2)\nto O(l). In Appendix B.5.3, we also discuss how Potetz and Lee (2008) reduce the complexity\nfor special kinds of higher-order clique, i.e., linear summations followed by non-linearities.\nFigure B.2a also shows what happens if we add an extra factor between nodes i and j.\nIn this case, the graph is no longer a tree, i.e., it contains a cycle. It is no longer possible\nto use the recursion formula (B.28), since ˆEi(xi) now appears in two different terms inside\nthe summation, i.e., as a child of both nodes j and k, and the same setting for xi may not\nminimize both. In other words, when loops exist, there is no ordering of the variables that\nallows the recursion (elimination) in (B.28) to be well-founded.\nIt is, however, possible to convert small loops into higher-order factors and to solve these\nas shown in Figure B.2b. However, graphs with long loops or meshes result in extremely\nlarge clique sizes and hence an amount of computation potentially exponential in the size of\nthe graph.\nB.5.3 Belief propagation\nBelief propagation is an inference technique originally developed for trees (Pearl 1988) but\nmore recently extended to “loopy” (cyclic) graphs such as MRFs (Frey and MacKay 1997;\nFreeman, Pasztor, and Carmichael 2000; Yedidia, Freeman, and Weiss 2001; Weiss and Free-\nman 2001a,b; Yuille 2002; Sun, Zheng, and Shum 2003; Felzenszwalb and Huttenlocher\n2006). It is closely related to dynamic programming, in that both techniques pass messages\nforward and backward over a tree or graph. In fact, one of the two variants of belief prop-\nagation, the max-product rule, performs the exact same computation (inference) as dynamic\nprogramming, albeit using probabilities instead of energies.\nRecall that the energy we are minimizing in MAP estimation (B.26) is the negative log\nlikelihood (B.12, B.13, and B.22) of a factored Gibbs posterior distribution,\np(x) =\nY\n(i,j)∈N\nφi,j(xi, xj),\n(B.29)",
  "791": "B.5 Markov random ﬁelds\n769\nwhere\nφi,j(xi, xj) = e−Vi,j(xi,xj)\n(B.30)\nare the pairwise interaction potentials. We can rewrite (B.27) as\n˜pk(x) =\nY\ni<k, j≤k\nφi,j(xi, xj) =\nY\ni∈Ck\n˜pi,k(x),\n(B.31)\nwhere\n˜pi,k(x) = φi,k(xi, xk)˜pi(x).\n(B.32)\nWe can therefore rewrite (B.28) as\nˆpk(xk) =\nmax\n{xi, i<k} ˜pk(x) =\nY\ni∈Ck\nˆpi,k(x),\n(B.33)\nwith\nˆpi,k(x) = max\nxi φi,k(xi, xk)ˆpi(x).\n(B.34)\nEquation (B.34) is the max update rule evaluated at all square box factors in Figure B.2a,\nwhile (B.33) is the product rule evaluated at the nodes. The probability distribution ˆpi,k(x)\nis often interpreted as a message passing information about child i to parent k and is hence\nwritten as mi,k(xk) (Yedidia, Freeman, and Weiss 2001) or µi→k(xk) (Bishop 2006).\nThe max-product rule can be used to compute the MAP estimate in a tree using the same\nkind of forward and backward sweep as in dynamic programming (which is sometimes called\nthe max-sum algorithm (Bishop 2006)). An alternative rule, known as the sum–product, sums\nover all possible values in (B.34) rather than taking the maximum, in essence computing\nthe expected distribution rather than the maximum likelihood distribution. This produces a\nset of probability estimates that can be used to compute the marginal distributions bi(xi) =\nP\nx\\xi p(x) (Pearl 1988; Yedidia, Freeman, and Weiss 2001; Bishop 2006).\nBelief propagation may not produce optimal estimates for cyclic graphs for the same\nreason that dynamic programming fails to work, i.e., because a node with multiple parents\nmay take on different optimal values for each of the parents, i.e., there is no unique elim-\nination ordering. Early algorithms for extending belief propagation to graphs with cycles,\ndubbed loopy belief propagation, performed the updates in parallel over the graph, i.e., us-\ning synchronous updates (Frey and MacKay 1997; Freeman, Pasztor, and Carmichael 2000;\nYedidia, Freeman, and Weiss 2001; Weiss and Freeman 2001a,b; Yuille 2002; Sun, Zheng,\nand Shum 2003; Felzenszwalb and Huttenlocher 2006).\nFor example, Felzenszwalb and Huttenlocher (2006) split an N4 graph into its red and\nblack (checkerboard) components and alternate between sending messages from the red nodes\nto the black and vice versa. They also use multi-grid (coarser level) updates to speed up the\nconvergence. As discussed previously, to reduce the complexity of the basic max-product",
  "792": "770\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nupdate rule (B.28) from O(l2) to O(l), they develop specialized update algorithms for sev-\neral cost functions Vi,k(xi, xk), including the Potts model (delta function), absolute values\n(total variation), and quadratic (Gaussian MRF). A related algorithm, mean ﬁeld diffusion\n(Scharstein and Szeliski 1998), also uses synchronous updates between nodes to compute\nmarginal distributions. Yuille (2010) discusses the relationships between mean ﬁeld theory\nand loopy belief propagation.\nMore recent loopy belief propagation algorithms and their variants use sequential scans\nthrough the graph (Szeliski, Zabih, Scharstein et al. 2008). For example, Tappen and Free-\nman (2003) pass messages from left to right along each row and then reverse the direction\nonce they reach the end. This is similar to treating each row as an independent tree (chain),\nexcept that messages from nodes above and below the row are also incorporated. They then\nperform similar computations along columns. These sequential updates allow the information\nto propagate much more quickly across the image than synchronous updates.\nThe other belief propagation variant tested by Szeliski, Zabih, Scharstein et al. (2008),\nwhich they call BP-S or TRW-S, is based on Kolmogorov’s (2006) sequential extension of\nthe tree-reweighted message passing of Wainwright, Jaakkola, and Willsky (2005). TRW\nﬁrst selects a set of trees from the neighborhood graph and computes a set of probability\ndistributions over each tree. These are then used to reweight the messages being passed\nduring loopy belief propagation. The sequential version of TRW, called TRW-S, processed\nnodes in scan-line order, with a forward and backward pass. In the forward pass, each node\nsends messages to its right and bottom neighbors. In the backward pass, messages are sent\nto the left and upper neighbors. TRW-S also computes a lower bound on the energy, which\nis used by Szeliski, Zabih, Scharstein et al. (2008) to estimate how close to the best possible\nsolution all of the MRF inference algorithms being evaluated get.\nAs with dynamic programming, belief propagation techniques also become less efﬁcient\nas the order of each factor clique increases. Potetz and Lee (2008) shows how this complex-\nity can be reduced back to linear in the clique order for continuous-valued problems where\nthe factors involve linear summations followed by a non-linearity, which is typical of more\nsophisticated MRF models such as ﬁelds of experts (Roth and Black 2009) and steerable ran-\ndom ﬁelds (Roth and Black 2007b). Kohli, Kumar, and Torr (2009) and Alahari, Kohli, and\nTorr (2011) develop alternative ways for dealing with higher-order cliques in the context of\ngraph cut algorithms.\nB.5.4 Graph cuts\nThe computer vision community has adopted “graph cuts” as an informal name to describe\na large family of MRF inference algorithms based on solving one or more min-cut or max-\nﬂow problems (Boykov, Veksler, and Zabih 2001; Boykov and Kolmogorov 2010; Boykov,",
  "793": "B.5 Markov random ﬁelds\n771\n  Object\nterminal\n  terminal\nBackground \np\nq\nr\nw\nv\nS\nT\nBackground \n  Object\n  terminal\nterminal\np\nq\nr\nw\nv\nS\nT\ncut\n(a)\n(b)\nFigure B.3 Graph cuts for minimizing binary sub-modular MRF energies (Boykov and Jolly\n2001) c⃝2001 IEEE: (a) energy function encoded as a max ﬂow problem; (b) the minimum\ncut determines the region boundary.\nVeksler, and Zabih 2010; Ishikawa and Veksler 2010).\nThe simplest example of an MRF graph cut is the polynomial-time algorithm for perform-\ning exact minimization of a binary MRF originally developed by Greig, Porteous, and Seheult\n(1989) and brought to the attention of the computer vision community by Boykov, Veksler,\nand Zabih (2001) and Boykov and Jolly (2001). The basic construction of the min-cut graph\nfrom an MRF energy function is shown in Figure B.3 and described in Sections 3.7.2 and\n5.5. In brief, the nodes in an MRF are connected to special source and sink nodes, and the\nminimum cut between these two nodes, whose cost is exactly that of the MRF energy un-\nder a binary assignment of labels, is computed using a polynomial-time max ﬂow algorithm\n(Goldberg and Tarjan 1988; Boykov and Kolmogorov 2004).\nAs discussed in Section 5.5, important extensions of this basic algorithm have been made\nfor the case of directed edges (Kolmogorov and Boykov 2005), larger neighborhoods (Boykov\nand Kolmogorov 2003; Kolmogorov and Boykov 2005), connectivity priors (Vicente, Kol-\nmogorov, and Rother 2008), and shape priors (Lempitsky and Boykov 2007; Lempitsky,\nBlake, and Rother 2008). Kolmogorov and Zabih (2004) formally characterize the class\nof binary energy potentials (regularity conditions) for which these algorithms ﬁnd the global\nminimum. Komodakis, Tziritas, and Paragios (2008) and Rother, Kolmogorov, Lempitsky et\nal. (2007) provide good algorithms for the cases when they do not.\nBinary MRF problems can also be approximately solved by turning them into continuous\n[0, 1] problems, solving them either as linear systems (Grady 2006; Sinop and Grady 2007;\nGrady and Alvino 2008; Grady 2008; Grady and Ali 2008; Singaraju, Grady, and Vidal 2008;\nCouprie, Grady, Najman et al. 2009) (the random walker model) or by computing geodesic",
  "794": "772\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ndistances (Bai and Sapiro 2009; Criminisi, Sharp, and Blake 2008) and then thresholding the\nresults. More details on these techniques are provided in Section 5.5 and a nice review can\nbe found in the work of Singaraju, Grady, Sinop et al. (2010). A different connection to\ncontinuous segmentation techniques, this time to the literature on level sets (Section 5.1.4),\nis made by Boykov, Kolmogorov, Cremers et al. (2006), who develop an approach to solving\nsurface propagation PDEs based on combinatorial graph cut algorithms—Boykov and Funka-\nLea (2006) discuss this and related techniques.\nMulti-valued MRF inference problems usually require solving a series of related binary\nMRF problems (Boykov, Veksler, and Zabih 2001), although for special cases, such as some\nconvex functions, a single graph cut may sufﬁce (Ishikawa 2003; Schlesinger and Flach\n2006). The seminal work in this area is that of Boykov, Veksler, and Zabih (2001), who intro-\nduced two algorithms, called the swap move and the expansion move, which are sketched in\nFigure B.4. The α–β-swap move selects two labels (usually by cycling through all possible\npairings) and then formulates a binary MRF problem that allows any pixels currently labeled\nas either α or β to optionally switch their values to the other label. The α-expansion move\nallows any pixel in the MRF to take on the α label or to keep its current identity. It is easy\nto see by inspection that both of these moves result in binary MRFs with well-deﬁned energy\nfunctions.\nBecause these algorithms use a binary MRF optimization inside their inner loop, they\nare subject to the constraints on the energy functions that occur in the binary labeling case\n(Kolmogorov and Zabih 2004). However, more recent algorithms such as those developed by\nKomodakis, Tziritas, and Paragios (2008) and Rother, Kolmogorov, Lempitsky et al. (2007)\ncan be used to provide approximate solutions for more general energy functions. Efﬁcient\nalgorithms for re-using previous solutions (ﬂow- and cut-recycling) have been developed for\non-line applications such as dynamic MRFs (Kohli and Torr 2005; Juan and Boykov 2006;\nAlahari, Kohli, and Torr 2011) and coarse-to-ﬁne banded graph cuts (Agarwala, Zheng, Pal et\nal. 2005; Lombaert, Sun, Grady et al. 2005; Juan and Boykov 2006). It is also now possible to\nminimize the number of labels used as part of the alpha-expansion process (Delong, Osokin,\nIsack et al. 2010).\nIn experimental comparisons, α-expansions usually converge faster to a good solution\nthan α–β-swaps (Szeliski, Zabih, Scharstein et al. 2008), especially for problems that in-\nvolve large regions of identical labels, such as the labeling of source imagery in image stitch-\ning (Figure 3.60). For truncated convex energy functions deﬁned over ordinal values, more\naccurate algorithms that consider complete ranges of labels inside each min-cut and often\nproduce lower energies have been developed (Veksler 2007; Kumar and Torr 2008; Kumar,\nVeksler, and Torr 2010). The whole ﬁeld of efﬁcient MRF inference algorithms is rapidly\ndeveloping, as witnessed by a recent special journal issue (Kohli and Torr 2008; Komodakis,\nTziritas, and Paragios 2008; Olsson, Eriksson, and Kahl 2008; Potetz and Lee 2008), articles",
  "795": "B.5 Markov random ﬁelds\n773\n(a) initial labeling\n(b) standard move\n(c) α-β-swap\n(d) α-expansion\nFigure B.4 Multi-level graph optimization from (Boykov, Veksler, and Zabih 2001) c⃝2001\nIEEE: (a) initial problem conﬁguration; (b) the standard move changes only one pixel; (c)\nthe α–β-swap optimally exchanges all α- and β-labeled pixels; (d) the α-expansion move\noptimally selects among current pixel values and the α label.\n(Alahari, Kohli, and Torr 2011), and a forthcoming book (Blake, Kohli, and Rother 2010).\nB.5.5 Linear programming\n8 Many successful algorithms for MRF optimization are based on the linear programming\n(LP) relaxation of the energy function (Weiss, Yanover, and Meltzer 2010). For some prac-\ntical MRF problems, LP-based techniques can produce globally minimal solutions (Meltzer,\nYanover, and Weiss 2005), even though MRF inference is in general NP-hard. In order to\ndescribe this relaxation, let us ﬁrst rewrite the energy function (B.26) as\nE(x)\n=\nX\n(i,j)∈N\nVi,j(xi, xj) +\nX\ni\nVi(xi)\n(B.35)\n=\nX\ni,j,α,β\nVi,j(α, β)xi,j;α,β +\nX\ni,α\nVi(α)xi;α\n(B.36)\nsubject to\nxi;α\n=\nX\nβ\nxi,j;α,β\n∀(i, j) ∈N, α,\n(B.37)\nxj;β\n=\nX\nα\nxi,j;α,β\n∀(i, j) ∈N, β,\nand\n(B.38)\nxi,α, xi,j;α,β\n∈\n{0, 1}.\n(B.39)\nHere, α and β range over label values and xi;α = δ(xi −α) and xij;αβ = δ(xi −α)δ(xj −β)\nare indicator variables of assignments xi = α and (xi, xj) = (α, β), respectively. The LP\nrelaxation is obtained by replacing the discreteness constraints (B.39) with linear constraints\nxij;αβ ∈[0, 1]. It is easy to show that the optimal value of (B.36) is a lower bound on (B.26).\n8 This section was contributed by Vladimir Kolmogorov. Thanks!",
  "796": "774\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nThis relaxation has been extensively studied in the literature, starting with the work of\nSchlesinger (1976). An important question is how to solve this LP efﬁciently. Unfortunately,\ngeneral-purpose LP solvers cannot handle large problems in vision (Yanover, Meltzer, and\nWeiss 2006). A large number of customized iterative techniques have been proposed. Most\nof these solve the dual problem, i.e., they formulate a lower bound on (B.36) and then try to\nmaximize this bound. The bound is often formulated using a convex combination of trees, as\nproposed in (Wainwright, Jaakkola, and Willsky 2005).\nThe LP lower bound can be maximized via a number of techniques, such as max-sum dif-\nfusion (Werner 2007), tree-reweighted message passing (TRW) (Wainwright, Jaakkola, and\nWillsky 2005; Kolmogorov 2006), subgradient methods (Schlesinger and Giginyak 2007a,b;\nKomodakis, Paragios, and Tziritas 2007), and Bregman projections (Ravikumar, Agarwal,\nand Wainwright 2008). Note that the max-sum diffusion and TRW algorithms are not guar-\nanteed to converge to a global maximum of LP—they may get stuck at a suboptimal point\n(Kolmogorov 2006; Werner 2007). However, in practice, this does not appear to be a problem\n(Kolmogorov 2006).\nFor some vision applications, algorithms based on relaxation (B.36) produce excellent\nresults. However, this is not guaranteed in all cases—after all, the problem is NP-hard.\nRecently, researchers have investigated alternative linear programming relaxations (Sontag\nand Jaakkola 2007; Sontag, Meltzer, Globerson et al. 2008; Komodakis and Paragios 2008;\nSchraudolph 2010). These algorithms are capable of producing tighter bounds compared to\n(B.36) at the expense of additional computational cost.\nLP relaxation and alpha expansion.\nSolving a linear program produces primal and dual\nsolutions that satisfy complementary slackness conditions. In general, the primal solution\nof (B.36) does not have to be integer-valued so, in practice, we may have to round it to\nobtain a valid labeling x. An alternative proposed by Komodakis and Tziritas (2007a); Ko-\nmodakis, Tziritas, and Paragios (2007) is to search for primal and dual solutions such that\nthey satisfy approximate complementary slackness conditions and the primal solution is al-\nready integer-valued. Several max-ﬂow-based algorithms are proposed by (Komodakis and\nTziritas 2007a; Komodakis, Tziritas, and Paragios 2007) for this purpose and the Fast-PD\nmethod (Komodakis, Tziritas, and Paragios 2007) is shown to perform best. In the case of\nmetric interactions, the default version of Fast-PD produces the same primal solution as the\nalpha-expansion algorithm (Boykov, Veksler, and Zabih 2001). This provides an interesting\ninterpretation of the alpha expansion algorithm as trying to approximately solve relaxation\n(B.36).\nUnlike the standard alpha expansion algorithm, Fast-PD also maintains a dual solution\nand thus runs faster in practice. Fast-PD can be extended to the case of semi-metric interac-\ntions (Komodakis, Tziritas, and Paragios 2007). The primal version of such extension was",
  "797": "B.6 Uncertainty estimation (error analysis)\n775\nalso given by Rother, Kumar, Kolmogorov et al. (2005).\nB.6 Uncertainty estimation (error analysis)\nIn addition to computing the most likely estimate, many applications require an estimate for\nthe uncertainty in this estimate.9 The most general way to do this is to compute a complete\nprobability distribution over all of the unknowns but this is generally intractable. The one spe-\ncial case where it is easy to obtain a simple description for this distribution is linear estimation\nproblems with Gaussian noise, where the joint energy function (negative log likelihood of the\nposterior estimate) is a quadratic. In this case, the posterior distribution is a multi-variate\nGaussian and the covariance can be computed directly from the inverse of the problem Hes-\nsian. (Another name for the inverse covariance matrix, which is equal to the Hessian in such\nsimple cases, is the information matrix.)\nEven here, however, the full covariance matrix may be too large to compute and store. For\nexample, in large structure from motion problems, a large sparse Hessian normally results in a\nfull dense covariance matrix. In such cases, it is often considered acceptable to report only the\nvariance in the estimated quantities or simple covariance estimates on individual parameters,\nsuch as 3D point positions or camera pose estimates (Szeliski 1990a). More insight into the\nproblem, e.g., the dominant modes of uncertainty, can be obtained using eigenvalue analysis\n(Szeliski and Kang 1997).\nFor problems where the posterior energy is non-quadratic, e.g., in non-linear or robustiﬁed\nleast squares, it is still often possible to obtain an estimate of the Hessian in the vicinity of the\noptimal solution. In this case, the Cramer–Rao lower bound on the uncertainty (covariance)\ncan be computed as the inverse of the Hessian. Another way of saying this is that while the\nlocal Hessian can underestimate how “wide” the energy function can be, the covariance can\nnever be smaller than the estimate based on this local quadratic approximation. It is also\npossible to estimate a different kind of uncertainty (min-marginal energies) in general MRFs\nwhere the MAP inference is performed using graph cuts (Kohli and Torr 2008).\nWhile many computer vision applications ignore uncertainty modeling, it is often useful\nto compute these estimates just to get an intuitive feeling for the reliability of the estimates.\nCertain applications, such as Kalman ﬁltering, require the computation of this uncertainty\n(either explicitly as posterior covariances or implicitly as inverse covariances) in order to\noptimally integrate new measurements with previously computed estimates.\n9 This is particularly true of classic photogrammetry applications, where the reporting of precision is almost\nalways considered mandatory (F¨orstner 2005).",
  "798": "776\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "799": "Appendix C\nSupplementary material\nC.1\nData sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778\nC.2\nSoftware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780\nC.3\nSlides and lectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 789\nC.4\nBibliography\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790",
  "800": "778\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIn this ﬁnal appendix, I summarize some of the supplementary materials that may be use-\nful to students, instructors, and researchers. The book’s Web site at http://szeliski.org/Book\ncontains updated lists of datasets and software, so please check there as well.\nC.1 Data sets\nOne of the keys to developing reliable vision algorithms is to test your procedures on chal-\nlenging and representative data sets. When ground truth or other people’s results are available,\nsuch test can be even more informative (and quantitative).\nOver the years, a large number of datasets have been developed for testing and evaluating\ncomputer vision algorithms. A number of these datasets (and software) are indexed on the\nComputer Vision Homepage.1 Some newer Web sites, such as CVonline (http://homepages.\ninf.ed.ac.uk/rbf/CVonline/), VisionBib.Com (http://datasets.visionbib.com/), and Computer\nVision online (http://computervisiononline.com/), have more recent pointers.\nBelow, I list some of the more popular data sets, grouped by the book chapters to which\nthey most closely correspond:\nChapter 2: Image formation\nCUReT: Columbia-Utrecht Reﬂectance and Texture Database, http://www1.cs.columbia.\nedu/CAVE/software/curet/ (Dana, van Ginneken, Nayar et al. 1999).\nMiddlebury Color Datasets: registered color images taken by different cameras to\nstudy how they transform gamuts and colors, http://vision.middlebury.edu/color/data/\n(Chakrabarti, Scharstein, and Zickler 2009).\nChapter 3: Image processing\nMiddlebury test datasets for evaluating MRF minimization/inference algorithms, http:\n//vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharstein et al. 2008).\nChapter 4: Feature detection and matching\nAfﬁne Covariant Features database for evaluating feature detector and descriptor match-\ning quality and repeatability, http://www.robots.ox.ac.uk/∼vgg/research/afﬁne/ (Miko-\nlajczyk and Schmid 2005; Mikolajczyk, Tuytelaars, Schmid et al. 2005).\nDatabase of matched image patches for learning and feature descriptor evaluation,\nhttp://cvlab.epﬂ.ch/∼brown/patchdata/patchdata.html (Winder and Brown 2007; Hua,\nBrown, and Winder 2007).\n1 http://www.cs.cmu.edu/∼cil/vision.html, although it has not been maintained since 2004.",
  "801": "C.1 Data sets\n779\nChapter 5: Segmentation\nBerkeley Segmentation Dataset and Benchmark of 1000 images labeled by 30 humans,\nalong with an evaluation, http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\ngrouping/segbench/ (Martin, Fowlkes, Tal et al. 2001).\nWeizmann segmentation evaluation database of 100 grayscale images with ground\ntruth segmentations, http://www.wisdom.weizmann.ac.il/∼vision/Seg Evaluation DB/\nindex.html (Alpert, Galun, Basri et al. 2007).\nChapter 8: Dense motion estimation\nThe Middlebury optic ﬂow evaluation Web site, http://vision.middlebury.edu/ﬂow/data\n(Baker, Scharstein, Lewis et al. 2009).\nThe Human-Assisted Motion Annotation database,\nhttp://people.csail.mit.edu/celiu/motionAnnotation/ (Liu, Freeman, Adelson et al. 2008)\nChapter 10: Computational photography\nHigh Dynamic Range radiance maps, http://www.debevec.org/Research/HDR/ (De-\nbevec and Malik 1997).\nAlpha matting evaluation Web site, http://alphamatting.com/ (Rhemann, Rother, Wang\net al. 2009).\nChapter 11: Stereo correspondence\nMiddlebury Stereo Datasets and Evaluation, http://vision.middlebury.edu/stereo/ (Scharstein\nand Szeliski 2002).\nStereo Classiﬁcation and Performance Evaluation of different aggregation costs for\nstereo matching, http://www.vision.deis.unibo.it/spe/SPEHome.aspx (Tombari, Mat-\ntoccia, Di Stefano et al. 2008).\nMiddlebury Multi-View Stereo Datasets, http://vision.middlebury.edu/mview/data/ (Seitz,\nCurless, Diebel et al. 2006).\nMulti-view and Oxford Colleges building reconstructions, http://www.robots.ox.ac.uk/\n∼vgg/data/data-mview.html.\nMulti-View Stereo Datasets, http://cvlab.epﬂ.ch/data/strechamvs/ (Strecha, Fransens,\nand Van Gool 2006).",
  "802": "780\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMulti-View Evaluation, http://cvlab.epﬂ.ch/∼strecha/multiview/ (Strecha, von Hansen,\nVan Gool et al. 2008).\nChapter 12: 3D reconstruction\nHumanEva: synchronized video and motion capture dataset for evaluation of artic-\nulated human motion, http://vision.cs.brown.edu/humaneva/ (Sigal, Balan, and Black\n2010).\nChapter 13: Image-based rendering\nThe (New) Stanford Light Field Archive, http://lightﬁeld.stanford.edu/ (Wilburn, Joshi,\nVaish et al. 2005).\nVirtual Viewpoint Video: multi-viewpoint video with per-frame depth maps, http:\n//research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/ (Zitnick, Kang, Uytten-\ndaele et al. 2004).\nChapter 14: Recognition\nFor a list of visual recognition datasets, see Tables 14.1–14.2. In addition to those,\nthere are also:\nBuffy pose classes, http://www.robots.ox.ac.uk/∼vgg/data/buffy pose classes/ and Buffy\nstickmen V2.1, http://www.robots.ox.ac.uk/∼vgg/data/stickmen/index.html (Ferrari, Marin-\nJimenez, and Zisserman 2009; Eichner and Ferrari 2009).\nH3D database of pose/joint annotated photographs of humans, http://www.eecs.berkeley.\nedu/∼lbourdev/h3d/ (Bourdev and Malik 2009).\nAction Recognition Datasets, http://www.cs.berkeley.edu/projects/vision/action, has point-\ners to several datasets for action and activity recognition, as well as some papers. The\nhuman action database at http://www.nada.kth.se/cvap/actions/ contains more action\nsequences.\nC.2 Software\nOne of the best sources for computer vision algorithms is the Open Source Computer Vision\n(OpenCV) library (http://opencv.willowgarage.com/wiki/), which was developed by Gary\nBradski and his colleagues at Intel and is now being maintained and extended at Willow\nGarage (Bradsky and Kaehler 2008). A partial list of the available functions, taken from\nhttp://opencv.willowgarage.com/documentation/cpp/ includes:",
  "803": "C.2 Software\n781\n• image processing and transforms (ﬁltering, morphology, pyramids);\n• geometric image transformations (rotations, resizing);\n• miscellaneous image transformations (Fourier transforms, distance transforms);\n• histograms;\n• segmentation (watershed, mean shift);\n• feature detection (Canny, Harris, Hough, MSER, SURF);\n• motion analysis and object tracking (Lucas–Kanade, mean shift);\n• camera calibration and 3D reconstruction;\n• machine learning (k nearest neighbors, support vector machines, decision trees, boost-\ning, random trees, expectation-maximization, and neural networks).\nThe Intel Performance Primitives (IPP) library, http://software.intel.com/en-us/intel-ipp/,\ncontains highly optimized code for a variety of image processing tasks. Many of the routines\nin OpenCV take advantage of this library, if it is installed, to run even faster. In terms of\nfunctionality, it has many of the same operators as those found in OpenCV, plus additional\nlibraries for image and video compression, signal and speech processing, and matrix algebra.\nThe MATLAB Image Processing Toolbox, http://www.mathworks.com/products/image/,\ncontains routines for spatial transformations (rotations, resizing), normalized cross-correla-\ntion, image analysis and statistics (edges, Hough transform), image enhancement (adaptive\nhistogram equalization, median ﬁltering) and restoration (deblurring), linear ﬁltering (con-\nvolution), image transforms (Fourier and DCT), and morphological operations (connected\ncomponents and distance transforms).\nTwo older libraries, which no longer appear to be under active development but contain\nmany useful routines, are VXL (C++ Libraries for Computer Vision Research and Implemen-\ntation, http://vxl.sourceforge.net/) and LTI-Lib 2 (http://www.ie.itcr.ac.cr/palvarado/ltilib-2/\nhomepage/).\nPhoto editing and viewing packages, such as Windows Live Photo Gallery, iPhoto, Picasa,\nGIMP, and IrfanView, can be useful for performing common processing tasks, converting for-\nmats, and viewing your results. They can also serve as interesting reference implementations\nfor image processing algorithms (such as tone correction or denoising) that you are trying to\ndevelop from scratch.\nThere are also software packages and infrastructure that can be helpful for building real-\ntime video processing demos. Vision on Tap (http://www.visionontap.com/) provides a Web",
  "804": "782\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nservice that will process your webcam video in real time (Chiu and Raskar 2009). Video-\nMan (VideoManager, http://videomanlib.sourceforge.net/) can be useful for getting real-time\nvideo-based demos and applications running. You can also use imread in MATLAB to read\ndirectly from any URL, such as a webcam.\nBelow, I list some additional software that can be found on the Web, grouped by the book\nchapters to which they most correspond:\nChapter 3: Image processing\nmatlabPyrTools—MATLAB source code for Laplacian pyramids, QMF/Wavelets, and\nsteerable pyramids, http://www.cns.nyu.edu/∼lcv/software.php (Simoncelli and Adel-\nson 1990a; Simoncelli, Freeman, Adelson et al. 1992).\nBLS-GSM image denoising, http://decsai.ugr.es/∼javier/denoise/ (Portilla, Strela, Wain-\nwright et al. 2003).\nFast bilateral ﬁltering code, http://people.csail.mit.edu/jiawen/#code (Chen, Paris, and\nDurand 2007).\nC++ implementation of the fast distance transform algorithm, http://people.cs.uchicago.\nedu/∼pff/dt/ (Felzenszwalb and Huttenlocher 2004a).\nGREYC’s Magic Image Converter, including image restoration software using regular-\nization and anisotropic diffusion, http://gmic.sourceforge.net/gimp.shtml (Tschumperl´e\nand Deriche 2005).\nChapter 4: Feature detection and matching\nVLFeat, an open and portable library of computer vision algorithms, http://vlfeat.org/\n(Vedaldi and Fulkerson 2008).\nSiftGPU: A GPU Implementation of Scale Invariant Feature Transform (SIFT), http:\n//www.cs.unc.edu/∼ccwu/siftgpu/ (Wu 2010).\nSURF: Speeded Up Robust Features, http://www.vision.ee.ethz.ch/∼surf/ (Bay, Tuyte-\nlaars, and Van Gool 2006).\nFAST corner detection, http://mi.eng.cam.ac.uk/∼er258/work/fast.html (Rosten and Drum-\nmond 2005, 2006).\nLinux binaries for afﬁne region detectors and descriptors, as well as MATLAB ﬁles to\ncompute repeatability and matching scores, http://www.robots.ox.ac.uk/∼vgg/research/\nafﬁne/.",
  "805": "C.2 Software\n783\nKanade–Lucas–Tomasi feature trackers: KLT, http://www.ces.clemson.edu/∼stb/klt/\n(Shi and Tomasi 1994); GPU-KLT, http://cs.unc.edu/∼cmzach/opensource.html (Zach,\nGallup, and Frahm 2008); and Lucas–Kanade 20 Years On, http://www.ri.cmu.edu/\nprojects/project 515.html (Baker and Matthews 2004).\nChapter 5: Segmentation\nEfﬁcient graph-based image segmentation, http://people.cs.uchicago.edu/∼pff/segment/\n(Felzenszwalb and Huttenlocher 2004b).\nEDISON, edge detection and image segmentation, http://coewww.rutgers.edu/riul/research/\ncode/EDISON/ (Meer and Georgescu 2001; Comaniciu and Meer 2002).\nNormalized cuts segmentation including intervening contours, http://www.cis.upenn.\nedu/∼jshi/software/ (Shi and Malik 2000; Malik, Belongie, Leung et al. 2001).\nSegmentation by weighted aggregation (SWA), http://www.cs.weizmann.ac.il/∼vision/\nSWA/ (Alpert, Galun, Basri et al. 2007).\nChapter 6: Feature-based alignment and calibration\nNon-iterative PnP algorithm, http://cvlab.epﬂ.ch/software/EPnP/ (Moreno-Noguer, Lep-\netit, and Fua 2007).\nTsai Camera Calibration Software, http://www-2.cs.cmu.edu/∼rgw/TsaiCode.html (Tsai\n1987).\nEasy Camera Calibration Toolkit, http://research.microsoft.com/en-us/um/people/zhang/\nCalib/ (Zhang 2000).\nCamera Calibration Toolbox for MATLAB, http://www.vision.caltech.edu/bouguetj/\ncalib doc/; a C version is included in OpenCV.\nMATLAB functions for multiple view geometry, http://www.robots.ox.ac.uk/∼vgg/hzbook/\ncode/ (Hartley and Zisserman 2004).\nChapter 7: Structure from motion\nSBA: A generic sparse bundle adjustment C/C++ package based on the Levenberg–\nMarquardt algorithm, http://www.ics.forth.gr/∼lourakis/sba/ (Lourakis and Argyros 2009).\nSimple sparse bundle adjustment (SSBA), http://cs.unc.edu/∼cmzach/opensource.html.",
  "806": "784\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBundler, structure from motion for unordered image collections, http://phototour.cs.\nwashington.edu/bundler/ (Snavely, Seitz, and Szeliski 2006).\nChapter 8: Dense motion estimation\nOptical ﬂow software, http://www.cs.brown.edu/∼black/code.html (Black and Anan-\ndan 1996).\nOptical ﬂow using total variation and conjugate gradient descent, http://people.csail.\nmit.edu/celiu/OpticalFlow/ (Liu 2009).\nTV-L1 optical ﬂow on the GPU, http://cs.unc.edu/∼cmzach/opensource.html (Zach,\nPock, and Bischof 2007a).\nelastix: a toolbox for rigid and nonrigid registration of images, http://elastix.isi.uu.nl/\n(Klein, Staring, and Pluim 2007).\nDeformable image registration using discrete optimization, http://www.mrf-registration.\nnet/deformable/index.html (Glocker, Komodakis, Tziritas et al. 2008).\nChapter 9: Image stitching\nMicrosoft Research Image Compositing Editor for stitching images, http://research.\nmicrosoft.com/en-us/um/redmond/groups/ivm/ice/.\nChapter 10: Computational photography\nHDRShop software for combining bracketed exposures into high-dynamic range radi-\nance images, http://projects.ict.usc.edu/graphics/HDRShop/.\nSuper-resolution code, http://www.robots.ox.ac.uk/∼vgg/software/SR/ (Pickup 2007;\nPickup, Capel, Roberts et al. 2007, 2009).\nChapter 11: Stereo correspondence\nStereoMatcher, standalone C++ stereo matching code, http://vision.middlebury.edu/\nstereo/code/ (Scharstein and Szeliski 2002).\nPatch-based multi-view stereo software (PMVS Version 2), http://grail.cs.washington.\nedu/software/pmvs/ (Furukawa and Ponce 2011).\nChapter 12: 3D reconstruction\nScanalyze: a system for aligning and merging range data, http://graphics.stanford.edu/\nsoftware/scanalyze/ (Curless and Levoy 1996).",
  "807": "C.2 Software\n785\nMeshLab: software for processing, editing, and visualizing unstructured 3D triangular\nmeshes, http://meshlab.sourceforge.net/.\nVRML viewers (various) are also a good way to visualize texture-mapped 3D models.\nSection 12.6.4: Whole body modeling and tracking\nBayesian 3D person tracking, http://www.cs.brown.edu/∼black/code.html (Sidenbladh,\nBlack, and Fleet 2000; Sidenbladh and Black 2003).\nHumanEva: baseline code for the tracking of articulated human motion, http://vision.\ncs.brown.edu/humaneva/ (Sigal, Balan, and Black 2010).\nSection 14.1.1: Face detection\nSample face detection code and evaluation tools,\nhttp://vision.ai.uiuc.edu/mhyang/face-detection-survey.html.\nSection 14.1.2: Pedestrian detection\nA simple object detector with boosting, http://people.csail.mit.edu/torralba/shortCourseRLOC/\nboosting/boosting.html (Hastie, Tibshirani, and Friedman 2001; Torralba, Murphy, and\nFreeman 2007).\nDiscriminatively trained deformable part models, http://people.cs.uchicago.edu/∼pff/\nlatent/ (Felzenszwalb, Girshick, McAllester et al. 2010).\nUpper-body detector, http://www.robots.ox.ac.uk/∼vgg/software/UpperBody/ (Ferrari,\nMarin-Jimenez, and Zisserman 2008).\n2D articulated human pose estimation software, http://www.vision.ee.ethz.ch/∼calvin/\narticulated human pose estimation code/ (Eichner and Ferrari 2009).\nSection 14.2.2: Active appearance and 3D shape models\nAAMtools: An active appearance modeling toolbox, http://cvsp.cs.ntua.gr/software/\nAAMtools/ (Papandreou and Maragos 2008).\nSection 14.3: Instance recognition\nFASTANN and FASTCLUSTER for approximate k-means (AKM), http://www.robots.\nox.ac.uk/∼vgg/software/ (Philbin, Chum, Isard et al. 2007).\nFeature matching using fast approximate nearest neighbors, http://people.cs.ubc.ca/\n∼mariusm/index.php/FLANN/FLANN (Muja and Lowe 2009).",
  "808": "786\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSection 14.4.1: Bag of words\nTwo bag of words classiﬁers, http://people.csail.mit.edu/fergus/iccv2005/bagwords.html\n(Fei-Fei and Perona 2005; Sivic, Russell, Efros et al. 2005).\nBag of features and hierarchical k-means, http://www.vlfeat.org/ (Nist´er and Stew´enius\n2006; Nowak, Jurie, and Triggs 2006).\nSection 14.4.2: Part-based models\nA simple parts and structure object detector, http://people.csail.mit.edu/fergus/iccv2005/\npartsstructure.html (Fischler and Elschlager 1973; Felzenszwalb and Huttenlocher 2005).\nSection 14.5.1: Machine learning software\nSupport vector machines (SVM) software (http://www.support-vector-machines.org/\nSVM soft.html) has pointers to lots of SVM libraries, including SVMlight, http://\nsvmlight.joachims.org/; LIBSVM, http://www.csie.ntu.edu.tw/∼cjlin/libsvm/ (Fan, Chen,\nand Lin 2005); and LIBLINEAR, http://www.csie.ntu.edu.tw/∼cjlin/liblinear/ (Fan,\nChang, Hsieh et al. 2008).\nKernel Machines: links to SVM, Gaussian processes, boosting, and other machine\nlearning algorithms, http://www.kernel-machines.org/software.\nMultiple kernels for image classiﬁcation, http://www.robots.ox.ac.uk/∼vgg/software/\nMKL/ (Varma and Ray 2007; Vedaldi, Gulshan, Varma et al. 2009).\nAppendix A.1–A.2: Matrix decompositions and linear least squares2\nBLAS (Basic Linear Algebra Subprograms), http://www.netlib.org/blas/ (Blackford,\nDemmel, Dongarra et al. 2002).\nLAPACK (Linear Algebra PACKage), http://www.netlib.org/lapack/ (Anderson, Bai,\nBischof et al. 1999).\nGotoBLAS, http://www.tacc.utexas.edu/tacc-projects/.\nATLAS (Automatically Tuned Linear Algebra Software), http://math-atlas.sourceforge.\nnet/ (Demmel, Dongarra, Eijkhout et al. 2005).\nIntel Math Kernel Library (MKL), http://software.intel.com/en-us/intel-mkl/.\nAMD Core Math Library (ACML), http://developer.amd.com/cpu/Libraries/acml/Pages/\ndefault.aspx.\n2 Thanks to Sameer Agarwal for suggesting and describing most of these sites.",
  "809": "C.2 Software\n787\nRobust PCA code, http://www.salle.url.edu/∼ftorre/papers/rpca2.html (De la Torre and\nBlack 2003).\nAppendix A.3: Non-linear least squares\nMINPACK, http://www.netlib.org/minpack/.\nlevmar: Levenberg–Marquardt nonlinear least squares algorithms, http://www.ics.forth.\ngr/∼lourakis/levmar/ (Madsen, Nielsen, and Tingleff 2004).\nAppendix A.4–A.5: Direct and iterative sparse matrix solvers\nSuiteSparse (various reordering algorithms, CHOLMOD) and SuiteSparse QR, http:\n//www.cise.uﬂ.edu/research/sparse/SuiteSparse/ (Davis 2006, 2008).\nPARDISO (iterative and sparse direct solution), http://www.pardiso-project.org/.\nTAUCS (sparse direct, iterative, out of core, preconditioners), http://www.tau.ac.il/\n∼stoledo/taucs/.\nHSL Mathematical Software Library, http://www.hsl.rl.ac.uk/index.html.\nTemplates for the solution of linear systems, http://www.netlib.org/linalg/html templates/\nTemplates.html (Barrett, Berry, Chan et al. 1994). Download the PDF for instructions\non how to get the software.\nITSOL, MIQR, and other sparse solvers, http://www-users.cs.umn.edu/∼saad/software/\n(Saad 2003).\nILUPACK, http://www-public.tu-bs.de/∼bolle/ilupack/.\nAppendix B: Bayesian modeling and inference\nMiddlebury source code for MRF minimization, http://vision.middlebury.edu/MRF/\ncode/ (Szeliski, Zabih, Scharstein et al. 2008).\nC++ code for efﬁcient belief propagation for early vision, http://people.cs.uchicago.\nedu/∼pff/bp/ (Felzenszwalb and Huttenlocher 2006).\nFastPD MRF optimization code, http://www.csd.uoc.gr/∼komod/FastPD (Komodakis\nand Tziritas 2007a; Komodakis, Tziritas, and Paragios 2008)",
  "810": "788\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\ndouble urand()\n{\nreturn ((double) rand()) / ((double) RAND MAX);\n}\nvoid grand(double& g1, double& g2)\n{\n#ifndef M PI\n#define M PI 3.14159265358979323846\n#endif // M PI\ndouble n1 = urand();\ndouble n2 = urand();\ndouble x1 = n1 + (n1 == 0); /* guard against log(0) */\ndouble sqlogn1 = sqrt(-2.0 * log (x1));\ndouble angl = (2.0 * M PI) * n2;\ng1 = sqlogn1 * cos(angl);\ng2 = sqlogn1 * sin(angl);\n}\nAlgorithm C.1 C algorithm for Gaussian random noise generation, using the Box–Muller\ntransform.\nGaussian noise generation.\nA lot of basic software packages come with a uniform random\nnoise generator (e.g., the rand() routine in Unix), but not all have a Gaussian random\nnoise generator. To compute a normally distributed random variable, you can use the Box–\nMuller transform (Box and Muller 1958), whose C code is given in Algorithm C.1—note that\nthis routine returns pairs of random variables. Alternative methods for generating Gaussian\nrandom numbers are given by Thomas, Luk, Leong et al. (2007).\nPseudocolor generation.\nIn many applications, it is convenient to be able to visualize the\nset of labels assigned to an image (or to image features such as lines). One of the easiest\nways to do this is to assign a unique color to each integer label. In my work, I have found it\nconvenient to distribute these labels in a quasi-uniform fashion around the RGB color cube\nusing the following idea.\nFor each (non-negative) label value, consider the bits as being split among the three color\nchannels, e.g., for a nine-bit value, the bits could be labeled RGBRGBRGB. After collecting\neach of the three color values, reverse the bits so that the low-order bits vary the most quickly.",
  "811": "C.3 Slides and lectures\n789\nIn practice, for eight-bit color channels, this bit reverse can be stored in a table or a complete\ntable mapping from labels to pseudocolors (say with 4092 entries) can be pre-computed.\nFigure 8.16 shows an example of such a pseudo-color mapping.\nGPU implementation\nThe advent of programmable GPUs with capabilities such as pixel shaders and compute\nshaders has led to the development of fast computer vision algorithms for real-time appli-\ncations such as segmentation, tracking, stereo, and motion estimation (Pock, Unger, Cremers\net al. 2008; Vineet and Narayanan 2008; Zach, Gallup, and Frahm 2008). A good source\nfor learning about such algorithms is the CVPR 2008 workshop on Visual Computer Vision\non GPUs (CVGPU), http://www.cs.unc.edu/∼jmf/Workshop on Computer Vision on GPU.\nhtml, whose papers can be found on the CVPR 2008 proceedings DVD. Additional sources\nfor GPU algorithms include the GPGPU Web site and workshops, http://gpgpu.org/, and the\nOpenVIDIA Web site, http://openvidia.sourceforge.net/index.php/OpenVIDIA.\nC.3 Slides and lectures\nAs I mentioned in the preface, I hope to post slides corresponding to the material in the book.\nUntil these are ready, your best bet is to look at the slides from the courses I have co-taught\nat the University of Washington, as well as related courses that have used a similar syllabus.\nHere is a partial list of such courses:\nUW 455: Undergraduate Computer Vision, http://www.cs.washington.edu/education/\ncourses/455/.\nUW 576: Graduate Computer Vision, http://www.cs.washington.edu/education/courses/\n576/.\nStanford CS233B: Introduction to Computer Vision, http://vision.stanford.edu/teaching/\ncs223b/.\nMIT 6.869: Advances in Computer Vision, http://people.csail.mit.edu/torralba/courses/\n6.869/6.869.computervision.htm.\nBerkeley CS 280: Computer Vision, http://www.eecs.berkeley.edu/∼trevor/CS280.html.\nUNC COMP 776: Computer Vision, http://www.cs.unc.edu/∼lazebnik/spring10/.\nMiddlebury CS 453: Computer Vision, http://www.cs.middlebury.edu/∼schar/courses/\ncs453-s10/.",
  "812": "790\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nRelated courses have also been taught on the topic of Computational Photography, e.g.,\nCMU 15-463: Computational Photography, http://graphics.cs.cmu.edu/courses/15-463/.\nMIT 6.815/6.865: Advanced Computational Photography, http://stellar.mit.edu/S/course/\n6/sp09/6.815/.\nStanford CS 448A: Computational photography on cell phones, http://graphics.stanford.\nedu/courses/cs448a-10/.\nSIGGRAPH courses on Computational Photography, http://web.media.mit.edu/∼raskar/\nphoto/.\nThere is also an excellent set of on-line lectures available on a range of computer vision\ntopics, such as belief propagation and graph cuts, at the UW-MSR Course of Vision Algo-\nrithms http://www.cs.washington.edu/education/courses/577/04sp/.\nC.4 Bibliography\nWhile a bibliography (BibTex .bib ﬁle) for all of the references cited in this book is avail-\nable on the book’s Web site, a much more comprehensive partially annotated bibliography\nof nearly all computer vision publications is maintained by Keith Price at http://iris.usc.edu/\nVision-Notes/bibliography/contents.html. There is also a searchable computer graphics bibli-\nography at http://www.siggraph.org/publications/bibliography/. Additional good sources for\ntechnical papers are Google Scholar and CiteSeerx.",
  "813": "References\nAbdel-Hakim, A. E. and Farag, A. A. (2006). CSIFT: A SIFT descriptor with color in-\nvariant characterstics. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2006), pp. 1978–1983, New York City, NY.\nAdelson, E. H. and Bergen, J. (1991). The plenoptic function and the elements of early\nvision. In Computational Models of Visual Processing, pp. 3–20.\nAdelson, E. H., Simoncelli, E., and Hingorani, R. (1987). Orthogonal pyramid transforms\nfor image coding. In SPIE Vol. 845, Visual Communications and Image Processing II,\npp. 50–58, Cambridge, Massachusetts.\nAdiv, G.\n(1989).\nInherent ambiguities in recovering 3-D motion and structure from a\nnoisy ﬂow ﬁeld.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n11(5):477–490.\nAgarwal, A. and Triggs, B. (2006). Recovering 3D human pose from monocular images.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 28(1):44–58.\nAgarwal, S. and Roth, D. (2002). Learning a sparse representation for object detection. In\nSeventh European Conference on Computer Vision (ECCV 2002), pp. 113–127, Copen-\nhagen.\nAgarwal, S., Snavely, N., Seitz, S. M., and Szeliski, R. (2010). Bundle adjustment in the\nlarge. In Eleventh European Conference on Computer Vision (ECCV 2010), Heraklion,\nCrete.\nAgarwal, S., Snavely, N., Simon, I., Seitz, S. M., and Szeliski, R. (2009). Building Rome\nin a day. In Twelfth IEEE International Conference on Computer Vision (ICCV 2009),\nKyoto, Japan.\nAgarwal, S., Furukawa, Y., Snavely, N., Curless, B., Seitz, S. M., and Szeliski, R. (2010).\nReconstructing Rome. Computer, 43(6):40–47.\nAgarwala, A. (2007). Efﬁcient gradient-domain compositing using quadtrees. ACM Trans-\nactions on Graphics, 26(3).",
  "814": "792\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAgarwala, A., Hertzmann, A., Seitz, S., and Salesin, D. (2004). Keyframe-based tracking\nfor rotoscoping and animation.\nACM Transactions on Graphics (Proc. SIGGRAPH\n2004), 23(3):584–591.\nAgarwala, A., Agrawala, M., Cohen, M., Salesin, D., and Szeliski, R.\n(2006).\nPho-\ntographing long scenes with multi-viewpoint panoramas. ACM Transactions on Graph-\nics (Proc. SIGGRAPH 2006), 25(3):853–861.\nAgarwala, A., Dontcheva, M., Agrawala, M., Drucker, S., Colburn, A., Curless, B., Salesin,\nD. H., and Cohen, M. F. (2004). Interactive digital photomontage. ACM Transactions\non Graphics (Proc. SIGGRAPH 2004), 23(3):292–300.\nAgarwala, A., Zheng, K. C., Pal, C., Agrawala, M., Cohen, M., Curless, B., Salesin, D., and\nSzeliski, R. (2005). Panoramic video textures. ACM Transactions on Graphics (Proc.\nSIGGRAPH 2005), 24(3):821–827.\nAggarwal, J. K. and Nandhakumar, N. (1988). On the computation of motion from se-\nquences of images—a review. Proceedings of the IEEE, 76(8):917–935.\nAgin, G. J. and Binford, T. O. (1976). Computer description of curved objects. IEEE\nTransactions on Computers, C-25(4):439–449.\nAhonen, T., Hadid, A., and Pietik¨ainen, M. (2006). Face description with local binary\npatterns: Application to face recognition. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 28(12):2037–2041.\nAkenine-M¨oller, T. and Haines, E. (2002). Real-Time Rendering. A K Peters, Wellesley,\nMassachusetts, second edition.\nAl-Baali, M. and Fletcher., R. (1986). An efﬁcient line search for nonlinear least squares.\nJournal Journal of Optimization Theory and Applications, 48(3):359–377.\nAlahari, K., Kohli, P., and Torr, P. (2011). Dynamic hybrid algorithms for discrete MAP\nMRF inference. IEEE Transactions on Pattern Analysis and Machine Intelligence.\nAlexa, M., Behr, J., Cohen-Or, D., Fleishman, S., Levin, D., and Silva, C. T.\n(2003).\nComputing and rendering point set surfaces. IEEE Transactions on Visualization and\nComputer Graphics, 9(1):3–15.\nAliaga, D. G., Funkhouser, T., Yanovsky, D., and Carlbom, I. (2003). Sea of images. IEEE\nComputer Graphics and Applications, 23(6):22–30.\nAllen, B., Curless, B., and Popovi´c, Z. (2003). The space of human body shapes: recon-\nstruction and parameterization from range scans. ACM Transactions on Graphics (Proc.\nSIGGRAPH 2003), 22(3):587–594.\nAllgower, E. L. and Georg, K. (2003). Introduction to Numerical Continuation Methods.\nSociety for Industrial and Applied Mathematics.",
  "815": "References\n793\nAloimonos, J. (1990). Perspective approximations. Image and Vision Computing, 8:177–\n192.\nAlpert, S., Galun, M., Basri, R., and Brandt, A. (2007). Image segmentation by probabilis-\ntic bottom-up aggregation and cue integration. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nAmini, A. A., Weymouth, T. E., and Jain, R. C. (1990). Using dynamic programming\nfor solving variational problems in vision. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 12(9):855–867.\nAnandan, P. (1984). Computing dense displacement ﬁelds with conﬁdence measures in\nscenes containing occlusion. In Image Understanding Workshop, pp. 236–246, New\nOrleans.\nAnandan, P. (1989). A computational framework and an algorithm for the measurement of\nvisual motion. International Journal of Computer Vision, 2(3):283–310.\nAnandan, P. and Irani, M. (2002). Factorization with uncertainty. International Journal of\nComputer Vision, 49(2-3):101–116.\nAnderson, E., Bai, Z., Bischof, C., Blackford, S., Demmel, J. W. et al. (1999). LAPACK\nUsers’ Guide. Society for Industrial and Applied Mathematics, 3rd edition.\nAndrieu, C., de Freitas, N., Doucet, A., and Jordan, M. I. (2003). An introduction to\nMCMC for machine learning. Machine Learning, 50(1-2):5–43.\nAndriluka, M., Roth, S., and Schiele, B. (2008). People-tracking-by-detection and people-\ndetection-by-tracking. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.\nAndriluka, M., Roth, S., and Schiele, B. (2009). Pictorial structures revisited: People\ndetection and articulated pose estimation. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR 2009), Miami Beach, FL.\nAndriluka, M., Roth, S., and Schiele, B. (2010). ocular 3d pose estimation and tracking\nby detection. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2010), San Francisco, CA.\nAnguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., and Davis, J.\n(2005).\nSCAPE: Shape completion and animation of people. ACM Transactions on Graphics\n(Proc. SIGGRAPH 2005), 24(3):408–416.\nAnsar, A., Castano, A., and Matthies, L. (2004). Enhanced real-time stereo using bilat-\neral ﬁltering. In International Symposium on 3D Data Processing, Visualization, and\nTransmission (3DPVT).",
  "816": "794\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAntone, M. and Teller, S. (2002). Scalable extrinsic calibration of omni-directional image\nnetworks. International Journal of Computer Vision, 49(2-3):143–174.\nArbel´aez, P., Maire, M., Fowlkes, C., and Malik, J. (2010). Contour Detection and Hierar-\nchical Image Segmentation. Technical Report UCB/EECS-2010-17, EECS Department,\nUniversity of California, Berkeley. Submitted to PAMI.\nArgyriou, V. and Vlachos, T. (2003). Estimation of sub-pixel motion using gradient cross-\ncorrelation. Electronic Letters, 39(13):980–982.\nArikan, O. and Forsyth, D. A. (2002). Interactive motion generation from examples. ACM\nTransactions on Graphics, 21(3):483–490.\nArnold, R. D. (1983). Automated Stereo Perception. Technical Report AIM-351, Artiﬁcial\nIntelligence Laboratory, Stanford University.\nArya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., and Wu, A. Y. (1998). An optimal\nalgorithm for approximate nearest neighbor searching in ﬁxed dimensions. Journal of\nthe ACM, 45(6):891–923.\nAshdown, I. (1993). Near-ﬁeld photometry: A new approach. Journal of the Illuminating\nEngineering Society, 22(1):163–180.\nAtkinson, K. B. (1996). Close Range Photogrammetry and Machine Vision. Whittles\nPublishing, Scotland, UK.\nAurich, V. and Weule, J. (1995). Non-linear Gaussian ﬁlters performing edge preserving\ndiffusion. In 17th DAGM-Symposium, pp. 538–545, Bielefeld.\nAvidan, S. (2001). Support vector tracking. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’2001), pp. 283–290, Kauai, Hawaii.\nAvidan, S., Baker, S., and Shan, Y. (2010). Special issue on Internet Vision. Proceedings\nof the IEEE, 98(8):1367–1369.\nAxelsson, O. (1996). Iterative Solution Methods. Cambridge University Press, Cambridge.\nAyache, N. (1989). Vision St´er´eoscopique et Perception Multisensorielle. InterEditions,\nParis.\nAzarbayejani, A. and Pentland, A. P. (1995). Recursive estimation of motion, structure,\nand focal length. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n17(6):562–575.\nAzuma, R. T., Baillot, Y., Behringer, R., Feiner, S. K., Julier, S., and MacIntyre, B. (2001).\nRecent advances in augmented reality. IEEE Computer Graphics and Applications,\n21(6):34–47.",
  "817": "References\n795\nBab-Hadiashar, A. and Suter, D. (1998a). Robust optic ﬂow computation. International\nJournal of Computer Vision, 29(1):59–77.\nBab-Hadiashar, A. and Suter, D.\n(1998b).\nRobust total least squares based optic ﬂow\ncomputation. In Asian Conference on Computer Vision (ACCV’98), pp. 566–573, Hong\nKong.\nBadra, F., Qumsieh, A., and Dudek, G. (1998). Rotation and zooming in image mosaic-\ning. In IEEE Workshop on Applications of Computer Vision (WACV’98), pp. 50–55,\nPrinceton.\nBae, S., Paris, S., and Durand, F. (2006). Two-scale tone management for photographic\nlook. ACM Transactions on Graphics, 25(3):637–645.\nBaeza-Yates, R. and Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison\nWesley.\nBai, X. and Sapiro, G. (2009). Geodesic matting: A framework for fast interactive im-\nage and video segmentation and matting. International Journal of Computer Vision,\n82(2):113–132.\nBajcsy, R. and Kovacic, S. (1989). Multiresolution elastic matching. Computer Vision,\nGraphics, and Image Processing, 46(1):1–21.\nBaker, H. H. (1977). Three-dimensional modeling. In Fifth International Joint Conference\non Artiﬁcial Intelligence (IJCAI-77), pp. 649–655.\nBaker, H. H. (1982). Depth from Edge and Intensity Based Stereo. Technical Report AIM-\n347, Artiﬁcial Intelligence Laboratory, Stanford University.\nBaker, H. H. (1989). Building surfaces of evolution: The weaving wall. International\nJournal of Computer Vision, 3(1):50–71.\nBaker, H. H. and Binford, T. O. (1981). Depth from edge and intensity based stereo. In\nIJCAI81, pp. 631–636.\nBaker, H. H. and Bolles, R. C. (1989). Generalizing epipolar-plane image analysis on the\nspatiotemporal surface. International Journal of Computer Vision, 3(1):33–49.\nBaker, S. and Kanade, T. (2002). Limits on super-resolution and how to break them. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 24(9):1167–1183.\nBaker, S. and Matthews, I. (2004). Lucas-Kanade 20 years on: A unifying framework: Part\n1: The quantity approximated, the warp update rule, and the gradient descent approxi-\nmation. International Journal of Computer Vision, 56(3):221–255.\nBaker, S. and Nayar, S. (1999). A theory of single-viewpoint catadioptric image formation.\nInternational Journal of Computer Vision, 5(2):175–196.",
  "818": "796\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBaker, S. and Nayar, S. K. (2001). Single viewpoint catadioptric cameras. In Benosman, R.\nand Kang, S. B. (eds), Panoramic Vision: Sensors, Theory, and Applications, pp. 39–71,\nSpringer, New York.\nBaker, S., Gross, R., and Matthews, I. (2003). Lucas-Kanade 20 Years On: A Unify-\ning Framework: Part 3. Technical Report CMU-RI-TR-03-35, The Robotics Institute,\nCarnegie Mellon University.\nBaker, S., Gross, R., and Matthews, I. (2004). Lucas-Kanade 20 Years On: A Unify-\ning Framework: Part 4. Technical Report CMU-RI-TR-04-14, The Robotics Institute,\nCarnegie Mellon University.\nBaker, S., Szeliski, R., and Anandan, P. (1998). A layered approach to stereo reconstruc-\ntion. In IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion (CVPR’98), pp. 434–441, Santa Barbara.\nBaker, S., Gross, R., Ishikawa, T., and Matthews, I. (2003). Lucas-Kanade 20 Years On:\nA Unifying Framework: Part 2. Technical Report CMU-RI-TR-03-01, The Robotics\nInstitute, Carnegie Mellon University.\nBaker, S., Black, M., Lewis, J. P., Roth, S., Scharstein, D., and Szeliski, R. (2007). A\ndatabase and evaluation methodology for optical ﬂow. In Eleventh International Con-\nference on Computer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nBaker, S., Scharstein, D., Lewis, J., Roth, S., Black, M. J., and Szeliski, R. (2009). A\nDatabase and Evaluation Methodology for Optical Flow. Technical Report MSR-TR-\n2009-179, Microsoft Research.\nBallard, D. H.\n(1981).\nGeneralizing the Hough transform to detect arbitrary patterns.\nPattern Recognition, 13(2):111–122.\nBallard, D. H. and Brown, C. M. (1982). Computer Vision. Prentice-Hall, Englewood\nCliffs, New Jersey.\nBanno, A., Masuda, T., Oishi, T., and Ikeuchi, K. (2008). Flying laser range sensor for\nlarge-scale site-modeling and its applications in Bayon digital archival project. Interna-\ntional Journal of Computer Vision, 78(2-3):207–222.\nBar-Hillel, A., Hertz, T., and Weinshall, D. (2005). Object class recognition by boosting\na part based model. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2005), pp. 701–708, San Diego, CA.\nBar-Joseph, Z., El-Yaniv, R., Lischinski, D., and Werman, M. (2001). Texture mixing and\ntexture movie synthesis using statistical learning. IEEE Transactions on Visualization\nand Computer Graphics, 7(2):120–135.",
  "819": "References\n797\nBar-Shalom, Y. and Fortmann, T. E. (1988). Tracking and data association. Academic\nPress, Boston.\nBarash, D. (2002). A fundamental relationship between bilateral ﬁltering, adaptive smooth-\ning, and the nonlinear diffusion equation. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 24(6):844–847.\nBarash, D. and Comaniciu, D. (2004). A common framework for nonlinear diffusion,\nadaptive smoothing, bilateral ﬁltering and mean shift. Image and Vision Computing,\n22(1):73–81.\nBarbu, A. and Zhu, S.-C. (2003). Graph partition by Swendsen–Wang cuts. In Ninth\nInternational Conference on Computer Vision (ICCV 2003), pp. 320–327, Nice, France.\nBarbu, A. and Zhu, S.-C. (2005). Generalizing Swendsen–Wang to sampling arbitrary pos-\nterior probabilities. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n27(9):1239–1253.\nBarkans, A. C. (1997). High quality rendering using the Talisman architecture. In Pro-\nceedings of the Eurographics Workshop on Graphics Hardware.\nBarnard, S. T. (1989). Stochastic stereo matching over scale. International Journal of\nComputer Vision, 3(1):17–32.\nBarnard, S. T. and Fischler, M. A. (1982). Computational stereo. Computing Surveys,\n14(4):553–572.\nBarnes, C., Jacobs, D. E., Sanders, J., Goldman, D. B., Rusinkiewicz, S., Finkelstein, A.,\nand Agrawala, M. (2008). Video puppetry: A performative interface for cutout anima-\ntion. ACM Transactions on Graphics, 27(5).\nBarreto, J. P. and Daniilidis, K. (2005). Fundamental matrix for cameras with radial distor-\ntion. In Tenth International Conference on Computer Vision (ICCV 2005), pp. 625–632,\nBeijing, China.\nBarrett, R., Berry, M., Chan, T. F., Demmel, J., Donato, J. et al. (1994). Templates for the\nSolution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. SIAM,\nPhiladelphia, PA.\nBarron, J. L., Fleet, D. J., and Beauchemin, S. S. (1994). Performance of optical ﬂow\ntechniques. International Journal of Computer Vision, 12(1):43–77.\nBarrow, H. G. and Tenenbaum, J. M. (1981). Computational vision. Proceedings of the\nIEEE, 69(5):572–595.\nBartels, R. H., Beatty, J. C., and Barsky, B. A. (1987). An Introduction to Splines for use\nin Computer Graphics and Geeometric Modeling. Morgan Kaufmann Publishers, Los\nAltos.",
  "820": "798\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBartoli, A. (2003). Towards gauge invariant bundle adjustment: A solution based on gauge\ndependent damping.\nIn Ninth International Conference on Computer Vision (ICCV\n2003), pp. 760–765, Nice, France.\nBartoli, A. and Sturm, P. (2003). Multiple-view structure and motion from line correspon-\ndences. In Ninth International Conference on Computer Vision (ICCV 2003), pp. 207–\n212, Nice, France.\nBartoli, A., Coquerelle, M., and Sturm, P.\n(2004).\nA framework for pencil-of-points\nstructure-from-motion. In Eighth European Conference on Computer Vision (ECCV\n2004), pp. 28–40, Prague.\nBascle, B., Blake, A., and Zisserman, A.\n(1996).\nMotion deblurring and super-\nresolution from an image sequence. In Fourth European Conference on Computer Vi-\nsion (ECCV’96), pp. 573–582, Cambridge, England.\nBathe, K.-J. (2007). Finite Element Procedures. Prentice-Hall, Inc., Englewood Cliffs,\nNew Jersey.\nBatra, D., Sukthankar, R., and Chen, T. (2008). Learning class-speciﬁc afﬁnities for im-\nage labelling. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nBaudisch, P., Tan, D., Steedly, D., Rudolph, E., Uyttendaele, M., Pal, C., and Szeliski, R.\n(2006). An exploration of user interface designs for real-time panoramic photography.\nAustralian Journal of Information Systems, 13(2).\nBaumberg, A.\n(2000).\nReliable feature matching across widely separated views.\nIn\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2000), pp. 774–781, Hilton Head Island.\nBaumberg, A. M. and Hogg, D. C. (1996). Generating spatiotemporal models from exam-\nples. Image and Vision Computing, 14(8):525–532.\nBaumgart, B. G.\n(1974).\nGeometric Modeling for Computer Vision.\nTechnical Re-\nport AIM-249, Artiﬁcial Intelligence Laboratory, Stanford University.\nBay, H., Ferrari, V., and Van Gool, L. (2005). Wide-baseline stereo matching with line seg-\nments. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’2005), pp. 329–336, San Diego, CA.\nBay, H., Tuytelaars, T., and Van Gool, L. (2006). SURF: Speeded up robust features. In\nNinth European Conference on Computer Vision (ECCV 2006), pp. 404–417.\nBayer, B. E. (1976). Color imaging array. US Patent No. 3,971,065.",
  "821": "References\n799\nBeardsley, P., Torr, P., and Zisserman, A. (1996). 3D model acquisition from extended\nimage sequences. In Fourth European Conference on Computer Vision (ECCV’96),\npp. 683–695, Cambridge, England.\nBeare, R. (2006). A locally constrained watershed transform. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 28(7):1063–1074.\nBecker, S. and Bove, V. M. (1995). Semiautomatic 3-D model extraction from uncalibrated\n2-D camera views. In SPIE Vol. 2410, Visual Data Exploration and Analysis II, pp. 447–\n461, San Jose.\nBeier, T. and Neely, S. (1992). Feature-based image metamorphosis. Computer Graphics\n(SIGGRAPH ’92), 26(2):35–42.\nBeis, J. S. and Lowe, D. G. (1999). Indexing without invariants in 3D object recognition.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 21(10):1000–1015.\nBelhumeur, P. N. (1996). A Bayesian approach to binocular stereopsis. International\nJournal of Computer Vision, 19(3):237–260.\nBelhumeur, P. N., Hespanha, J. P., and Kriegman, D. J. (1997). Eigenfaces vs. Fisher-\nfaces: Recognition using class speciﬁc linear projection. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 19(7):711–720.\nBelongie, S. and Malik, J. (1998). Finding boundaries in natural images: a new method us-\ning point descriptors and area completion. In Fifth European Conference on Computer\nVision (ECCV’98), pp. 751–766, Freiburg, Germany.\nBelongie, S., Malik, J., and Puzicha, J. (2002). Shape matching and object recognition\nusing shape contexts. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n24(4):509–522.\nBelongie, S., Fowlkes, C., Chung, F., and Malik, J. (2002). Spectral partitioning with\nindeﬁnite kernels using the Nystr¨om extension. In Seventh European Conference on\nComputer Vision (ECCV 2002), pp. 531–543, Copenhagen.\nBennett, E., Uyttendaele, M., Zitnick, L., Szeliski, R., and Kang, S. B. (2006). Video\nand image Bayesian demosaicing with a two color image prior. In Ninth European\nConference on Computer Vision (ECCV 2006), pp. 508–521, Graz.\nBenosman, R. and Kang, S. B. (eds). (2001). Panoramic Vision: Sensors, Theory, and\nApplications, Springer, New York.\nBerg, T.\n(2008).\nInternet vision.\nSUNY Stony Brook Course CSE 690, http://www.\ntamaraberg.com/teaching/Fall 08/.",
  "822": "800\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBerg, T. and Forsyth, D. (2006). Animals on the web. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR’2006), pp. 1463–1470, New\nYork City, NY.\nBergen, J. R., Anandan, P., Hanna, K. J., and Hingorani, R.\n(1992).\nHierarchical\nmodel-based motion estimation. In Second European Conference on Computer Vision\n(ECCV’92), pp. 237–252, Santa Margherita Liguere, Italy.\nBergen, J. R., Burt, P. J., Hingorani, R., and Peleg, S. (1992). A three-frame algorithm for\nestimating two-component image motion. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 14(9):886–896.\nBerger, J. O. (1993). Statistical Decision Theory and Bayesian Analysis. Springer, New\nYork, second edition.\nBertalmio, M., Sapiro, G., Caselles, V., and Ballester, C. (2000). Image inpainting. In\nACM SIGGRAPH 2000 Conference Proceedings, pp. 417–424.\nBertalmio, M., Vese, L., Sapiro, G., and Osher, S. (2003). Simultaneous structure and\ntexture image inpainting. IEEE Transactions on Image Processing, 12(8):882–889.\nBertero, M., Poggio, T. A., and Torre, V. (1988). Ill-posed problems in early vision. Pro-\nceedings of the IEEE, 76(8):869–889.\nBesag, J. (1986). On the statistical analysis of dirty pictures. Journal of the Royal Statisti-\ncal Society B, 48(3):259–302.\nBesl, P. (1989). Active optical range imaging sensors. In Sanz, J. L. (ed.), Advances in\nMachine Vision, chapter 1, pp. 1–63, Springer-Verlag.\nBesl, P. J. and Jain, R. C.\n(1985).\nThree-dimensional object recognition.\nComputing\nSurveys, 17(1):75–145.\nBesl, P. J. and McKay, N. D. (1992). A method for registration of 3-D shapes. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 14(2):239–256.\nBetrisey, C., Blinn, J. F., Dresevic, B., Hill, B., Hitchcock, G. et al. (2000). Displaced ﬁlter-\ning for patterned displays. In Society for Information Display Symposium,, pp. 296–299.\nBeymer, D. (1996). Feature correspondence by interleaving shape and texture computa-\ntions. In IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion (CVPR’96), pp. 921–928, San Francisco.\nBhat, D. N. and Nayar, S. K. (1998). Ordinal measures for image correspondence. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 20(4):415–423.\nBickel, B., Botsch, M., Angst, R., Matusik, W., Otaduy, M., Pﬁster, H., and Gross, M.\n(2007).\nMulti-scale capture of facial geometry and motion.\nACM Transactions on\nGraphics, 26(3).",
  "823": "References\n801\nBillinghurst, M., Kato, H., and Poupyrev, I. (2001). The MagicBook: a transitional AR\ninterface. Computers & Graphics, 25:745–753.\nBimber, O. (2006). Computational photography—the next big step. Computer, 39(8):28–\n29.\nBirchﬁeld, S. and Tomasi, C. (1998). A pixel dissimilarity measure that is insensitive\nto image sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n20(4):401–406.\nBirchﬁeld, S. and Tomasi, C. (1999). Depth discontinuities by pixel-to-pixel stereo. Inter-\nnational Journal of Computer Vision, 35(3):269–293.\nBirchﬁeld, S. T., Natarajan, B., and Tomasi, C. (2007). Correspondence as energy-based\nsegmentation. Image and Vision Computing, 25(8):1329–1340.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer, New York,\nNY.\nBitouk, D., Kumar, N., Dhillon, S., Belhumeur, P., and Nayar, S. K. (2008). Face swapping:\nAutomatically replacing faces in photographs. ACM Transactions on Graphics, 27(3).\nBj¨orck, A. (1996). Numerical Methods for Least Squares Problems. Society for Industrial\nand Applied Mathematics.\nBj¨orck, A. and Dahlquist, G. (2010). Numerical Methods in Scientiﬁc Computing. Vol-\nume II, Society for Industrial and Applied Mathematics.\nBlack, M., Yacoob, Y., Jepson, A. D., and Fleet, D. J. (1997). Learning parameterized\nmodels of image motion. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’97), pp. 561–567, San Juan, Puerto Rico.\nBlack, M. J. and Anandan, P. (1996). The robust estimation of multiple motions: Para-\nmetric and piecewise-smooth ﬂow ﬁelds. Computer Vision and Image Understanding,\n63(1):75–104.\nBlack, M. J. and Jepson, A. D.\n(1996).\nEstimating optical ﬂow in segmented images\nusing variable-order parametric models with local deformations. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 18(10):972–986.\nBlack, M. J. and Jepson, A. D. (1998). EigenTracking: robust matching and tracking of ar-\nticulated objects using a view-based representation. International Journal of Computer\nVision, 26(1):63–84.\nBlack, M. J. and Rangarajan, A.\n(1996).\nOn the uniﬁcation of line processes, outlier\nrejection, and robust statistics with applications in early vision. International Journal\nof Computer Vision, 19(1):57–91.",
  "824": "802\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBlack, M. J., Sapiro, G., Marimont, D. H., and Heeger, D. (1998). Robust anisotropic\ndiffusion. IEEE Transactions on Image Processing, 7(3):421–432.\nBlackford, L. S., Demmel, J., Dongarra, J., Duff, I., Hammarling, S. et al. (2002). An\nupdated set of basic linear algebra subprograms (BLAS). ACM Transactions on Math-\nematical Software, 28(2):135–151.\nBlake, A. and Isard, M. (1998). Active Contours: The Application of Techniques from\nGraphics, Vision, Control Theory and Statistics to Visual Tracking of Shapes in Motion.\nSpringer Verlag, London.\nBlake, A. and Zisserman, A.\n(1987).\nVisual Reconstruction.\nMIT Press, Cambridge,\nMassachusetts.\nBlake, A., Curwen, R., and Zisserman, A. (1993). A framework for spatio-temporal control\nin the tracking of visual contour. International Journal of Computer Vision, 11(2):127–\n145.\nBlake, A., Kohli, P., and Rother, C. (eds). (2010). Advances in Markov Random Fields,\nMIT Press.\nBlake, A., Zimmerman, A., and Knowles, G. (1985). Surface descriptions from stereo and\nshading. Image and Vision Computing, 3(4):183–191.\nBlake, A., Rother, C., Brown, M., Perez, P., and Torr, P. (2004). Interactive image segmen-\ntation using an adaptive GMMRF model. In Eighth European Conference on Computer\nVision (ECCV 2004), pp. 428–441, Prague.\nBlanz, V. and Vetter, T. (1999). A morphable model for the synthesis of 3D faces. In ACM\nSIGGRAPH 1999 Conference Proceedings, pp. 187–194.\nBlanz, V. and Vetter, T. (2003). Face recognition based on ﬁtting a 3D morphable model.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 25():1063–1074.\nBleyer, M., Gelautz, M., Rother, C., and Rhemann, C. (2009). A stereo approach that\nhandles the matting problem via image warping. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2009), Miami Beach, FL.\nBlinn, J. (1998). Dirty Pixels. Morgan Kaufmann Publishers, San Francisco.\nBlinn, J. F. (1994a). Jim Blinn’s corner: Compositing, part 1: Theory. IEEE Computer\nGraphics and Applications, 14(5):83–87.\nBlinn, J. F. (1994b). Jim Blinn’s corner: Compositing, part 2: Practice. IEEE Computer\nGraphics and Applications, 14(6):78–82.\nBlinn, J. F. and Newell, M. E. (1976). Texture and reﬂection in computer generated images.\nCommunications of the ACM, 19(10):542–547.",
  "825": "References\n803\nBlostein, D. and Ahuja, N. (1987). Shape from texture: Integrating texture-element ex-\ntraction and surface estimation. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 11(12):1233–1251.\nBobick, A. F. (1997). Movement, activity and action: the role of knowledge in the percep-\ntion of motion. Proceedings of the Royal Society of London, B 352:1257–1265.\nBobick, A. F. and Intille, S. S. (1999). Large occlusion stereo. International Journal of\nComputer Vision, 33(3):181–200.\nBoden, M. A. (2006). Mind As Machine: A History of Cognitive Science. Oxford Univer-\nsity Press, Oxford, England.\nBogart, R. G. (1991). View correlation. In Arvo, J. (ed.), Graphics Gems II, pp. 181–190,\nAcademic Press, Boston.\nBoiman, O., Shechtman, E., and Irani, M. (2008). In defense of nearest-neighbor based\nimage classiﬁcation. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.\nBoissonat, J.-D. (1984). Representing 2D and 3D shapes with the Delaunay triangulation.\nIn Seventh International Conference on Pattern Recognition (ICPR’84), pp. 745–748,\nMontreal, Canada.\nBolles, R. C., Baker, H. H., and Hannah, M. J. (1993). The JISCT stereo evaluation. In\nImage Understanding Workshop, pp. 263–274.\nBolles, R. C., Baker, H. H., and Marimont, D. H. (1987). Epipolar-plane image analysis:\nAn approach to determining structure from motion. International Journal of Computer\nVision, 1:7–55.\nBookstein, F. L.\n(1989).\nPrincipal warps: Thin-plate splines and the decomposition\nof deformations.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n11(6):567–585.\nBorenstein, E. and Ullman, S. (2008). Combined top-down/bottom-up segmentation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 30(12):2109–2125.\nBorgefors, G.\n(1986).\nDistance transformations in digital images.\nComputer Vision,\nGraphics and Image Processing, 34(3):227–248.\nBouchard, G. and Triggs, B. (2005). Hierarchical part-based visual object categorization.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 709–714, San Diego, CA.\nBougnoux, S. (1998). From projective to Euclidean space under any practical situation,\na criticism of self-calibration. In Sixth International Conference on Computer Vision\n(ICCV’98), pp. 790–798, Bombay.",
  "826": "804\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBouguet, J.-Y. and Perona, P. (1999). 3D photography using shadows in dual-space geom-\netry. International Journal of Computer Vision, 35(2):129–149.\nBoult, T. E. and Kender, J. R. (1986). Visual surface reconstruction using sparse depth data.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’86), pp. 68–76, Miami Beach.\nBourdev, L. and Malik, J. (2009). Poselets: Body part detectors trained using 3D hu-\nman pose annotations. In Twelfth International Conference on Computer Vision (ICCV\n2009), Kyoto, Japan.\nBovik, A. (ed.). (2000). Handbook of Image and Video Processing, Academic Press, San\nDiego.\nBowyer, K. W., Kranenburg, C., and Dougherty, S. (2001). Edge detector evaluation using\nempirical ROC curves. Computer Vision and Image Understanding, 84(1):77–103.\nBox, G. E. P. and Muller, M. E. (1958). A note on the generation of random normal\ndeviates. Annals of Mathematical Statistics, 29(2).\nBoyer, E. and Berger, M. O. (1997). 3D surface reconstruction using occluding contours.\nInternational Journal of Computer Vision, 22(3):219–233.\nBoykov, Y. and Funka-Lea, G. (2006). Graph cuts and efﬁcient N-D image segmentation.\nInternational Journal of Computer Vision, 70(2):109–131.\nBoykov, Y. and Jolly, M.-P. (2001). Interactive graph cuts for optimal boundary and re-\ngion segmentation of objects in N-D images. In Eighth International Conference on\nComputer Vision (ICCV 2001), pp. 105–112, Vancouver, Canada.\nBoykov, Y. and Kolmogorov, V.\n(2003).\nComputing geodesics and minimal surfaces\nvia graph cuts. In Ninth International Conference on Computer Vision (ICCV 2003),\npp. 26–33, Nice, France.\nBoykov, Y. and Kolmogorov, V. (2004). An experimental comparison of min-cut/max-ﬂow\nalgorithms for energy minimization in vision. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 26(9):1124–1137.\nBoykov, Y. and Kolmogorov, V. (2010). Basic graph cut algorithms. In Blake, A., Kohli,\nP., and Rother, C. (eds), Advances in Markov Random Fields, MIT Press.\nBoykov, Y., Veksler, O., and Zabih, R. (1998). A variable window approach to early vision.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1283–1294.\nBoykov, Y., Veksler, O., and Zabih, R. (2001). Fast approximate energy minimization\nvia graph cuts.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n23(11):1222–1239.",
  "827": "References\n805\nBoykov, Y., Veksler, O., and Zabih, R. (2010). Optimizing multi-label MRFs by move\nmaking algorithms. In Blake, A., Kohli, P., and Rother, C. (eds), Advances in Markov\nRandom Fields, MIT Press.\nBoykov, Y., Kolmogorov, V., Cremers, D., and Delong, A. (2006). An integral solution\nto surface evolution PDEs via Geo-cuts. In Ninth European Conference on Computer\nVision (ECCV 2006), pp. 409–422.\nBracewell, R. N. (1986). The Fourier Transform and its Applications. McGraw-Hill, New\nYork, 2nd edition.\nBradley, D., Boubekeur, T., and Heidrich, W. (2008). Accurate multi-view reconstruction\nusing robust binocular stereo and surface meshing. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nBradsky, G. and Kaehler, A. (2008). Learning OpenCV: Computer Vision with the OpenCV\nLibrary. O’Reilly, Sebastopol, CA.\nBrandt, A. (1986). Algebraic multigrid theory: The symmetric case. Applied Mathematics\nand Computation, 19(1-4):23–56.\nBregler, C. and Malik, J.\n(1998).\nTracking people with twists and exponential maps.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’98), pp. 8–15, Santa Barbara.\nBregler, C., Covell, M., and Slaney, M. (1997). Video rewrite: Driving visual speech with\naudio. In ACM SIGGRAPH 1997 Conference Proceedings, pp. 353–360.\nBregler, C., Malik, J., and Pullen, K. (2004). Twist based acquisition and tracking of animal\nand human kinematics. International Journal of Computer Vision, 56(3):179–194.\nBreu, H., Gil, J., Kirkpatrick, D., and Werman, M. (1995). Linear time Euclidean dis-\ntance transform algorithms. IEEE Transactions on Pattern Analysis and Machine Intel-\nligence, 17(5):529–533.\nBrice, C. R. and Fennema, C. L. (1970). Scene analysis using regions. Artiﬁcial Intelli-\ngence, 1(3-4):205–226.\nBriggs, W. L., Henson, V. E., and McCormick, S. F. (2000). A Multigrid Tutorial. Society\nfor Industrial and Applied Mathematics, Philadelphia, second edition.\nBrillaut-O’Mahoney, B. (1991). New method for vanishing point detection. Computer\nVision, Graphics, and Image Processing, 54(2):289–300.\nBrinkmann, R. (2008). The Art and Science of Digital Compositing. Morgan Kaufmann\nPublishers, San Francisco, 2nd edition.",
  "828": "806\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nBrooks, R. A. (1981). Symbolic reasoning among 3-D models and 2-D images. Artiﬁcial\nIntelligence, 17:285–348.\nBrown, D. C. (1971). Close-range camera calibration. Photogrammetric Engineering,\n37(8):855–866.\nBrown, L. G. (1992). A survey of image registration techniques. Computing Surveys,\n24(4):325–376.\nBrown, M. and Lowe, D. (2002). Invariant features from interest point groups. In British\nMachine Vision Conference, pp. 656–665, Cardiff, Wales.\nBrown, M. and Lowe, D. (2003). Unsupervised 3D object recognition and reconstruc-\ntion in unordered datasets. In International Conference on 3D Imaging and Modelling,\npp. 1218–1225, Nice, France.\nBrown, M. and Lowe, D. (2007). Automatic panoramic image stitching using invariant\nfeatures. International Journal of Computer Vision, 74(1):59–73.\nBrown, M., Hartley, R., and Nist´er, D. (2007). Minimal solutions for panoramic stitching.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.\nBrown, M., Szeliski, R., and Winder, S. (2004). Multi-Image Matching Using Multi-Scale\nOriented Patches. Technical Report MSR-TR-2004-133, Microsoft Research.\nBrown, M., Szeliski, R., and Winder, S. (2005). Multi-image matching using multi-scale\noriented patches. In IEEE Computer Society Conference on Computer Vision and Pat-\ntern Recognition (CVPR’2005), pp. 510–517, San Diego, CA.\nBrown, M. Z., Burschka, D., and Hager, G. D. (2003). Advances in computational stereo.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 25(8):993–1008.\nBrox, T., Bregler, C., and Malik, J. (2009). Large displacement optical ﬂow. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009),\nMiami Beach, FL.\nBrox, T., Bruhn, A., Papenberg, N., and Weickert, J. (2004). High accuracy optical ﬂow\nestimation based on a theory for warping. In Eighth European Conference on Computer\nVision (ECCV 2004), pp. 25–36, Prague.\nBrubaker, S. C., Wu, J., Sun, J., Mullin, M. D., and Rehg, J. M. (2008). On the design of\ncascades of boosted ensembles for face detection. International Journal of Computer\nVision, 77(1-3):65–86.\nBruhn, A., Weickert, J., and Schn¨orr, C.\n(2005).\nLucas/Kanade meets Horn/Schunck:\nCombining local and global optic ﬂow methods. International Journal of Computer\nVision, 61(3):211–231.",
  "829": "References\n807\nBruhn, A., Weickert, J., Kohlberger, T., and Schn¨orr, C. (2006). A multigrid platform for\nreal-time motion computation with discontinuity-preserving variational methods. Inter-\nnational Journal of Computer Vision, 70(3):257–277.\nBuades, A., Coll, B., and Morel, J.-M. (2008). Nonlocal image and movie denoising.\nInternational Journal of Computer Vision, 76(2):123–139.\nB˘alan, A. O. and Black, M. J. (2008). The naked truth: Estimating body shape under\nclothing. In Tenth European Conference on Computer Vision (ECCV 2008), pp. 15–29,\nMarseilles.\nBuchanan, A. and Fitzgibbon, A. (2005). Damped Newton algorithms for matrix factoriza-\ntion with missing data. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2005), pp. 316–322, San Diego, CA.\nBuck, I., Finkelstein, A., Jacobs, C., Klein, A., Salesin, D. H., Seims, J., Szeliski, R., and\nToyama, K. (2000). Performance-driven hand-drawn animation. In Symposium on Non\nPhotorealistic Animation and Rendering, pp. 101–108, Annecy.\nBuehler, C., Bosse, M., McMillan, L., Gortler, S. J., and Cohen, M. F. (2001). Unstructured\nLumigraph rendering. In ACM SIGGRAPH 2001 Conference Proceedings, pp. 425–\n432.\nBugayevskiy, L. M. and Snyder, J. P. (1995). Map Projections: A Reference Manual. CRC\nPress.\nBurger, W. and Burge, M. J. (2008). Digital Image Processing: An Algorithmic Introduc-\ntion Using Java. Springer, New York, NY.\nBurl, M. C., Weber, M., and Perona, P. (1998). A probabilistic approach to object recog-\nnition using local photometry and global geometry. In Fifth European Conference on\nComputer Vision (ECCV’98), pp. 628–641, Freiburg, Germany.\nBurns, J. B., Hanson, A. R., and Riseman, E. M. (1986). Extracting straight lines. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, PAMI-8(4):425–455.\nBurns, P. D. and Williams, D. (1999). Using slanted edge analysis for color registration\nmeasurement. In IS&T PICS Conference, pp. 51–53.\nBurt, P. J. and Adelson, E. H. (1983a). The Laplacian pyramid as a compact image code.\nIEEE Transactions on Communications, COM-31(4):532–540.\nBurt, P. J. and Adelson, E. H. (1983b). A multiresolution spline with applications to image\nmosaics. ACM Transactions on Graphics, 2(4):217–236.\nBurt, P. J. and Kolczynski, R. J. (1993). Enhanced image capture through fusion. In\nFourth International Conference on Computer Vision (ICCV’93), pp. 173–182, Berlin,\nGermany.",
  "830": "808\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nByr¨od, M. and øAstr¨om, K. (2009). Bundle adjustment using conjugate gradients with\nmultiscale preconditioning. In British Machine Vision Conference (BMVC 2009).\nCai, D., He, X., Hu, Y., Han, J., and Huang, T.\n(2007).\nLearning a spatially smooth\nsubspace for face recognition. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nCampbell, N. D. F., Vogiatzis, G., Hern´andez, C., and Cipolla, R. (2008). Using multiple\nhypotheses to improve depth-maps for multi-view stereo. In Tenth European Confer-\nence on Computer Vision (ECCV 2008), pp. 766–779, Marseilles.\nCan, A., Stewart, C., Roysam, B., and Tanenbaum, H. (2002). A feature-based, robust,\nhierarchical algorithm for registering pairs of images of the curved human retina. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 24(3):347–364.\nCanny, J. (1986). A computational approach to edge detection. IEEE Transactions on\nPattern Analysis and Machine Intelligence, PAMI-8(6):679–698.\nCao, Z., Yin, Q., Tang, X., and Sun, J.\n(2010).\nFace recognition with learning-based\ndescriptor.\nIn IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2010), San Francisco, CA.\nCapel, D. (2004). Image Mosaicing and Super-resolution. Distinguished Dissertation\nSeries, British Computer Society, Springer-Verlag.\nCapel, D. and Zisserman, A. (1998). Automated mosaicing with super-resolution zoom.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’98), pp. 885–891, Santa Barbara.\nCapel, D. and Zisserman, A.\n(2000).\nSuper-resolution enhancement of text image se-\nquences. In Fifteenth International Conference on Pattern Recognition (ICPR’2000),\npp. 600–605, Barcelona, Spain.\nCapel, D. and Zisserman, A. (2003). Computer vision applied to super resolution. IEEE\nSignal Processing Magazine, 20(3):75–86.\nCapel, D. P. (2001). Super-resolution and Image Mosaicing. Ph.D. thesis, University of\nOxford.\nCaprile, B. and Torre, V. (1990). Using vanishing points for camera calibration. Interna-\ntional Journal of Computer Vision, 4(2):127–139.\nCarneiro, G. and Jepson, A. (2005). The distinctiveness, detectability, and robustness of\nlocal image features. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2005), pp. 296–301, San Diego, CA.\nCarneiro, G. and Lowe, D. (2006). Sparse ﬂexible models of local features. In Ninth\nEuropean Conference on Computer Vision (ECCV 2006), pp. 29–43.",
  "831": "References\n809\nCarnevali, P., Coletti, L., and Patarnello, S. (1985). Image processing by simulated anneal-\ning. IBM Journal of Research and Development, 29(6):569–579.\nCarranza, J., Theobalt, C., Magnor, M. A., and Seidel, H.-P. (2003). Free-viewpoint video\nof human actors. ACM Transactions on Graphics (Proc. SIGGRAPH 2003), 22(3):569–\n577.\nCarroll, R., Agrawala, M., and Agarwala, A. (2009). Optimizing content-preserving pro-\njections for wide-angle images. ACM Transactions on Graphics, 28(3).\nCaselles, V., Kimmel, R., and Sapiro, G. (1997). Geodesic active contours. International\nJournal of Computer Vision, 21(1):61–79.\nCatmull, E. and Smith, A. R. (1980). 3-D transformations of images in scanline order.\nComputer Graphics (SIGGRAPH ’80), 14(3):279–285.\nCelniker, G. and Gossard, D. (1991). Deformable curve and surface ﬁnite-elements for\nfree-form shape design. Computer Graphics (SIGGRAPH ’91), 25(4):257–266.\nChakrabarti, A., Scharstein, D., and Zickler, T. (2009). An empirical camera model for\ninternet color vision. In British Machine Vision Conference (BMVC 2009), London,\nUK.\nCham, T. J. and Cipolla, R. (1998). A statistical framework for long-range feature matching\nin uncalibrated image mosaicing. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’98), pp. 442–447, Santa Barbara.\nCham, T.-J. and Rehg, J. M. (1999). A multiple hypothesis approach to ﬁgure tracking.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’99), pp. 239–245, Fort Collins.\nChampleboux, G., Lavall´ee, S., Sautot, P., and Cinquin, P. (1992). Accurate calibration of\ncameras and range imaging sensors, the NPBS method. In IEEE International Confer-\nence on Robotics and Automation, pp. 1552–1558, Nice, France.\nChampleboux, G., Lavall´ee, S., Szeliski, R., and Brunie, L. (1992). From accurate range\nimaging sensor calibration to accurate model-based 3-D object localization. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’92),\npp. 83–89, Champaign, Illinois.\nChan, A. B. and Vasconcelos, N. (2009). Layered dynamic textures. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 31(10):1862–1879.\nChan, T. F. and Vese, L. A. (1992). Active contours without edges. IEEE Transactions on\nImage Processing, 10(2):266–277.\nChan, T. F., Osher, S., and Shen, J. (2001). The digital TV ﬁlter and nonlinear denoising.\nIEEE Transactions on Image Processing, 10(2):231–241.",
  "832": "810\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nChang, M. M., Tekalp, A. M., and Sezan, M. I. (1997). Simultaneous motion estimation\nand segmentation. IEEE Transactions on Image Processing, 6(9):1326–1333.\nChaudhuri, S. (2001). Super-Resolution Imaging. Springer.\nChaudhuri, S. and Rajagopalan, A. N. (1999). Depth from Defocus: A Real Aperture\nImaging Approach. Springer.\nCheeseman, P., Kanefsky, B., Hanson, R., and Stutz, J. (1993). Super-Resolved Surface\nReconstruction From Multiple Images. Technical Report FIA-93-02, NASA Ames Re-\nsearch Center, Artiﬁcial Intelligence Branch.\nChellappa, R., Wilson, C., and Sirohey, S. (1995). Human and machine recognition of\nfaces: A survey. Proceedings of the IEEE, 83(5):705–740.\nChen, B., Neubert, B., Ofek, E., Deussen, O., and Cohen, M. F. (2009). Integrated videos\nand maps for driving directions. In UIST ’09: Proceedings of the 22nd annual ACM\nsymposium on User interface software and technology, pp. 223–232, Victoria, BC,\nCanada, New York, NY, USA.\nChen, C.-Y. and Klette, R. (1999). Image stitching - comparisons and new techniques. In\nComputer Analysis of Images and Patterns (CAIP’99), pp. 615–622, Ljubljana.\nChen, J. and Chen, B. (2008). Architectural modeling from sparsely scanned range data.\nInternational Journal of Computer Vision, 78(2-3):223–236.\nChen, J., Paris, S., and Durand, F. (2007). Real-time edge-aware image processing with the\nbilateral grid. ACM Transactions on Graphics, 26(3).\nChen, S. and Williams, L. (1993). View interpolation for image synthesis. In ACM SIG-\nGRAPH 1993 Conference Proceedings, pp. 279–288.\nChen, S. E. (1995). QuickTime VR – an image-based approach to virtual environment nav-\nigation. In ACM SIGGRAPH 1995 Conference Proceedings, pp. 29–38, Los Angeles.\nChen, Y. and Medioni, G. (1992). Object modeling by registration of multiple range im-\nages. Image and Vision Computing, 10(3):145–155.\nCheng, L., Vishwanathan, S. V. N., and Zhang, X. (2008). Consistent image analogies\nusing semi-supervised learning. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nCheng, Y. (1995). Mean shift, mode seeking, and clustering. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 17(8):790–799.\nChiang, M.-C. and Boult, T. E. (1996). Efﬁcient image warping and super-resolution. In\nIEEE Workshop on Applications of Computer Vision (WACV’96), pp. 56–61, Sarasota.",
  "833": "References\n811\nChiu, K. and Raskar, R. (2009). Computer vision on tap. In Second IEEE Workshop on\nInternet Vision, Miami Beach, Florida.\nChou, P. B. and Brown, C. M. (1990). The theory and practice of Bayesian image labeling.\nInternational Journal of Computer Vision, 4(3):185–210.\nChristensen, G., Joshi, S., and Miller, M.\n(1997).\nVolumetric transformation of brain\nanatomy. IEEE Transactions on Medical Imaging, 16(6):864–877.\nChristy, S. and Horaud, R. (1996). Euclidean shape and motion from multiple perspec-\ntive views by afﬁne iterations. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 18(11):1098–1104.\nChuang, Y.-Y., Curless, B., Salesin, D. H., and Szeliski, R. (2001). A Bayesian approach to\ndigital matting. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2001), pp. 264–271, Kauai, Hawaii.\nChuang, Y.-Y., Agarwala, A., Curless, B., Salesin, D. H., and Szeliski, R. (2002). Video\nmatting of complex scenes. ACM Transactions on Graphics (Proc. SIGGRAPH 2002),\n21(3):243–248.\nChuang, Y.-Y., Goldman, D. B., Curless, B., Salesin, D. H., and Szeliski, R.\n(2003).\nShadow matting. ACM Transactions on Graphics (Proc. SIGGRAPH 2003), 22(3):494–\n500.\nChuang, Y.-Y., Goldman, D. B., Zheng, K. C., Curless, B., Salesin, D. H., and Szeliski,\nR. (2005). Animating pictures with stochastic motion textures. ACM Transactions on\nGraphics (Proc. SIGGRAPH 2005), 24(3):853–860.\nChuang, Y.-Y., Zongker, D., Hindorff, J., Curless, B., Salesin, D. H., and Szeliski, R.\n(2000). Environment matting extensions: Towards higher accuracy and real-time cap-\nture. In ACM SIGGRAPH 2000 Conference Proceedings, pp. 121–130, New Orleans.\nChui, C. K. (1992). Wavelet Analysis and Its Applications. Academic Press, New York.\nChum, O. and Matas, J. (2005). Matching with PROSAC—progressive sample consensus.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 220–226, San Diego, CA.\nChum, O. and Matas, J. (2010). Large-scale discovery of spatially related images. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 32(2):371–377.\nChum, O., Philbin, J., and Zisserman, A. (2008). Near duplicate image detection: min-\nhash and tf-idf weighting. In British Machine Vision Conference (BMVC 2008), Leeds,\nEngland.",
  "834": "812\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nChum, O., Philbin, J., Sivic, J., Isard, M., and Zisserman, A. (2007). Total recall: Auto-\nmatic query expansion with a generative feature model for object retrieval. In Eleventh\nInternational Conference on Computer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nCipolla, R. and Blake, A. (1990). The dynamic analysis of apparent contours. In Third\nInternational Conference on Computer Vision (ICCV’90), pp. 616–623, Osaka, Japan.\nCipolla, R. and Blake, A. (1992). Surface shape from the deformation of apparent contours.\nInternational Journal of Computer Vision, 9(2):83–112.\nCipolla, R. and Giblin, P. (2000). Visual Motion of Curves and Surfaces. Cambridge\nUniversity Press, Cambridge.\nCipolla, R., Drummond, T., and Robertson, D. P. (1999). Camera calibration from van-\nishing points in images of architectural scenes. In British Machine Vision Conference\n(BMVC99).\nClaus, D. and Fitzgibbon, A. (2005). A rational function lens distortion model for gen-\neral cameras. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2005), pp. 213–219, San Diego, CA.\nClowes, M. B. (1971). On seeing things. Artiﬁcial Intelligence, 2:79–116.\nCohen, L. D. and Cohen, I. (1993). Finite-element methods for active contour models and\nballoons for 2-D and 3-D images. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 15(11):1131–1147.\nCohen, M. and Wallace, J. (1993). Radiosity and Realistic Image Synthesis. Morgan\nKaufmann.\nCohen, M. F. and Szeliski, R. (2006). The Moment Camera. Computer, 39(8):40–45.\nCollins, R. T. (1996). A space-sweep approach to true multi-image matching. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’96),\npp. 358–363, San Francisco.\nCollins, R. T. and Liu, Y. (2003). On-line selection of discriminative tracking features. In\nNinth International Conference on Computer Vision (ICCV 2003), pp. 346–352, Nice,\nFrance.\nCollins, R. T. and Weiss, R. S. (1990). Vanishing point calculation as a statistical inference\non the unit sphere. In Third International Conference on Computer Vision (ICCV’90),\npp. 400–403, Osaka, Japan.\nComaniciu, D. and Meer, P. (2002). Mean shift: A robust approach toward feature space\nanalysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603–\n619.",
  "835": "References\n813\nComaniciu, D. and Meer, P. (2003). An algorithm for data-driven bandwidth selection.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 25(2):281–288.\nConn, A. R., Gould, N. I. M., and Toint, P. L. (2000). Trust-Region Methods. Society for\nIndustrial and Applied Mathematics, Philadephia.\nCook, R. L. and Torrance, K. E. (1982). A reﬂectance model for computer graphics. ACM\nTransactions on Graphics, 1(1):7–24.\nCoorg, S. and Teller, S. (2000). Spherical mosaics with quaternions and dense correlation.\nInternational Journal of Computer Vision, 37(3):259–273.\nCootes, T., Edwards, G. J., and Taylor, C. J. (2001). Active appearance models. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 23(6):681–685.\nCootes, T., Cooper, D., Taylor, C., and Graham, J. (1995). Active shape models—their\ntraining and application. Computer Vision and Image Understanding, 61(1):38–59.\nCootes, T., Taylor, C., Lanitis, A., Cooper, D., and Graham, J. (1993). Building and using\nﬂexible models incorporating grey-level information. In Fourth International Confer-\nence on Computer Vision (ICCV’93), pp. 242–246, Berlin, Germany.\nCootes, T. F. and Taylor, C. J. (2001). Statistical models of appearance for medical image\nanalysis and computer vision. In Medical Imaging.\nCoquillart, S. (1990). Extended free-form deformations: A sculpturing tool for 3D geo-\nmetric modeling. Computer Graphics (SIGGRAPH ’90), 24(4):187–196.\nCormen, T. H. (2001). Introduction to Algorithms. MIT Press, Cambridge, Massachusetts.\nCornelis, N., Leibe, B., Cornelis, K., and Van Gool, L. (2008). 3D urban scene modeling\nintegrating recognition and reconstruction. International Journal of Computer Vision,\n78(2-3):121–141.\nCorso, J. and Hager, G. (2005). Coherent regions for concise and stable image description.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 184–190, San Diego, CA.\nCosteira, J. and Kanade, T. (1995). A multi-body factorization method for motion analy-\nsis. In Fifth International Conference on Computer Vision (ICCV’95), pp. 1071–1076,\nCambridge, Massachusetts.\nCosten, N., Cootes, T. F., Edwards, G. J., and Taylor, C. J. (1999). Simultaneous extrac-\ntion of functional face subspaces. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’99), pp. 492–497, Fort Collins.\nCouprie, C., Grady, L., Najman, L., and Talbot, H. (2009). Power watersheds: A new image\nsegmentation framework extending graph cuts, random walker and optimal spanning",
  "836": "814\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nforest. In Twelfth International Conference on Computer Vision (ICCV 2009), Kyoto,\nJapan.\nCour, T., B´en´ezit, F., and Shi, J. (2005). Spectral segmentation with multiscale graph\ndecomposition. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2005), pp. 1123–1130, San Diego, CA.\nCox, D., Little, J., and O’Shea, D. (2007). Ideals, Varieties, and Algorithms: An Introduc-\ntion to Computational Algebraic Geometry and Commutative Algebra. Springer.\nCox, I. J. (1994). A maximum likelihood N-camera stereo algorithm. In IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR’94), pp. 733–\n739, Seattle.\nCox, I. J., Roy, S., and Hingorani, S. L. (1995). Dynamic histogram warping of image pairs\nfor constant image brightness. In IEEE International Conference on Image Processing\n(ICIP’95), pp. 366–369.\nCox, I. J., Hingorani, S. L., Rao, S. B., and Maggs, B. M. (1996). A maximum likelihood\nstereo algorithm. Computer Vision and Image Understanding, 63(3):542–567.\nCrandall, D. and Huttenlocher, D. (2007). Composite models of objects and scenes for\ncategory recognition. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2007), Minneapolis, MN.\nCrandall, D., Felzenszwalb, P., and Huttenlocher, D. (2005). Spatial priors for part-based\nrecognition using statistical models. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR’2005), pp. 10–17, San Diego, CA.\nCrandall, D., Backstrom, L., Huttenlocher, D., and Kleinberg, J. (2009). Mapping the\nworld’s photos. In 18th Int. World Wide Web Conference, pp. 761–770, Madrid.\nCrandall, D. J. and Huttenlocher, D. P. (2006). Weakly supervised learning of part-based\nspatial models for visual object recognition. In Ninth European Conference on Com-\nputer Vision (ECCV 2006), pp. 16–29.\nCrane, R. (1997). A Simpliﬁed Approach to Image Processing. Prentice Hall, Upper Saddle\nRiver, NJ.\nCraswell, N. and Szummer, M. (2007). Random walks on the click graph. In ACM SIGIR\nConference on Research and Development in Informaion Retrieval, pp. 239–246, New\nYork, NY.\nCremers, D. and Soatto, S. (2005). Motion competition: A variational framework for\npiecewise parametric motion segmentation. International Journal of Computer Vision,\n62(3):249–265.",
  "837": "References\n815\nCremers, D., Rousson, M., and Deriche, R. (2007). A review of statistical approaches\nto level set segmentation: integrating color, texture, motion and shape. International\nJournal of Computer Vision, 72(2):195–215.\nCrevier, D. (1993). AI: The Tumultuous Search for Artiﬁcial Intelligence. BasicBooks,\nNew York, NY.\nCriminisi, A., P´erez, P., and Toyama, K. (2004). Region ﬁlling and object removal by\nexemplar-based inpainting. IEEE Transactions on Image Processing, 13(9):1200–1212.\nCriminisi, A., Reid, I., and Zisserman, A. (2000). Single view metrology. International\nJournal of Computer Vision, 40(2):123–148.\nCriminisi, A., Sharp, T., and Blake, A. (2008). Geos: Geodesic image segmentation. In\nTenth European Conference on Computer Vision (ECCV 2008), pp. 99–112, Marseilles.\nCriminisi, A., Cross, G., Blake, A., and Kolmogorov, V. (2006). Bilayer segmentation\nof live video. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2006), pp. 53–60, New York City, NY.\nCriminisi, A., Shotton, J., Blake, A., and Torr, P. (2003). Gaze manipulation for one-to-one\nteleconferencing. In Ninth International Conference on Computer Vision (ICCV 2003),\npp. 191–198, Nice, France.\nCriminisi, A., Kang, S. B., Swaminathan, R., Szeliski, R., and Anandan, P. (2005). Extract-\ning layers and analyzing their specular properties using epipolar-plane-image analysis.\nComputer Vision and Image Understanding, 97(1):51–85.\nCriminisi, A., Shotton, J., Blake, A., Rother, C., and Torr, P. H. S. (2007). Efﬁcient dense\nstereo with occlusion by four-state dynamic programming. International Journal of\nComputer Vision, 71(1):89–110.\nCrow, F. C. (1984). Summed-area table for texture mapping. Computer Graphics (SIG-\nGRAPH ’84), 18(3):207–212.\nCrowley, J. L. and Stern, R. M. (1984). Fast computation of the difference of low-pass\ntransform. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(2):212–\n222.\nCsurka, G. and Perronnin, F. (2008). A simple high performance approach to semantic\nsegmentation. In British Machine Vision Conference (BMVC 2008), Leeds.\nCsurka, G., Dance, C. R., Perronnin, F., and Willamowski, J. (2006). Generic visual cate-\ngorization using weak geometry. In Ponce, J., Hebert, M., Schmid, C., and Zisserman,\nA. (eds), Toward Category-Level Object Recognition, pp. 207–224, Springer, New York.",
  "838": "816\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nCsurka, G., Dance, C. R., Fan, L., Willamowski, J., and Bray, C. (2004). Visual categoriza-\ntion with bags of keypoints. In ECCV International Workshop on Statistical Learning\nin Computer Vision, Prague.\nCui, J., Yang, Q., Wen, F., Wu, Q., Zhang, C., Van Gool, L., and Tang, X. (2008). Trans-\nductive object cutout. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.\nCurless, B. (1999). From range scans to 3D models. Computer Graphics, 33(4):38–41.\nCurless, B. and Levoy, M. (1995). Better optical triangulation through spacetime anal-\nysis. In Fifth International Conference on Computer Vision (ICCV’95), pp. 987–994,\nCambridge, Massachusetts.\nCurless, B. and Levoy, M. (1996). A volumetric method for building complex models from\nrange images. In ACM SIGGRAPH 1996 Conference Proceedings, pp. 303–312, New\nOrleans.\nCutler, R. and Davis, L. S. (2000). Robust real-time periodic motion detection, analysis,\nand applications. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n22(8):781–796.\nCutler, R. and Turk, M. (1998). View-based interpretation of real-time optical ﬂow for\ngesture recognition. In IEEE International Conference on Automatic Face and Gesture\nRecognition, pp. 416–421, Nara, Japan.\nDai, S., Baker, S., and Kang, S. B. (2009). An MRF-based deinterlacing algorithm with\nexemplar-based reﬁnement. IEEE Transactions on Image Processing, 18(5):956–968.\nDalal, N. and Triggs, B. (2005). Histograms of oriented gradients for human detection.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 886–893, San Diego, CA.\nDalal, N., Triggs, B., and Schmid, C. (2006). Human detection using oriented histograms\nof ﬂow and appearance. In Ninth European Conference on Computer Vision (ECCV\n2006), pp. 428–441.\nDana, K. J., van Ginneken, B., Nayar, S. K., and Koenderink, J. J. (1999). Reﬂectance and\ntexture of real world surfaces. ACM Transactions on Graphics, 18(1):1–34.\nDanielsson, P. E. (1980). Euclidean distance mapping. Computer Graphics and Image\nProcessing, 14(3):227–248.\nDarrell, T. and Pentland, A. (1991). Robust estimation of a multi-layered motion represen-\ntation. In IEEE Workshop on Visual Motion, pp. 173–178, Princeton, New Jersey.\nDarrell, T. and Pentland, A. (1995). Cooperative robust estimation using layers of support.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 17(5):474–487.",
  "839": "References\n817\nDarrell, T. and Simoncelli, E. (1993). “Nulling” ﬁlters and the separation of transparent\nmotion. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’93), pp. 738–739, New York.\nDarrell, T., Gordon, G., Harville, M., and Woodﬁll, J. (2000). Integrated person tracking\nusing stereo, color, and pattern detection. International Journal of Computer Vision,\n37(2):175–185.\nDarrell, T., Baker, H., Crow, F., Gordon, G., and Woodﬁll, J. (1997). Magic morphin\nmirror: face-sensitive distortion and exaggeration. In ACM SIGGRAPH 1997 Visual\nProceedings, Los Angeles.\nDatta, R., Joshi, D., Li, J., and Wang, J. Z. (2008). Image retrieval: Ideas, inﬂuences, and\ntrends of the new age. ACM Computing Surveys, 40(2).\nDaugman, J. (2004). How iris recognition works. IEEE Transactions on Circuits and\nSystems for Video Technology, 14(1):21–30.\nDavid, P., DeMenthon, D., Duraiswami, R., and Samet, H. (2004). SoftPOSIT: Simultane-\nous pose and correspondence determination. International Journal of Computer Vision,\n59(3):259–284.\nDavies, R., Twining, C., and Taylor, C. (2008). Statistical Models of Shape. Springer-\nVerlag, London.\nDavis, J. (1998). Mosaics of scenes with moving objects. In IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition (CVPR’98), pp. 354–360, Santa\nBarbara.\nDavis, J., Ramamoorthi, R., and Rusinkiewicz, S. (2003). Spacetime stereo: A unifying\nframework for depth from triangulation.\nIn IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’2003), pp. 359–366, Madison, WI.\nDavis, J., Nahab, D., Ramamoorthi, R., and Rusinkiewicz, S. (2005). Spacetime stereo: A\nunifying framework for depth from triangulation. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 27(2):296–302.\nDavis, L. (1975). A survey of edge detection techniques. Computer Graphics and Image\nProcessing, 4(3):248–270.\nDavis, T. A. (2006). Direct Methods for Sparse Linear Systems. SIAM.\nDavis, T. A. (2008). Multifrontal multithreaded rank-revealing sparse QR factorization.\nACM Trans. on Mathematical Software, (submitted).\nDavison, A., Reid, I., Molton, N. D., and Stasse, O. (2007). MonoSLAM: Real-time sin-\ngle camera SLAM. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n29(6):1052–1067.",
  "840": "818\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nde Agapito, L., Hayman, E., and Reid, I. (2001). Self-calibration of rotating and zooming\ncameras. International Journal of Computer Vision, 45(2):107–127.\nde Berg, M., Cheong, O., van Kreveld, M., and Overmars, M. (2006). Computational\nGeometry: Algorithms and Applications. Springer, New York, NY, third edition.\nDe Bonet, J. (1997). Multiresolution sampling procedure for analysis and synthesis of\ntexture images. In ACM SIGGRAPH 1997 Conference Proceedings, pp. 361–368, Los\nAngeles.\nDe Bonet, J. S. and Viola, P. (1999). Poxels: Probabilistic voxelized volume reconstruc-\ntion. In Seventh International Conference on Computer Vision (ICCV’99), pp. 418–425,\nKerkyra, Greece.\nDe Castro, E. and Morandi, C. (1987). Registration of translated and rotated images using\nﬁnite Fourier transforms. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, PAMI-9(5):700–703.\nde Haan, G. and Bellers, E. B. (1998). Deinterlacing—an overview. Proceedings of the\nIEEE, 86:1839–1857.\nDe la Torre, F. and Black, M. J. (2003). A framework for robust subspace learning. Inter-\nnational Journal of Computer Vision, 54(1/2/3):117–142.\nDebevec, P. (1998). Rendering synthetic objects into real scenes: Bridging traditional and\nimage-based graphics with global illumination and high dynamic range photography. In\nACM SIGGRAPH 1998 Conference Proceedings, pp. 189–198.\nDebevec, P. (2006). Virtual cinematography: Relighting through computation. Computer,\n39(8):57–65.\nDebevec, P., Hawkins, T., Tchou, C., Duiker, H.-P., Sarokin, W., and Sagar, M. (2000).\nAcquiring the reﬂectance ﬁeld of a human face. In ACM SIGGRAPH 2000 Conference\nProceedings, pp. 145–156.\nDebevec, P., Wenger, A., Tchou, C., Gardner, A., Waese, J., and Hawkins, T.\n(2002).\nA lighting reproduction approach to live-action compositing. ACM Transactions on\nGraphics (Proc. SIGGRAPH 2002), 21(3):547–556.\nDebevec, P. E. (1999). Image-based modeling and lighting. Computer Graphics, 33(4):46–\n50.\nDebevec, P. E. and Malik, J. (1997). Recovering high dynamic range radiance maps from\nphotographs. In ACM SIGGRAPH 1997 Conference Proceedings, pp. 369–378.\nDebevec, P. E., Taylor, C. J., and Malik, J. (1996). Modeling and rendering architec-\nture from photographs: A hybrid geometry- and image-based approach. In ACM SIG-\nGRAPH 1996 Conference Proceedings, pp. 11–20, New Orleans.",
  "841": "References\n819\nDebevec, P. E., Yu, Y., and Borshukov, G. D. (1998). Efﬁcient view-dependent image-based\nrendering with projective texture-mapping. In Eurographics Rendering Workshop 1998,\npp. 105–116.\nDeCarlo, D. and Santella, A. (2002). Stylization and abstraction of photographs. ACM\nTransactions on Graphics (Proc. SIGGRAPH 2002), 21(3):769–776.\nDeCarlo, D., Metaxas, D., and Stone, M. (1998). An anthropometric face model using\nvariational techniques. In ACM SIGGRAPH 1998 Conference Proceedings, pp. 67–74.\nDelingette, H., Hebert, M., and Ikeuichi, K. (1992). Shape representation and image seg-\nmentation using deformable surfaces. Image and Vision Computing, 10(3):132–144.\nDellaert, F. and Collins, R. (1999). Fast image-based tracking by selective pixel integration.\nIn ICCV Workshop on Frame-Rate Vision, pp. 1–22.\nDelong, A., Osokin, A., Isack, H. N., and Boykov, Y. (2010). Fast approximate energy\nminimization with label costs. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2010), San Francisco, CA.\nDeMenthon, D. I. and Davis, L. S. (1995). Model-based object pose in 25 lines of code.\nInternational Journal of Computer Vision, 15(1-2):123–141.\nDemmel, J., Dongarra, J., Eijkhout, V., Fuentes, E., Petitet, A. et al. (2005). Self-adapting\nlinear algebra algorithms and software. Proceedings of the IEEE, 93(2):293–312.\nDempster, A., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incom-\nplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39(1):1–38.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A large-\nscale hierarchical image database. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2009), Miami Beach, FL.\nDeriche, R. (1987). Using Canny’s criteria to derive a recursively implemented optimal\nedge detector. International Journal of Computer Vision, 1(2):167–187.\nDeriche, R. (1990). Fast algorithms for low-level vision. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 12(1):78–87.\nDeutscher, J. and Reid, I. (2005). Articulated body motion capture by stochastic search.\nInternational Journal of Computer Vision, 61(2):185–205.\nDeutscher, J., Blake, A., and Reid, I. (2000). Articulated body motion capture by annealed\nparticle ﬁltering. In IEEE Computer Society Conference on Computer Vision and Pat-\ntern Recognition (CVPR’2000), pp. 126–133, Hilton Head Island.\nDev, P. (1974). Segmentation Processes in Visual Perception: A Cooperative Neural Model.\nCOINS Technical Report 74C-5, University of Massachusetts at Amherst.",
  "842": "820\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nDhond, U. R. and Aggarwal, J. K. (1989). Structure from stereo—a review. IEEE Trans-\nactions on Systems, Man, and Cybernetics, 19(6):1489–1510.\nDick, A., Torr, P. H. S., and Cipolla, R. (2004). Modelling and interpretation of architecture\nfrom several images. International Journal of Computer Vision, 60(2):111–134.\nDickinson, S., Leonardis, A., Schiele, B., and Tarr, M. J. (eds). (2007). Object Catego-\nrization: Computer and Human Vision Perspectives, Cambridge University Press, New\nYork.\nDickmanns, E. D. and Graefe, V. (1988). Dynamic monocular machine vision. Machine\nVision and Applications, 1:223–240.\nDiebel, J. (2006). Representing Attitude: Euler Angles, Quaternions, and Rotation Vectors.\nTechnical Report, Stanford University. http://ai.stanford.edu/∼diebel/attitude.html.\nDiebel, J. R., Thrun, S., and Br¨unig, M. (2006). A Bayesian method for probable surface\nreconstruction and decimation. ACM Transactions on Graphics, 25(1).\nDimitrijevic, M., Lepetit, V., and Fua, P.\n(2006).\nHuman body pose detection us-\ning Bayesian spatio-temporal templates. Computer Vision and Image Understanding,\n104(2-3):127–139.\nDinh, H. Q., Turk, G., and Slabaugh, G. (2002). Reconstructing surfaces by volumetric\nregularization using radial basis functions. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 24(10):1358–1371.\nDivvala, S., Hoiem, D., Hays, J., Efros, A. A., and Hebert, M. (2009). An empirical study\nof context in object detection. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2009), Miami, FL.\nDodgson, N. A. (1992). Image Resampling. Technical Report TR261, Wolfson College\nand Computer Laboratory, University of Cambridge.\nDoll`ar, P., Belongie, S., and Perona, P. (2010). The fastest pedestrian detector in the west.\nIn British Machine Vision Conference (BMVC 2010), Aberystwyth, Wales, UK.\nDoll`ar, P., Wojek, C., Schiele, B., and Perona, P. (2009). Pedestrian detection: A bench-\nmark. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR 2009), Miami Beach, FL.\nDoretto, G. and Soatto, S. (2006). Dynamic shape and appearance models. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 28(12):2006–2019.\nDoretto, G., Chiuso, A., Wu, Y. N., and Soatto, S. (2003). Dynamic textures. International\nJournal of Computer Vision, 51(2):91–109.",
  "843": "References\n821\nDork´o, G. and Schmid, C. (2003). Selection of scale-invariant parts for object class recog-\nnition. In Ninth International Conference on Computer Vision (ICCV 2003), pp. 634–\n640, Nice, France.\nDorsey, J., Rushmeier, H., and Sillion, F. (2007). Digital Modeling of Material Appear-\nance. Morgan Kaufmann, San Francisco.\nDouglas, D. H. and Peucker, T. K. (1973). Algorithms for the reduction of the number of\npoints required to represent a digitized line or its caricature. The Canadian Cartogra-\npher, 10(2):112–122.\nDrori, I., Cohen-Or, D., and Yeshurun, H. (2003). Fragment-based image completion.\nACM Transactions on Graphics (Proc. SIGGRAPH 2003), 22(3):303–312.\nDuda, R. O. and Hart, P. E. (1972). Use of the Hough transform to detect lines and curves\nin pictures. Communications of the ACM, 15(1):11–15.\nDuda, R. O., Hart, P. E., and Stork, D. G. (2001). Pattern Classiﬁcation. John Wiley &\nSons, New York, 2nd edition.\nDupuis, P. and Oliensis, J. (1994). An optimal control formulation and related numer-\nical methods for a problem in shape reconstruction. Annals of Applied Probability,\n4(2):287–346.\nDurand, F. and Dorsey, J. (2002). Fast bilateral ﬁltering for the display of high-dynamic-\nrange images. ACM Transactions on Graphics (Proc. SIGGRAPH 2002), 21(3):257–\n266.\nDurand, F. and Szeliski, R. (2007). Computational photography. IEEE Computer Graphics\nand Applications, 27(2):21–22. Guest Editors’ Introduction to Special Issue.\nDurbin, R. and Willshaw, D. (1987). An analogue approach to the traveling salesman\nproblem using an elastic net method. Nature, 326:689–691.\nDurbin, R., Szeliski, R., and Yuille, A. (1989). An analysis of the elastic net approach to\nthe travelling salesman problem. Neural Computation, 1(3):348–358.\nEck, M., DeRose, T., Duchamp, T., Hoppe, H., Lounsbery, M., and Stuetzle, W. (1995).\nMultiresolution analysis of arbitrary meshes. In ACM SIGGRAPH 1995 Conference\nProceedings, pp. 173–182, Los Angeles.\nEden, A., Uyttendaele, M., and Szeliski, R. (2006). Seamless image stitching of scenes\nwith large motions and exposure differences. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’2006), pp. 2498–2505, New York,\nNY.\nEfros, A. A. and Freeman, W. T. (2001). Image quilting for texture synthesis and transfer.\nIn ACM SIGGRAPH 2001 Conference Proceedings, pp. 341–346.",
  "844": "822\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nEfros, A. A. and Leung, T. K. (1999). Texture synthesis by non-parametric sampling.\nIn Seventh International Conference on Computer Vision (ICCV’99), pp. 1033–1038,\nKerkyra, Greece.\nEfros, A. A., Berg, A. C., Mori, G., and Malik, J. (2003). Recognizing action at a distance.\nIn Ninth International Conference on Computer Vision (ICCV 2003), pp. 726–733, Nice,\nFrance.\nEichner, M. and Ferrari, V. (2009). Better appearance models for pictorial structures. In\nBritish Machine Vision Conference (BMVC 2009).\nEisemann, E. and Durand, F. (2004). Flash photography enhancement via intrinsic relight-\ning. ACM Transactions on Graphics, 23(3):673–678.\nEisert, P., Steinbach, E., and Girod, B. (2000). Automatic reconstruction of stationary 3-D\nobjects from multiple uncalibrated camera views. IEEE Transactions on Circuits and\nSystems for Video Technology, 10(2):261–277.\nEisert, P., Wiegand, T., and Girod, B. (2000). Model-aided coding: a new approach to in-\ncorporate facial animation into motion-compensated video coding. IEEE Transactions\non Circuits and Systems for Video Technology, 10(3):344–358.\nEkman, P. and Friesen, W. V. (1978). Facial Action Coding System: A Technique for the\nMeasurement of Facial Movement. Consulting Psychologists press, Palo Alto, CA.\nEl-Melegy, M. and Farag, A. (2003). Nonmetric lens distortion calibration: Closed-form\nsolutions, robust estimation and model selection. In Ninth International Conference on\nComputer Vision (ICCV 2003), pp. 554–559, Nice, France.\nElder, J. H. (1999). Are edges incomplete?\nInternational Journal of Computer Vision,\n34(2/3):97–122.\nElder, J. H. and Goldberg, R. M. (2001). Image editing in the contour domain. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 23(3):291–296.\nElder, J. H. and Zucker, S. W. (1998). Local scale control for edge detection and blur esti-\nmation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(7):699–\n716.\nEngels, C., Stew´enius, H., and Nist´er, D. (2006). Bundle adjustment rules. In Photogram-\nmetric Computer Vision (PCV’06), Bonn, Germany.\nEngl, H. W., Hanke, M., and Neubauer, A. (1996). Regularization of Inverse Problems.\nKluwer Academic Publishers, Dordrecht.\nEnqvist, O., Josephson, K., and Kahl, F. (2009). Optimal correspondences from pairwise\nconstraints.\nIn Twelfth International Conference on Computer Vision (ICCV 2009),\nKyoto, Japan.",
  "845": "References\n823\nEstrada, F. J. and Jepson, A. D. (2009). Benchmarking image segmentation algorithms.\nInternational Journal of Computer Vision, 85(2):167–181.\nEstrada, F. J., Jepson, A. D., and Chennubhotla, C. (2004). Spectral embedding and min-cut\nfor image segmentation. In British Machine Vision Conference (BMVC 2004), pp. 317–\n326, London.\nEvangelidis, G. D. and Psarakis, E. Z. (2008). Parametric image alignment using enhanced\ncorrelation coefﬁcient maximization. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 30(10):1858–1865.\nEveringham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. (2008).\nThe PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results. http://www.\npascal-network.org/challenges/VOC/voc2008/workshop/index.html.\nEveringham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. (2010). The\nPASCAL visual object classes (VOC) challenge. International Journal of Computer\nVision, 88(2):147–168.\nEzzat, T., Geiger, G., and Poggio, T. (2002). Trainable videorealistic speech animation.\nACM Transactions on Graphics (Proc. SIGGRAPH 2002), 21(3):388–398.\nFabbri, R., Costa, L. D. F., Torelli, J. C., and Bruno, O. M. (2008). 2D Euclidean distance\ntransform algorithms: A comparative survey. ACM Computing Surveys, 40(1).\nFairchild, M. D. (2005). Color Appearance Models. Wiley, 2nd edition.\nFan, R.-E., Chen, P.-H., and Lin, C.-J. (2005). Working set selection using second order in-\nformation for training support vector machines. Journal of Machine Learning Research,\n6:1889–1918.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). LIBLINEAR:\nA library for large linear classiﬁcation. Journal of Machine Learning Research, 9:1871–\n1874.\nFarbman, Z., Fattal, R., Lischinski, D., and Szeliski, R. (2008). Edge-preserving decom-\npositions for multi-scale tone and detail manipulation. ACM Transactions on Graphics\n(Proc. SIGGRAPH 2008), 27(3).\nFarenzena, M., Fusiello, A., and Gherardi, R. (2009). Structure-and-motion pipeline on a\nhierarchical cluster tree. In IEEE International Workshop on 3D Digital Imaging and\nModeling (3DIM 2009), Kyoto, Japan.\nFarin, G. (1992). From conics to NURBS: A tutorial and survey. IEEE Computer Graphics\nand Applications, 12(5):78–86.\nFarin, G. E. (1996). Curves and Surfaces for Computer Aided Geometric Design: A Prac-\ntical Guide. Academic Press, Boston, Massachusetts, 4th edition.",
  "846": "824\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFattal, R. (2007). Image upsampling via imposed edge statistics. ACM Transactions on\nGraphics, 26(3).\nFattal, R. (2009). Edge-avoiding wavelets and their applications. ACM Transactions on\nGraphics, 28(3).\nFattal, R., Lischinski, D., and Werman, M. (2002). Gradient domain high dynamic range\ncompression. ACM Transactions on Graphics (Proc. SIGGRAPH 2002), 21(3):249–\n256.\nFaugeras, O. (1993). Three-dimensional computer vision: A geometric viewpoint. MIT\nPress, Cambridge, Massachusetts.\nFaugeras, O. and Keriven, R.\n(1998).\nVariational principles, surface evolution, PDEs,\nlevel set methods, and the stereo problem. IEEE Transactions on Image Processing,\n7(3):336–344.\nFaugeras, O. and Luong, Q.-T. (2001). The Geometry of Multiple Images. MIT Press,\nCambridge, MA.\nFaugeras, O. D. (1992). What can be seen in three dimensions with an uncalibrated stereo\nrig?\nIn Second European Conference on Computer Vision (ECCV’92), pp. 563–578,\nSanta Margherita Liguere, Italy.\nFaugeras, O. D. and Hebert, M. (1987). The representation, recognition and positioning of\n3-D shapes from range data. In Kanade, T. (ed.), Three-Dimensional Machine Vision,\npp. 301–353, Kluwer Academic Publishers, Boston.\nFaugeras, O. D., Luong, Q.-T., and Maybank, S. J. (1992). Camera self-calibration: Theory\nand experiments. In Second European Conference on Computer Vision (ECCV’92),\npp. 321–334, Santa Margherita Liguere, Italy.\nFavaro, P. and Soatto, S. (2006). 3-D Shape Estimation and Image Restoration: Exploiting\nDefocus and Motion-Blur. Springer.\nFawcett, T.\n(2006).\nAn introduction to ROC analysis.\nPattern Recognition Letters,\n27(8):861–874.\nFei-Fei, L. and Perona, P. (2005). A Bayesian hierarchical model for learning natural scene\ncategories. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2005), pp. 524–531, San Diego, CA.\nFei-Fei, L., Fergus, R., and Perona, P. (2006). One-shot learning of object categories. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 28(4):594–611.\nFei-Fei, L., Fergus, R., and Torralba, A. (2009). ICCV 2009 short course on recognizing\nand learning object categories. In Twelfth International Conference on Computer Vision\n(ICCV 2009), Kyoto, Japan. http://people.csail.mit.edu/torralba/shortCourseRLOC/.",
  "847": "References\n825\nFeilner, M., Van De Ville, D., and Unser, M. (2005). An orthogonal family of quincunx\nwavelets with continuously adjustable order. IEEE Transactions on Image Processing,\n14(4):499–520.\nFeldmar, J. and Ayache, N. (1996). Rigid, afﬁne, and locally afﬁne registration of free-form\nsurfaces. International Journal of Computer Vision, 18(2):99–119.\nFellbaum, C. (ed.). (1998). WordNet: An Electronic Lexical Database, Bradford Books.\nFelzenszwalb, P., McAllester, D., and Ramanan, D. (2008). A discriminatively trained,\nmultiscale, deformable part model. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nFelzenszwalb, P. F. and Huttenlocher, D. P. (2004a). Distance Transforms of Sampled\nFunctions. Technical Report TR2004-1963, Cornell University Computing and Infor-\nmation Science.\nFelzenszwalb, P. F. and Huttenlocher, D. P. (2004b). Efﬁcient graph-based image segmen-\ntation. International Journal of Computer Vision, 59(2):167–181.\nFelzenszwalb, P. F. and Huttenlocher, D. P. (2005). Pictorial structures for object recogni-\ntion. International Journal of Computer Vision, 61(1):55–79.\nFelzenszwalb, P. F. and Huttenlocher, D. P. (2006). Efﬁcient belief propagation for early\nvision. International Journal of Computer Vision, 70(1):41–54.\nFelzenszwalb, P. F., Girshick, R. B., McAllester, D., and Ramanan, D. (2010). Object de-\ntection with discriminatively trained part-based models. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 32(9):1627–1645.\nFerencz, A., Learned-Miller, E. G., and Malik, J. (2008). Learning to locate informative\nfeatures for visual identiﬁcation. International Journal of Computer Vision, 77(1-3):3–\n24.\nFergus, R. (2007a). Combined segmentation and recognition. In CVPR 2007 Short Course\non Recognizing and Learning Object Categories. http://people.csail.mit.edu/torralba/\nshortCourseRLOC/.\nFergus, R. (2007b). Part-based models. In CVPR 2007 Short Course on Recognizing and\nLearning Object Categories. http://people.csail.mit.edu/torralba/shortCourseRLOC/.\nFergus, R. (2009). Classical methods for object recognition. In ICCV 2009 Short Course\non Recognizing and Learning Object Categories, Kyoto, Japan. http://people.csail.mit.\nedu/torralba/shortCourseRLOC/.\nFergus, R., Perona, P., and Zisserman, A. (2004). A visual category ﬁlter for Google\nimages. In Eighth European Conference on Computer Vision (ECCV 2004), pp. 242–\n256, Prague.",
  "848": "826\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFergus, R., Perona, P., and Zisserman, A. (2005). A sparse object category model for\nefﬁcient learning and exhaustive recognition. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’2005), pp. 380–387, San Diego, CA.\nFergus, R., Perona, P., and Zisserman, A.\n(2007).\nWeakly supervised scale-invariant\nlearning of models for visual recognition. International Journal of Computer Vision,\n71(3):273–303.\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. (2005). Learning object categories\nfrom Google’s image search. In Tenth International Conference on Computer Vision\n(ICCV 2005), pp. 1816–1823, Beijing, China.\nFergus, R., Singh, B., Hertzmann, A., Roweis, S. T., and Freeman, W. T. (2006). Removing\ncamera shake from a single photograph. ACM Transactions on Graphics, 25(3):787–\n794.\nFerrari, V., Marin-Jimenez, M., and Zisserman, A. (2009). Pose search: retrieving peo-\nple using their pose. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2009), Miami Beach, FL.\nFerrari, V., Marin-Jimenez, M. J., and Zisserman, A. (2008). Progressive search space\nreduction for human pose estimation. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nFerrari, V., Tuytelaars, T., and Van Gool, L. (2006a). Object detection by contour segment\nnetworks. In Ninth European Conference on Computer Vision (ECCV 2006), pp. 14–28.\nFerrari, V., Tuytelaars, T., and Van Gool, L. (2006b). Simultaneous object recognition and\nsegmentation from single or multiple model views. International Journal of Computer\nVision, 67(2):159–188.\nField, D. J. (1987). Relations between the statistics of natural images and the response\nproperties of cortical cells. Journal of the Optical Society of America A, 4(12):2379–\n2394.\nFinkelstein, A. and Salesin, D. H. (1994). Multiresolution curves. In ACM SIGGRAPH\n1994 Conference Proceedings, pp. 261–268.\nFischler, M. A. and Bolles, R. C. (1981). Random sample consensus: A paradigm for\nmodel ﬁtting with applications to image analysis and automated cartography. Commu-\nnications of the ACM, 24(6):381–395.\nFischler, M. A. and Elschlager, R. A. (1973). The representation and matching of pictorial\nstructures. IEEE Transactions on Computers, 22(1):67–92.\nFischler, M. A. and Firschein, O. (1987). Readings in Computer Vision. Morgan Kaufmann\nPublishers, Inc., Los Altos.",
  "849": "References\n827\nFischler, M. A., Firschein, O., Barnard, S. T., Fua, P. V., and Leclerc, Y. (1989). The Vision\nProblem: Exploiting Parallel Computation.\nTechnical Note 458, SRI International,\nMenlo Park.\nFitzgibbon, A. W. and Zisserman, A. (1998). Automatic camera recovery for closed and\nopen image sequences. In Fifth European Conference on Computer Vision (ECCV’98),\npp. 311–326, Freiburg, Germany.\nFitzgibbon, A. W., Cross, G., and Zisserman, A. (1998). Automatic 3D model construction\nfor turn-table sequences. In European Workshop on 3D Structure from Multiple Images\nof Large-Scale Environments (SMILE), pp. 155–170, Freiburg.\nFleet, D. and Jepson, A. (1990). Computation of component image velocity from local\nphase information. International Journal of Computer Vision, 5(1):77–104.\nFleuret, F. and Geman, D. (2001). Coarse-to-ﬁne face detection. International Journal of\nComputer Vision, 41(1/2):85–107.\nFlickner, M., Sawhney, H., Niblack, W., Ashley, J., Huang, Q. et al. (1995). Query by\nimage and video content: The QBIC system. Computer, 28(9):23–32.\nFoley, J. D., van Dam, A., Feiner, S. K., and Hughes, J. F. (1995). Computer Graphics:\nPrinciples and Practice. Addison-Wesley, Reading, MA, 2 edition.\nF¨orstner, W. (1986). A feature-based correspondence algorithm for image matching. Intl.\nArch. Photogrammetry & Remote Sensing, 26(3):150–166.\nF¨orstner, W. (2005). Uncertainty and projective geometry. In Bayro-Corrochano, E. (ed.),\nHandbook of Geometric Computing, pp. 493–534, Springer, New York.\nForsyth, D. and Ponce, J. (2003). Computer Vision: A Modern Approach. Prentice Hall,\nUpper Saddle River, NJ.\nForsyth, D. A. and Fleck, M. M. (1999). Automatic detection of human nudes. Interna-\ntional Journal of Computer Vision, 32(1):63–77.\nForsyth, D. A., Arikan, O., Ikemoto, L., O’Brien, J., and Ramanan, D. (2006). Computa-\ntional studies of human motion: Part 1, tracking and motion synthesis. Foundations and\nTrends in Computer Graphics and Computer Vision, 1(2/3):77–254.\nFossati, A., Dimitrijevic, M., Lepetit, V., and Fua, P. (2007). Bridging the gap between\ndetection and tracking for 3D monocular video-based motion capture. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007),\nMinneapolis, MN.\nFrahm, J.-M. and Koch, R. (2003). Camera calibration with known rotation. In Ninth Inter-\nnational Conference on Computer Vision (ICCV 2003), pp. 1418–1425, Nice, France.",
  "850": "828\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFreeman, M. (2008). Mastering HDR Photography. Amphoto Books, New York.\nFreeman, W., Perona, P., and Sch¨olkopf, B. (2008). Guest editorial: Special issue on\nmachine learning for vision. International Journal of Computer Vision, 77(1-3):1.\nFreeman, W. T. (1992). Steerable Filters and Local Analysis of Image Structure. Ph.D.\nthesis, Massachusetts Institute of Technology.\nFreeman, W. T. and Adelson, E. H. (1991). The design and use of steerable ﬁlters. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 13(9):891–906.\nFreeman, W. T., Jones, T. R., and Pasztor, E. C. (2002). Example-based super-resolution.\nIEEE Computer Graphics and Applications, 22(2):56–65.\nFreeman, W. T., Pasztor, E. C., and Carmichael, O. T. (2000). Learning low-level vision.\nInternational Journal of Computer Vision, 40(1):25–47.\nFrey, B. J. and MacKay, D. J. C. (1997). A revolution: Belief propagation in graphs with\ncycles. In Advances in Neural Information Processing Systems.\nFriedman, J., Hastie, T., and Tibshirani, R. (2000). Additive logistic regression: a statistical\nview of boosting. Annals of Statistics, 38(2):337–374.\nFrisken, S. F., Perry, R. N., Rockwood, A. P., and Jones, T. R. (2000). Adaptively sam-\npled distance ﬁelds: A general representation of shape for computer graphics. In ACM\nSIGGRAPH 2000 Conference Proceedings, pp. 249–254.\nFritz, M. and Schiele, B. (2008). Decomposition, discovery and detection of visual cate-\ngories using topic models. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2008), Anchorage, AK.\nFrome, A., Singer, Y., Sha, F., and Malik, J. (2007). Learning globally-consistent local\ndistance functions for shape-based image retrieval and classiﬁcation. In Eleventh Inter-\nnational Conference on Computer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nFua, P. (1993). A parallel stereo algorithm that produces dense depth maps and preserves\nimage features. Machine Vision and Applications, 6(1):35–49.\nFua, P. and Leclerc, Y. G. (1995). Object-centered surface reconstruction: Combining\nmulti-image stereo and shading. International Journal of Computer Vision, 16(1):35–\n56.\nFua, P. and Sander, P. (1992). Segmenting unstructured 3D points into surfaces. In Second\nEuropean Conference on Computer Vision (ECCV’92), pp. 676–680, Santa Margherita\nLiguere, Italy.\nFuh, C.-S. and Maragos, P. (1991). Motion displacement estimation using an afﬁne model\nfor image matching. Optical Engineering, 30(7):881–887.",
  "851": "References\n829\nFukunaga, K. and Hostetler, L. D. (1975). The estimation of the gradient of a density\nfunction, with applications in pattern recognition. IEEE Transactions on Information\nTheory, 21:32–40.\nFurukawa, Y. and Ponce, J. (2007). Accurate, dense, and robust multi-view stereopsis.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.\nFurukawa, Y. and Ponce, J. (2008). Accurate calibration from multi-view stereo and bundle\nadjustment. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nFurukawa, Y. and Ponce, J. (2009). Carved visual hulls for image-based modeling. Inter-\nnational Journal of Computer Vision, 81(1):53–67.\nFurukawa, Y. and Ponce, J. (2011). Accurate, dense, and robust multi-view stereopsis.\nIEEE Transactions on Pattern Analysis and Machine Intelligence.\nFurukawa, Y., Curless, B., Seitz, S. M., and Szeliski, R. (2009a). Manhattan-world stereo.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2009), Miami, FL.\nFurukawa, Y., Curless, B., Seitz, S. M., and Szeliski, R. (2009b). Reconstructing building\ninteriors from images. In Twelfth IEEE International Conference on Computer Vision\n(ICCV 2009), Kyoto, Japan.\nFurukawa, Y., Curless, B., Seitz, S. M., and Szeliski, R. (2010). Towards internet-scale\nmulti-view stereo.\nIn IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2010), San Francisco, CA.\nFusiello, A., Roberto, V., and Trucco, E. (1997). Efﬁcient stereo with multiple windowing.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’97), pp. 858–863, San Juan, Puerto Rico.\nFusiello, A., Trucco, E., and Verri, A. (2000). A compact algorithm for rectiﬁcation of\nstereo pairs. Machine Vision and Applications, 12(1):16–22.\nGai, J. and Kang, S. B. (2009). Matte-based restoration of vintage video. IEEE Transac-\ntions on Image Processing, 18:2185–2197.\nGal, R., Wexler, Y., Ofek, E., Hoppe, H., and Cohen-Or, D. (2010). Seamless montage for\ntexturing models. In Proceedings of Eurographics 2010.\nGallagher, A. C. and Chen, T. (2008). Multi-image graph cut clothing segmentation for\nrecognizing people. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.",
  "852": "830\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nGallup, D., Frahm, J.-M., Mordohai, P., and Pollefeys, M.\n(2008).\nVariable base-\nline/resolution stereo. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.\nGamble, E. and Poggio, T. (1987). Visual integration and detection of discontinuities:\nthe key role of intensity edges.\nA. I. Memo 970, Artiﬁcial Intelligence Laboratory,\nMassachusetts Institute of Technology.\nGammeter, S., Bossard, L., Quack, T., and Van Gool, L. (2009). I know what you did\nlast summer: Object-level auto-annotation of holiday snaps. In Twelfth International\nConference on Computer Vision (ICCV 2009), Kyoto, Japan.\nGao, W., Chen, Y., Wang, R., Shan, S., and Jiang, D. (2003). Learning and synthesizing\nMPEG-4 compatible 3-D face animation from video sequence. IEEE Transactions on\nCircuits and Systems for Video Technology, 13(11):1119–1128.\nGarding, J. (1992). Shape from texture for smooth curved surfaces in perspective projec-\ntion. Journal of Mathematical Imaging and Vision, 2:329–352.\nGargallo, P., Prados, E., and Sturm, P. (2007). Minimizing the reprojection error in surface\nreconstruction from images. In Eleventh International Conference on Computer Vision\n(ICCV 2007), Rio de Janeiro, Brazil.\nGavrila, D. M. (1999). The visual analysis of human movement: A survey. Computer\nVision and Image Understanding, 73(1):82–98.\nGavrila, D. M. and Davis, L. S. (1996). 3D model-based tracking of humans in action: A\nmulti-view approach. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’96), pp. 73–80, San Francisco.\nGavrila, D. M. and Philomin, V. (1999). Real-time object detection for smart vehicles. In\nSeventh International Conference on Computer Vision (ICCV’99), pp. 87–93, Kerkyra,\nGreece.\nGeiger, D. and Girosi, F. (1991). Parallel and deterministic algorithms for MRFs: Sur-\nface reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n13(5):401–412.\nGeiger, D., Ladendorf, B., and Yuille, A. (1992). Occlusions and binocular stereo. In\nSecond European Conference on Computer Vision (ECCV’92), pp. 425–433, Santa\nMargherita Liguere, Italy.\nGelb, A. (ed.).\n(1974).\nApplied Optimal Estimation.\nMIT Press, Cambridge, Mas-\nsachusetts.\nGeller, T. (2008). Overcoming the uncanny valley. IEEE Computer Graphics and Applica-\ntions, 28(4):11–17.",
  "853": "References\n831\nGeman, S. and Geman, D.\n(1984).\nStochastic relaxation, Gibbs distribution, and the\nBayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, PAMI-6(6):721–741.\nGennert, M. A. (1988). Brightness-based stereo matching. In Second International Con-\nference on Computer Vision (ICCV’88), pp. 139–143, Tampa.\nGersho, A. and Gray, R. M.\n(1991).\nVector Quantization and Signal Compression.\nSpringer.\nGershun, A. (1939). The light ﬁeld. Journal of Mathematics and Physics, XVIII:51–151.\nGevers, T., van de Weijer, J., and Stokman, H. (2006). Color feature detection. In Lukac,\nR. and Plataniotis, K. N. (eds), Color Image Processing: Methods and Applications,\nCRC Press.\nGiblin, P. and Weiss, R. (1987). Reconstruction of surfaces from proﬁles. In First Interna-\ntional Conference on Computer Vision (ICCV’87), pp. 136–144, London, England.\nGionis, A., Indyk, P., and Motwani, R.\n(1999).\nSimilarity search in high dimensions\nvia hashing. In 25th International Conference on Very Large Data Bases (VLDB’99),\npp. 518–529.\nGirod, B., Greiner, G., and Niemann, H. (eds). (2000). Principles of 3D Image Analysis\nand Synthesis, Kluwer, Boston.\nGlassner, A. S. (1995). Principles of Digital Image Synthesis. Morgan Kaufmann Publish-\ners, San Francisco.\nGleicher, M. (1995). Image snapping. In ACM SIGGRAPH 1995 Conference Proceedings,\npp. 183–190.\nGleicher, M.\n(1997).\nProjective registration with difference decomposition.\nIn IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’97),\npp. 331–337, San Juan, Puerto Rico.\nGleicher, M. and Witkin, A. (1992). Through-the-lens camera control. Computer Graphics\n(SIGGRAPH ’92), 26(2):331–340.\nGlocker, B., Komodakis, N., Tziritas, G., Navab, N., and Paragios, N. (2008). Dense image\nregistration through MRFs and efﬁcient linear programming. Medical Image Analysis,\n12(6):731–741.\nGlocker, B., Paragios, N., Komodakis, N., Tziritas, G., and Navab, N. (2008). Optical\nﬂow estimation with uncertainties through dynamic MRFs. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (CVPR 2008), Anchorage,\nAK.",
  "854": "832\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nGluckman, J. (2006a). Higher order image pyramids. In Ninth European Conference on\nComputer Vision (ECCV 2006), pp. 308–320.\nGluckman, J. (2006b). Scale variant image pyramids. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR’2006), pp. 1069–1075, New\nYork City, NY.\nGoesele, M., Curless, B., and Seitz, S. (2006). Multi-view stereo revisited. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR’2006),\npp. 2402–2409, New York City, NY.\nGoesele, M., Fuchs, C., and Seidel, H.-P.\n(2003).\nAccuracy of 3D range scanners by\nmeasurement of the slanted edge modulation transfer function. In Fourth International\nConference on 3-D Digital Imaging and Modeling, Banff.\nGoesele, M., Snavely, N., Curless, B., Hoppe, H., and Seitz, S. M. (2007). Multi-view\nstereo for community photo collections. In Eleventh International Conference on Com-\nputer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nGold, S., Rangarajan, A., Lu, C., Pappu, S., and Mjolsness, E. (1998). New algorithms for\n2D and 3D point matching: Pose estimation and correspondence. Pattern Recognition,\n31(8):1019–1031.\nGoldberg, A. V. and Tarjan, R. E. (1988). A new approach to the maximum-ﬂow problem.\nJournal of the ACM, 35(4):921–940.\nGoldluecke, B. and Cremers, D. (2009). Superresolution texture maps for multiview re-\nconstruction. In Twelfth International Conference on Computer Vision (ICCV 2009),\nKyoto, Japan.\nGoldman, D. B. (2011). Vignette and exposure calibration and compensation. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence.\nGolovinskiy, A., Matusik, W., ster, H. P., Rusinkiewicz, S., and Funkhouser, T. (2006).\nA statistical model for synthesis of detailed facial geometry. ACM Transactions on\nGraphics, 25(3):1025–1034.\nGolub, G. and Van Loan, C. F. (1996). Matrix Computation, third edition. The John\nHopkins University Press, Baltimore and London.\nGomes, J. and Velho, L. (1997). Image Processing for Computer Graphics. Springer-\nVerlag, New York.\nGomes, J., Darsa, L., Costa, B., and Velho, L. (1999). Warping and Morphing of Graphical\nObjects. Morgan Kaufmann Publishers, San Francisco.",
  "855": "References\n833\nGong, M., Yang, R., Wang, L., and Gong, M. (2007). A performance study on different\ncost aggregation approaches used in realtime stereo matching. International Journal of\nComputer Vision, 75(2):283–296.\nGonzales, R. C. and Woods, R. E. (2008). Digital Image Processing. Prentice-Hall, Upper\nSaddle River, NJ, 3rd edition.\nGooch, B. and Gooch, A. (2001). Non-Photorealistic Rendering. A K Peters, Ltd, Natick,\nMassachusetts.\nGordon, I. and Lowe, D. G. (2006). What and where: 3D object recognition with accurate\npose. In Ponce, J., Hebert, M., Schmid, C., and Zisserman, A. (eds), Toward Category-\nLevel Object Recognition, pp. 67–82, Springer, New York.\nGorelick, L., Blank, M., Shechtman, E., Irani, M., and Basri, R.\n(2007).\nActions as\nspace-time shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n29(12):2247–2253.\nGortler, S. J. and Cohen, M. F. (1995). Hierarchical and variational geometric modeling\nwith wavelets. In Symposium on Interactive 3D Graphics, pp. 35–43, Monterey, CA.\nGortler, S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. F. (1996). The Lumigraph. In\nACM SIGGRAPH 1996 Conference Proceedings, pp. 43–54, New Orleans.\nGoshtasby, A. (1989). Correction of image deformation from lens distortion using B´ezier\npatches. Computer Vision, Graphics, and Image Processing, 47(4):385–394.\nGoshtasby, A. (2005). 2-D and 3-D Image Registration. Wiley, New York.\nGotchev, A. and Rosenhahn, B. (eds). (2009). Proceedings of the 3DTV Conference: The\nTrue Vision—Capture, Transmission and Display of 3D Video, IEEE Computer Society\nPress.\nGovindu, V. M. (2006). Revisiting the brightness constraint: Probabilistic formulation and\nalgorithms. In Ninth European Conference on Computer Vision (ECCV 2006), pp. 177–\n188.\nGrady, L. (2006). Random walks for image segmentation. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 28(11):1768–1783.\nGrady, L. (2008). A lattice-preserving multigrid method for solving the inhomogeneous\nPoisson equations used in image analysis. In Tenth European Conference on Computer\nVision (ECCV 2008), pp. 252–264, Marseilles.\nGrady, L. and Ali, S. (2008). Fast approximate random walker segmentation using eigen-\nvector precomputation. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.",
  "856": "834\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nGrady, L. and Alvino, C. (2008). Reformulating and optimizing the Mumford–Shah func-\ntional on a graph — a faster, lower energy solution. In Tenth European Conference on\nComputer Vision (ECCV 2008), pp. 248–261, Marseilles.\nGrauman, K. and Darrell, T. (2005). Efﬁcient image matching with distributions of lo-\ncal invariant features. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2005), pp. 627–634, San Diego, CA.\nGrauman, K. and Darrell, T. (2007a). Pyramid match hashing: Sub-linear time index-\ning over partial correspondences. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nGrauman, K. and Darrell, T. (2007b). The pyramid match kernel: Efﬁcient learning with\nsets of features. Journal of Machine Learning Research, 8:725–760.\nGrauman, K., Shakhnarovich, G., and Darrell, T. (2003). Inferring 3D structure with a\nstatistical image-based shape model. In Ninth International Conference on Computer\nVision (ICCV 2003), pp. 641–648, Nice, France.\nGreene, N. (1986). Environment mapping and other applications of world projections.\nIEEE Computer Graphics and Applications, 6(11):21–29.\nGreene, N. and Heckbert, P. (1986). Creating raster Omnimax images from multiple per-\nspective views using the elliptical weighted average ﬁlter. IEEE Computer Graphics\nand Applications, 6(6):21–27.\nGreig, D., Porteous, B., and Seheult, A. (1989). Exact maximum a posteriori estimation\nfor binary images. Journal of the Royal Statistical Society, Series B, 51(2):271–279.\nGremban, K. D., Thorpe, C. E., and Kanade, T. (1988). Geometric camera calibration\nusing systems of linear equations. In IEEE International Conference on Robotics and\nAutomation, pp. 562–567, Philadelphia.\nGrifﬁn, G., Holub, A., and Perona, P. (2007). Caltech-256 Object Category Dataset. Tech-\nnical Report 7694, California Institute of Technology.\nGrimson, W. E. L. (1983). An implementation of a computational theory of visual surface\ninterpolation. Computer Vision, Graphics, and Image Processing, 22:39–69.\nGrimson, W. E. L. (1985). Computational experiments with a feature based stereo al-\ngorithm.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-\n7(1):17–34.\nGross, R., Matthews, I., and Baker, S. (2006). Active appearance models with occlusion.\nImage and Vision Computing, 24(6):593–604.\nGross, R., Shi, J., and Cohn, J. F. (2005). Quo vadis face recognition? In IEEE Workshop\non Empirical Evaluation Methods in Computer Vision, San Diego.",
  "857": "References\n835\nGross, R., Baker, S., Matthews, I., and Kanade, T. (2005). Face recognition across pose\nand illumination. In Li, S. Z. and Jain, A. K. (eds), Handbook of Face Recognition,\nSpringer.\nGross, R., Sweeney, L., De la Torre, F., and Baker, S. (2008). Semi-supervised learning of\nmulti-factor models for face de-identiﬁcation. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nGross, R., Matthews, I., Cohn, J., Kanade, T., and Baker, S. (2010). Multi-PIE. Image and\nVision Computing, 28(5):807–813.\nGrossberg, M. D. and Nayar, S. K. (2001). A general imaging model and a method for\nﬁnding its parameters. In Eighth International Conference on Computer Vision (ICCV\n2001), pp. 108–115, Vancouver, Canada.\nGrossberg, M. D. and Nayar, S. K. (2004). Modeling the space of camera response func-\ntions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(10):1272–\n1282.\nGu, C., Lim, J., Arbelaez, P., and Malik, J. (2009). Recognition using regions. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR\n2009), Miami Beach, FL.\nGu, X., Gortler, S. J., and Hoppe, H. (2002). Geometry images. ACM Transactions on\nGraphics, 21(3):355–361.\nGuan, P., Weiss, A., Bˇalan, A. O., and Black, M. J. (2009). Estimating human shape and\npose from a single image. In Twelfth International Conference on Computer Vision\n(ICCV 2009), Kyoto, Japan.\nGuennebaud, G. and Gross, M. (2007). Algebraic point set surfaces. ACM Transactions on\nGraphics, 26(3).\nGuennebaud, G., Germann, M., and Gross, M. (2008). Dynamic sampling and rendering\nof algebraic point set surfaces. Computer Graphics Forum, 27(2):653–662.\nGuenter, B., Grimm, C., Wood, D., Malvar, H., and Pighin, F. (1998). Making faces. In\nACM SIGGRAPH 1998 Conference Proceedings, pp. 55–66.\nGuillaumin, M., Verbeek, J., and Schmid, C. (2009). Is that you? Metric learning ap-\nproaches for face identiﬁcation. In Twelfth International Conference on Computer Vi-\nsion (ICCV 2009), Kyoto, Japan.\nGulbins, J. and Gulbins, R. (2009). Photographic Multishot Techniques: High Dynamic\nRange, Super-Resolution, Extended Depth of Field, Stitching. Rocky Nook.",
  "858": "836\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nHabbecke, M. and Kobbelt, L. (2007). A surface-growing approach to multi-view stereo\nreconstruction. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2007), Minneapolis, MN.\nHager, G. D. and Belhumeur, P. N. (1998). Efﬁcient region tracking with parametric models\nof geometry and illumination. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 20(10):1025–1039.\nHall, R. (1989). Illumination and Color in Computer Generated Imagery. Springer-Verlag,\nNew York.\nHaller, M., Billinghurst, M., and Thomas, B. (2007). Emerging Technologies of Augmented\nReality: Interfaces and Design. IGI Publishing.\nHampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. (1986). Robust\nStatistics : The Approach Based on Inﬂuence Functions. Wiley, New York.\nHan, F. and Zhu, S.-C. (2005). Bottom-up/top-down image parsing by attribute graph gram-\nmar. In Tenth International Conference on Computer Vision (ICCV 2005), pp. 1778–\n1785, Beijing, China.\nHanna, K. J. (1991). Direct multi-resolution estimation of ego-motion and structure from\nmotion. In IEEE Workshop on Visual Motion, pp. 156–162, Princeton, New Jersey.\nHannah, M. J. (1974). Computer Matching of Areas in Stereo Images. Ph.D. thesis, Stan-\nford University.\nHannah, M. J. (1988). Test results from SRI’s stereo system. In Image Understanding\nWorkshop, pp. 740–744, Cambridge, Massachusetts.\nHansen, M., Anandan, P., Dana, K., van der Wal, G., and Burt, P. (1994). Real-time scene\nstabilization and mosaic construction. In IEEE Workshop on Applications of Computer\nVision (WACV’94), pp. 54–62, Sarasota.\nHanson, A. R. and Riseman, E. M. (eds). (1978). Computer Vision Systems, Academic\nPress, New York.\nHaralick, R. M. and Shapiro, L. G. (1985). Image segmentation techniques. Computer\nVision, Graphics, and Image Processing, 29(1):100–132.\nHaralick, R. M. and Shapiro, L. G. (1992). Computer and Robot Vision. Addison-Wesley,\nReading, MA.\nHaralick, R. M., Lee, C.-N., Ottenberg, K., and N¨olle, M. (1994). Review and analysis of\nsolutions of the three point perspective pose estimation problem. International Journal\nof Computer Vision, 13(3):331–356.",
  "859": "References\n837\nHardie, R. C., Barnard, K. J., and Armstrong, E. E. (1997). Joint MAP registration and\nhigh-resolution image estimation using a sequence of undersampled images.\nIEEE\nTransactions on Image Processing, 6(12):1621–1633.\nHaritaoglu, I., Harwood, D., and Davis, L. S. (2000). W4: Real-time surveillance of people\nand their activities. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n22(8):809–830.\nHarker, M. and O’Leary, P. (2008). Least squares surface reconstruction from measured\ngradient ﬁelds. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nHarris, C. and Stephens, M. J. (1988). A combined corner and edge detector. In Alvey\nVision Conference, pp. 147–152.\nHartley, R. and Kang, S. B. (2007). Parameter-free radial distortion correction with center\nof distortion estimation. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 31(8):1309–1321.\nHartley, R., Gupta, R., and Chang, T. (1992). Estimation of relative camera positions for\nuncalibrated cameras. In Second European Conference on Computer Vision (ECCV’92),\npp. 579–587, Santa Margherita Liguere, Italy.\nHartley, R. I. (1994a). Projective reconstruction and invariants from multiple images. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 16(10):1036–1041.\nHartley, R. I.\n(1994b).\nSelf-calibration from multiple views of a rotating camera.\nIn\nThird European Conference on Computer Vision (ECCV’94), pp. 471–478, Stockholm,\nSweden.\nHartley, R. I. (1997a). In defense of the 8-point algorithm. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 19(6):580–593.\nHartley, R. I. (1997b). Self-calibration of stationary cameras. International Journal of\nComputer Vision, 22(1):5–23.\nHartley, R. I. (1998). Chirality. International Journal of Computer Vision, 26(1):41–61.\nHartley, R. I. and Kang, S. B. (2005). Parameter-free radial distortion correction with\ncentre of distortion estimation. In Tenth International Conference on Computer Vision\n(ICCV 2005), pp. 1834–1841, Beijing, China.\nHartley, R. I. and Sturm, P. (1997). Triangulation. Computer Vision and Image Under-\nstanding, 68(2):146–157.\nHartley, R. I. and Zisserman, A. (2004). Multiple View Geometry. Cambridge University\nPress, Cambridge, UK.",
  "860": "838\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nHartley, R. I., Hayman, E., de Agapito, L., and Reid, I. (2000). Camera calibration and\nthe search for inﬁnity. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2000), pp. 510–517, Hilton Head Island.\nHasinoff, S. W. and Kutulakos, K. N. (2008). Light-efﬁcient photography. In Tenth Euro-\npean Conference on Computer Vision (ECCV 2008), pp. 45–59, Marseilles.\nHasinoff, S. W., Durand, F., and Freeman, W. T. (2010). Noise-optimal capture for high dy-\nnamic range photography. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2010), San Francisco, CA.\nHasinoff, S. W., Kang, S. B., and Szeliski, R. (2006). Boundary matting for view synthesis.\nComputer Vision and Image Understanding, 103(1):22–32.\nHasinoff, S. W., Kutulakos, K. N., Durand, F., and Freeman, W. T.\n(2009).\nTime-\nconstrained photography.\nIn Twelfth International Conference on Computer Vision\n(ICCV 2009), Kyoto, Japan.\nHastie, T., Tibshirani, R., and Friedman, J. (2001). The Elements of Statistical Learning:\nData Mining, Inference, and Prediction. Springer-Verlag, New York.\nHayes, B. (2008). Computational photography. American Scientist, 96:94–99.\nHays, J. and Efros, A. A. (2007). Scene completion using millions of photographs. ACM\nTransactions on Graphics, 26(3).\nHays, J., Leordeanu, M., Efros, A. A., and Liu, Y. (2006). Discovering texture regularity as\na higher-order correspondence problem. In Ninth European Conference on Computer\nVision (ECCV 2006), pp. 522–535.\nHe, L.-W. and Zhang, Z. (2005). Real-time whiteboard capture and processing using a\nvideo camera for teleconferencing. In IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP 2005), pp. 1113–1116, Philadelphia.\nHe, X. and Zemel, R. S. (2008). Learning hybrid models for image annotation with partially\nlabeled data. In Advances in Neural Information Processing Systems.\nHe, X., Zemel, R. S., and Carreira-Perpi˜n´an, M. A. (2004). Multiscale conditional random\nﬁelds for image labeling. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’2004), pp. 695–702, Washington, DC.\nHe, X., Zemel, R. S., and Ray, D. (2006). Learning and incorporating top-down cues in\nimage segmentation. In Ninth European Conference on Computer Vision (ECCV 2006),\npp. 338–351.\nHealey, G. E. and Kondepudy, R.\n(1994).\nRadiometric CCD camera calibration and\nnoise estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n16(3):267–276.",
  "861": "References\n839\nHealey, G. E. and Shafer, S. A. (1992). Color. Physics-Based Vision: Principles and\nPractice, Jones & Bartlett, Cambridge, MA.\nHeath, M. D., Sarkar, S., Sanocki, T., and Bowyer, K. W. (1998). Comparison of edge\ndetectors. Computer Vision and Image Understanding, 69(1):38–54.\nHebert, M. (2000). Active and passive range sensing for robotics. In IEEE International\nConference on Robotics and Automation, pp. 102–110, San Francisco.\nHecht, E. (2001). Optics. Pearson Addison Wesley, Reading, MA, 4th edition.\nHeckbert, P. (1986). Survey of texture mapping. IEEE Computer Graphics and Applica-\ntions, 6(11):56–67.\nHeckbert, P. (1989). Fundamentals of Texture Mapping and Image Warping. Master’s\nthesis, The University of California at Berkeley.\nHeeger, D. J. (1988). Optical ﬂow using spatiotemporal ﬁlters. International Journal of\nComputer Vision, 1(1):279–302.\nHeeger, D. J. and Bergen, J. R. (1995). Pyramid-based texture analysis/synthesis. In ACM\nSIGGRAPH 1995 Conference Proceedings, pp. 229–238.\nHeisele, B., Serre, T., and Poggio, T. (2007). A component-based framework for face\ndetection and identiﬁcation. International Journal of Computer Vision, 74(2):167–181.\nHeisele, B., Ho, P., Wu, J., and Poggio, T. (2003). Face recognition: component-based\nversus global approaches. Computer Vision and Image Understanding, 91(1-2):6–21.\nHerley, C. (2005). Automatic occlusion removal from minimum number of images. In In-\nternational Conference on Image Processing (ICIP 2005), pp. 1046–1049–16, Genova.\nHernandez, C. and Schmitt, F. (2004). Silhouette and stereo fusion for 3D object modeling.\nComputer Vision and Image Understanding, 96(3):367–392.\nHernandez, C. and Vogiatzis, G. (2010). Self-calibrating a real-time monocular 3d facial\ncapture system. In Fifth International Symposium on 3D Data Processing, Visualization\nand Transmission (3DPVT’10), Paris.\nHernandez, C., Vogiatzis, G., and Cipolla, R. (2007). Probabilistic visibility for multi-\nview stereo. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2007), Minneapolis, MN.\nHernandez, C., Vogiatzis, G., Brostow, G. J., Stenger, B., and Cipolla, R. (2007). Non-\nrigid photometric stereo with colored lights. In Eleventh International Conference on\nComputer Vision (ICCV 2007), Rio de Janeiro, Brazil.",
  "862": "840\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nHershberger, J. and Snoeyink, J.\n(1992).\nSpeeding Up the Douglas-Peucker Line-\nSimpliﬁcation Algorithm. Technical Report TR-92-07, Computer Science Department,\nThe University of British Columbia.\nHertzmann, A., Jacobs, C. E., Oliver, N., Curless, B., and Salesin, D. H. (2001). Image\nanalogies. In ACM SIGGRAPH 2001 Conference Proceedings, pp. 327–340.\nHiep, V. H., Keriven, R., Pons, J.-P., and Labatut, P. (2009). Towards high-resolution large-\nscale multi-view stereo. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2009), Miami Beach, FL.\nHillman, P., Hannah, J., and Renshaw, D. (2001). Alpha channel estimation in high resolu-\ntion images and image sequences. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’2001), pp. 1063–1068, Kauai, Hawaii.\nHilton, A., Fua, P., and Ronfard, R. (2006). Modeling people: Vision-based understanding\nof a person’s shape, appearance, movement, and behaviour. Computer Vision and Image\nUnderstanding, 104(2-3):87–89.\nHilton, A., Stoddart, A. J., Illingworth, J., and Windeatt, T. (1996). Reliable surface re-\nconstruction from multiple range images. In Fourth European Conference on Computer\nVision (ECCV’96), pp. 117–126, Cambridge, England.\nHinckley, K., Sinclair, M., Hanson, E., Szeliski, R., and Conway, M. (1999). The Video-\nMouse: a camera-based multi-degree-of-freedom input device. In 12th annual ACM\nsymposium on User interface software and technology, pp. 103–112.\nHinterstoisser, S., Benhimane, S., Navab, N., Fua, P., and Lepetit, V.\n(2008).\nOnline\nlearning of patch perspective rectiﬁcation for efﬁcient object detection. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2008),\nAnchorage, AK.\nHinton, G. E. (1977). Relaxation and its Role in Vision. Ph.D. thesis, University of Edin-\nburgh.\nHirschm¨uller, H. (2008). Stereo processing by semiglobal matching and mutual informa-\ntion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(2):328–341.\nHirschm¨uller, H. and Scharstein, D. (2009). Evaluation of stereo matching costs on im-\nages with radiometric differences. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 31(9):1582–1599.\nHjaltason, G. R. and Samet, H. (2003). Index-driven similarity search in metric spaces.\nACM Transactions on Database Systems, 28(4):517–580.\nHofmann, T. (1999). Probabilistic latent semantic indexing. In ACM SIGIR Conference on\nResearch and Development in Informaion Retrieval, pp. 50–57, Berkeley, CA.",
  "863": "References\n841\nHogg, D. (1983). Model-based vision: A program to see a walking person. Image and\nVision Computing, 1(1):5–20.\nHoiem, D., Efros, A. A., and Hebert, M. (2005a). Automatic photo pop-up. ACM Transac-\ntions on Graphics (Proc. SIGGRAPH 2005), 24(3):577–584.\nHoiem, D., Efros, A. A., and Hebert, M. (2005b). Geometric context from a single im-\nage. In Tenth International Conference on Computer Vision (ICCV 2005), pp. 654–661,\nBeijing, China.\nHoiem, D., Efros, A. A., and Hebert, M. (2008a). Closing the loop in scene interpretation.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2008), Anchorage, AK.\nHoiem, D., Efros, A. A., and Hebert, M. (2008b). Putting objects in perspective. Interna-\ntional Journal of Computer Vision, 80(1):3–15.\nHoiem, D., Rother, C., and Winn, J. (2007). 3D LayoutCRF for multi-view object class\nrecognition and segmentation. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nHoover, A., Jean-Baptiste, G., Jiang, X., Flynn, P. J., Bunke, H. et al. (1996). An exper-\nimental comparison of range image segmentation algorithms. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 18(7):673–689.\nHoppe, H. (1996). Progressive meshes. In ACM SIGGRAPH 1996 Conference Proceed-\nings, pp. 99–108, New Orleans.\nHoppe, H., DeRose, T., Duchamp, T., McDonald, J., and Stuetzle, W.\n(1992).\nSur-\nface reconstruction from unorganized points. Computer Graphics (SIGGRAPH ’92),\n26(2):71–78.\nHorn, B. K. P. (1974). Determining lightness from an image. Computer Graphics and\nImage Processing, 3(1):277–299.\nHorn, B. K. P. (1975). Obtaining shape from shading information. In Winston, P. H. (ed.),\nThe Psychology of Computer Vision, pp. 115–155, McGraw-Hill, New York.\nHorn, B. K. P. (1977). Understanding image intensities. Artiﬁcial Intelligence, 8(2):201–\n231.\nHorn, B. K. P. (1986). Robot Vision. MIT Press, Cambridge, Massachusetts.\nHorn, B. K. P. (1987). Closed-form solution of absolute orientation using unit quaternions.\nJournal of the Optical Society of America A, 4(4):629–642.\nHorn, B. K. P. (1990). Height and gradient from shading. International Journal of Com-\nputer Vision, 5(1):37–75.",
  "864": "842\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nHorn, B. K. P. and Brooks, M. J. (1986). The variational approach to shape from shading.\nComputer Vision, Graphics, and Image Processing, 33:174–208.\nHorn, B. K. P. and Brooks, M. J. (eds). (1989). Shape from Shading, MIT Press, Cam-\nbridge, Massachusetts.\nHorn, B. K. P. and Schunck, B. G. (1981). Determining optical ﬂow. Artiﬁcial Intelligence,\n17:185–203.\nHorn, B. K. P. and Weldon Jr., E. J. (1988). Direct methods for recovering motion. Inter-\nnational Journal of Computer Vision, 2(1):51–76.\nHornung, A., Zeng, B., and Kobbelt, L.\n(2008).\nImage selection for improved multi-\nview stereo. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nHorowitz, S. L. and Pavlidis, T. (1976). Picture segmentation by a tree traversal algorithm.\nJournal of the ACM, 23(2):368–388.\nHorry, Y., Anjyo, K.-I., and Arai, K. (1997). Tour into the picture: Using a spidery mesh\ninterface to make animation from a single image. In ACM SIGGRAPH 1997 Conference\nProceedings, pp. 225–232.\nHough, P. V. C. (1962). Method and means for recognizing complex patterns. U. S. Patent,\n3,069,654.\nHouhou, N., Thiran, J.-P., and Bresson, X. (2008). Fast texture segmentation using the\nshape operator and active contour. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nHowe, N. R., Leventon, M. E., and Freeman, W. T. (2000). Bayesian reconstruction of 3D\nhuman motion from single-camera video. In Advances in Neural Information Process-\ning Systems.\nHsieh, Y. C., McKeown, D., and Perlant, F. P. (1992). Performance evaluation of scene\nregistration and stereo matching for cartographic feature extraction. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 14(2):214–238.\nHu, W., Tan, T., Wang, L., and Maybank, S. (2004). A survey on visual surveillance of\nobject motion and behaviors. IEEE Transactions on Systems, Man, and Cybernetics,\nPart C: Applications and Reviews, 34(3):334–352.\nHua, G., Brown, M., and Winder, S. (2007). Discriminant embedding for local image\ndescriptors. In Eleventh International Conference on Computer Vision (ICCV 2007),\nRio de Janeiro, Brazil.",
  "865": "References\n843\nHuang, G. B., Ramesh, M., Berg, T., and Learned-Miller, E. (2007). Labeled Faces in\nthe Wild: A Database for Studying Face Recognition in Unconstrained Environments.\nTechnical Report 07-49, University of Massachusetts, Amherst.\nHuang, T. S. (1981). Image Sequence Analysis. Springer-Verlag, Berlin, Heidelberg.\nHuber, P. J. (1981). Robust Statistics. John Wiley & Sons, New York.\nHuffman, D. A. (1971). Impossible objects and nonsense sentences. Machine Intelligence,\n8:295–323.\nHuguet, F. and Devernay, F. (2007). A variational method for scene ﬂow estimation from\nstereo sequences.\nIn Eleventh International Conference on Computer Vision (ICCV\n2007), Rio de Janeiro, Brazil.\nHuttenlocher, D. P., Klanderman, G., and Rucklidge, W. (1993). Comparing images using\nthe Hausdorff distance. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 15(9):850–863.\nHuynh, D. Q., Hartley, R., and Heyden, A. (2003). Outlier correcton in image sequences\nfor the afﬁne camera. In Ninth International Conference on Computer Vision (ICCV\n2003), pp. 585–590, Nice, France.\nIddan, G. J. and Yahav, G. (2001). 3D imaging in the studio (and elsewhere...). In Three-\nDimensional Image Capture and Applications IV, pp. 48–55.\nIgarashi, T., Nishino, K., and Nayar, S. (2007). The appearance of human skin: A survey.\nFoundations and Trends in Computer Graphics and Computer Vision, 3(1):1–95.\nIkeuchi, K. (1981). Shape from regular patterns. Artiﬁcial Intelligence, 22(1):49–75.\nIkeuchi, K. and Horn, B. K. P.\n(1981).\nNumerical shape from shading and occluding\nboundaries. Artiﬁcial Intelligence, 17:141–184.\nIkeuchi, K. and Miyazaki, D. (eds). (2007). Digitally Archiving Cultural Objects, Springer,\nBoston, MA.\nIkeuchi, K. and Sato, Y. (eds). (2001). Modeling From Reality, Kluwer Academic Publish-\ners, Boston.\nIllingworth, J. and Kittler, J. (1988). A survey of the Hough transform. Computer Vision,\nGraphics, and Image Processing, 44:87–116.\nIntille, S. S. and Bobick, A. F. (1994). Disparity-space images and large occlusion stereo.\nIn Third European Conference on Computer Vision (ECCV’94), Stockholm, Sweden.\nIrani, M. and Anandan, P. (1998). Video indexing based on mosaic representations. Pro-\nceedings of the IEEE, 86(5):905–921.",
  "866": "844\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIrani, M. and Peleg, S. (1991). Improving resolution by image registration. Graphical\nModels and Image Processing, 53(3):231–239.\nIrani, M., Hsu, S., and Anandan, P. (1995). Video compression using mosaic representa-\ntions. Signal Processing: Image Communication, 7:529–552.\nIrani, M., Rousso, B., and Peleg, S. (1994). Computing occluding and transparent motions.\nInternational Journal of Computer Vision, 12(1):5–16.\nIrani, M., Rousso, B., and Peleg, S. (1997). Recovery of ego-motion using image stabiliza-\ntion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(3):268–272.\nIsaksen, A., McMillan, L., and Gortler, S. J. (2000). Dynamically reparameterized light\nﬁelds. In ACM SIGGRAPH 2000 Conference Proceedings, pp. 297–306.\nIsard, M. and Blake, A. (1998). CONDENSATION—conditional density propagation for\nvisual tracking. International Journal of Computer Vision, 29(1):5–28.\nIshiguro, H., Yamamoto, M., and Tsuji, S. (1992). Omni-directional stereo. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 14(2):257–262.\nIshikawa, H. (2003). Exact optimization for Markov random ﬁelds with convex priors.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 25(10):1333–1336.\nIshikawa, H. and Veksler, O. (2010). Convex and truncated convex priors for multi-label\nMRFs. In Blake, A., Kohli, P., and Rother, C. (eds), Advances in Markov Random\nFields, MIT Press.\nIsidoro, J. and Sclaroff, S. (2003). Stochastic reﬁnement of the visual hull to satisfy pho-\ntometric and silhouette consistency constraints. In Ninth International Conference on\nComputer Vision (ICCV 2003), pp. 1335–1342, Nice, France.\nIvanchenko, V., Shen, H., and Coughlan, J. (2009). Elevation-based stereo implemented\nin real-time on a GPU. In IEEE Workshop on Applications of Computer Vision (WACV\n2009), Snowbird, Utah.\nJacobs, C. E., Finkelstein, A., and Salesin, D. H.\n(1995).\nFast multiresolution image\nquerying. In ACM SIGGRAPH 1995 Conference Proceedings, pp. 277–286.\nJ¨ahne, B. (1997). Digital Image Processing. Springer-Verlag, Berlin.\nJain, A. K. and Dubes, R. C. (1988). Algorithms for Clustering Data. Prentice Hall,\nEnglewood Cliffs, New Jersey.\nJain, A. K., Bolle, R. M., and Pankanti, S. (eds). (1999). Biometrics: Personal Identiﬁca-\ntion in Networked Society, Kluwer.\nJain, A. K., Duin, R. P. W., and Mao, J. (2000). Statistical pattern recognition: A review.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 22(1):4–37.",
  "867": "References\n845\nJain, A. K., Topchy, A., Law, M. H. C., and Buhmann, J. M. (2004). Landscape of clus-\ntering algorithms. In International Conference on Pattern Recognition (ICPR 2004),\npp. 260–263.\nJenkin, M. R. M., Jepson, A. D., and Tsotsos, J. K. (1991). Techniques for disparity\nmeasurement. CVGIP: Image Understanding, 53(1):14–30.\nJensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. (2001). A practical model for\nsubsurface light transport. In ACM SIGGRAPH 2001 Conference Proceedings, pp. 511–\n518.\nJeong, Y., Nist´er, D., Steedly, D., Szeliski, R., and Kweon, I.-S. (2010). Pushing the enve-\nlope of modern methods for bundle adjustment. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2010), San Francisco, CA.\nJia, J. and Tang, C.-K. (2003). Image registration with global and local luminance align-\nment. In Ninth International Conference on Computer Vision (ICCV 2003), pp. 156–\n163, Nice, France.\nJia, J., Sun, J., Tang, C.-K., and Shum, H.-Y. (2006). Drag-and-drop pasting. ACM Trans-\nactions on Graphics, 25(3):631–636.\nJiang, Z., Wong, T.-T., and Bao, H. (2003). Practical super-resolution from dynamic video\nsequences. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2003), pp. 549–554, Madison, WI.\nJohnson, A. E. and Hebert, M. (1999). Using spin images for efﬁcient object recognition in\ncluttered 3D scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n21(5):433–448.\nJohnson, A. E. and Kang, S. B. (1997). Registration and integration of textured 3-D data.\nIn International Conference on Recent Advances in 3-D Digital Imaging and Modeling,\npp. 234–241, Ottawa.\nJojic, N. and Frey, B. J. (2001). Learning ﬂexible sprites in video layers. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR’2001),\npp. 199–206, Kauai, Hawaii.\nJones, D. G. and Malik, J. (1992). A computational framework for determining stereo\ncorrespondence from a set of linear spatial ﬁlters. In Second European Conference on\nComputer Vision (ECCV’92), pp. 397–410, Santa Margherita Liguere, Italy.\nJones, M. J. and Rehg, J. M. (2001). Statistical color models with application to skin\ndetection. International Journal of Computer Vision, 46(1):81–96.\nJoshi, N., Matusik, W., and Avidan, S. (2006). Natural video matting using camera arrays.\nACM Transactions on Graphics, 25(3):779–786.",
  "868": "846\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nJoshi, N., Szeliski, R., and Kriegman, D. J. (2008). PSF estimation using sharp edge\nprediction. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nJoshi, N., Zitnick, C. L., Szeliski, R., and Kriegman, D. J.\n(2009).\nImage deblurring\nand denoising using color priors. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2009), Miami, FL.\nJu, S. X., Black, M. J., and Jepson, A. D. (1996). Skin and bones: Multi-layer, locally\nafﬁne, optical ﬂow and regularization with transparency. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (CVPR’96), pp. 307–314, San\nFrancisco.\nJu, S. X., Black, M. J., and Yacoob, Y. (1996). Cardboard people: a parameterized model\nof articulated image motion. In 2nd International Conference on Automatic Face and\nGesture Recognition, pp. 38–44, Killington, VT.\nJuan, O. and Boykov, Y. (2006). Active graph cuts. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’2006), pp. 1023–1029, New York\nCity, NY.\nJurie, F. and Dhome, M. (2002). Hyperplane approximation for template matching. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 24(7):996–1000.\nJurie, F. and Schmid, C. (2004). Scale-invariant shape features for recognition of object\ncategories. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2004), pp. 90–96, Washington, DC.\nKadir, T., Zisserman, A., and Brady, M. (2004). An afﬁne invariant salient region detec-\ntor. In Eighth European Conference on Computer Vision (ECCV 2004), pp. 228–241,\nPrague.\nKaftory, R., Schechner, Y., and Zeevi, Y. (2007). Variational distance-dependent image\nrestoration. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2007), Minneapolis, MN.\nKahl, F. and Hartley, R. (2008). Multiple-view geometry under the l∞-norm. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 30(11):1603–1617.\nKakadiaris, I. and Metaxas, D. (2000). Model-based estimation of 3D human motion.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 22(12):1453–1459.\nKakumanu, P., Makrogiannis, S., and Bourbakis, N. (2007). A survey of skin-color model-\ning and detection methods. Pattern Recognition, 40(3):1106–1122.\nKamvar, S. D., Klein, D., and Manning, C. D. (2002). Interpreting and extending classical\nagglomerative clustering algorithms using a model-based approach. In International",
  "869": "References\n847\nConference on Machine Learning, pp. 283–290.\nKanade, T. (1977). Computer Recognition of Human Faces. Birkhauser, Basel.\nKanade, T. (1980). A theory of the origami world. Artiﬁcial Intelligence, 13:279–311.\nKanade, T. (ed.). (1987). Three-Dimensional Machine Vision, Kluwer Academic Publish-\ners, Boston.\nKanade, T. (1994). Development of a video-rate stereo machine. In Image Understanding\nWorkshop, pp. 549–557, Monterey.\nKanade, T. and Okutomi, M. (1994). A stereo matching algorithm with an adaptive win-\ndow: Theory and experiment. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 16(9):920–932.\nKanade, T., Rander, P. W., and Narayanan, P. J. (1997). Virtualized reality: constructing\nvirtual worlds from real scenes. IEEE MultiMedia Magazine, 4(1):34–47.\nKanade, T., Yoshida, A., Oda, K., Kano, H., and Tanaka, M. (1996). A stereo machine for\nvideo-rate dense depth mapping and its new applications. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (CVPR’96), pp. 196–202, San\nFrancisco.\nKanatani, K. and Morris, D. D. (2001). Gauges and gauge transformations for uncertainty\ndescription of geometric structure with indeterminacy. IEEE Transactions on Informa-\ntion Theory, 47(5):2017–2028.\nKang, S. B. (1998). Depth Painting for Image-based Rendering Applications. Technical\nReport, Compaq Computer Corporation, Cambridge Research Lab.\nKang, S. B. (1999). A survey of image-based rendering techniques. In Videometrics VI,\npp. 2–16, San Jose.\nKang, S. B. (2001). Radial distortion snakes. IEICE Trans. Inf. & Syst., E84-D(12):1603–\n1611.\nKang, S. B. and Jones, M. (2002). Appearance-based structure from motion using linear\nclasses of 3-D models. International Journal of Computer Vision, 49(1):5–22.\nKang, S. B. and Szeliski, R. (1997). 3-D scene data recovery using omnidirectional multi-\nbaseline stereo. International Journal of Computer Vision, 25(2):167–183.\nKang, S. B. and Szeliski, R. (2004). Extracting view-dependent depth maps from a collec-\ntion of images. International Journal of Computer Vision, 58(2):139–163.\nKang, S. B. and Weiss, R. (1997). Characterization of errors in compositing panoramic\nimages. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’97), pp. 103–109, San Juan, Puerto Rico.",
  "870": "848\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKang, S. B. and Weiss, R. (1999). Characterization of errors in compositing panoramic\nimages. Computer Vision and Image Understanding, 73(2):269–280.\nKang, S. B. and Weiss, R. (2000). Can we calibrate a camera using an image of a ﬂat,\ntextureless Lambertian surface?\nIn Sixth European Conference on Computer Vision\n(ECCV 2000), pp. 640–653, Dublin, Ireland.\nKang, S. B., Szeliski, R., and Anandan, P. (2000). The geometry-image representation\ntradeoff for rendering. In International Conference on Image Processing (ICIP-2000),\npp. 13–16, Vancouver.\nKang, S. B., Szeliski, R., and Chai, J. (2001). Handling occlusions in dense multi-view\nstereo. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’2001), pp. 103–110, Kauai, Hawaii.\nKang, S. B., Szeliski, R., and Shum, H.-Y. (1997). A parallel feature tracker for extended\nimage sequences. Computer Vision and Image Understanding, 67(3):296–310.\nKang, S. B., Szeliski, R., and Uyttendaele, M. (2004). Seamless Stitching using Multi-\nPerspective Plane Sweep. Technical Report MSR-TR-2004-48, Microsoft Research.\nKang, S. B., Li, Y., Tong, X., and Shum, H.-Y. (2006). Image-based rendering. Founda-\ntions and Trends in Computer Graphics and Computer Vision, 2(3):173–258.\nKang, S. B., Uyttendaele, M., Winder, S., and Szeliski, R. (2003). High dynamic range\nvideo. ACM Transactions on Graphics (Proc. SIGGRAPH 2003), 22(3):319–325.\nKang, S. B., Webb, J., Zitnick, L., and Kanade, T. (1995). A multibaseline stereo system\nwith active illumination and real-time image acquisition. In Fifth International Confer-\nence on Computer Vision (ICCV’95), pp. 88–93, Cambridge, Massachusetts.\nKannala, J., Rahtu, E., Brandt, S. S., and Heikkila, J. (2008). Object recognition and seg-\nmentation by non-rigid quasi-dense matching. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nKass, M. (1988). Linear image features in stereopsis. International Journal of Computer\nVision, 1(4):357–368.\nKass, M., Witkin, A., and Terzopoulos, D. (1988). Snakes: Active contour models. Inter-\nnational Journal of Computer Vision, 1(4):321–331.\nKato, H., Billinghurst, M., Poupyrev, I., Imamoto, K., and Tachibana, K. (2000). Virtual\nobject manipulation on a table-top AR environment. In International Symposium on\nAugmented Reality (ISAR 2000).\nKaufman, L. and Rousseeuw, P. J. (1990). Finding Groups in Data: An Introduction to\nCluster Analysis. John Wiley & Sons, Hoboken.",
  "871": "References\n849\nKazhdan, M., Bolitho, M., and Hoppe, H. (2006). Poisson surface reconstruction. In\nEurographics Symposium on Geometry Processing, pp. 61–70.\nKe, Y. and Sukthankar, R. (2004). PCA-SIFT: a more distinctive representation for local\nimage descriptors.\nIn IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2004), pp. 506–513, Washington, DC.\nKehl, R. and Van Gool, L. (2006). Markerless tracking of complex human motions from\nmultiple views. Computer Vision and Image Understanding, 104(2-3):190–209.\nKenney, C., Zuliani, M., and Manjunath, B. (2005). An axiomatic approach to corner de-\ntection. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’2005), pp. 191–197, San Diego, CA.\nKeren, D., Peleg, S., and Brada, R. (1988). Image sequence enhancement using sub-pixel\ndisplacements. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’88), pp. 742–746, Ann Arbor, Michigan.\nKim, J., Kolmogorov, V., and Zabih, R. (2003). Visual correspondence using energy min-\nimization and mutual information.\nIn Ninth International Conference on Computer\nVision (ICCV 2003), pp. 1033–1040, Nice, France.\nKimmel, R. (1999). Demosaicing: image reconstruction from color CCD samples. IEEE\nTransactions on Image Processing, 8(9):1221–1228.\nKimura, S., Shinbo, T., Yamaguchi, H., Kawamura, E., and Nakano, K.\n(1999).\nA\nconvolver-based real-time stereo machine (SAZAN). In IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition (CVPR’99), pp. 457–463, Fort\nCollins.\nKindermann, R. and Snell, J. L. (1980). Markov Random Fields and Their Applications.\nAmerican Mathematical Society.\nKing, D. (1997). The Commissar Vanishes. Henry Holt and Company.\nKirby, M. and Sirovich, L. (1990). Application of the Karhunen–L`oeve procedure for the\ncharacterization of human faces. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 12(1):103–108.\nKirkpatrick, S., Gelatt, C. D. J., and Vecchi, M. P. (1983). Optimization by simulated\nannealing. Science, 220:671–680.\nKirovski, D., Jojic, N., and Jancke, G. (2004). Tamper-resistant biometric IDs. In ISSE\n2004 - Securing Electronic Business Processes: Highlights of the Information Security\nSolutions Europe 2004 Conference, pp. 160–175.\nKittler, J. and F¨oglein, J. (1984). Contextual classiﬁcation of multispectral pixel data.\nImage and Vision Computing, 2(1):13–29.",
  "872": "850\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKlaus, A., Sormann, M., and Karner, K. (2006). Segment-based stereo matching using be-\nlief propagation and a self-adapting dissimilarity measure. In International Conference\non Pattern Recognition (ICPR 2006), pp. 15–18.\nKlein, G. and Murray, D. (2007). Parallel tracking and mapping for small AR workspaces.\nIn International Symposium on Mixed and Augmented Reality (ISMAR 2007), Nara.\nKlein, G. and Murray, D. (2008). Improving the agility of keyframe-based slam. In Tenth\nEuropean Conference on Computer Vision (ECCV 2008), pp. 802–815, Marseilles.\nKlein, S., Staring, M., and Pluim, J. P. W. (2007). Evaluation of optimization methods\nfor nonrigid medical image registration using mutual information and B-splines. IEEE\nTransactions on Image Processing, 16(12):2879–2890.\nKlinker, G. J. (1993). A Physical Approach to Color Image Understanding. A K Peters,\nWellesley, Massachusetts.\nKlinker, G. J., Shafer, S. A., and Kanade, T. (1990). A physical approach to color image\nunderstanding. International Journal of Computer Vision, 4(1):7–38.\nKoch, R., Pollefeys, M., and Van Gool, L. J. (2000). Realistic surface reconstruction of\n3D scenes from uncalibrated image sequences. Journal Visualization and Computer\nAnimation, 11:115–127.\nKoenderink, J. J. (1990). Solid Shape. MIT Press, Cambridge, Massachusetts.\nKoethe, U. (2003). Integrated edge and junction detection with the boundary tensor. In\nNinth International Conference on Computer Vision (ICCV 2003), pp. 424–431, Nice,\nFrance.\nKohli, P. (2007). Minimizing Dynamic and Higher Order Energy Functions using Graph\nCuts. Ph.D. thesis, Oxford Brookes University.\nKohli, P. and Torr, P. H. S. (2005). Effciently solving dynamic Markov random ﬁelds\nusing graph cuts. In Tenth International Conference on Computer Vision (ICCV 2005),\npp. 922–929, Beijing, China.\nKohli, P. and Torr, P. H. S. (2007). Dynamic graph cuts for efﬁcient inference in markov\nrandom ﬁelds.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n29(12):2079–2088.\nKohli, P. and Torr, P. H. S. (2008). Measuring uncertainty in graph cut solutions. Computer\nVision and Image Understanding, 112(1):30–38.\nKohli, P., Kumar, M. P., and Torr, P. H. S. (2009). P3 & beyond: Move making algorithms\nfor solving higher order functions. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 31(9):1645–1656.",
  "873": "References\n851\nKohli, P., Ladick´y, L., and Torr, P. H. S. (2009). Robust higher order potentials for enforc-\ning label consistency. International Journal of Computer Vision, 82(3):302–324.\nKokaram, A. (2004). On missing data treatment for degraded video and ﬁlm archives: a sur-\nvey and a new Bayesian approach. IEEE Transactions on Image Processing, 13(3):397–\n415.\nKolev, K. and Cremers, D. (2008). Integration of multiview stereo and silhouettes via\nconvex functionals on convex domains. In Tenth European Conference on Computer\nVision (ECCV 2008), pp. 752–765, Marseilles.\nKolev, K. and Cremers, D. (2009). Continuous ratio optimization via convex relaxation\nwith applications to multiview 3D reconstruction. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR 2009), Miami Beach, FL.\nKolev, K., Klodt, M., Brox, T., and Cremers, D. (2009). Continuous global optimization in\nmultiview 3D reconstruction. International Journal of Computer Vision, 84(1):80–96.\nKoller, D. and Friedman, N.\n(2009).\nProbabilistic Graphical Models: Principles and\nTechniques. MIT Press, Cambridge, Massachusetts.\nKolmogorov, V. (2006). Convergent tree-reweighted message passing for energy minimiza-\ntion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):1568–\n1583.\nKolmogorov, V. and Boykov, Y. (2005). What metrics can be approximated by geo-cuts,\nor global optimization of length/area and ﬂux. In Tenth International Conference on\nComputer Vision (ICCV 2005), pp. 564–571, Beijing, China.\nKolmogorov, V. and Zabih, R. (2002). Multi-camera scene reconstruction via graph cuts.\nIn Seventh European Conference on Computer Vision (ECCV 2002), pp. 82–96, Copen-\nhagen.\nKolmogorov, V. and Zabih, R. (2004). What energy functions can be minimized via graph\ncuts? IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):147–159.\nKolmogorov, V., Criminisi, A., Blake, A., Cross, G., and Rother, C. (2006). Probabilistic\nfusion of stereo with color and contrast for bi-layer segmentation. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 28(9):1480–1492.\nKomodakis, N. and Paragios, N. (2008). Beyond loose LP-relaxations: Optimizing MRFs\nby repairing cycles. In Tenth European Conference on Computer Vision (ECCV 2008),\npp. 806–820, Marseilles.\nKomodakis, N. and Tziritas, G. (2007a). Approximate labeling via graph cuts based on\nlinear programming. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n29(8):1436–1453.",
  "874": "852\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKomodakis, N. and Tziritas, G. (2007b). Image completion using efﬁcient belief prop-\nagation via priority scheduling and dynamic pruning.\nIEEE Transactions on Image\nProcessing, 29(11):2649–2661.\nKomodakis, N., Paragios, N., and Tziritas, G. (2007). MRF optimization via dual decom-\nposition: Message-passing revisited. In Eleventh International Conference on Com-\nputer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nKomodakis, N., Tziritas, G., and Paragios, N. (2007). Fast, approximately optimal solu-\ntions for single and dynamic MRFs. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nKomodakis, N., Tziritas, G., and Paragios, N.\n(2008).\nPerformance vs computational\nefﬁciency for optimizing single and dynamic MRFs: Setting the state of the art with\nprimal dual strategies. Computer Vision and Image Understanding, 112(1):14–29.\nKonolige, K. (1997). Small vision systems: Hardware and implementation. In Eighth\nInternational Symposium on Robotics Research, pp. 203–212, Hayama, Japan.\nKopf, J., Cohen, M. F., Lischinski, D., and Uyttendaele, M. (2007). Joint bilateral upsam-\npling. ACM Transactions on Graphics, 26(3).\nKopf, J., Uyttendaele, M., Deussen, O., and Cohen, M. F. (2007). Capturing and viewing\ngigapixel images. ACM Transactions on Graphics, 26(3).\nKopf, J., Lischinski, D., Deussen, O., Cohen-Or, D., and Cohen, M.\n(2009).\nLocally\nadapted projections to reduce panorama distortions. Computer Graphics Forum (Pro-\nceedings of EGSR 2009), 28(4).\nKoutis, I. (2007). Combinatorial and algebraic tools for optimal multilevel algorithms.\nPh.D. thesis, Carnegie Mellon University. Technical Report CMU-CS-07-131.\nKoutis, I. and Miller, G. L. (2008). Graph partitioning into isolated, high conductance\nclusters: theory, computation and applications to preconditioning. In Symposium on\nParallel Algorithms and Architectures, pp. 137–145, Munich.\nKoutis, I., Miller, G. L., and Tolliver, D. (2009). Combinatorial preconditioners and mul-\ntilevel solvers for problems in computer vision and image processing. In 5th Interna-\ntional Symposium on Visual Computing (ISVC09), Las Vegas.\nKovar, L., Gleicher, M., and Pighin, F. (2002). Motion graphs. ACM Transactions on\nGraphics, 21(3):473–482.\nKoˇseck´a, J. and Zhang, W. (2005). Extraction, matching and pose recovery based on dom-\ninant rectangular structures. Computer Vision and Image Understanding, 100(3):174–\n293.",
  "875": "References\n853\nKraus, K. (1997). Photogrammetry. D¨ummler, Bonn.\nKrishnan, D. and Fergus, R.\n(2009).\nFast image deconvolution using hyper-Laplacian\npriors. In Advances in Neural Information Processing Systems.\nKuglin, C. D. and Hines, D. C. (1975). The phase correlation image alignment method. In\nIEEE 1975 Conference on Cybernetics and Society, pp. 163–165, New York.\nKulis, B. and Grauman, K. (2009). Kernelized locality-sensitive hashing for scalable image\nsearch. In Twelfth International Conference on Computer Vision (ICCV 2009), Kyoto,\nJapan.\nKumar, M. P. (2008). Combinatorial and Convex Optimization for Probabilistic Models in\nComputer Vision. Ph.D. thesis, Oxford Brookes University.\nKumar, M. P. and Torr, P. H. S. (2006). Fast memory-efﬁcient generalized belief propaga-\ntion. In Ninth European Conference on Computer Vision (ECCV 2006), pp. 451–463.\nKumar, M. P. and Torr, P. H. S. (2008). Improved moves for truncated convex models. In\nAdvances in Neural Information Processing Systems.\nKumar, M. P., Torr, P. H. S., and Zisserman, A. (2008). Learning layered motion segmen-\ntations of video. International Journal of Computer Vision, 76(3):301–319.\nKumar, M. P., Torr, P. H. S., and Zisserman, A. (2010). OBJCUT: Efﬁcient segmenta-\ntion using top-down and bottom-up cues. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 32(3).\nKumar, M. P., Veksler, O., and Torr, P. H. S. (2010). Improved moves for truncated convex\nmodels. Journal of Machine Learning Research, (sumbitted).\nKumar, M. P., Zisserman, A., and H.S.Torr, P. (2009). Efﬁcient discriminative learning\nof parts-based models. In Twelfth International Conference on Computer Vision (ICCV\n2009), Kyoto, Japan.\nKumar, R., Anandan, P., and Hanna, K. (1994). Direct recovery of shape from multiple\nviews: A parallax based approach.\nIn Twelfth International Conference on Pattern\nRecognition (ICPR’94), pp. 685–688, Jerusalem, Israel.\nKumar, R., Anandan, P., Irani, M., Bergen, J., and Hanna, K. (1995). Representation of\nscenes from collections of images. In IEEE Workshop on Representations of Visual\nScenes, pp. 10–17, Cambridge, Massachusetts.\nKumar, S. and Hebert, M. (2003). Discriminative random ﬁelds: A discriminative frame-\nwork for contextual interaction in classiﬁcation. In Ninth International Conference on\nComputer Vision (ICCV 2003), pp. 1150–1157, Nice, France.",
  "876": "854\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKumar, S. and Hebert, M. (2006). Discriminative random ﬁelds. International Journal of\nComputer Vision, 68(2):179–202.\nKundur, D. and Hatzinakos, D. (1996). Blind image deconvolution. IEEE Signal Process-\ning Magazine, 13(3):43–64.\nKutulakos, K. N. (2000). Approximate N-view stereo. In Sixth European Conference on\nComputer Vision (ECCV 2000), pp. 67–83, Dublin, Ireland.\nKutulakos, K. N. and Seitz, S. M. (2000). A theory of shape by space carving. Interna-\ntional Journal of Computer Vision, 38(3):199–218.\nKwatra, V., Essa, I., Bobick, A., and Kwatra, N. (2005). Graphcut textures: Image and\nvideo synthesis using graph cuts. ACM Transactions on Graphics (Proc. SIGGRAPH\n2005), 24(5):795–802.\nKwatra, V., Sch¨odl, A., Essa, I., Turk, G., and Bobick, A. (2003). Graphcut textures:\nImage and video synthesis using graph cuts. ACM Transactions on Graphics (Proc.\nSIGGRAPH 2003), 22(3):277–286.\nKybic, J. and Unser, M. (2003). Fast parametric elastic image registration. IEEE Transac-\ntions on Image Processing, 12(11):1427–1442.\nLafferty, J., McCallum, A., and Pereira, F. (2001). Conditional random ﬁelds: Probabilistic\nmodels for segmenting and labeling sequence data.\nIn International Conference on\nMachine Learning.\nLafortune, E. P. F., Foo, S.-C., Torrance, K. E., and Greenberg, D. P. (1997). Non-linear\napproximation of reﬂectance functions. In ACM SIGGRAPH 1997 Conference Proceed-\nings, pp. 117–126, Los Angeles.\nLai, S.-H. and Vemuri, B. C. (1997). Physically based adaptive preconditioning for early\nvision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(6):594–\n607.\nLalonde, J.-F., Hoiem, D., Efros, A. A., Rother, C., Winn, J., and Criminisi, A. (2007).\nPhoto clip art. ACM Transactions on Graphics, 26(3).\nLampert, C. H. (2008). Kernel methods in computer vision. Foundations and Trends in\nComputer Graphics and Computer Vision, 4(3):193–285.\nLanger, M. S. and Zucker, S. W. (1994). Shape from shading on a cloudy day. Journal\nOptical Society America, A, 11(2):467–478.\nLanitis, A., Taylor, C. J., and Cootes, T. F. (1997). Automatic interpretation and coding of\nface images using ﬂexible models. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 19(7):742–756.",
  "877": "References\n855\nLarlus, D. and Jurie, F. (2008). Combining appearance models and Markov random ﬁelds\nfor category level object segmentation. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nLarson, G. W. (1998). LogLuv encoding for full-gamut, high-dynamic range images. Jour-\nnal of Graphics Tools, 3(1):15–31.\nLarson, G. W., Rushmeier, H., and Piatko, C. (1997). A visibility matching tone reproduc-\ntion operator for high dynamic range scenes. IEEE Transactions on Visualization and\nComputer Graphics, 3(4):291–306.\nLaurentini, A. (1994). The visual hull concept for silhouette-based image understanding.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 16(2):150–162.\nLavall´ee, S. and Szeliski, R. (1995). Recovering the position and orientation of free-form\nobjects from image contours using 3-D distance maps. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 17(4):378–390.\nLaveau, S. and Faugeras, O. D. (1994). 3-D scene representation as a collection of images.\nIn Twelfth International Conference on Pattern Recognition (ICPR’94), pp. 689–691,\nJerusalem, Israel.\nLazebnik, S., Schmid, C., and Ponce, J. (2005). A sparse texture representation using\nlocal afﬁne regions. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n27(8):1265–1278.\nLazebnik, S., Schmid, C., and Ponce, J. (2006). Beyond bags of features: Spatial pyramid\nmatching for recognizing natural scene categories. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR’2006), pp. 2169–2176, New\nYork City, NY.\nLe Gall, D. (1991). MPEG: A video compression standard for multimedia applications.\nCommunications of the ACM, 34(4):46–58.\nLeclerc, Y. G.\n(1989).\nConstructing simple stable descriptions for image partitioning.\nInternational Journal of Computer Vision, 3(1):73–102.\nLeCun, Y., Huang, F. J., and Bottou, L. (2004). Learning methods for generic object\nrecognition with invariance to pose and lighting. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’2004), pp. 97–104, Washington,\nDC.\nLee, J., Chai, J., Reitsma, P. S. A., Hodgins, J. K., and Pollard, N. S. (2002). Interactive\ncontrol of avatars animated with human motion data. ACM Transactions on Graphics,\n21(3):491–500.",
  "878": "856\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLee, M.-C., ge Chen, W., lung Bruce Lin, C., Gu, C., Markoc, T., Zabinsky, S. I., and\nSzeliski, R. (1997). A layered video object coding system using sprite and afﬁne motion\nmodel. IEEE Transactions on Circuits and Systems for Video Technology, 7(1):130–\n145.\nLee, M. E. and Redner, R. A. (1990). A note on the use of nonlinear ﬁltering in computer\ngraphics. IEEE Computer Graphics and Applications, 10(3):23–29.\nLee, M. W. and Cohen, I. (2006). A model-based approach for estimating human 3D poses\nin static images.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n28(6):905–916.\nLee, S., Wolberg, G., and Shin, S. Y. (1996). Data interpolation using multilevel b-splines.\nIEEE Transactions on Visualization and Computer Graphics, 3(3):228–244.\nLee, S., Wolberg, G., Chwa, K.-Y., and Shin, S. Y. (1996). Image metamorphosis with scat-\ntered feature constraints. IEEE Transactions on Visualization and Computer Graphics,\n2(4):337–354.\nLee, Y. D., Terzopoulos, D., and Waters, K. (1995). Realistic facial modeling for animation.\nIn ACM SIGGRAPH 1995 Conference Proceedings, pp. 55–62.\nLei, C. and Yang, Y.-H. (2009). Optical ﬂow estimation on coarse-to-ﬁne region-trees using\ndiscrete optimization. In Twelfth International Conference on Computer Vision (ICCV\n2009), Kyoto, Japan.\nLeibe, B. and Schiele, B. (2003). Analyzing appearance and contour based methods for\nobject categorization. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2003), pp. 409–415, Madison, WI.\nLeibe, B., Leonardis, A., and Schiele, B.\n(2008).\nRobust object detection with inter-\nleaved categorization and segmentation.\nInternational Journal of Computer Vision,\n77(1-3):259–289.\nLeibe, B., Seemann, E., and Schiele, B. (2005). Pedestrian detection in crowded scenes.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 878–885, San Diego, CA.\nLeibe, B., Cornelis, N., Cornelis, K., and Van Gool, L. (2007). Dynamic 3D scene analysis\nfrom a moving vehicle. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2007), Minneapolis, MN.\nLeibowitz, D. (2001). Camera Calibration and Reconstruction of Geometry from Images.\nPh.D. thesis, University of Oxford.\nLempitsky, V. and Boykov, Y. (2007). Global optimization for shape ﬁtting. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007),",
  "879": "References\n857\nMinneapolis, MN.\nLempitsky, V. and Ivanov, D. (2007). Seamless mosaicing of image-based texture maps.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.\nLempitsky, V., Blake, A., and Rother, C. (2008). Image segmentation by branch-and-\nmincut. In Tenth European Conference on Computer Vision (ECCV 2008), pp. 15–29,\nMarseilles.\nLempitsky, V., Roth, S., and Rother., C. (2008). FlowFusion: Discrete-continuous opti-\nmization for optical ﬂow estimation. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nLempitsky, V., Rother, C., and Blake, A. (2007). Logcut - efﬁcient graph cut optimization\nfor Markov random ﬁelds. In Eleventh International Conference on Computer Vision\n(ICCV 2007), Rio de Janeiro, Brazil.\nLengyel, J. and Snyder, J. (1997). Rendering with coherent layers. In ACM SIGGRAPH\n1997 Conference Proceedings, pp. 233–242, Los Angeles.\nLensch, H. P. A., Kautz, J., Goesele, M., Heidrich, W., and Seidel, H.-P. (2003). Image-\nbased reconstruction of spatial appearance and geometric detail. ACM Transactions on\nGraphics, 22(2):234–257.\nLeonardis, A. and Bischof, H. (2000). Robust recognition using eigenimages. Computer\nVision and Image Understanding, 78(1):99–118.\nLeonardis, A., Jakliˇc, A., and Solina, F. (1997). Superquadrics for segmenting and mod-\neling range data. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n19(11):1289–1295.\nLepetit, V. and Fua, P. (2005). Monocular model-based 3D tracking of rigid objects. Foun-\ndations and Trends in Computer Graphics and Computer Vision, 1(1).\nLepetit, V., Pilet, J., and Fua, P. (2004). Point matching as a classiﬁcation problem for fast\nand robust object pose estimation. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’2004), pp. 244–250, Washington, DC.\nLepetit, V., Pilet, J., and Fua, P. (2006). Keypoint recognition using randomized trees.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 28(9):1465–1479.\nLeung, T. K., Burl, M. C., and Perona, P. (1995). Finding faces in cluttered scenes using\nrandom labeled graph matching. In Fifth International Conference on Computer Vision\n(ICCV’95), pp. 637–644, Cambridge, Massachusetts.\nLevenberg, K. (1944). A method for the solution of certain problems in least squares.\nQuarterly of Applied Mathematics, 2:164–168.",
  "880": "858\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLevin, A. (2006). Blind motion deblurring using image statistics. In Advances in Neural\nInformation Processing Systems.\nLevin, A. and Szeliski, R. (2004). Visual odometry and map correlation. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR’2004),\npp. 611–618, Washington, DC.\nLevin, A. and Szeliski, R.\n(2006).\nMotion Uncertainty and Field of View.\nTechnical\nReport MSR-TR-2006-37, Microsoft Research.\nLevin, A. and Weiss, Y. (2006). Learning to combine bottom-up and top-down segmenta-\ntion. In Ninth European Conference on Computer Vision (ECCV 2006), pp. 581–594.\nLevin, A. and Weiss, Y. (2007). User assisted separation of reﬂections from a single image\nusing a sparsity prior. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n29(9):1647–1654.\nLevin, A., Acha, A. R., and Lischinski, D. (2008). Spectral matting. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 30(10):1699–1712.\nLevin, A., Lischinski, D., and Weiss, Y. (2004). Colorization using optimization. ACM\nTransactions on Graphics, 23(3):689–694.\nLevin, A., Lischinski, D., and Weiss, Y. (2008). A closed form solution to natural image\nmatting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(2):228–\n242.\nLevin, A., Zomet, A., and Weiss, Y. (2004). Separating reﬂections from a single image\nusing local features. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2004), pp. 306–313, Washington, DC.\nLevin, A., Fergus, R., Durand, F., and Freeman, W. T. (2007). Image and depth from a\nconventional camera with a coded aperture. ACM Transactions on Graphics, 26(3).\nLevin, A., Weiss, Y., Durand, F., and Freeman, B. (2009). Understanding and evaluating\nblind deconvolution algorithms. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2009), Miami Beach, FL.\nLevin, A., Zomet, A., Peleg, S., and Weiss, Y. (2004). Seamless image stitching in the\ngradient domain. In Eighth European Conference on Computer Vision (ECCV 2004),\npp. 377–389, Prague.\nLevoy, M. (1988). Display of surfaces from volume data. IEEE Computer Graphics and\nApplications, 8(3):29–37.\nLevoy, M. (2006). Light ﬁelds and computational imaging. Computer, 39(8):46–55.",
  "881": "References\n859\nLevoy, M. (2008). Technical perspective: Computational photography on large collections\nof images. Communications of the ACM, 51(10):86.\nLevoy, M. and Hanrahan, P. (1996). Light ﬁeld rendering. In ACM SIGGRAPH 1996\nConference Proceedings, pp. 31–42, New Orleans.\nLevoy, M. and Whitted, T. (1985). The Use of Points as a Display Primitive. Technical\nReport 85-022, University of North Carolina at Chapel Hill.\nLevoy, M., Ng, R., Adams, A., Footer, M., and Horowitz, M. (2006). Light ﬁeld mi-\ncroscopy. ACM Transactions on Graphics, 25(3):924–934.\nLevoy, M., Pulli, K., Curless, B., Rusinkiewicz, S., Koller, D. et al. (2000). The digital\nMichelangelo project: 3D scanning of large statues. In ACM SIGGRAPH 2000 Confer-\nence Proceedings, pp. 131–144.\nLew, M. S., Sebe, N., Djeraba, C., and Jain, R. (2006). Content-based multimedia in-\nformation retrieval: State of the art and challenges. ACM Transactions on Multimedia\nComputing, Communications and Applications, 2(1):1–19.\nLeyvand, T., Cohen-Or, D., Dror, G., and Lischinski, D. (2008). Data-driven enhancement\nof facial attractiveness. ACM Transactions on Graphics, 27(3).\nLhuillier, M. and Quan, L. (2002). Match propagation for image-based modeling and ren-\ndering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(8):1140–\n1146.\nLhuillier, M. and Quan, L. (2005). A quasi-dense approach to surface reconstruction from\nuncalibrated images. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n27(3):418–433.\nLi, H. and Hartley, R. (2007). The 3D–3D registration problem revisited. In Eleventh\nInternational Conference on Computer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nLi, L.-J. and Fei-Fei, L. (2010). Optimol: Automatic object picture collection via incre-\nmental model learning. International Journal of Computer Vision, 88(2):147–168.\nLi, S. (1995). Markov Random Field Modeling in Computer Vision. Springer-Verlag.\nLi, S. Z. and Jain, A. K. (eds). (2005). Handbook of Face Recognition, Springer.\nLi, X., Wu, C., Zach, C., Lazebnik, S., and Frahm, J.-M. (2008). Modeling and recog-\nnition of landmark image collections using iconic scene graphs. In Tenth European\nConference on Computer Vision (ECCV 2008), pp. 427–440, Marseilles.\nLi, Y. and Huttenlocher, D. P. (2008). Learning for optical ﬂow using stochastic optimiza-\ntion. In Tenth European Conference on Computer Vision (ECCV 2008), pp. 379–391,\nMarseilles.",
  "882": "860\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLi, Y., Crandall, D. J., and Huttenlocher, D. P. (2009). Landmark classiﬁcation in large-\nscale image collections. In Twelfth International Conference on Computer Vision (ICCV\n2009), Kyoto, Japan.\nLi, Y., Wang, T., and Shum, H.-Y. (2002). Motion texture: a two-level statistical model for\ncharacter motion synthesis. ACM Transactions on Graphics, 21(3):465–472.\nLi, Y., Shum, H.-Y., Tang, C.-K., and Szeliski, R. (2004). Stereo reconstruction from\nmultiperspective panoramas. IEEE Transactions on Pattern Analysis and Machine In-\ntelligence, 26(1):44–62.\nLi, Y., Sun, J., Tang, C.-K., and Shum, H.-Y. (2004). Lazy snapping. ACM Transactions\non Graphics (Proc. SIGGRAPH 2004), 23(3):303–308.\nLiang, L., Xiao, R., Wen, F., and Sun, J. (2008). Face alignment via component-based dis-\ncriminative search. In Tenth European Conference on Computer Vision (ECCV 2008),\npp. 72–85, Marseilles.\nLiang, L., Liu, C., Xu, Y.-Q., Guo, B., and Shum, H.-Y. (2001). Real-time texture synthesis\nby patch-based sampling. ACM Transactions on Graphics, 20(3):127–150.\nLiebowitz, D. and Zisserman, A. (1998). Metric rectiﬁcation for perspective images of\nplanes. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’98), pp. 482–488, Santa Barbara.\nLim, J. (1990). Two-Dimensional Signal and Image Processing. Prentice-Hall, Engle-\nwood, NJ.\nLim, J. J., Arbel´aez, P., Gu, C., and Malik, J. (2009). Context by region ancestry. In Twelfth\nInternational Conference on Computer Vision (ICCV 2009), Kyoto, Japan.\nLin, D., Kapoor, A., Hua, G., and Baker, S. (2010). Joint people, event, and location recog-\nnition in personal photo collections using cross-domain context. In Eleventh European\nConference on Computer Vision (ECCV 2010), Heraklion, Crete.\nLin, W.-C., Hays, J., Wu, C., Kwatra, V., and Liu, Y. (2006). Quantitative evaluation of\nnear regular texture synthesis algorithms. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’2006), pp. 427–434, New York City,\nNY.\nLindeberg, T. (1990). Scale-space for discrete signals. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 12(3):234–254.\nLindeberg, T. (1993). Detecting salient blob-like image structures and their scales with\na scale-space primal sketch: a method for focus-of-attention. International Journal of\nComputer Vision, 11(3):283–318.",
  "883": "References\n861\nLindeberg, T. (1994). Scale-space theory: A basic tool for analysing structures at different\nscales. Journal of Applied Statistics, 21(2):224–270.\nLindeberg, T. (1998a). Edge detection and ridge detection with automatic scale selection.\nInternational Journal of Computer Vision, 30(2):116–154.\nLindeberg, T. (1998b). Feature detection with automatic scale selection. International\nJournal of Computer Vision, 30(2):79–116.\nLindeberg, T. and Garding, J.\n(1997).\nShape-adapted smoothing in estimation of 3-D\nshape cues from afﬁne deformations of local 2-D brightness structure. Image and Vision\nComputing, 15(6):415–434.\nLippman, A. (1980). Movie maps: An application of the optical videodisc to computer\ngraphics. Computer Graphics (SIGGRAPH ’80), 14(3):32–43.\nLischinski, D., Farbman, Z., Uyttendaele, M., and Szeliski, R. (2006a). Interactive local\nadjustment of tonal values. ACM Transactions on Graphics (Proc. SIGGRAPH 2006),\n25(3):646–653.\nLischinski, D., Farbman, Z., Uyttendaele, M., and Szeliski, R. (2006b). Interactive local\nadjustment of tonal values. ACM Transactions on Graphics, 25(3):646–653.\nLitvinov, A. and Schechner, Y. Y. (2005). Radiometric framework for image mosaicking.\nJournal of the Optical Society of America A, 22(5):839–848.\nLitwinowicz, P. (1997). Processing images and video for an impressionist effect. In ACM\nSIGGRAPH 1997 Conference Proceedings, pp. 407–414.\nLitwinowicz, P. and Williams, L.\n(1994).\nAnimating images with drawings.\nIn ACM\nSIGGRAPH 1994 Conference Proceedings, pp. 409–412.\nLiu, C. (2009). Beyond Pixels: Exploring New Representations and Applications for Mo-\ntion Analysis. Ph.D. thesis, Massachusetts Institute of Technology.\nLiu, C., Yuen, J., and Torralba, A. (2009). Nonparametric scene parsing: Label transfer via\ndense scene alignment. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2009), Miami Beach, FL.\nLiu, C., Freeman, W. T., Adelson, E., and Weiss, Y.\n(2008).\nHuman-assisted motion\nannotation. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nLiu, C., Szeliski, R., Kang, S. B., Zitnick, C. L., and Freeman, W. T. (2008). Automatic\nestimation and removal of noise from a single image. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 30(2):299–314.",
  "884": "862\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLiu, F., Gleicher, M., Jin, H., and Agarwala, A. (2009). Content-preserving warps for 3d\nvideo stabilization. ACM Transactions on Graphics, 28(3).\nLiu, X., Chen, T., and Kumar, B. V. (2003). Face authentication for multiple subjects using\neigenﬂow. Pattern Recognition, 36(2):313–328.\nLiu, Y., Collins, R. T., and Tsin, Y. (2004). A computational model for periodic pattern per-\nception based on frieze and wallpaper groups. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 26(3):354–371.\nLiu, Y., Lin, W.-C., and Hays, J. (2004). Near-regular texture analysis and manipulation.\nACM Transactions on Graphics, 23(3):368–376.\nLivingstone, M. (2008). Vision and Art: The Biology of Seeing. Abrams, New York.\nLobay, A. and Forsyth, D. A. (2006). Shape from texture without boundaries. International\nJournal of Computer Vision, 67(1):71–91.\nLombaert, H., Sun, Y., Grady, L., and Xu, C. (2005). A multilevel banded graph cuts\nmethod for fast image segmentation. In Tenth International Conference on Computer\nVision (ICCV 2005), pp. 259–265, Beijing, China.\nLongere, P., Delahunt, P. B., Zhang, X., and Brainard, D. H. (2002). Perceptual assessment\nof demosaicing algorithm performance. Proceedings of the IEEE, 90(1):123–132.\nLonguet-Higgins, H. C. (1981). A computer algorithm for reconstructing a scene from two\nprojections. Nature, 293:133–135.\nLoop, C. and Zhang, Z. (1999). Computing rectifying homographies for stereo vision.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’99), pp. 125–131, Fort Collins.\nLorensen, W. E. and Cline, H. E. (1987). Marching cubes: A high resolution 3D surface\nconstruction algorithm. Computer Graphics (SIGGRAPH ’87), 21(4):163–169.\nLorusso, A., Eggert, D., and Fisher, R. B. (1995). A comparison of four algorithms for\nestimating 3-D rigid transformations. In British Machine Vision Conference (BMVC95),\npp. 237–246, Birmingham, England.\nLourakis, M. I. A. and Argyros, A. A. (2009). SBA: A software package for generic sparse\nbundle adjustment. ACM Transactions on Mathematical Software, 36(1).\nLowe, D. G. (1988). Organization of smooth image curves at multiple scales. In Second\nInternational Conference on Computer Vision (ICCV’88), pp. 558–567, Tampa.\nLowe, D. G. (1989). Organization of smooth image curves at multiple scales. International\nJournal of Computer Vision, 3(2):119–130.",
  "885": "References\n863\nLowe, D. G. (1999). Object recognition from local scale-invariant features. In Seventh\nInternational Conference on Computer Vision (ICCV’99), pp. 1150–1157, Kerkyra,\nGreece.\nLowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. Interna-\ntional Journal of Computer Vision, 60(2):91–110.\nLucas, B. D. and Kanade, T. (1981). An iterative image registration technique with an\napplication in stereo vision. In Seventh International Joint Conference on Artiﬁcial\nIntelligence (IJCAI-81), pp. 674–679, Vancouver.\nLuong, Q.-T. and Faugeras, O. D. (1996). The fundamental matrix: Theory, algorithms,\nand stability analysis. International Journal of Computer Vision, 17(1):43–75.\nLuong, Q.-T. and Vi´eville, T. (1996). Canonical representations for the geometries of\nmultiple projective views. Computer Vision and Image Understanding, 64(2):193–229.\nLyu, S. and Simoncelli, E. (2008). Nonlinear image representation using divisive nor-\nmalization. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nLyu, S. and Simoncelli, E. (2009). Modeling multiscale subbands of photographic images\nwith ﬁelds of Gaussian scale mixtures. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 31(4):693–706.\nMa, W.-C., Jones, A., Chiang, J.-Y., Hawkins, T., Frederiksen, S., Peers, P., Vukovic,\nM., Ouhyoung, M., and Debevec, P.\n(2008).\nFacial performance synthesis using\ndeformation-driven polynomial displacement maps. ACM Transactions on Graphics,\n27(5).\nMa, Y., Derksen, H., Hong, W., and Wright, J. (2007). Segmentation of multivariate mixed\ndata via lossy data coding and compression. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 29(9):1546–1562.\nMacDonald, L. (ed.). (2006). Digital Heritage: Applying Digital Imaging to Cultural\nHeritage, Butterworth-Heinemann.\nMadsen, K., Nielsen, H. B., and Tingleff, O. (2004). Methods for non-linear least squares\nproblems. Informatics and Mathematical Modelling, Technical University of Denmark\n(DTU).\nMaes, F., Collignon, A., Vandermeulen, D., Marchal, G., and Suetens, P. (1997). Multi-\nmodality image registration by maximization of mutual information. IEEE Transactions\non Medical Imaging, 16(2):187–198.\nMagnor, M. (2005). Video-Based Rendering. A. K. Peters, Wellesley, MA.",
  "886": "864\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMagnor, M. and Girod, B.\n(2000).\nData compression for light-ﬁeld rendering.\nIEEE\nTransactions on Circuits and Systems for Video Technology, 10(3):338–343.\nMagnor, M., Ramanathan, P., and Girod, B. (2003). Multi-view coding for image-based\nrendering using 3-D scene geometry. IEEE Transactions on Circuits and Systems for\nVideo Technology, 13(11):1092–1106.\nMahajan, D., Huang, F.-C., Matusik, W., Ramamoorthi, R., and Belhumeur, P. (2009).\nMoving gradients: A path-based method for plausible image interpolation. ACM Trans-\nactions on Graphics, 28(3).\nMaimone, M., Cheng, Y., and Matthies, L. (2007). Two years of visual odometry on the\nMars exploration rovers. Journal of Field Robotics, 24(3).\nMaire, M., Arbelaez, P., Fowlkes, C., and Malik, J.\n(2008).\nUsing contours to detect\nand localize junctions in natural images. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nMaitin-Shepard, J., Cusumano-Towner, M., Lei, J., and Abbeel, P. (2010). Cloth grasp\npoint detection based on multiple-view geometric cues with application to robotic towel\nfolding. In IEEE International Conference on Robotics and Automation, Anchorage,\nAK.\nMaitre, M., Shinagawa, Y., and Do, M. N. (2008). Symmetric multi-view stereo reconstruc-\ntion from planar camera arrays. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nMaji, S., Berg, A., and Malik, J. (2008). Classiﬁcation using intersection kernel support\nvector machines is efﬁcient. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2008), Anchorage, AK.\nMalik, J. and Rosenholtz, R. (1997). Computing local surface orientation and shape from\ntexture for curved surfaces. International Journal of Computer Vision, 23(2):149–168.\nMalik, J., Belongie, S., Leung, T., and Shi, J. (2001). Contour and texture analysis for\nimage segmentation. International Journal of Computer Vision, 43(1):7–27.\nMalisiewicz, T. and Efros, A. A. (2008). Recognition by association via learning per-\nexemplar distances. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.\nMalladi, R., Sethian, J. A., and Vemuri, B. C. (1995). Shape modeling with front propaga-\ntion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(2):158–176.\nMallat, S. G. (1989). A theory for multiresolution signal decomposition: the wavelet rep-\nresentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-\n11(7):674–693.",
  "887": "References\n865\nMalvar, H. S. (1990). Lapped transforms for efﬁcient transform/subband coding. IEEE\nTransactions on Acoustics, Speech, and Signal Processing, 38(6):969–978.\nMalvar, H. S. (1998). Biorthogonal and nonuniform lapped transforms for transform cod-\ning with reduced blocking and ringing artifacts. IEEE Transactions on Signal Process-\ning, 46(4):1043–1053.\nMalvar, H. S. (2000). Fast progressive image coding without wavelets. In IEEE Data\nCompressions Conference, pp. 243–252, Snowbird, UT.\nMalvar, H. S., He, L.-W., and Cutler, R.\n(2004).\nHigh-quality linear interpolation for\ndemosaicing of Bayer-patterned color images. In IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP’04), pp. 485–488, Montreal.\nMancini, T. A. and Wolff, L. B. (1992). 3D shape and light source location from depth and\nreﬂectance. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’92), pp. 707–709, Champaign, Illinois.\nManjunathi, B. S. and Ma, W. Y. (1996). Texture features for browsing and retrieval of im-\nage data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(8):837–\n842.\nMann, S. and Picard, R. W. (1994). Virtual bellows: Constructing high-quality images\nfrom video. In First IEEE International Conference on Image Processing (ICIP-94),\npp. 363–367, Austin.\nMann, S. and Picard, R. W. (1995). On being ‘undigital’ with digital cameras: Extend-\ning dynamic range by combining differently exposed pictures. In IS&T’s 48th Annual\nConference, pp. 422–428, Washington, D. C.\nManning, C. D., Raghavan, P., and Sch¨utze, H. (2008). Introduction to Information Re-\ntrieval. Cambridge University Press.\nMarquardt, D. W. (1963). An algorithm for least-squares estimation of nonlinear parame-\nters. Journal of the Society for Industrial and Applied Mathematics, 11(2):431–441.\nMarr, D. (1982). Vision: A Computational Investigation into the Human Representation\nand Processing of Visual Information. W. H. Freeman, San Francisco.\nMarr, D. and Hildreth, E. (1980). Theory of edge detection. Proceedings of the Royal\nSociety of London, B 207:187–217.\nMarr, D. and Nishihara, H. K. (1978). Representation and recognition of the spatial orga-\nnization of three-dimensional shapes. Proc. Roy. Soc. London, B, 200:269–294.\nMarr, D. and Poggio, T. (1976). Cooperative computation of stereo disparity. Science,\n194:283–287.",
  "888": "866\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMarr, D. C. and Poggio, T. (1979). A computational theory of human stereo vision. Pro-\nceedings of the Royal Society of London, B 204:301–328.\nMarroquin, J., Mitter, S., and Poggio, T. (1985). Probabilistic solution of ill-posed prob-\nlems in computational vision. In Image Understanding Workshop, pp. 293–309, Miami\nBeach.\nMarroquin, J., Mitter, S., and Poggio, T.\n(1987).\nProbabilistic solution of ill-posed\nproblems in computational vision.\nJournal of the American Statistical Association,\n82(397):76–89.\nMarroquin, J. L. (1983). Design of Cooperative Networks. Working Paper 253, Artiﬁcial\nIntelligence Laboratory, Massachusetts Institute of Technology.\nMartin, D., Fowlkes, C., and Malik, J. (2004). Learning to detect natural image boundaries\nusing local brightness, color, and texture cues. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 26(5):530–549.\nMartin, D., Fowlkes, C., Tal, D., and Malik, J. (2001). A database of human segmented\nnatural images and its application to evaluating segmentation algorithms and measuring\necological statistics. In Eighth International Conference on Computer Vision (ICCV\n2001), pp. 416–423, Vancouver, Canada.\nMartin, W. N. and Aggarwal, J. K. (1983). Volumetric description of objects from mul-\ntiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-\n5(2):150–158.\nMartinec, D. and Pajdla, T. (2007). Robust rotation and translation estimation in multiview\nreconstruction. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2007), Minneapolis, MN.\nMassey, M. and Bender, W. (1996). Salient stills: Process and practice. IBM Systems\nJournal, 35(3&4):557–573.\nMatas, J., Chum, O., Urban, M., and Pajdla, T. (2004). Robust wide baseline stereo from\nmaximally stable extremal regions. Image and Vision Computing, 22(10):761–767.\nMatei, B. C. and Meer, P. (2006). Estimation of nonlinear errors-in-variables models for\ncomputer vision applications.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 28(10):1537–1552.\nMatsushita, Y. and Lin, S.\n(2007).\nRadiometric calibration from noise distributions.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.\nMatsushita, Y., Ofek, E., Ge, W., Tang, X., and Shum, H.-Y. (2006). Full-frame video sta-\nbilization with motion inpainting. IEEE Transactions on Pattern Analysis and Machine",
  "889": "References\n867\nIntelligence, 28(7):1150–1163.\nMatthews, I. and Baker, S. (2004). Active appearance models revisited. International\nJournal of Computer Vision, 60(2):135–164.\nMatthews, I., Xiao, J., and Baker, S. (2007). 2D vs. 3D deformable face models: Represen-\ntational power, construction, and real-time ﬁtting. International Journal of Computer\nVision, 75(1):93–113.\nMatthies, L., Kanade, T., and Szeliski, R.\n(1989).\nKalman ﬁlter-based algorithms for\nestimating depth from image sequences.\nInternational Journal of Computer Vision,\n3(3):209–236.\nMatusik, W., Buehler, C., and McMillan, L. (2001). Polyhedral visual hulls for real-time\nrendering. In 12th Eurographics Workshop on Rendering Techniques, pp. 115–126,\nLondon.\nMatusik, W., Buehler, C., Raskar, R., Gortler, S. J., and McMillan, L. (2000). Image-based\nvisual hulls. In ACM SIGGRAPH 2000 Conference Proceedings, pp. 369–374.\nMayhew, J. E. W. and Frisby, J. P. (1980). The computation of binocular edges. Perception,\n9:69–87.\nMayhew, J. E. W. and Frisby, J. P. (1981). Psychophysical and computational studies\ntowards a theory of human stereopsis. Artiﬁcial Intelligence, 17(1-3):349–408.\nMcCamy, C. S., Marcus, H., and Davidson, J. G. (1976). A color-rendition chart. Journal\nof Applied Photogrammetric Engineering, 2(3):95–99.\nMcCane, B., Novins, K., Crannitch, D., and Galvin, B. (2001). On benchmarking optical\nﬂow. Computer Vision and Image Understanding, 84(1):126–143.\nMcGuire, M., Matusik, W., Pﬁster, H., Hughes, J. F., and Durand, F. (2005). Defocus video\nmatting. ACM Transactions on Graphics (Proc. SIGGRAPH 2005), 24(3):567–576.\nMcInerney, T. and Terzopoulos, D. (1993). A ﬁnite element model for 3D shape reconstruc-\ntion and nonrigid motion tracking. In Fourth International Conference on Computer\nVision (ICCV’93), pp. 518–523, Berlin, Germany.\nMcInerney, T. and Terzopoulos, D. (1996). Deformable models in medical image analysis:\nA survey. Medical Image Analysis, 1(2):91–108.\nMcInerney, T. and Terzopoulos, D.\n(1999).\nTopology adaptive deformable surfaces\nfor medical image volume segmentation.\nIEEE Transactions on Medical Imaging,\n18(10):840–850.\nMcInerney, T. and Terzopoulos, D. (2000). T-snakes: Topology adaptive snakes. Medical\nImage Analysis, 4:73–91.",
  "890": "868\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMcLauchlan, P. F.\n(2000).\nA batch/recursive algorithm for 3D scene reconstruction.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2000), pp. 738–743, Hilton Head Island.\nMcLauchlan, P. F. and Jaenicke, A. (2002). Image mosaicing using sequential bundle\nadjustment. Image and Vision Computing, 20(9-10):751–759.\nMcLean, G. F. and Kotturi, D. (1995). Vanishing point detection by line clustering. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 17(11):1090–1095.\nMcMillan, L. and Bishop, G. (1995). Plenoptic modeling: An image-based rendering\nsystem. In ACM SIGGRAPH 1995 Conference Proceedings, pp. 39–46.\nMcMillan, L. and Gortler, S. (1999). Image-based rendering: A new interface between\ncomputer vision and computer graphics. Computer Graphics, 33(4):61–64.\nMeehan, J. (1990). Panoramic Photography. Watson-Guptill.\nMeer, P. and Georgescu, B. (2001). Edge detection with embedded conﬁdence. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 23(12):1351–1365.\nMeil˘a, M. and Shi, J. (2000). Learning segmentation by random walks. In Advances in\nNeural Information Processing Systems.\nMeil˘a, M. and Shi, J. (2001). A random walks view of spectral segmentation. In Workshop\non Artiﬁcial Intelligence and Statistics, pp. 177–182, Key West, FL.\nMeltzer, J. and Soatto, S. (2008). Edge descriptors for robust wide-baseline correspon-\ndence. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR 2008), Anchorage, AK.\nMeltzer, T., Yanover, C., and Weiss, Y. (2005). Globally optimal solutions for energy min-\nimization in stereo vision using reweighted belief propagation. In Tenth International\nConference on Computer Vision (ICCV 2005), pp. 428–435, Beijing, China.\nM´emin, E. and P´erez, P. (2002). Hierarchical estimation and segmentation of dense motion\nﬁelds. International Journal of Computer Vision, 44(2):129–155.\nMenet, S., Saint-Marc, P., and Medioni, G. (1990a). Active contour models: overview,\nimplementation and applications. In IEEE International Conference on Systems, Man\nand Cybernetics, pp. 194–199, Los Angeles.\nMenet, S., Saint-Marc, P., and Medioni, G. (1990b). B-snakes: implementation and appli-\ncations to stereo. In Image Understanding Workshop, pp. 720–726, Pittsburgh.\nMerrell, P., Akbarzadeh, A., Wang, L., Mordohai, P., Frahm, J.-M., Yang, R., Nister, D.,\nand Pollefeys, M. (2007). Real-time visibility-based fusion of depth maps. In Eleventh\nInternational Conference on Computer Vision (ICCV 2007), Rio de Janeiro, Brazil.",
  "891": "References\n869\nMertens, T., Kautz, J., and Reeth, F. V. (2007). Exposure fusion. In Proceedings of Paciﬁc\nGraphics 2007, pp. 382–390.\nMetaxas, D. and Terzopoulos, D. (2002). Dynamic deformation of solid primitives with\nconstraints. ACM Transactions on Graphics (Proc. SIGGRAPH 2002), 21(3):309–312.\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953).\nEquations of state calculations by fast computing machines.\nJournal of Chemical\nPhysics, 21:1087–1091.\nMeyer, C. D. (2000). Matrix Analysis and Applied Linear Algebra. Society for Industrial\nand Applied Mathematics, Philadephia.\nMeyer, Y. (1993). Wavelets: Algorithms and Applications. Society for Industrial and\nApplied Mathematics, Philadephia.\nMikolajczyk, K. and Schmid, C. (2004). Scale & afﬁne invariant interest point detectors.\nInternational Journal of Computer Vision, 60(1):63–86.\nMikolajczyk, K. and Schmid, C. (2005). A performance evaluation of local descriptors.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 27(10):1615–1630.\nMikolajczyk, K., Schmid, C., and Zisserman, A. (2004). Human detection based on a prob-\nabilistic assembly of robust part detectors. In Eighth European Conference on Computer\nVision (ECCV 2004), pp. 69–82, Prague.\nMikolajczyk, K., Tuytelaars, T., Schmid, C., Zisserman, A., Matas, J., Schaffalitzky, F.,\nKadir, T., and Van Gool, L. J. (2005). A comparison of afﬁne region detectors. Inter-\nnational Journal of Computer Vision, 65(1-2):43–72.\nMilgram, D. L. (1975). Computer methods for creating photomosaics. IEEE Transactions\non Computers, C-24(11):1113–1119.\nMilgram, D. L. (1977). Adaptive techniques for photomosaicking. IEEE Transactions on\nComputers, C-26(11):1175–1180.\nMiller, I., Campbell, M., Huttenlocher, D., Kline, F.-R., Nathan, A. et al. (2008). Team\nCornell’s Skynet: Robust perception and planning in an urban environment. Journal of\nField Robotics, 25(8):493–527.\nMitiche, A. and Bouthemy, P. (1996). Computation and analysis of image motion: A\nsynopsis of current problems and methods. International Journal of Computer Vision,\n19(1):29–55.\nMitsunaga, T. and Nayar, S. K. (1999). Radiometric self calibration. In IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR’99), pp. 374–\n380, Fort Collins.",
  "892": "870\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMittal, A. and Davis, L. S. (2003). M2 tracker: A multi-view approach to segmenting\nand tracking people in a cluttered scene. International Journal of Computer Vision,\n51(3):189–203.\nMiˇcuˇs´ık, B. and Koˇseck´a, J. (2009). Piecewise planar city 3D modeling from street view\npanoramic sequences. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2009), Miami Beach, FL.\nMiˇcuˇs`ık, B., Wildenauer, H., and Koˇseck´a, J. (2008). Detection and matching of rectilinear\nstructures.\nIn IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nMoeslund, T. B. and Granum, E. (2001). A survey of computer vision-based human motion\ncapture. Computer Vision and Image Understanding, 81(3):231–268.\nMoeslund, T. B., Hilton, A., and Kr¨uger, V.\n(2006).\nA survey of advances in vision-\nbased human motion capture and analysis. Computer Vision and Image Understanding,\n104(2-3):90–126.\nMoezzi, S., Katkere, A., Kuramura, D., and Jain, R. (1996). Reality modeling and visu-\nalization from multiple video sequences. IEEE Computer Graphics and Applications,\n16(6):58–63.\nMoghaddam, B. and Pentland, A. (1997). Probabilistic visual learning for object represen-\ntation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):696–\n710.\nMoghaddam, B., Jebara, T., and Pentland, A. (2000). Bayesian face recognition. Pattern\nRecognition, 33(11):1771–1782.\nMohan, A., Papageorgiou, C., and Poggio, T. (2001). Example-based object detection in\nimages by components. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 23(4):349–361.\nM¨oller, K. D. (1988). Optics. University Science Books, Mill Valley, CA.\nMontemerlo, M., Becker, J., Bhat, S., Dahlkamp, H., Dolgov, D. et al. (2008). Junior: The\nStanford entry in the Urban Challenge. Journal of Field Robotics, 25(9):569–597.\nMoon, P. and Spencer, D. E. (1981). The Photic Field. MIT Press, Cambridge, Mas-\nsachusetts.\nMoons, T., Van Gool, L., and Vergauwen, M. (2010). 3D reconstruction from multiple\nimages. Foundations and Trends in Computer Graphics and Computer Vision, 4(4).\nMoosmann, F., Nowak, E., and Jurie, F. (2008). Randomized clustering forests for im-\nage classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n30(9):1632–1646.",
  "893": "References\n871\nMoravec, H.\n(1977).\nTowards automatic visual obstacle avoidance.\nIn Fifth Interna-\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI’77), p. 584, Cambridge, Mas-\nsachusetts.\nMoravec, H. (1983). The Stanford cart and the CMU rover. Proceedings of the IEEE,\n71(7):872–884.\nMoreno-Noguer, F., Lepetit, V., and Fua, P. (2007). Accurate non-iterative O(n) solution\nto the PnP problem. In Eleventh International Conference on Computer Vision (ICCV\n2007), Rio de Janeiro, Brazil.\nMori, G. (2005). Guiding model search using segmentation. In Tenth International Con-\nference on Computer Vision (ICCV 2005), pp. 1417–1423, Beijing, China.\nMori, G., Ren, X., Efros, A., and Malik, J. (2004). Recovering human body conﬁgurations:\nCombining segmentation and recognition. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’2004), pp. 326–333, Washington, DC.\nMori, M. (1970). The uncanny valley. Energy, 7(4):33–35. http://www.androidscience.\ncom/theuncannyvalley/proceedings2005/uncannyvalley.html.\nMorimoto, C. and Chellappa, R. (1997). Fast 3D stabilization and mosaic construction.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’97), pp. 660–665, San Juan, Puerto Rico.\nMorita, T. and Kanade, T. (1997). A sequential factorization method for recovering shape\nand motion from image streams. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 19(8):858–867.\nMorris, D. D. and Kanade, T. (1998). A uniﬁed factorization algorithm for points, line\nsegments and planes with uncertainty models. In Sixth International Conference on\nComputer Vision (ICCV’98), pp. 696–702, Bombay.\nMorrone, M. and Burr, D. (1988). Feature detection in human vision: A phase dependent\nenergy model. Proceedings of the Royal Society of London B, 235:221–245.\nMortensen, E. N. (1999). Vision-assisted image editing. Computer Graphics, 33(4):55–57.\nMortensen, E. N. and Barrett, W. A. (1995). Intelligent scissors for image composition. In\nACM SIGGRAPH 1995 Conference Proceedings, pp. 191–198.\nMortensen, E. N. and Barrett, W. A.\n(1998).\nInteractive segmentation with intelligent\nscissors. Graphical Models and Image Processing, 60(5):349–384.\nMortensen, E. N. and Barrett, W. A. (1999). Toboggan-based intelligent scissors with a\nfour parameter edge model. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’99), pp. 452–458, Fort Collins.",
  "894": "872\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMueller, P., Zeng, G., Wonka, P., and Van Gool, L. (2007). Image-based procedural mod-\neling of facades. ACM Transactions on Graphics, 26(3).\nM¨uhlich, M. and Mester, R. (1998). The role of total least squares in motion analysis.\nIn Fifth European Conference on Computer Vision (ECCV’98), pp. 305–321, Freiburg,\nGermany.\nMuja, M. and Lowe, D. G. (2009). Fast approximate nearest neighbors with automatic\nalgorithm conﬁguration. In International Conference on Computer Vision Theory and\nApplications (VISAPP), Lisbon, Portugal.\nMumford, D. and Shah, J. (1989). Optimal approximations by piecewise smooth functions\nand variational problems. Comm. Pure Appl. Math., XLII(5):577–685.\nMunder, S. and Gavrila, D. M. (2006). An experimental study on pedestrian classiﬁcation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 28(11):1863–1868.\nMundy, J. L. (2006). Object recognition in the geometric era: A retrospective. In Ponce,\nJ., Hebert, M., Schmid, C., and Zisserman, A. (eds), Toward Category-Level Object\nRecognition, pp. 3–28, Springer, New York.\nMundy, J. L. and Zisserman, A. (eds). (1992). Geometric Invariance in Computer Vision.\nMIT Press, Cambridge, Massachusetts.\nMurase, H. and Nayar, S. K. (1995). Visual learning and recognition of 3-D objects from\nappearance. International Journal of Computer Vision, 14(1):5–24.\nMurphy, E. P. (2005). A Testing Procedure to Characterize Color and Spatial Quality of\nDigital Cameras Used to Image Cultural Heritage. Master’s thesis, Rochester Institute\nof Technology.\nMurphy, K., Torralba, A., and Freeman, W. T. (2003). Using the forest to see the trees: A\ngraphical model relating features, objects, and scenes. In Advances in Neural Informa-\ntion Processing Systems.\nMurphy-Chutorian, E. and Trivedi, M. M.\n(2009).\nHead pose estimation in computer\nvision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n31(4):607–626.\nMurray, R. M., Li, Z. X., and Sastry, S. S. (1994). A Mathematical Introduction to Robotic\nManipulation. CRC Press.\nMutch, J. and Lowe, D. G.\n(2008).\nObject class recognition and localization using\nsparse features with limited receptive ﬁelds. International Journal of Computer Vision,\n80(1):45–57.",
  "895": "References\n873\nNagel, H. H. (1986). Image sequences—ten (octal) years—from phenomenology towards\na theoretical foundation. In Eighth International Conference on Pattern Recognition\n(ICPR’86), pp. 1174–1185, Paris.\nNagel, H.-H. and Enkelmann, W. (1986). An investigation of smoothness constraints for\nthe estimation of displacement vector ﬁelds from image sequences. IEEE Transactions\non Pattern Analysis and Machine Intelligence, PAMI-8(5):565–593.\nNakamura, Y., Matsuura, T., Satoh, K., and Ohta, Y. (1996). Occlusion detectable stereo—\nocclusion patterns in camera matrix. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR’96), pp. 371–378, San Francisco.\nNakao, T., Kashitani, A., and Kaneyoshi, A. (1998). Scanning a document with a small\ncamera attached to a mouse. In IEEE Workshop on Applications of Computer Vision\n(WACV’98), pp. 63–68, Princeton.\nNalwa, V. S. (1987). Edge-detector resolution improvement by image interpolation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, PAMI-9(3):446–451.\nNalwa, V. S. (1993). A Guided Tour of Computer Vision. Addison-Wesley, Reading, MA.\nNalwa, V. S. and Binford, T. O. (1986). On detecting edges. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, PAMI-8(6):699–714.\nNarasimhan, S. G. and Nayar, S. K. (2005). Enhancing resolution along multiple imaging\ndimensions using assorted pixels. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 27(4):518–530.\nNarayanan, P., Rander, P., and Kanade, T. (1998). Constructing virtual worlds using dense\nstereo. In Sixth International Conference on Computer Vision (ICCV’98), pp. 3–10,\nBombay.\nNayar, S., Watanabe, M., and Noguchi, M. (1995). Real-time focus range sensor. In Fifth\nInternational Conference on Computer Vision (ICCV’95), pp. 995–1001, Cambridge,\nMassachusetts.\nNayar, S. K. (2006). Computational cameras: Redeﬁning the image. Computer, 39(8):30–\n38.\nNayar, S. K. and Branzoi, V. (2003). Adaptive dynamic range imaging: Optical control of\npixel exposures over space and time. In Ninth International Conference on Computer\nVision (ICCV 2003), pp. 1168–1175, Nice, France.\nNayar, S. K. and Mitsunaga, T. (2000). High dynamic range imaging: Spatially varying\npixel exposures. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2000), pp. 472–479, Hilton Head Island.",
  "896": "874\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nNayar, S. K. and Nakagawa, Y. (1994). Shape from focus. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 16(8):824–831.\nNayar, S. K., Ikeuchi, K., and Kanade, T. (1991). Shape from interreﬂections. Interna-\ntional Journal of Computer Vision, 6(3):173–195.\nNayar, S. K., Watanabe, M., and Noguchi, M. (1996). Real-time focus range sensor. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 18(12):1186–1198.\nNegahdaripour, S. (1998). Revised deﬁnition of optical ﬂow: Integration of radiometric\nand geometric cues for dynamic scene analysis. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 20(9):961–979.\nNehab, D., Rusinkiewicz, S., Davis, J., and Ramamoorthi, R. (2005). Efﬁciently combining\npositions and normals for precise 3d geometry. ACM Transactions on Graphics (Proc.\nSIGGRAPH 2005), 24(3):536–543.\nNene, S. and Nayar, S. K. (1997). A simple algorithm for nearest neighbor search in\nhigh dimensions. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n19(9):989–1003.\nNene, S. A., Nayar, S. K., and Murase, H. (1996). Columbia Object Image Library (COIL-\n100). Technical Report CUCS-006-96, Department of Computer Science, Columbia\nUniversity.\nNetravali, A. and Robbins, J. (1979). Motion-compensated television coding: Part 1. Bell\nSystem Tech., 58(3):631–670.\nNevatia, R. (1977). A color edge detector and its use in scene segmentation. IEEE Trans-\nactions on Systems, Man, and Cybernetics, SMC-7(11):820–826.\nNevatia, R. and Binford, T. (1977). Description and recognition of curved objects. Artiﬁ-\ncial Intelligence, 8:77–98.\nNg, A. Y., Jordan, M. I., and Weiss, Y. (2001). On spectral clustering: Analysis and an\nalgorithm. In Advances in Neural Information Processing Systems, pp. 849–854.\nNg, R. (2005). Fourier slice photography. ACM Transactions on Graphics (Proc. SIG-\nGRAPH 2005), 24(3):735–744.\nNg, R., Levoy, M., Br´eedif, M., Duval, G., Horowitz, M., and Hanrahan, P. (2005). Light\nField Photography with a Hand-held Plenoptic Camera. Technical Report CSTR 2005-\n02, Stanford University.\nNielsen, M., Florack, L. M. J., and Deriche, R. (1997). Regularization, scale-space, and\nedge-detection ﬁlters. Journal of Mathematical Imaging and Vision, 7(4):291–307.",
  "897": "References\n875\nNielson, G. M. (1993). Scattered data modeling. IEEE Computer Graphics and Applica-\ntions, 13(1):60–70.\nNir, T., Bruckstein, A. M., and Kimmel, R. (2008). Over-parameterized variational optical\nﬂow. International Journal of Computer Vision, 76(2):205–216.\nNishihara, H. K. (1984). Practical real-time imaging stereo matcher. OptEng, 23(5):536–\n545.\nNist´er, D. (2003). Preemptive RANSAC for live structure and motion estimation. In Ninth\nInternational Conference on Computer Vision (ICCV 2003), pp. 199–206, Nice, France.\nNist´er, D. (2004). An efﬁcient solution to the ﬁve-point relative pose problem. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 26(6):756–777.\nNist´er, D. and Stew´enius, H.\n(2006).\nScalable recognition with a vocabulary tree.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2006), pp. 2161–2168, New York City, NY.\nNist´er, D. and Stew´enius, H.\n(2008).\nLinear time maximally stable extremal regions.\nIn Tenth European Conference on Computer Vision (ECCV 2008), pp. 183–196, Mar-\nseilles.\nNist´er, D., Naroditsky, O., and Bergen, J. (2006). Visual odometry for ground vehicle\napplications. Journal of Field Robotics, 23(1):3–20.\nNoborio, H., Fukada, S., and Arimoto, S. (1988). Construction of the octree approximat-\ning three-dimensional objects by using multiple views. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, PAMI-10(6):769–782.\nNocedal, J. and Wright, S. J. (2006). Numerical Optimization. Springer, New York, second\nedition.\nNomura, Y., Zhang, L., and Nayar, S. K. (2007). Scene collages and ﬂexible camera arrays.\nIn Eurographics Symposium on Rendering.\nNordstr¨om, N. (1990). Biased anisotropic diffusion: A uniﬁed regularization and diffusion\napproach to edge detection. Image and Vision Computing, 8(4):318–327.\nNowak, E., Jurie, F., and Triggs, B. (2006). Sampling strategies for bag-of-features im-\nage classiﬁcation. In Ninth European Conference on Computer Vision (ECCV 2006),\npp. 490–503.\nObdrˇz´alek, S. and Matas, J. (2006). Object recognition using local afﬁne frames on maxi-\nmally stable extremal regions. In Ponce, J., Hebert, M., Schmid, C., and Zisserman, A.\n(eds), Toward Category-Level Object Recognition, pp. 83–104, Springer, New York.",
  "898": "876\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nOh, B. M., Chen, M., Dorsey, J., and Durand, F. (2001). Image-based modeling and photo\nediting. In ACM SIGGRAPH 2001 Conference Proceedings, pp. 433–442.\nOhlander, R., Price, K., and Reddy, D. R. (1978). Picture segmentation using a recursive\nregion splitting method. Computer Graphics and Image Processing, 8(3):313–333.\nOhta, Y. and Kanade, T.\n(1985).\nStereo by intra- and inter-scanline search using dy-\nnamic programming. IEEE Transactions on Pattern Analysis and Machine Intelligence,\nPAMI-7(2):139–154.\nOhtake, Y., Belyaev, A., Alexa, M., Turk, G., and Seidel, H.-P. (2003). Multi-level par-\ntition of unity implicits. ACM Transactions on Graphics (Proc. SIGGRAPH 2003),\n22(3):463–470.\nOkutomi, M. and Kanade, T. (1992). A locally adaptive window for signal matching.\nInternational Journal of Computer Vision, 7(2):143–162.\nOkutomi, M. and Kanade, T. (1993). A multiple baseline stereo. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 15(4):353–363.\nOkutomi, M. and Kanade, T. (1994). A stereo matching algorithm with an adaptive win-\ndow: Theory and experiment. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 16(9):920–932.\nOliensis, J. (2005). The least-squares error for structure from inﬁnitesimal motion. Inter-\nnational Journal of Computer Vision, 61(3):259–299.\nOliensis, J. and Hartley, R. (2007). Iterative extensions of the Sturm/Triggs algorithm:\nConvergence and nonconvergence. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 29(12):2217–2233.\nOliva, A. and Torralba, A. (2001). Modeling the shape of the scene: a holistic representa-\ntion of the spatial envelope. International Journal of Computer Vision, 42(3):145–175.\nOliva, A. and Torralba, A. (2007). The role of context in object recognition. Trends in\nCognitive Sciences, 11(12):520–527.\nOlsson, C., Eriksson, A. P., and Kahl, F. (2008). Improved spectral relaxation methods for\nbinary quadratic optimization problems. Computer Vision and Image Understanding,\n112(1):3–13.\nOmer, I. and Werman, M.\n(2004).\nColor lines: Image speciﬁc color representation.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2004), pp. 946–953, Washington, DC.\nOng, E.-J., Micilotta, A. S., Bowden, R., and Hilton, A.\n(2006).\nViewpoint invari-\nant exemplar-based 3D human tracking. Computer Vision and Image Understanding,\n104(2-3):178–189.",
  "899": "References\n877\nOpelt, A., Pinz, A., and Zisserman, A. (2006). A boundary-fragment-model for object\ndetection. In Ninth European Conference on Computer Vision (ECCV 2006), pp. 575–\n588.\nOpelt, A., Pinz, A., Fussenegger, M., and Auer, P. (2006). Generic object recognition with\nboosting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(3):614–\n641.\nOpenGL-ARB. (1997). OpenGL Reference Manual: The Ofﬁcial Reference Document to\nOpenGL, Version 1.1. Addison-Wesley, Reading, MA, 2nd edition.\nOppenheim, A. V. and Schafer, A. S. (1996). Signals and Systems. Prentice Hall, Engle-\nwood Cliffs, New Jersey, 2nd edition.\nOppenheim, A. V., Schafer, R. W., and Buck, J. R. (1999). Discrete-Time Signal Process-\ning. Prentice Hall, Englewood Cliffs, New Jersey, 2nd edition.\nOren, M. and Nayar, S. (1997). A theory of specular surface geometry. International\nJournal of Computer Vision, 24(2):105–124.\nO’Rourke, J. and Badler, N. I. (1980). Model-based image analysis of human motion using\nconstraint propagation. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 2(6):522–536.\nOsher, S. and Paragios, N. (eds). (2003). Geometric Level Set Methods in Imaging, Vision,\nand Graphics, Springer.\nOsuna, E., Freund, R., and Girosi, F. (1997). Training support vector machines: An ap-\nplication to face detection. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’97), pp. 130–136, San Juan, Puerto Rico.\nO’Toole, A. J., Jiang, F., Roark, D., and Abdi, H. (2006). Predicting human face recogni-\ntion. In Zhao, W.-Y. and Chellappa, R. (eds), Face Processing: Advanced Methods and\nModels, Elsevier.\nO’Toole, A. J., Phillips, P. J., Jiang, F., Ayyad, J., P´enard, N., and Abdi, H. (2009). Face\nrecognition algorithms surpass humans matching faces over changes in illumination.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 29(9):1642–1646.\nOtt, M., Lewis, J. P., and Cox, I. J. (1993). Teleconferencing eye contact using a virtual\ncamera. In INTERACT’93 and CHI’93 conference companion on Human factors in\ncomputing systems, pp. 109–110, Amsterdam.\nOtte, M. and Nagel, H.-H. (1994). Optical ﬂow estimation: advances and comparisons.\nIn Third European Conference on Computer Vision (ECCV’94), pp. 51–60, Stockholm,\nSweden.",
  "900": "878\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nOztireli, C., Guennebaud, G., and Gross, M. (2008). Feature preserving point set surfaces.\nComputer Graphics Forum, 28(2):493–501.\n¨Ozuysal, M., Calonder, M., Lepetit, V., and Fua, P. (2010). Fast keypoint recognition using\nrandom ferns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(3).\nPaglieroni, D. W. (1992). Distance transforms: Properties and machine vision applications.\nGraphical Models and Image Processing, 54(1):56–74.\nPal, C., Szeliski, R., Uyttendaele, M., and Jojic, N. (2004). Probability models for high\ndynamic range imaging. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’2004), pp. 173–180, Washington, DC.\nPalmer, S. E. (1999). Vision Science: Photons to Phenomenology. The MIT Press, Cam-\nbridge, Massachusetts.\nPankanti, S., Bolle, R. M., and Jain, A. K. (2000). Biometrics: The future of identiﬁcation.\nComputer, 21(2):46–49.\nPapageorgiou, C. and Poggio, T. (2000). A trainable system for object detection. Interna-\ntional Journal of Computer Vision, 38(1):15–33.\nPapandreou, G. and Maragos, P. (2008). Adaptive and constrained algorithms for inverse\ncompositional active appearance model ﬁtting. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nPapenberg, N., Bruhn, A., Brox, T., Didas, S., and Weickert, J. (2006). Highly accurate\noptic ﬂow computation with theoretically justiﬁed warping. International Journal of\nComputer Vision, 67(2):141–158.\nPapert,\nS.\n(1966).\nThe Summer Vision Project.\nTechnical Report AIM-\n100,\nArtiﬁcial\nIntelligence\nGroup,\nMassachusetts\nInstitute\nof\nTechnology.\nhttp://hdl.handle.net/1721.1/6125.\nParagios, N. and Deriche, R. (2000). Geodesic active contours and level sets for the de-\ntection and tracking of moving objects. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 22(3):266–280.\nParagios, N. and Sgallari, F. (2009). Special issue on scale space and variational methods\nin computer vision. International Journal of Computer Vision, 84(2).\nParagios, N., Faugeras, O. D., Chan, T., and Schn¨orr, C. (eds). (2005). Third International\nWorkshop on Variational, Geometric, and Level Set Methods in Computer Vision (VLSM\n2005), Springer.\nParis, S. and Durand, F. (2006). A fast approximation of the bilateral ﬁlter using a signal\nprocessing approach. In Ninth European Conference on Computer Vision (ECCV 2006),\npp. 568–580.",
  "901": "References\n879\nParis, S. and Durand, F. (2007). A topological approach to hierarchical segmentation using\nmean shift. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2007), Minneapolis, MN.\nParis, S., Kornprobst, P., Tumblin, J., and Durand, F. (2008). Bilateral ﬁltering: Theory\nand applications. Foundations and Trends in Computer Graphics and Computer Vision,\n4(1):1–73.\nPark, M., Brocklehurst, K., Collins, R. T., and Liu, Y. (2009). Deformed lattice detection in\nreal-world images using mean-shift belief propagation. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 31(10):1804–1816.\nPark, S. C., Park, M. K., and Kang, M. G. (2003). Super-resolution image reconstruction:\nA technical overview. IEEE Signal Processing Magazine, 20:21–36.\nParke, F. I. and Waters, K. (1996). Computer Facial Animation. A K Peters, Wellesley,\nMassachusetts.\nParker, J. A., Kenyon, R. V., and Troxel, D. E. (1983). Comparison of interpolating meth-\nods for image resampling. IEEE Transactions on Medical Imaging, MI-2(1):31–39.\nPattanaik, S. N., Ferwerda, J. A., Fairchild, M. D., and Greenberg, D. P. (1998). A multi-\nscale model of adaptation and spatial vision for realistic image display. In ACM SIG-\nGRAPH 1998 Conference Proceedings, pp. 287–298, Orlando.\nPauly, M., Keiser, R., Kobbelt, L. P., and Gross, M.\n(2003).\nShape modeling with\npoint-sampled geometry. ACM Transactions on Graphics (Proc. SIGGRAPH 2003),\n21(3):641–650.\nPavlidis, T. (1977). Structural Pattern Recognition. Springer-Verlag, Berlin; New York.\nPavlidis, T. and Liow, Y.-T. (1990). Integrating region growing and edge detection. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 12(3):225–233.\nPavlovi´c, V., Sharma, R., and Huang, T. S. (1997). Visual interpretation of hand gestures\nfor human-computer interaction: A review. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 19(7):677–695.\nPearl, J.\n(1988).\nProbabilistic reasoning in intelligent systems: networks of plausible\ninference. Morgan Kaufmann Publishers, Los Altos.\nPeleg, R., Ben-Ezra, M., and Pritch, Y. (2001). Omnistereo: Panoramic stereo imaging.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 23(3):279–290.\nPeleg, S. (1981). Elimination of seams from photomosaics. Computer Vision, Graphics,\nand Image Processing, 16(1):1206–1210.",
  "902": "880\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPeleg, S. and Herman, J. (1997). Panoramic mosaics by manifold projection. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’97),\npp. 338–343, San Juan, Puerto Rico.\nPeleg, S. and Rav-Acha, A. (2006). Lucas-Kanade without iterative warping. In Interna-\ntional Conference on Image Processing (ICIP-2006), pp. 1097–1100, Atlanta.\nPeleg, S., Rousso, B., Rav-Acha, A., and Zomet, A. (2000). Mosaicing on adaptive mani-\nfolds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(10):1144–\n1154.\nPenev, P. and Atick, J. (1996). Local feature analysis: A general statistical theory for object\nrepresentation. Network Computation and Neural Systems, 7:477–500.\nPentland, A. P. (1984). Local shading analysis. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, PAMI-6(2):170–179.\nPentland, A. P. (1986). Perceptual organization and the representation of natural form.\nArtiﬁcial Intelligence, 28(3):293–331.\nPentland, A. P. (1987). A new sense for depth of ﬁeld. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, PAMI-9(4):523–531.\nPentland, A. P. (1994). Interpolation using wavelet bases. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 16(4):410–414.\nP´erez, P., Blake, A., and Gangnet, M. (2001). JetStream: Probabilistic contour extraction\nwith particles. In Eighth International Conference on Computer Vision (ICCV 2001),\npp. 524–531, Vancouver, Canada.\nP´erez, P., Gangnet, M., and Blake, A. (2003). Poisson image editing. ACM Transactions\non Graphics (Proc. SIGGRAPH 2003), 22(3):313–318.\nPerona, P. (1995). Deformable kernels for early vision. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 17(5):488–499.\nPerona, P. and Malik, J. (1990a). Detecting and localizing edges composed of steps, peaks\nand roofs. In Third International Conference on Computer Vision (ICCV’90), pp. 52–\n57, Osaka, Japan.\nPerona, P. and Malik, J. (1990b). Scale space and edge detection using anisotropic diffu-\nsion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(7):629–639.\nPeters, J. and Reif, U. (2008). Subdivision Surfaces. Springer.\nPetschnigg, G., Agrawala, M., Hoppe, H., Szeliski, R., Cohen, M., and Toyama, K. (2004).\nDigital photography with ﬂash and no-ﬂash image pairs. ACM Transactions on Graph-\nics (Proc. SIGGRAPH 2004), 23(3):664–672.",
  "903": "References\n881\nPﬁster, H., Zwicker, M., van Baar, J., and Gross, M. (2000). Surfels: Surface elements as\nrendering primitives. In ACM SIGGRAPH 2000 Conference Proceedings, pp. 335–342.\nPﬂugfelder, R. (2008). Self-calibrating Cameras in Video Surveillance. Ph.D. thesis, Graz\nUniversity of Technology.\nPhilbin, J. and Zisserman, A. (2008). Object mining using a matching graph on very large\nimage collections.\nIn Indian Conference on Computer Vision, Graphics and Image\nProcessing, Bhubaneswar, India.\nPhilbin, J., Chum, O., Isard, M., Sivic, J., and Zisserman, A. (2007). Object retrieval with\nlarge vocabularies and fast spatial matching. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nPhilbin, J., Chum, O., Sivic, J., Isard, M., and Zisserman, A. (2008). Lost in quantization:\nImproving particular object retrieval in large scale image databases. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2008),\nAnchorage, AK.\nPhillips, P. J., Moon, H., Rizvi, S. A., and Rauss, P. J. (2000). The FERET evaluation\nmethodology for face recognition algorithms. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 22(10):1090–1104.\nPhillips, P. J., Scruggs, W. T., O’Toole, A. J., Flynn, P. J., Bowyer, K. W. et al. (2010).\nFRVT 2006 and ICE 2006 large-scale experimental results. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 32(5):831–846.\nPhong, B. T. (1975). Illumination for computer generated pictures. Communications of the\nACM, 18(6):311–317.\nPickup, L. C. (2007). Machine Learning in Multi-frame Image Super-resolution. Ph.D.\nthesis, University of Oxford.\nPickup, L. C. and Zisserman, A. (2009). Automatic retrieval of visual continuity errors\nin movies. In ACM International Conference on Image and Video Retrieval, Santorini,\nGreece.\nPickup, L. C., Capel, D. P., Roberts, S. J., and Zisserman, A. (2007). Overcoming reg-\nistration uncertainty in image super-resolution: Maximize or marginalize? EURASIP\nJournal on Advances in Signal Processing, 2010(Article ID 23565).\nPickup, L. C., Capel, D. P., Roberts, S. J., and Zisserman, A. (2009). Bayesian methods for\nimage super-resolution. The Computer Journal, 52.\nPighin, F., Szeliski, R., and Salesin, D. H. (2002). Modeling and animating realistic faces\nfrom images. International Journal of Computer Vision, 50(2):143–169.",
  "904": "882\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPighin, F., Hecker, J., Lischinski, D., Salesin, D. H., and Szeliski, R. (1998). Synthesizing\nrealistic facial expressions from photographs. In ACM SIGGRAPH 1998 Conference\nProceedings, pp. 75–84, Orlando.\nPilet, J., Lepetit, V., and Fua, P. (2008). Fast non-rigid surface detection, registration, and\nrealistic augmentation. International Journal of Computer Vision, 76(2).\nPinz, A. (2005). Object categorization. Foundations and Trends in Computer Graphics\nand Computer Vision, 1(4):255–353.\nPizer, S. M., Amburn, E. P., Austin, J. D., Cromartie, R., Geselowitz, A. et al. (1987).\nAdaptive histogram equalization and its variations. Computer Vision, Graphics, and\nImage Processing, 39(3):355–368.\nPlatel, B., Balmachnova, E., Florack, L., and ter Haar Romeny, B. (2006). Top-points as\ninterest points for image matching. In Ninth European Conference on Computer Vision\n(ECCV 2006), pp. 418–429.\nPlatt, J. C. (2000). Optimal ﬁltering for patterned displays. IEEE Signal Processing Let-\nters, 7(7):179–180.\nPock, T., Unger, M., Cremers, D., and Bischof, H. (2008). Fast and exact solution of total\nvariation models on the GPU. In CVPR 2008 Workshop on Visual Computer Vision on\nGPUs (CVGPU), Anchorage, AK.\nPoelman, C. J. and Kanade, T. (1997). A paraperspective factorization method for shape\nand motion recovery. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n19(3):206–218.\nPoggio, T. and Koch, C. (1985). Ill-posed problems in early vision: from computational\ntheory to analogue networks. Proceedings of the Royal Society of London, B 226:303–\n323.\nPoggio, T., Gamble, E., and Little, J. (1988). Parallel integration of vision modules. Sci-\nence, 242(4877):436–440.\nPoggio, T., Torre, V., and Koch, C. (1985). Computational vision and regularization theory.\nNature, 317(6035):314–319.\nPoggio, T., Little, J., Gamble, E., Gillet, W., Geiger, D. et al. (1988). The MIT vision\nmachine. In Image Understanding Workshop, pp. 177–198, Boston.\nPolana, R. and Nelson, R. C. (1997). Detection and recognition of periodic, nonrigid\nmotion. International Journal of Computer Vision, 23(3):261–282.\nPollard, S. B., Mayhew, J. E. W., and Frisby, J. P. (1985). PMF: A stereo correspondence\nalgorithm using a disparity gradient limit. Perception, 14:449–470.",
  "905": "References\n883\nPollefeys, M. and Van Gool, L. (2002). From images to 3D models. Communications of\nthe ACM, 45(7):50–55.\nPollefeys, M., Nist´er, D., Frahm, J.-M., Akbarzadeh, A., Mordohai, P. et al. (2008). De-\ntailed real-time urban 3D reconstruction from video. International Journal of Computer\nVision, 78(2-3):143–167.\nPonce, J., Hebert, M., Schmid, C., and Zisserman, A. (eds). (2006). Toward Category-Level\nObject Recognition, Springer, New York.\nPonce, J., Berg, T., Everingham, M., Forsyth, D., Hebert, M. et al. (2006). Dataset issues\nin object recognition. In Ponce, J., Hebert, M., Schmid, C., and Zisserman, A. (eds),\nToward Category-Level Object Recognition, pp. 29–48, Springer, New York.\nPons, J.-P., Keriven, R., and Faugeras, O. (2005). Modelling dynamic scenes by register-\ning multi-view image sequences. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’2005), pp. 822–827, San Diego, CA.\nPons, J.-P., Keriven, R., and Faugeras, O. (2007). Multi-view stereo reconstruction and\nscene ﬂow estimation with a global image-based matching score. International Journal\nof Computer Vision, 72(2):179–193.\nPorter, T. and Duff, T. (1984). Compositing digital images. Computer Graphics (SIG-\nGRAPH ’84), 18(3):253–259.\nPortilla, J. and Simoncelli, E. P. (2000). A parametric texture model based on joint statistics\nof complex wavelet coefﬁcients. International Journal of Computer Vision, 40(1):49–\n71.\nPortilla, J., Strela, V., Wainwright, M., and Simoncelli, E. P. (2003). Image denoising\nusing scale mixtures of Gaussians in the wavelet domain. IEEE Transactions on Image\nProcessing, 12(11):1338–1351.\nPotetz, B. and Lee, T. S. (2008). Efﬁcient belief propagation for higher-order cliques using\nlinear constraint nodes. Computer Vision and Image Understanding, 112(1):39–54.\nPotmesil, M. (1987). Generating octree models of 3D objects from their silhouettes in a\nsequence of images. Computer Vision, Graphics, and Image Processing, 40:1–29.\nPratt, W. K. (2007). Digital Image Processing. Wiley-Interscience, Hoboken, NJ, 4th\nedition.\nPrazdny, K. (1985). Detection of binocular disparities. Biological Cybernetics, 52:93–99.\nPritchett, P. and Zisserman, A. (1998). Wide baseline stereo matching. In Sixth Interna-\ntional Conference on Computer Vision (ICCV’98), pp. 754–760, Bombay.",
  "906": "884\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nProesmans, M., Van Gool, L., and Defoort, F. (1998). Reading between the lines – a\nmethod for extracting dynamic 3D with texture. In Sixth International Conference on\nComputer Vision (ICCV’98), pp. 1081–1086, Bombay.\nProtter, M. and Elad, M. (2009). Super resolution with probabilistic motion estimation.\nIEEE Transactions on Image Processing, 18(8):1899–1904.\nPullen, K. and Bregler, C. (2002). Motion capture assisted animation: texturing and syn-\nthesis. ACM Transactions on Graphics, 21(3):501–508.\nPulli, K. (1999). Multiview registration for large data sets. In Second International Confer-\nence on 3D Digital Imaging and Modeling (3DIM’99), pp. 160–168, Ottawa, Canada.\nPulli, K., Abi-Rached, H., Duchamp, T., Shapiro, L., and Stuetzle, W. (1998). Acquisi-\ntion and visualization of colored 3D objects. In International Conference on Pattern\nRecognition (ICPR’98), pp. 11–15.\nQuack, T., Leibe, B., and Van Gool, L.\n(2008).\nWorld-scale mining of objects and\nevents from community photo collections. In Conference on Image and Video Retrieval,\npp. 47–56, Niagara Falls.\nQuam, L. H.\n(1984).\nHierarchical warp stereo.\nIn Image Understanding Workshop,\npp. 149–155, New Orleans.\nQuan, L. and Lan, Z. (1999). Linear N-point camera pose determination. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 21(8):774–780.\nQuan, L. and Mohr, R.\n(1989).\nDetermining perspective structures using hierarchical\nHough transform. Pattern Recognition Letters, 9(4):279–286.\nRabinovich, A., Vedaldi, A., Galleguillos, C., Wiewiora, E., and Belongie, S.\n(2007).\nObjects in context. In Eleventh International Conference on Computer Vision (ICCV\n2007), Rio de Janeiro, Brazil.\nRademacher, P. and Bishop, G. (1998). Multiple-center-of-projection images. In ACM\nSIGGRAPH 1998 Conference Proceedings, pp. 199–206, Orlando.\nRaginsky, M. and Lazebnik, S. (2009). Locality-sensitive binary codes from shift-invariant\nkernels. In Advances in Neural Information Processing Systems.\nRaman, S. and Chaudhuri, S. (2007). A matte-less, variational approach to automatic scene\ncompositing. In Eleventh International Conference on Computer Vision (ICCV 2007),\nRio de Janeiro, Brazil.\nRaman, S. and Chaudhuri, S. (2009). Bilateral ﬁlter based compositing for variable expo-\nsure photography. In Proceedings of Eurographics 2009.",
  "907": "References\n885\nRamanan, D. and Baker, S. (2009). Local distance functions: A taxonomy, new algorithms,\nand an evaluation.\nIn Twelfth International Conference on Computer Vision (ICCV\n2009), Kyoto, Japan.\nRamanan, D., Forsyth, D., and Zisserman, A. (2005). Strike a pose: Tracking people by\nﬁnding stylized poses. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2005), pp. 271–278, San Diego, CA.\nRamanarayanan, G. and Bala, K. (2007). Constrained texture synthesis via energy mini-\nmization. IEEE Transactions on Visualization and Computer Graphics, 13(1):167–178.\nRamer, U. (1972). An iterative procedure for the polygonal approximation of plane curves.\nComputer Graphics and Image Processing, 1(3):244–256.\nRamnath, K., Koterba, S., Xiao, J., Hu, C., Matthews, I., Baker, S., Cohn, J., and Kanade, T.\n(2008). Multi-view AAM ﬁtting and construction. International Journal of Computer\nVision, 76(2):183–204.\nRaskar, R. and Tumblin, J. (2010). Computational Photography: Mastering New Tech-\nniques for Lenses, Lighting, and Sensors. A K Peters, Wellesley, Massachusetts.\nRaskar, R., Tan, K.-H., Feris, R., Yu, J., and Turk, M. (2004). Non-photorealistic camera:\nDepth edge detection and stylized rendering using multi-ﬂash imaging. ACM Transac-\ntions on Graphics, 23(3):679–688.\nRav-Acha, A., Kohli, P., Fitzgibbon, A., and Rother, C. (2008). Unwrap mosaics: A new\nrepresentation for video editing. ACM Transactions on Graphics, 27(3).\nRav-Acha, A., Pritch, Y., Lischinski, D., and Peleg, S. (2005). Dynamosaics: Video mo-\nsaics with non-chronological time. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’2005), pp. 58–65, San Diego, CA.\nRavikumar, P., Agarwal, A., and Wainwright, M. J. (2008). Message-passing for graph-\nstructured linear programs: Proximal projections, convergence and rounding schemes.\nIn International Conference on Machine Learning, pp. 800–807.\nRay, S. F. (2002). Applied Photographic Optics. Focal Press, Oxford, 3rd edition.\nRehg, J. and Kanade, T. (1994). Visual tracking of high DOF articulated structures: an\napplication to human hand tracking. In Third European Conference on Computer Vision\n(ECCV’94), pp. 35–46, Stockholm, Sweden.\nRehg, J. and Witkin, A. (1991). Visual tracking with deformation models. In IEEE Inter-\nnational Conference on Robotics and Automation, pp. 844–850, Sacramento.\nRehg, J., Morris, D. D., and Kanade, T. (2003). Ambiguities in visual tracking of ar-\nticulated objects using two- and three-dimensional models. International Journal of\nRobotics Research, 22(6):393–418.",
  "908": "886\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nReichenbach, S. E., Park, S. K., and Narayanswamy, R. (1991). Characterizing digital\nimage acquisition devices. Optical Engineering, 30(2):170–177.\nReinhard, E., Stark, M., Shirley, P., and Ferwerda, J. (2002). Photographic tone repro-\nduction for digital images. ACM Transactions on Graphics (Proc. SIGGRAPH 2002),\n21(3):267–276.\nReinhard, E., Ward, G., Pattanaik, S., and Debevec, P.\n(2005).\nHigh Dynamic Range\nImaging: Acquisition, Display, and Image-Based Lighting. Morgan Kaufmann.\nRhemann, C., Rother, C., and Gelautz, M. (2008). Improving color modeling for alpha\nmatting. In British Machine Vision Conference (BMVC 2008), Leeds.\nRhemann, C., Rother, C., Rav-Acha, A., and Sharp, T. (2008). High resolution matting via\ninteractive trimap segmentation. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nRhemann, C., Rother, C., Wang, J., Gelautz, M., Kohli, P., and Rott, P. (2009). A per-\nceptually motivated online benchmark for image matting. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (CVPR 2009), Miami Beach,\nFL.\nRichardson, I. E. G. (2003). H.264 and MPEG-4 Video Compression: Video Coding for\nNext Generation Multimedia. Wiley.\nRioul, O. and Vetterli, M. (1991). Wavelets and signal processing. IEEE Signal Processing\nMagazine, 8(4):14–38.\nRioux, M. and Bird, T. (1993). White laser, synced scan. IEEE Computer Graphics and\nApplications, 13(3):15–17.\nRioux, M., Bechthold, G., Taylor, D., and Duggan, M. (1987). Design of a large depth\nof view three-dimensional camera for robot vision. Optical Engineering, 26(12):1245–\n1250.\nRiseman, E. M. and Arbib, M. A. (1977). Computational techniques in the visual segmen-\ntation of static scenes. Computer Graphics and Image Processing, 6(3):221–276.\nRitter, G. X. and Wilson, J. N. (2000). Handbook of Computer Vision Algorithms in Image\nAlgebra. CRC Press, Boca Raton, 2nd edition.\nRobert, C. P.\n(2007).\nThe Bayesian Choice: From Decision-Theoretic Foundations to\nComputational Implementation. Springer-Verlag, New York.\nRoberts, L. G. (1965). Machine perception of three-dimensional solids. In Tippett, J. T.,\nBorkowitz, D. A., Clapp, L. C., Koester, C. J., and Vanderburgh Jr., A. (eds), Opti-\ncal and Electro-Optical Information Processing, pp. 159–197, MIT Press, Cambridge,\nMassachusetts.",
  "909": "References\n887\nRobertson, D. and Cipolla, R. (2004). An image-based system for urban navigation. In\nBritish Machine Vision Conference, pp. 656–665, Kingston.\nRobertson, D. P. and Cipolla, R. (2002). Building architectural models from many views\nusing map constraints. In Seventh European Conference on Computer Vision (ECCV\n2002), pp. 155–169, Copenhagen.\nRobertson, D. P. and Cipolla, R. (2009). Architectural modelling. In Varga, M. (ed.),\nPractical Image Processing and Computer Vision, John Wiley.\nRobertson, N. and Reid, I. (2006). A general method for human activity recognition in\nvideo. Computer Vision and Image Understanding, 104(2-3):232–248.\nRoble, D. (1999). Vision in ﬁlm and special effects. Computer Graphics, 33(4):58–60.\nRoble, D. and Zafar, N. B. (2009). Don’t trust your eyes: cutting-edge visual effects.\nComputer, 42(7):35–41.\nRogez, G., Rihan, J., Ramalingam, S., Orrite, C., and Torr, P. H. S. (2008). Randomized\ntrees for human pose detection. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nRogmans, S., Lu, J., Bekaert, P., and Lafruit, G. (2009). Real-time stereo-based views\nsynthesis algorithms: A uniﬁed framework and evaluation on commodity GPUs. Signal\nProcessing: Image Communication, 24:49–64.\nRohr, K. (1994). Towards model-based recognition of human movements in image se-\nquences. Computer Vision, Graphics, and Image Processing, 59(1):94–115.\nRom´an, A. and Lensch, H. P. A. (2006). Automatic multiperspective images. In Euro-\ngraphics Symposium on Rendering, pp. 83–92.\nRom´an, A., Garg, G., and Levoy, M. (2004). Interactive design of multi-perspective images\nfor visualizing urban landscapes. In IEEE Visualization 2004, pp. 537–544, Minneapo-\nlis.\nRomdhani, S. and Vetter, T. (2003). Efﬁcient, robust and accurate ﬁtting of a 3D morphable\nmodel. In Ninth International Conference on Computer Vision (ICCV 2003), pp. 59–66,\nNice, France.\nRomdhani, S., Torr, P. H. S., Sch¨olkopf, B., and Blake, A. (2001). Computationally ef-\nﬁcient face detection. In Eighth International Conference on Computer Vision (ICCV\n2001), pp. 695–700, Vancouver, Canada.\nRosales, R. and Sclaroff, S.\n(2000).\nInferring body pose without tracking body parts.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2000), pp. 721–727, Hilton Head Island.",
  "910": "888\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nRosenfeld, A. (1980). Quadtrees and pyramids for pattern recognition and image process-\ning. In Fifth International Conference on Pattern Recognition (ICPR’80), pp. 802–809,\nMiami Beach.\nRosenfeld, A. (ed.). (1984). Multiresolution Image Processing and Analysis, Springer-\nVerlag, New York.\nRosenfeld, A. and Davis, L. S. (1979). Image segmentation and image models. Proceed-\nings of the IEEE, 67(5):764–772.\nRosenfeld, A. and Kak, A. C. (1976). Digital Picture Processing. Academic Press, New\nYork.\nRosenfeld, A. and Pfaltz, J. L. (1966). Sequential operations in digital picture processing.\nJournal of the ACM, 13(4):471–494.\nRosenfeld, A., Hummel, R. A., and Zucker, S. W. (1976). Scene labeling by relaxation\noperations. IEEE Transactions on Systems, Man, and Cybernetics, SMC-6:420–433.\nRosten, E. and Drummond, T. (2005). Fusing points and lines for high performance track-\ning. In Tenth International Conference on Computer Vision (ICCV 2005), pp. 1508–\n1515, Beijing, China.\nRosten, E. and Drummond, T. (2006). Machine learning for high-speed corner detection.\nIn Ninth European Conference on Computer Vision (ECCV 2006), pp. 430–443.\nRoth, S. and Black, M. J. (2007a). On the spatial statistics of optical ﬂow. International\nJournal of Computer Vision, 74(1):33–50.\nRoth, S. and Black, M. J. (2007b). Steerable random ﬁelds. In Eleventh International\nConference on Computer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nRoth, S. and Black, M. J. (2009). Fields of experts. International Journal of Computer\nVision, 82(2):205–229.\nRother, C. (2002). A new approach for vanishing point detection in architectural environ-\nments. Image and Vision Computing, 20(9-10):647–656.\nRother, C. (2003). Linear multi-view reconstruction of points, lines, planes and cameras\nusing a reference plane. In Ninth International Conference on Computer Vision (ICCV\n2003), pp. 1210–1217, Nice, France.\nRother, C. and Carlsson, S. (2002). Linear multi view reconstruction and camera recovery\nusing a reference plane. International Journal of Computer Vision, 49(2/3):117–141.\nRother, C., Kolmogorov, V., and Blake, A. (2004). “GrabCut”—interactive foreground\nextraction using iterated graph cuts. ACM Transactions on Graphics (Proc. SIGGRAPH\n2004), 23(3):309–314.",
  "911": "References\n889\nRother, C., Bordeaux, L., Hamadi, Y., and Blake, A. (2006). Autocollage. ACM Transac-\ntions on Graphics, 25(3):847–852.\nRother, C., Kohli, P., Feng, W., and Jia, J. (2009). Minimizing sparse higher order energy\nfunctions of discrete variables. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2009), Miami Beach, FL.\nRother, C., Kolmogorov, V., Lempitsky, V., and Szummer, M. (2007). Optimizing binary\nMRFs via extended roof duality. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nRother, C., Kumar, S., Kolmogorov, V., and Blake, A.\n(2005).\nDigital tapestry.\nIn\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 589–596, San Diego, CA.\nRothganger, F., Lazebnik, S., Schmid, C., and Ponce, J.\n(2006).\n3D object modeling\nand recognition using local afﬁne-invariant image descriptors and multi-view spatial\nconstraints. International Journal of Computer Vision, 66(3):231–259.\nRousseeuw, P. J. (1984). Least median of squares regresssion. Journal of the American\nStatistical Association, 79:871–880.\nRousseeuw, P. J. and Leroy, A. M.\n(1987).\nRobust Regression and Outlier Detection.\nWiley, New York.\nRousson, M. and Paragios, N. (2008). Prior knowledge, level set representations, and visual\ngrouping. International Journal of Computer Vision, 76(3):231–243.\nRoweis, S. (1998). EM algorithms for PCA and SPCA. In Advances in Neural Information\nProcessing Systems, pp. 626–632.\nRowland, D. A. and Perrett, D. I. (1995). Manipulating facial appearance through shape\nand color. IEEE Computer Graphics and Applications, 15(5):70–76.\nRowley, H. A., Baluja, S., and Kanade, T. (1998a). Neural network-based face detection.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1):23–38.\nRowley, H. A., Baluja, S., and Kanade, T. (1998b). Rotation invariant neural network-\nbased face detection. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’98), pp. 38–44, Santa Barbara.\nRoy, S. and Cox, I. J. (1998). A maximum-ﬂow formulation of the N-camera stereo corre-\nspondence problem. In Sixth International Conference on Computer Vision (ICCV’98),\npp. 492–499, Bombay.\nRozenfeld, S., Shimshoni, I., and Lindenbaum, M. (2007). Dense mirroring surface re-\ncovery from 1d homographies and sparse correspondences. In IEEE Computer Society",
  "912": "890\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nConference on Computer Vision and Pattern Recognition (CVPR 2007), Minneapolis,\nMN.\nRubner, Y., Tomasi, C., and Guibas, L. J. (2000). The earth mover’s distance as a metric\nfor image retrieval. International Journal of Computer Vision, 40(2):99–121.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal representa-\ntions by error propagation. In Rumelhart, D. E., McClelland, J. L., and the PDP research\ngroup (eds), Parallel distributed processing: Explorations in the microstructure of cog-\nnition, pp. 318–362, Bradford Books, Cambridge, Massachusetts.\nRusinkiewicz, S. and Levoy, M. (2000). Qsplat: A multiresolution point rendering system\nfor large meshes. In ACM SIGGRAPH 2000 Conference Proceedings, pp. 343–352.\nRuss, J. C. (2007). The Image Processing Handbook. CRC Press, Boca Raton, 5th edition.\nRussell, B., Efros, A., Sivic, J., Freeman, W., and Zisserman, A. (2006). Using multiple\nsegmentations to discover objects and their extent in image collections. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR’2006),\npp. 1605–1612, New York City, NY.\nRussell, B. C., Torralba, A., Murphy, K. P., and Freeman, W. T. (2008). LabelMe: A\ndatabase and web-based tool for image annotation. International Journal of Computer\nVision, 77(1-3):157–173.\nRussell, B. C., Torralba, A., Liu, C., Fergus, R., and Freeman, W. T. (2007). Object recog-\nnition by scene alignment. In Advances in Neural Information Processing Systems.\nRuzon, M. A. and Tomasi, C. (2000). Alpha estimation in natural images. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR’2000),\npp. 18–25, Hilton Head Island.\nRuzon, M. A. and Tomasi, C.\n(2001).\nEdge, junction, and corner detection using\ncolor distributions. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n23(11):1281–1295.\nRyan, T. W., Gray, R. T., and Hunt, B. R. (1980). Prediction of correlation errors in stereo-\npair images. Optical Engineering, 19(3):312–322.\nSaad, Y. (2003). Iterative Methods for Sparse Linear Systems. Society for Industrial and\nApplied Mathematics, second edition.\nSaint-Marc, P., Chen, J. S., and Medioni, G.\n(1991).\nAdaptive smoothing: A general\ntool for early vision. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n13(6):514–529.",
  "913": "References\n891\nSaito, H. and Kanade, T. (1999). Shape reconstruction in projective grid space from large\nnumber of images. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’99), pp. 49–54, Fort Collins.\nSamet, H. (1989). The Design and Analysis of Spatial Data Structures. Addison-Wesley,\nReading, Massachusetts.\nSander, P. T. and Zucker, S. W. (1990). Inferring surface trace and differential structure\nfrom 3-D images. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n12(9):833–854.\nSapiro, G. (2001). Geometric Partial Differential Equations and Image Analysis. Cam-\nbridge University Press.\nSato, Y. and Ikeuchi, K. (1996). Reﬂectance analysis for 3D computer graphics model\ngeneration. Graphical Models and Image Processing, 58(5):437–451.\nSato, Y., Wheeler, M., and Ikeuchi, K. (1997). Object shape and reﬂectance modeling\nfrom observation. In ACM SIGGRAPH 1997 Conference Proceedings, pp. 379–387,\nLos Angeles.\nSavarese, S. and Fei-Fei, L. (2007). 3D generic object categorization, localization and pose\nestimation. In Eleventh International Conference on Computer Vision (ICCV 2007),\nRio de Janeiro, Brazil.\nSavarese, S. and Fei-Fei, L. (2008). View synthesis for recognizing unseen poses of object\nclasses. In Tenth European Conference on Computer Vision (ECCV 2008), pp. 602–615,\nMarseilles.\nSavarese, S., Chen, M., and Perona, P. (2005). Local shape from mirror reﬂections. Inter-\nnational Journal of Computer Vision, 64(1):31–67.\nSavarese, S., Andreetto, M., Rushmeier, H. E., Bernardini, F., and Perona, P. (2007). 3D re-\nconstruction by shadow carving: Theory and practical evaluation. International Journal\nof Computer Vision, 71(3):305–336.\nSawhney, H. S. (1994). Simplifying motion and structure analysis using planar parallax and\nimage warping. In Twelfth International Conference on Pattern Recognition (ICPR’94),\npp. 403–408, Jerusalem, Israel.\nSawhney, H. S. and Ayer, S. (1996). Compact representation of videos through domi-\nnant multiple motion estimation. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 18(8):814–830.\nSawhney, H. S. and Hanson, A. R. (1991). Identiﬁcation and 3D description of ‘shallow’\nenvironmental structure over a sequence of images. In IEEE Computer Society Con-",
  "914": "892\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nference on Computer Vision and Pattern Recognition (CVPR’91), pp. 179–185, Maui,\nHawaii.\nSawhney, H. S. and Kumar, R. (1999). True multi-image alignment and its application to\nmosaicing and lens distortion correction. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 21(3):235–243.\nSawhney, H. S., Kumar, R., Gendel, G., Bergen, J., Dixon, D., and Paragano, V. (1998).\nVideoBrush: Experiences with consumer video mosaicing. In IEEE Workshop on Ap-\nplications of Computer Vision (WACV’98), pp. 56–62, Princeton.\nSawhney, H. S., Arpa, A., Kumar, R., Samarasekera, S., Aggarwal, M., Hsu, S., Nister, D.,\nand Hanna, K. (2002). Video ﬂashlights: real time rendering of multiple videos for\nimmersive model visualization. In Proceedings of the 13th Eurographics Workshop on\nRendering, pp. 157–168, Pisa, Italy.\nSaxena, A., Sun, M., and Ng, A. Y. (2009). Make3D: Learning 3D scene structure from\na single still image. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n31(5):824–840.\nSchaffalitzky, F. and Zisserman, A. (2000). Planar grouping for automatic detection of\nvanishing lines and points. Image and Vision Computing, 18:647–658.\nSchaffalitzky, F. and Zisserman, A. (2002). Multi-view matching for unordered image\nsets, or “How do I organize my holiday snaps?”. In Seventh European Conference on\nComputer Vision (ECCV 2002), pp. 414–431, Copenhagen.\nScharr, H., Black, M. J., and Haussecker, H. W. (2003). Image statistics and anisotropic dif-\nfusion. In Ninth International Conference on Computer Vision (ICCV 2003), pp. 840–\n847, Nice, France.\nScharstein, D. (1994). Matching images by comparing their gradient ﬁelds. In Twelfth\nInternational Conference on Pattern Recognition (ICPR’94), pp. 572–575, Jerusalem,\nIsrael.\nScharstein, D. (1999). View Synthesis Using Stereo Vision. Volume 1583, Springer-Verlag.\nScharstein, D. and Pal, C. (2007). Learning conditional random ﬁelds for stereo. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR\n2007), Minneapolis, MN.\nScharstein, D. and Szeliski, R. (1998). Stereo matching with nonlinear diffusion. Interna-\ntional Journal of Computer Vision, 28(2):155–174.\nScharstein, D. and Szeliski, R. (2002). A taxonomy and evaluation of dense two-frame\nstereo correspondence algorithms. International Journal of Computer Vision, 47(1):7–\n42.",
  "915": "References\n893\nScharstein, D. and Szeliski, R. (2003). High-accuracy stereo depth maps using structured\nlight. In IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion (CVPR’2003), pp. 195–202, Madison, WI.\nSchechner, Y. Y., Nayar, S. K., and Belhumeur, P. N. (2009). Multiplexing for optimal\nlighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(8):1339–\n1354.\nSchindler, G., Brown, M., and Szeliski, R. (2007). City-scale location recognition. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR\n2007), Minneapolis, MN.\nSchindler, G., Krishnamurthy, P., Lublinerman, R., Liu, Y., and Dellaert, F. (2008). Detect-\ning and matching repeated patterns for automatic geo-tagging in urban environments.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2008), Anchorage, AK.\nSchlesinger, D. and Flach, B. (2006). Transforming an arbitrary minsum problem into a\nbinary one. Technical Report TUD-FI06-01, Dresden University of Technology.\nSchlesinger, M. I. (1976). Syntactic analysis of two-dimensional visual signals in noisy\nconditions. Kibernetika, 4:113–130.\nSchlesinger, M. I. and Giginyak, V. V. (2007a). Solution to structural recognition (max,+)-\nproblems by their equivalent transformations – part 1. Control Systems and Computers,\n2007(1):3–15.\nSchlesinger, M. I. and Giginyak, V. V. (2007b). Solution to structural recognition (max,+)-\nproblems by their equivalent transformations – part 2. Control Systems and Computers,\n2007(2):3–18.\nSchmid, C. and Mohr, R. (1997). Local grayvalue invariants for image retrieval. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 19(5):530–534.\nSchmid, C. and Zisserman, A. (1997). Automatic line matching across views. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’97),\npp. 666–671, San Juan, Puerto Rico.\nSchmid, C., Mohr, R., and Bauckhage, C. (2000). Evaluation of interest point detectors.\nInternational Journal of Computer Vision, 37(2):151–172.\nSchneiderman, H. and Kanade, T. (2004). Object detection using the statistics of parts.\nInternational Journal of Computer Vision, 56(3):151–177.\nSch¨odl, A. and Essa, I. (2002). Controlled animation of video sprites. In ACM Symposium\non Computater Animation, San Antonio.",
  "916": "894\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSch¨odl, A., Szeliski, R., Salesin, D. H., and Essa, I. (2000). Video textures. In ACM\nSIGGRAPH 2000 Conference Proceedings, pp. 489–498, New Orleans.\nSchoenemann, T. and Cremers, D. (2008). High resolution motion layer decomposition\nusing dual-space graph cuts. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2008), Anchorage, AK.\nSch¨olkopf, B. and Smola, A. (eds).\n(2002).\nLearning with Kernels: Support Vector\nMachines, Regularization, Optimization and Beyond.\nMIT Press, Cambridge, Mas-\nsachusetts.\nSchraudolph, N. N. (2010). Polynomial-time exact inference in NP-hard binary MRFs via\nreweighted perfect matching. In 13th International Conference on Artiﬁcial Intelligence\nand Statistics (AISTATS), pp. 717–724.\nSchr¨oder, P. and Sweldens, W. (1995). Spherical wavelets: Efﬁciently representing func-\ntions on the sphere. In ACM SIGGRAPH 1995 Conference Proceedings, pp. 161–172.\nSchultz, R. R. and Stevenson, R. L. (1996). Extraction of high-resolution frames from\nvideo sequences. IEEE Transactions on Image Processing, 5(6):996–1011.\nSclaroff, S. and Isidoro, J. (2003). Active blobs: region-based, deformable appearance\nmodels. Computer Vision and Image Understanding, 89(2-3):197–225.\nScott, G. L. and Longuet-Higgins, H. C. (1990). Feature grouping by relocalization of\neigenvectors of the proximity matrix. In British Machine Vision Conference, pp. 103–\n108.\nSebastian, T. B. and Kimia, B. B. (2005). Curves vs. skeletons in object recognition. Signal\nProcessing, 85(2):246–263.\nSederberg, T. W. and Parry, S. R.\n(1986).\nFree-form deformations of solid geometric\nmodels. Computer Graphics (SIGGRAPH ’86), 20(4):151–160.\nSederberg, T. W., Gao, P., Wang, G., and Mu, H. (1993). 2D shape blending: An intrinsic\nsolution to the vertex path problem. In ACM SIGGRAPH 1993 Conference Proceedings,\npp. 15–18.\nSeitz, P. (1989). Using local orientation information as image primitive for robust ob-\nject recognition. In SPIE Vol. 1199, Visual Communications and Image Processing IV,\npp. 1630–1639.\nSeitz, S. (2001). The space of all stereo images. In Eighth International Conference on\nComputer Vision (ICCV 2001), pp. 26–33, Vancouver, Canada.\nSeitz, S. and Szeliski, R. (1999). Applications of computer vision to computer graphics.\nComputer Graphics, 33(4):35–37. Guest Editors’ introduction to the Special Issue.",
  "917": "References\n895\nSeitz, S., Curless, B., Diebel, J., Scharstein, D., and Szeliski, R. (2006). A comparison and\nevaluation of multi-view stereo reconstruction algorithms. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (CVPR’2006), pp. 519–526,\nNew York, NY.\nSeitz, S. M. and Baker, S. (2009). Filter ﬂow. In Twelfth International Conference on\nComputer Vision (ICCV 2009), Kyoto, Japan.\nSeitz, S. M. and Dyer, C. M. (1996). View morphing. In ACM SIGGRAPH 1996 Confer-\nence Proceedings, pp. 21–30, New Orleans.\nSeitz, S. M. and Dyer, C. M. (1997). Photorealistic scene reconstruction by voxel coloring.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’97), pp. 1067–1073, San Juan, Puerto Rico.\nSeitz, S. M. and Dyer, C. M. (1999). Photorealistic scene reconstruction by voxel coloring.\nInternational Journal of Computer Vision, 35(2):151–173.\nSeitz, S. M. and Dyer, C. R. (1997). View invariant analysis of cyclic motion. International\nJournal of Computer Vision, 25(3):231–251.\nSerra, J. (1982). Image Analysis and Mathematical Morphology. Academic Press, New\nYork.\nSerra, J. and Vincent, L. (1992). An overview of morphological ﬁltering. Circuits, Systems\nand Signal Processing, 11(1):47–108.\nSerre, T., Wolf, L., and Poggio, T. (2005). Object recognition with features inspired by\nvisual cortex. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2005), pp. 994–1000, San Diego, CA.\nSethian, J. (1999). Level Set Methods and Fast Marching Methods. Cambridge University\nPress, Cambridge, 2nd edition.\nShade, J., Gortler, S., He, L., and Szeliski, R. (1998). Layered depth images. In ACM\nSIGGRAPH 1998 Conference Proceedings, pp. 231–242, Orlando.\nShade, J., Lischinski, D., Salesin, D., DeRose, T., and Snyder, J. (1996). Hierarchical\nimages caching for accelerated walkthroughs of complex environments. In ACM SIG-\nGRAPH 1996 Conference Proceedings, pp. 75–82, New Orleans.\nShafer, S. A. (1985). Using color to separate reﬂection components. COLOR Research and\nApplications, 10(4):210–218.\nShafer, S. A., Healey, G., and Wolff, L. (1992). Physics-Based Vision: Principles and\nPractice. Jones & Bartlett, Cambridge, MA.",
  "918": "896\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nShaﬁque, K. and Shah, M. (2005). A noniterative greedy algorithm for multiframe point\ncorrespondence.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n27(1):51–65.\nShah, J. (1993). A nonlinear diffusion model for discontinuous disparity and half-occlusion\nin stereo.\nIn IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’93), pp. 34–40, New York.\nShakhnarovich, G., Darrell, T., and Indyk, P. (eds). (2006). Nearest-Neighbor Methods in\nLearning and Vision: Theory and Practice, MIT Press.\nShakhnarovich, G., Viola, P., and Darrell, T. (2003). Fast pose estimation with parameter-\nsensitive hashing. In Ninth International Conference on Computer Vision (ICCV 2003),\npp. 750–757, Nice, France.\nShan, Y., Liu, Z., and Zhang, Z. (2001). Model-based bundle adjustment with application\nto face modeling. In Eighth International Conference on Computer Vision (ICCV 2001),\npp. 644–641, Vancouver, Canada.\nSharon, E., Galun, M., Sharon, D., Basri, R., and Brandt, A. (2006). Hierarchy and adap-\ntivity in segmenting visual scenes. Nature, 442(7104):810–813.\nShashua, A. and Toelg, S. (1997). The quadric reference surface: Theory and applications.\nInternational Journal of Computer Vision, 23(2):185–198.\nShashua, A. and Wexler, Y. (2001). Q-warping: Direct computation of quadratic reference\nsurfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(8):920–\n925.\nShaw, D. and Barnes, N. (2006). Perspective rectangle detection. In Workshop on Applica-\ntions of Computer Vision at ECCV’2006.\nShewchuk, J. R.\n(1994).\nAn introduction to the conjugate gradient method without\nthe agonizing pain. Unpublished manuscript, available on author’s homepage (http:\n//www.cs.berkeley.edu/∼jrs/). An earlier version appeared as a Carnegie Mellon Uni-\nversity Technical Report, CMU-CS-94-125.\nShi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 8(22):888–905.\nShi, J. and Tomasi, C. (1994). Good features to track. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR’94), pp. 593–600, Seattle.\nShimizu, M. and Okutomi, M. (2001). Precise sub-pixel estimation on area-based match-\ning. In Eighth International Conference on Computer Vision (ICCV 2001), pp. 90–97,\nVancouver, Canada.",
  "919": "References\n897\nShirley, P. (2005). Fundamentals of Computer Graphics. A K Peters, Wellesley, Mas-\nsachusetts, second edition.\nShizawa, M. and Mase, K. (1991). A uniﬁed computational theory of motion transparency\nand motion boundaries based on eigenenergy analysis. In IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition (CVPR’91), pp. 289–295, Maui,\nHawaii.\nShoemake, K. (1985). Animating rotation with quaternion curves. Computer Graphics\n(SIGGRAPH ’85), 19(3):245–254.\nShotton, J., Blake, A., and Cipolla, R. (2005). Contour-based learning for object detection.\nIn Tenth International Conference on Computer Vision (ICCV 2005), pp. 503–510, Bei-\njing, China.\nShotton, J., Johnson, M., and Cipolla, R. (2008). Semantic texton forests for image catego-\nrization and segmentation. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2008), Anchorage, AK.\nShotton, J., Winn, J., Rother, C., and Criminisi, A. (2009). Textonboost for image under-\nstanding: Multi-class object recognition and segmentation by jointly modeling appear-\nance, shape and context. International Journal of Computer Vision, 81(1):2–23.\nShufelt, J. (1999). Performance evaluation and analysis of vanishing point detection tech-\nniques. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(3):282–\n288.\nShum, H.-Y. and He, L.-W. (1999). Rendering with concentric mosaics. In ACM SIG-\nGRAPH 1999 Conference Proceedings, pp. 299–306, Los Angeles.\nShum, H.-Y. and Szeliski, R. (1999). Stereo reconstruction from multiperspective panora-\nmas. In Seventh International Conference on Computer Vision (ICCV’99), pp. 14–21,\nKerkyra, Greece.\nShum, H.-Y. and Szeliski, R. (2000). Construction of panoramic mosaics with global and\nlocal alignment. International Journal of Computer Vision, 36(2):101–130. Erratum\npublished July 2002, 48(2):151–152.\nShum, H.-Y., Chan, S.-C., and Kang, S. B. (2007). Image-Based Rendering. Springer,\nNew York, NY.\nShum, H.-Y., Han, M., and Szeliski, R. (1998). Interactive construction of 3D models from\npanoramic mosaics. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’98), pp. 427–433, Santa Barbara.\nShum, H.-Y., Ikeuchi, K., and Reddy, R. (1995). Principal component analysis with miss-\ning data and its application to polyhedral modeling.\nIEEE Transactions on Pattern",
  "920": "898\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nAnalysis and Machine Intelligence, 17(9):854–867.\nShum, H.-Y., Kang, S. B., and Chan, S.-C. (2003). Survey of image-based representations\nand compression techniques. IEEE Transactions on Circuits and Systems for Video\nTechnology, 13(11):1020–1037.\nShum, H.-Y., Wang, L., Chai, J.-X., and Tong, X. (2002). Rendering by manifold hopping.\nInternational Journal of Computer Vision, 50(2):185–201.\nShum, H.-Y., Sun, J., Yamazaki, S., Li, Y., and Tang, C.-K. (2004). Pop-up light ﬁeld: An\ninteractive image-based modeling and rendering system. ACM Transactions on Graph-\nics, 23(2):143–162.\nSidenbladh, H. and Black, M. J. (2003). Learning the statistics of people in images and\nvideo. International Journal of Computer Vision, 54(1):189–209.\nSidenbladh, H., Black, M. J., and Fleet, D. J. (2000). Stochastic tracking of 3D human\nﬁgures using 2D image motion. In Sixth European Conference on Computer Vision\n(ECCV 2000), pp. 702–718, Dublin, Ireland.\nSigal, L. and Black, M. J. (2006). Predicting 3D people from 2D pictures. In AMDO 2006 -\nIV Conference on Articulated Motion and Deformable Objects, pp. 185–195, Mallorca,\nSpain.\nSigal, L., Balan, A., and Black, M. J. (2010). Humaneva: Synchronized video and mo-\ntion capture dataset and baseline algorithm for evaluation of articulated human motion.\nInternational Journal of Computer Vision, 87(1-2):4–27.\nSigal, L., Bhatia, S., Roth, S., Black, M. J., and Isard, M. (2004). Tracking loose-limbed\npeople. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’2004), pp. 421–428, Washington, DC.\nSillion, F. and Puech, C. (1994). Radiosity and Global Illumination. Morgan Kaufmann.\nSim, T., Baker, S., and Bsat, M.\n(2003).\nThe CMU pose, illumination, and expres-\nsion database.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n25(12):1615–1618.\nSimard, P. Y., Bottou, L., Haffner, P., and Cun, Y. L. (1998). Boxlets: a fast convolution\nalgorithm for signal processing and neural networks. In Advances in Neural Information\nProcessing Systems 13, pp. 571–577.\nSimon, I. and Seitz, S. M.\n(2008).\nScene segmentation using the wisdom of crowds.\nIn Tenth European Conference on Computer Vision (ECCV 2008), pp. 541–553, Mar-\nseilles.",
  "921": "References\n899\nSimon, I., Snavely, N., and Seitz, S. M. (2007). Scene summarization for online image\ncollections. In Eleventh International Conference on Computer Vision (ICCV 2007),\nRio de Janeiro, Brazil.\nSimoncelli, E. P. (1999). Bayesian denoising of visual images in the wavelet domain.\nIn M¨uller, P. and Vidakovic, B. (eds), Bayesian Inference in Wavelet Based Models,\npp. 291–308, Springer-Verlag, New York.\nSimoncelli, E. P. and Adelson, E. H. (1990a). Non-separable extensions of quadrature\nmirror ﬁlters to multiple dimensions. Proceedings of the IEEE, 78(4):652–664.\nSimoncelli, E. P. and Adelson, E. H. (1990b). Subband transforms. In Woods, J. (ed.),\nSubband Coding, pp. 143–191, Kluwer Academic Press, Norwell, MA.\nSimoncelli, E. P., Adelson, E. H., and Heeger, D. J. (1991). Probability distributions of\noptic ﬂow. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’91), pp. 310–315, Maui, Hawaii.\nSimoncelli, E. P., Freeman, W. T., Adelson, E. H., and Heeger, D. J. (1992). Shiftable\nmultiscale transforms. IEEE Transactions on Information Theory, 38(3):587–607.\nSingaraju, D., Grady, L., and Vidal, R. (2008). Interactive image segmentation via mini-\nmization of quadratic energies on directed graphs. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nSingaraju, D., Rother, C., and Rhemann, C. (2009). New appearance models for natural\nimage matting. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2009), Miami Beach, FL.\nSingaraju, D., Grady, L., Sinop, A. K., and Vidal, R. (2010). A continuous valued MRF for\nimage segmentation. In Blake, A., Kohli, P., and Rother, C. (eds), Advances in Markov\nRandom Fields, MIT Press.\nSinha, P., Balas, B., Ostrovsky, Y., and Russell, R. (2006). Face recognition by humans:\nNineteen results all computer vision researchers should know about. Proceedings of the\nIEEE, 94(11):1948–1962.\nSinha, S., Mordohai, P., and Pollefeys, M. (2007). Multi-view stereo via graph cuts on the\ndual of an adaptive tetrahedral mesh. In Eleventh International Conference on Com-\nputer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nSinha, S. N. and Pollefeys, M. (2005). Multi-view reconstruction using photo-consistency\nand exact silhouette constraints: A maximum-ﬂow formulation. In Tenth International\nConference on Computer Vision (ICCV 2005), pp. 349–356, Beijing, China.\nSinha, S. N., Steedly, D., and Szeliski, R. (2009). Piecewise planar stereo for image-based\nrendering. In Twelfth IEEE International Conference on Computer Vision (ICCV 2009),",
  "922": "900\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nKyoto, Japan.\nSinha, S. N., Steedly, D., Szeliski, R., Agrawala, M., and Pollefeys, M. (2008). Interactive\n3D architectural modeling from unordered photo collections. ACM Transactions on\nGraphics (Proc. SIGGRAPH Asia 2008), 27(5).\nSinop, A. K. and Grady, L. (2007). A seeded image segmentation framework unifying\ngraph cuts and random walker which yields a new algorithm. In Eleventh International\nConference on Computer Vision (ICCV 2007), Rio de Janeiro, Brazil.\nSivic, J. and Zisserman, A. (2003). Video Google: A text retrieval approach to object\nmatching in videos.\nIn Ninth International Conference on Computer Vision (ICCV\n2003), pp. 1470–1477, Nice, France.\nSivic, J. and Zisserman, A. (2009). Efﬁcient visual search of videos cast as text retrieval.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 31(4):591–606.\nSivic, J., Everingham, M., and Zisserman, A. (2009). “Who are you?”—Learning person\nspeciﬁc classiﬁers from video. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2009), Miami Beach, FL.\nSivic, J., Zitnick, C. L., and Szeliski, R.\n(2006).\nFinding people in repeated shots of\nthe same scene. In British Machine Vision Conference (BMVC 2006), pp. 909–918,\nEdinburgh.\nSivic, J., Russell, B., Zisserman, A., Freeman, W. T., and Efros, A. A. (2008). Unsuper-\nvised discovery of visual object class hierarchies. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nSivic, J., Russell, B. C., Efros, A. A., Zisserman, A., and Freeman, W. T. (2005). Dis-\ncovering objects and their localization in images. In Tenth International Conference on\nComputer Vision (ICCV 2005), pp. 370–377, Beijing, China.\nSlabaugh, G. G., Culbertson, W. B., Slabaugh, T. G., Culbertson, B., Malzbender, T., and\nStevens, M. (2004). Methods for volumetric reconstruction of visual scenes. Interna-\ntional Journal of Computer Vision, 57(3):179–199.\nSlama, C. C. (ed.). (1980). Manual of Photogrammetry. American Society of Photogram-\nmetry, Falls Church, Virginia, fourth edition.\nSmelyanskiy, V. N., Cheeseman, P., Maluf, D. A., and Morris, R. D. (2000). Bayesian\nsuper-resolved surface reconstruction from images. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR’2000), pp. 375–382, Hilton\nHead Island.",
  "923": "References\n901\nSmeulders, A. W. M., Worring, M., Santini, S., Gupta, A., and Jain, R. C. (2000). Content-\nbased image retrieval at the end of the early years. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 22(12):477–490.\nSminchisescu, C. and Triggs, B. (2001). Covariance scaled sampling for monocular 3D\nbody tracking. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2001), pp. 447–454, Kauai, Hawaii.\nSminchisescu, C., Kanaujia, A., and Metaxas, D. (2006). Conditional models for con-\ntextual human motion recognition. Computer Vision and Image Understanding, 104(2-\n3):210–220.\nSminchisescu, C., Kanaujia, A., Li, Z., and Metaxas, D. (2005). Discriminative density\npropagation for 3D human motion estimation. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’2005), pp. 390–397, San Diego,\nCA.\nSmith, A. R. and Blinn, J. F. (1996). Blue screen matting. In ACM SIGGRAPH 1996\nConference Proceedings, pp. 259–268, New Orleans.\nSmith, B. M., Zhang, L., Jin, H., and Agarwala, A. (2009). Light ﬁeld video stabilization.\nIn Twelfth International Conference on Computer Vision (ICCV 2009), Kyoto, Japan.\nSmith, S. M. and Brady, J. M.\n(1997).\nSUSAN—a new approach to low level image\nprocessing. International Journal of Computer Vision, 23(1):45–78.\nSmolic, A. and Kauff, P. (2005). Interactive 3-D video representation and coding technolo-\ngies. Proceedings of the IEEE, 93(1):98–110.\nSnavely, N., Seitz, S. M., and Szeliski, R. (2006). Photo tourism: Exploring photo collec-\ntions in 3D. ACM Transactions on Graphics (Proc. SIGGRAPH 2006), 25(3):835–846.\nSnavely, N., Seitz, S. M., and Szeliski, R. (2008a). Modeling the world from Internet photo\ncollections. International Journal of Computer Vision, 80(2):189–210.\nSnavely, N., Seitz, S. M., and Szeliski, R. (2008b). Skeletal graphs for efﬁcient structure\nfrom motion. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nSnavely, N., Garg, R., Seitz, S. M., and Szeliski, R. (2008). Finding paths through the\nworld’s photos. ACM Transactions on Graphics (Proc. SIGGRAPH 2008), 27(3).\nSnavely, N., Simon, I., Goesele, M., Szeliski, R., and Seitz, S. M.\n(2010).\nScene re-\nconstruction and visualization from community photo collections. Proceedings of the\nIEEE, 98(8):1370–1390.",
  "924": "902\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSoatto, S., Yezzi, A. J., and Jin, H. (2003). Tales of shape and radiance in multiview stereo.\nIn Ninth International Conference on Computer Vision (ICCV 2003), pp. 974–981, Nice,\nFrance.\nSoille, P. (2006). Morphological image compositing. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 28(5):673–683.\nSolina, F. and Bajcsy, R.\n(1990).\nRecovery of parametric models from range images:\nThe case for superquadrics with global deformations. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 12(2):131–147.\nSontag, D. and Jaakkola, T. (2007). New outer bounds on the marginal polytope. In\nAdvances in Neural Information Processing Systems.\nSontag, D., Meltzer, T., Globerson, A., Jaakkola, T., and Weiss, Y. (2008). Tightening LP\nrelaxations for MAP using message passing. In Uncertainty in Artiﬁcial Intelligence\n(UAI).\nSoucy, M. and Laurendeau, D. (1992). Multi-resolution surface modeling from multiple\nrange views. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’92), pp. 348–353, Champaign, Illinois.\nSrinivasan, S., Chellappa, R., Veeraraghavan, A., and Aggarwal, G. (2005). Electronic\nimage stabilization and mosaicking algorithms. In Bovik, A. (ed.), Handbook of Image\nand Video Processing, Academic Press.\nSrivasan, P., Liang, P., and Hackwood, S. (1990). Computational geometric methods in\nvolumetric intersections for 3D reconstruction. Pattern Recognition, 23(8):843–857.\nStamos, I., Liu, L., Chen, C., Wolberg, G., Yu, G., and Zokai, S. (2008). Integrating\nautomated range registration with multiview geometry for the photorealistic modeling\nof large-scale scenes. International Journal of Computer Vision, 78(2-3):237–260.\nStark, J. A. (2000). Adaptive image contrast enhancement using generalizations of his-\ntogram equalization. IEEE Transactions on Image Processing, 9(5):889–896.\nStauffer, C. and Grimson, W.\n(1999).\nAdaptive background mixture models for real-\ntime tracking. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’99), pp. 246–252, Fort Collins.\nSteedly, D. and Essa, I. (2001). Propagation of innovative information in non-linear least-\nsquares structure from motion. In Eighth International Conference on Computer Vision\n(ICCV 2001), pp. 223–229, Vancouver, Canada.\nSteedly, D., Essa, I., and Dellaert, F. (2003). Spectral partitioning for structure from mo-\ntion. In Ninth International Conference on Computer Vision (ICCV 2003), pp. 996–\n1003, Nice, France.",
  "925": "References\n903\nSteedly, D., Pal, C., and Szeliski, R. (2005). Efﬁciently registering video into panoramic\nmosaics.\nIn Tenth International Conference on Computer Vision (ICCV 2005),\npp. 1300–1307, Beijing, China.\nSteele, R. and Jaynes, C. (2005). Feature uncertainty arising from covariant image noise.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 1063–1070, San Diego, CA.\nSteele, R. M. and Jaynes, C. (2006). Overconstrained linear estimation of radial distortion\nand multi-view geometry. In Ninth European Conference on Computer Vision (ECCV\n2006), pp. 253–264.\nStein, A., Hoiem, D., and Hebert, M. (2007). Learning to extract object boundaries using\nmotion cues. In Eleventh International Conference on Computer Vision (ICCV 2007),\nRio de Janeiro, Brazil.\nStein, F. and Medioni, G. (1992). Structural indexing: Efﬁcient 3-D object recognition.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2):125–145.\nStein, G. (1995). Accurate internal camera calibration using rotation, with analysis of\nsources of error.\nIn Fifth International Conference on Computer Vision (ICCV’95),\npp. 230–236, Cambridge, Massachusetts.\nStein, G. (1997). Lens distortion calibration using point correspondences. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR’97),\npp. 602–608, San Juan, Puerto Rico.\nStenger, B., Thayananthan, A., Torr, P. H. S., and Cipolla, R. (2006). Model-based hand\ntracking using a hierarchical bayesian ﬁlter. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 28(9):1372–1384.\nStewart, C. V. (1999). Robust parameter estimation in computer vision. SIAM Reviews,\n41(3):513–537.\nStiller, C. and Konrad, J. (1999). Estimating motion in image sequences: A tutorial on\nmodeling and computation of 2D motion. IEEE Signal Processing Magazine, 16(4):70–\n91.\nStollnitz, E. J., DeRose, T. D., and Salesin, D. H. (1996). Wavelets for Computer Graphics:\nTheory and Applications. Morgan Kaufmann, San Francisco.\nStrang, G. (1988). Linear Algebra and its Applications. Harcourt, Brace, Jovanovich,\nPublishers, San Diego, 3rd edition.\nStrang, G. (1989). Wavelets and dilation equations: A brief introduction. SIAM Reviews,\n31(4):614–627.",
  "926": "904\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nStrecha, C., Fransens, R., and Van Gool, L. (2006). Combined depth and outlier estimation\nin multi-view stereo. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2006), pp. 2394–2401, New York City, NY.\nStrecha, C., Tuytelaars, T., and Van Gool, L. (2003). Dense matching of multiple wide-\nbaseline views. In Ninth International Conference on Computer Vision (ICCV 2003),\npp. 1194–1201, Nice, France.\nStrecha, C., von Hansen, W., Van Gool, L., Fua, P., and Thoennessen, U. (2008). On\nbenchmarking camera calibration and multi-view stereo. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (CVPR 2008), Anchorage,\nAK.\nSturm, P. (2005). Multi-view geometry for general camera models. In IEEE Computer So-\nciety Conference on Computer Vision and Pattern Recognition (CVPR’2005), pp. 206–\n212, San Diego, CA.\nSturm, P. and Ramalingam, S. (2004). A generic concept for camera calibration. In Eighth\nEuropean Conference on Computer Vision (ECCV 2004), pp. 1–13, Prague.\nSturm, P. and Triggs, W. (1996). A factorization based algorithm for multi-image projective\nstructure and motion. In Fourth European Conference on Computer Vision (ECCV’96),\npp. 709–720, Cambridge, England.\nSu, H., Sun, M., Fei-Fei, L., and Savarese, S. (2009). Learning a dense multi-view repre-\nsentation for detection, viewpoint classiﬁcation and synthesis of object categories. In\nTwelfth International Conference on Computer Vision (ICCV 2009), Kyoto, Japan.\nSudderth, E. B., Torralba, A., Freeman, W. T., and Willsky, A. S. (2008). Describing visual\nscenes using transformed objects and parts. International Journal of Computer Vision,\n77(1-3):291–330.\nSullivan, S. and Ponce, J.\n(1998).\nAutomatic model construction and pose estimation\nfrom photographs using triangular splines. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 20(10):1091–1096.\nSun, D., Roth, S., Lewis, J. P., and Black, M. J. (2008). Learning optical ﬂow. In Tenth\nEuropean Conference on Computer Vision (ECCV 2008), pp. 83–97, Marseilles.\nSun, J., Zheng, N., and Shum, H. (2003). Stereo matching using belief propagation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 25(7):787–800.\nSun, J., Jia, J., Tang, C.-K., and Shum, H.-Y. (2004). Poisson matting. ACM Transactions\non Graphics (Proc. SIGGRAPH 2004), 23(3):315–321.\nSun, J., Li, Y., Kang, S. B., and Shum, H.-Y. (2006). Flash matting. ACM Transactions on\nGraphics, 25(3):772–778.",
  "927": "References\n905\nSun, J., Yuan, L., Jia, J., and Shum, H.-Y. (2004). Image completion with structure propa-\ngation. ACM Transactions on Graphics (Proc. SIGGRAPH 2004), 24(3):861–868.\nSun, M., Su, H., Savarese, S., and Fei-Fei, L. (2009). A multi-view probabilistic model\nfor 3D object classes. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2009), Miami Beach, FL.\nSung, K.-K. and Poggio, T. (1998). Example-based learning for view-based human face\ndetection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1):39–\n51.\nSutherland, I. E. (1974). Three-dimensional data input by tablet. Proceedings of the IEEE,\n62(4):453–461.\nSwain, M. J. and Ballard, D. H. (1991). Color indexing. International Journal of Computer\nVision, 7(1):11–32.\nSwaminathan, R., Kang, S. B., Szeliski, R., Criminisi, A., and Nayar, S. K. (2002). On\nthe motion and appearance of specularities in image sequences. In Seventh European\nConference on Computer Vision (ECCV 2002), pp. 508–523, Copenhagen.\nSweldens, W. (1996). Wavelets and the lifting scheme: A 5 minute tour. Z. Angew. Math.\nMech., 76 (Suppl. 2):41–44.\nSweldens, W. (1997). The lifting scheme: A construction of second generation wavelets.\nSIAM J. Math. Anal., 29(2):511–546.\nSwendsen, R. H. and Wang, J.-S. (1987). Nonuniversal critical dynamics in Monte Carlo\nsimulations. Physical Review Letters, 58(2):86–88.\nSzeliski, R. (1986). Cooperative Algorithms for Solving Random-Dot Stereograms. Tech-\nnical Report CMU-CS-86-133, Computer Science Department, Carnegie Mellon Uni-\nversity.\nSzeliski, R.\n(1989).\nBayesian Modeling of Uncertainty in Low-Level Vision.\nKluwer\nAcademic Publishers, Boston.\nSzeliski, R. (1990a). Bayesian modeling of uncertainty in low-level vision. International\nJournal of Computer Vision, 5(3):271–301.\nSzeliski, R. (1990b). Fast surface interpolation using hierarchical basis functions. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 12(6):513–528.\nSzeliski, R. (1991a). Fast shape from shading. CVGIP: Image Understanding, 53(2):129–\n153.\nSzeliski, R. (1991b). Shape from rotation. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’91), pp. 625–630, Maui, Hawaii.",
  "928": "906\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSzeliski, R. (1993). Rapid octree construction from image sequences. CVGIP: Image\nUnderstanding, 58(1):23–32.\nSzeliski, R. (1994). Image mosaicing for tele-reality applications. In IEEE Workshop on\nApplications of Computer Vision (WACV’94), pp. 44–53, Sarasota.\nSzeliski, R. (1996). Video mosaics for virtual environments. IEEE Computer Graphics\nand Applications, 16(2):22–30.\nSzeliski, R. (1999). A multi-view approach to motion and stereo. In IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR’99), pp. 157–\n163, Fort Collins.\nSzeliski, R. (2006a). Image alignment and stitching: A tutorial. Foundations and Trends in\nComputer Graphics and Computer Vision, 2(1):1–104.\nSzeliski, R. (2006b). Locally adapted hierarchical basis preconditioning. ACM Transac-\ntions on Graphics (Proc. SIGGRAPH 2006), 25(3):1135–1143.\nSzeliski, R. and Coughlan, J. (1997). Spline-based image registration. International Jour-\nnal of Computer Vision, 22(3):199–218.\nSzeliski, R. and Golland, P. (1999). Stereo matching with transparency and matting. Inter-\nnational Journal of Computer Vision, 32(1):45–61. Special Issue for Marr Prize papers.\nSzeliski, R. and Hinton, G. (1985). Solving random-dot stereograms using the heat equa-\ntion. In IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion (CVPR’85), pp. 284–288, San Francisco.\nSzeliski, R. and Ito, M. R. (1986). New Hermite cubic interpolator for two-dimensional\ncurve generation. IEE Proceedings E, 133(6):341–347.\nSzeliski, R. and Kang, S. B. (1994). Recovering 3D shape and motion from image streams\nusing nonlinear least squares. Journal of Visual Communication and Image Represen-\ntation, 5(1):10–28.\nSzeliski, R. and Kang, S. B.\n(1995).\nDirect methods for visual scene reconstruction.\nIn IEEE Workshop on Representations of Visual Scenes, pp. 26–33, Cambridge, Mas-\nsachusetts.\nSzeliski, R. and Kang, S. B. (1997). Shape ambiguities in structure from motion. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 19(5):506–512.\nSzeliski, R. and Lavall´ee, S. (1996). Matching 3-D anatomical surfaces with non-rigid de-\nformations using octree-splines. International Journal of Computer Vision, 18(2):171–\n186.",
  "929": "References\n907\nSzeliski, R. and Scharstein, D. (2004). Sampling the disparity space image. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 26(3):419–425.\nSzeliski, R. and Shum, H.-Y. (1996). Motion estimation with quadtree splines. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 18(12):1199–1210.\nSzeliski, R. and Shum, H.-Y. (1997). Creating full view panoramic image mosaics and\ntexture-mapped models. In ACM SIGGRAPH 1997 Conference Proceedings, pp. 251–\n258, Los Angeles.\nSzeliski, R. and Tonnesen, D. (1992). Surface modeling with oriented particle systems.\nComputer Graphics (SIGGRAPH ’92), 26(2):185–194.\nSzeliski, R. and Torr, P. (1998). Geometrically constrained structure from motion: Points\non planes. In European Workshop on 3D Structure from Multiple Images of Large-Scale\nEnvironments (SMILE), pp. 171–186, Freiburg, Germany.\nSzeliski, R. and Weiss, R. (1998). Robust shape recovery from occluding contours using a\nlinear smoother. International Journal of Computer Vision, 28(1):27–44.\nSzeliski, R., Avidan, S., and Anandan, P.\n(2000).\nLayer extraction from multiple im-\nages containing reﬂections and transparency. In IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’2000), pp. 246–253, Hilton Head\nIsland.\nSzeliski, R., Tonnesen, D., and Terzopoulos, D. (1993a). Curvature and continuity control\nin particle-based surface models. In SPIE Vol. 2031, Geometric Methods in Computer\nVision II, pp. 172–181, San Diego.\nSzeliski, R., Tonnesen, D., and Terzopoulos, D. (1993b). Modeling surfaces of arbitrary\ntopology with dynamic particles. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’93), pp. 82–87, New York.\nSzeliski, R., Uyttendaele, M., and Steedly, D. (2008). Fast Poisson Blending using Multi-\nSplines. Technical Report MSR-TR-2008-58, Microsoft Research.\nSzeliski, R., Winder, S., and Uyttendaele, M. (2010). High-quality multi-pass image re-\nsampling. Technical Report MSR-TR-2010-10, Microsoft Research.\nSzeliski, R., Zabih, R., Scharstein, D., Veksler, O., Kolmogorov, V., Agarwala, A., Tappen,\nM., and Rother, C. (2008). A comparative study of energy minimization methods for\nMarkov random ﬁelds with smoothness-based priors. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 30(6):1068–1080.\nSzummer, M. and Picard, R. W. (1996). Temporal texture modeling. In IEEE International\nConference on Image Processing (ICIP-96), pp. 823–826, Lausanne.",
  "930": "908\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTabb, M. and Ahuja, N. (1997). Multiscale image segmentation by integrated edge and\nregion detection. IEEE Transactions on Image Processing, 6(5):642–655.\nTaguchi, Y., Wilburn, B., and Zitnick, C. L. (2008). Stereo reconstruction with mixed pixels\nusing adaptive over-segmentation. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nTanaka, M. and Okutomi, M. (2008). Locally adaptive learning for translation-variant MRF\nimage priors. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2008), Anchorage, AK.\nTao, H., Sawhney, H. S., and Kumar, R. (2001). A global matching framework for stereo\ncomputation. In Eighth International Conference on Computer Vision (ICCV 2001),\npp. 532–539, Vancouver, Canada.\nTappen, M. F. (2007). Utilizing variational optimization to learn Markov random ﬁelds.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.\nTappen, M. F. and Freeman, W. T. (2003). Comparison of graph cuts with belief propaga-\ntion for stereo, using identical MRF parameters. In Ninth International Conference on\nComputer Vision (ICCV 2003), pp. 900–907, Nice, France.\nTappen, M. F., Freeman, W. T., and Adelson, E. H. (2005). Recovering intrinsic images\nfrom a single image. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n27(9):1459–1472.\nTappen, M. F., Russell, B. C., and Freeman, W. T. (2003). Exploiting the sparse derivative\nprior for super-resolution and image demosaicing. In Third International Workshop on\nStatistical and Computational Theories of Vision, Nice, France.\nTappen, M. F., Liu, C., Freeman, W., and Adelson, E. (2007). Learning Gaussian con-\nditional random ﬁelds for low-level vision. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nTardif, J.-P. (2009). Non-iterative approach for fast and accurate vanishing point detection.\nIn Twelfth International Conference on Computer Vision (ICCV 2009), Kyoto, Japan.\nTardif, J.-P., Sturm, P., and Roy, S. (2007). Plane-based self-calibration of radial distortion.\nIn Eleventh International Conference on Computer Vision (ICCV 2007), Rio de Janeiro,\nBrazil.\nTardif, J.-P., Sturm, P., Trudeau, M., and Roy, S. (2009). Calibration of cameras with\nradially symmetric distortion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 31(9):1552–1566.",
  "931": "References\n909\nTaubin, G. (1995). Curve and surface smoothing without shrinkage. In Fifth International\nConference on Computer Vision (ICCV’95), pp. 852–857, Cambridge, Massachusetts.\nTaubman, D. S. and Marcellin, M. W. (2002). Jpeg2000: standard for interactive imaging.\nProceedings of the IEEE, 90(8):1336–1357.\nTaylor, C. J. (2003). Surface reconstruction from feature based stereo. In Ninth Interna-\ntional Conference on Computer Vision (ICCV 2003), pp. 184–190, Nice, France.\nTaylor, C. J., Debevec, P. E., and Malik, J. (1996). Reconstructing polyhedral models of\narchitectural scenes from photographs. In Fourth European Conference on Computer\nVision (ECCV’96), pp. 659–668, Cambridge, England.\nTaylor, C. J., Kriegman, D. J., and Anandan, P.\n(1991).\nStructure and motion in two\ndimensions from multiple images: A least squares approach. In IEEE Workshop on\nVisual Motion, pp. 242–248, Princeton, New Jersey.\nTaylor, P. (2009). Text-to-Speech Synthesis. Cambridge University Press, Cambridge.\nTek, K. and Kimia, B. B. (2003). Symmetry maps of free-form curve segments via wave\npropagation. International Journal of Computer Vision, 54(1-3):35–81.\nTekalp, M. (1995). Digital Video Processing. Prentice Hall, Upper Saddle River, NJ.\nTelea, A. (2004). An image inpainting technique based on fast marching method. Journal\nof Graphics Tools, 9(1):23–34.\nTeller, S., Antone, M., Bodnar, Z., Bosse, M., Coorg, S., Jethwa, M., and Master, N. (2003).\nCalibrated, registered images of an extended urban area. International Journal of Com-\nputer Vision, 53(1):93–107.\nTeodosio, L. and Bender, W. (1993). Salient video stills: Content and context preserved.\nIn ACM Multimedia 93, pp. 39–46, Anaheim, California.\nTerzopoulos, D. (1983). Multilevel computational processes for visual surface reconstruc-\ntion. Computer Vision, Graphics, and Image Processing, 24:52–96.\nTerzopoulos, D. (1986a). Image analysis using multigrid relaxation methods. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, PAMI-8(2):129–139.\nTerzopoulos, D.\n(1986b).\nRegularization of inverse visual problems involving discon-\ntinuities.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-\n8(4):413–424.\nTerzopoulos, D. (1988). The computation of visible-surface representations. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, PAMI-10(4):417–438.\nTerzopoulos, D. (1999). Visual modeling for computer animation: Graphics with a vision.\nComputer Graphics, 33(4):42–45.",
  "932": "910\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTerzopoulos, D. and Fleischer, K. (1988). Deformable models. The Visual Computer,\n4(6):306–331.\nTerzopoulos, D. and Metaxas, D. (1991). Dynamic 3D models with local and global de-\nformations: Deformable superquadrics. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 13(7):703–714.\nTerzopoulos, D. and Szeliski, R. (1992). Tracking with Kalman snakes. In Blake, A. and\nYuille, A. L. (eds), Active Vision, pp. 3–20, MIT Press, Cambridge, Massachusetts.\nTerzopoulos, D. and Waters, K.\n(1990).\nAnalysis of facial images using physical and\nanatomical models. In Third International Conference on Computer Vision (ICCV’90),\npp. 727–732, Osaka, Japan.\nTerzopoulos, D. and Witkin, A. (1988). Physically-based models with rigid and deformable\ncomponents. IEEE Computer Graphics and Applications, 8(6):41–51.\nTerzopoulos, D., Witkin, A., and Kass, M. (1987). Symmetry-seeking models and 3D\nobject reconstruction. International Journal of Computer Vision, 1(3):211–221.\nTerzopoulos, D., Witkin, A., and Kass, M. (1988). Constraints on deformable models:\nRecovering 3D shape and nonrigid motion. Artiﬁcial Intelligence, 36(1):91–123.\nThayananthan, A., Iwasaki, M., and Cipolla, R. (2008). Principled fusion of high-level\nmodel and low-level cues for motion segmentation. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nThirthala, S. and Pollefeys, M. (2005). The radial trifocal tensor: A tool for calibrating\nthe radial distortion of wide-angle cameras. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’2005), pp. 321–328, San Diego, CA.\nThomas, D. B., Luk, W., Leong, P. H., and Villasenor, J. D. (2007). Gaussian random\nnumber generators. ACM Computing Surveys, 39(4).\nThrun, S., Burgard, W., and Fox, D. (2005). Probabilistic Robotics. The MIT Press,\nCambridge, Massachusetts.\nThrun, S., Montemerlo, M., Dahlkamp, H., Stavens, D., Aron, A. et al. (2006). Stanley, the\nrobot that won the DARPA Grand Challenge. Journal of Field Robotics, 23(9):661–692.\nTian, Q. and Huhns, M. N. (1986). Algorithms for subpixel registration. Computer Vision,\nGraphics, and Image Processing, 35:220–233.\nTikhonov, A. N. and Arsenin, V. Y. (1977). Solutions of Ill-Posed Problems. V. H. Winston,\nWashington, D. C.\nTipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis.\nJournal of the Royal Statistical Society, Series B, 61(3):611–622.",
  "933": "References\n911\nToint, P. L. (1987). On large scale nonlinear least squares calculations. SIAM J. Sci. Stat.\nComput., 8(3):416–435.\nTola, E., Lepetit, V., and Fua, P. (2010). DAISY: An efﬁcient dense descriptor applied to\nwide-baseline stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n32(5):815–830.\nTolliver, D. and Miller, G. (2006). Graph partitioning by spectral rounding: Applications\nin image segmentation and clustering. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR’2006), pp. 1053–1060, New York City,\nNY.\nTomasi, C. and Kanade, T. (1992). Shape and motion from image streams under orthogra-\nphy: A factorization method. International Journal of Computer Vision, 9(2):137–154.\nTomasi, C. and Manduchi, R. (1998). Bilateral ﬁltering for gray and color images. In Sixth\nInternational Conference on Computer Vision (ICCV’98), pp. 839–846, Bombay.\nTombari, F., Mattoccia, S., and Di Stefano, L. (2007). Segmentation-based adaptive support\nfor accurate stereo correspondence. In Paciﬁc-Rim Symposium on Image and Video\nTechnology.\nTombari, F., Mattoccia, S., Di Stefano, L., and Addimanda, E. (2008). Classiﬁcation and\nevaluation of cost aggregation methods for stereo correspondence. In IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR 2008), An-\nchorage, AK.\nTommasini, T., Fusiello, A., Trucco, E., and Roberto, V. (1998). Making good features\ntrack better. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’98), pp. 178–183, Santa Barbara.\nTorborg, J. and Kajiya, J. T. (1996). Talisman: Commodity realtime 3D graphics for the\nPC. In ACM SIGGRAPH 1996 Conference Proceedings, pp. 353–363, New Orleans.\nTorr, P. H. S. (2002). Bayesian model estimation and selection for epipolar geometry and\ngeneric manifold ﬁtting. International Journal of Computer Vision, 50(1):35–61.\nTorr, P. H. S. and Fitzgibbon, A. W. (2004). Invariant ﬁtting of two view geometry. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 26(5):648–650.\nTorr, P. H. S. and Murray, D. (1997). The development and comparison of robust meth-\nods for estimating the fundamental matrix. International Journal of Computer Vision,\n24(3):271–300.\nTorr, P. H. S., Szeliski, R., and Anandan, P. (1999). An integrated Bayesian approach to\nlayer extraction from image sequences. In Seventh International Conference on Com-\nputer Vision (ICCV’99), pp. 983–990, Kerkyra, Greece.",
  "934": "912\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTorr, P. H. S., Szeliski, R., and Anandan, P. (2001). An integrated Bayesian approach\nto layer extraction from image sequences. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 23(3):297–303.\nTorralba, A. (2003). Contextual priming for object detection. International Journal of\nComputer Vision, 53(2):169–191.\nTorralba, A.\n(2007).\nClassiﬁer-based methods.\nIn CVPR 2007 Short Course on\nRecognizing and Learning Object Categories.\nhttp://people.csail.mit.edu/torralba/\nshortCourseRLOC/.\nTorralba, A. (2008). Object recognition and scene understanding. MIT Course 6.870,\nhttp://people.csail.mit.edu/torralba/courses/6.870/6.870.recognition.htm.\nTorralba, A., Freeman, W. T., and Fergus, R. (2008). 80 million tiny images: a large\ndataset for non-parametric object and scene recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 30(11):1958–1970.\nTorralba, A., Murphy, K. P., and Freeman, W. T. (2004). Contextual models for object\ndetection using boosted random ﬁelds. In Advances in Neural Information Processing\nSystems.\nTorralba, A., Murphy, K. P., and Freeman, W. T. (2007). Sharing visual features for mul-\nticlass and multiview object detection.\nIEEE Transactions on Pattern Analysis and\nMachine Intelligence, 29(5):854–869.\nTorralba, A., Weiss, Y., and Fergus, R. (2008). Small codes and large databases of images\nfor object recognition. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2008), Anchorage, AK.\nTorralba, A., Murphy, K. P., Freeman, W. T., and Rubin, M. A. (2003). Context-based\nvision system for place and object recognition. In Ninth International Conference on\nComputer Vision (ICCV 2003), pp. 273–280, Nice, France.\nTorrance, K. E. and Sparrow, E. M. (1967). Theory for off-specular reﬂection from rough-\nened surfaces. Journal of the Optical Society of America A, 57(9):1105–1114.\nTorresani, L., Hertzmann, A., and Bregler, C. (2008). Non-rigid structure-from-motion:\nEstimating shape and motion with hierarchical priors. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 30(5):878–892.\nToyama, K. (1998). Prolegomena for Robust Face Tracking. Technical Report MSR-TR-\n98-65, Microsoft Research.\nToyama, K., Krumm, J., Brumitt, B., and Meyers, B. (1999). Wallﬂower: Principles and\npractice of background maintenance. In Seventh International Conference on Computer\nVision (ICCV’99), pp. 255–261, Kerkyra, Greece.",
  "935": "References\n913\nTran, S. and Davis, L. (2002). 3D surface reconstruction using graph cuts with surface con-\nstraints. In Seventh European Conference on Computer Vision (ECCV 2002), pp. 219–\n231, Copenhagen.\nTrefethen, L. N. and Bau, D. (1997). Numerical Linear Algebra. SIAM.\nTreisman, A. (1985). Preattentive processing in vision. Computer Vision, Graphics, and\nImage Processing, 31(2):156–177.\nTriggs, B. (1996). Factorization methods for projective structure and motion. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’96),\npp. 845–851, San Francisco.\nTriggs, B. (2004). Detecting keypoints with stable position, orientation, and scale un-\nder illumination changes. In Eighth European Conference on Computer Vision (ECCV\n2004), pp. 100–113, Prague.\nTriggs, B., McLauchlan, P. F., Hartley, R. I., and Fitzgibbon, A. W. (1999). Bundle adjust-\nment — a modern synthesis. In International Workshop on Vision Algorithms, pp. 298–\n372, Kerkyra, Greece.\nTrobin, W., Pock, T., Cremers, D., and Bischof, H. (2008). Continuous energy minimiza-\ntion via repeated binary fusion. In Tenth European Conference on Computer Vision\n(ECCV 2008), pp. 677–690, Marseilles.\nTroccoli, A. and Allen, P. (2008). Building illumination coherent 3D models of large-scale\noutdoor scenes. International Journal of Computer Vision, 78(2-3):261–280.\nTrottenberg, U., Oosterlee, C. W., and Schuller, A. (2000). Multigrid. Academic Press.\nTrucco, E. and Verri, A. (1998). Introductory Techniques for 3-D Computer Vision. Pren-\ntice Hall, Upper Saddle River, NJ.\nTsai, P. S. and Shah, M. (1994). Shape from shading using linear approximation. Image\nand Vision Computing, 12:487–498.\nTsai, R. Y. (1987). A versatile camera calibration technique for high-accuracy 3D machine\nvision metrology using off-the-shelf TV cameras and lenses. IEEE Journal of Robotics\nand Automation, RA-3(4):323–344.\nTschumperl´e, D. (2006). Curvature-preserving regularization of multi-valued images using\nPDEs. In Ninth European Conference on Computer Vision (ECCV 2006), pp. 295–307.\nTschumperl´e, D. and Deriche, R. (2005). Vector-valued image regularization with PDEs: A\ncommon framework for different applications. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 27:506–517.",
  "936": "914\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTsin, Y., Kang, S. B., and Szeliski, R. (2006). Stereo matching with linear superposition\nof layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(2):290–\n301.\nTsin, Y., Ramesh, V., and Kanade, T.\n(2001).\nStatistical calibration of CCD imaging\nprocess. In Eighth International Conference on Computer Vision (ICCV 2001), pp. 480–\n487, Vancouver, Canada.\nTu, Z., Chen, X., Yuille, A. L., and Zhu, S.-C. (2005). Image parsing: Unifying segmenta-\ntion, detection, and recognition. International Journal of Computer Vision, 63(2):113–\n140.\nTumblin, J. and Rushmeier, H. E. (1993). Tone reproduction for realistic images. IEEE\nComputer Graphics and Applications, 13(6):42–48.\nTumblin, J. and Turk, G. (1999). LCIS: A boundary hierarchy for detail-preserving contrast\nreduction. In ACM SIGGRAPH 1999 Conference Proceedings, pp. 83–90, Los Angeles.\nTumblin, J., Agrawal, A., and Raskar, R.\n(2005).\nWhy I want a gradient camera.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 103–110, San Diego, CA.\nTurcot, P. and Lowe, D. G. (2009). Better matching with fewer features: The selection of\nuseful features in large database recognition problems. In ICCV Workshop on Emergent\nIssues in Large Amounts of Visual Data (WS-LAVD), Kyoto, Japan.\nTurk, G. and Levoy, M. (1994). Zippered polygonal meshes from range images. In ACM\nSIGGRAPH 1994 Conference Proceedings, pp. 311–318.\nTurk, G. and O’Brien, J. (2002). Modelling with implicit surfaces that interpolate. ACM\nTransactions on Graphics, 21(4):855–873.\nTurk, M. and Pentland, A. (1991a). Eigenfaces for recognition. Journal of Cognitive\nNeuroscience, 3(1):71–86.\nTurk, M. and Pentland, A. (1991b). Face recognition using eigenfaces. In IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR’91), pp. 586–\n591, Maui, Hawaii.\nTuytelaars, T. and Mikolajczyk, K. (2007). Local invariant feature detectors. Foundations\nand Trends in Computer Graphics and Computer Vision, 3(1).\nTuytelaars, T. and Van Gool, L. (2004). Matching widely separated views based on afﬁne\ninvariant regions. International Journal of Computer Vision, 59(1):61–85.\nTuytelaars, T., Van Gool, L., and Proesmans, M. (1997). The cascaded Hough transform.\nIn International Conference on Image Processing (ICIP’97), pp. 736–739.",
  "937": "References\n915\nUllman, S. (1979). The interpretation of structure from motion. Proceedings of the Royal\nSociety of London, B-203:405–426.\nUnnikrishnan, R., Pantofaru, C., and Hebert, M. (2007). Toward objective evaluation of\nimage segmentation algorithms. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 29(6):828–944.\nUnser, M. (1999). Splines: A perfect ﬁt for signal and image processing. IEEE Signal\nProcessing Magazine, 16(6):22–38.\nUrmson, C., Anhalt, J., Bagnell, D., Baker, C., Bittner, R. et al. (2008). Autonomous\ndriving in urban environments: Boss and the urban challenge. Journal of Field Robotics,\n25(8):425–466.\nUrtasun, R., Fleet, D. J., and Fua, P. (2006). Temporal motion models for monocular\nand multiview 3D human body tracking. Computer Vision and Image Understanding,\n104(2-3):157–177.\nUyttendaele, M., Eden, A., and Szeliski, R. (2001). Eliminating ghosting and exposure\nartifacts in image mosaics. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’2001), pp. 509–516, Kauai, Hawaii.\nUyttendaele, M., Criminisi, A., Kang, S. B., Winder, S., Hartley, R., and Szeliski, R.\n(2004). Image-based interactive exploration of real-world environments. IEEE Com-\nputer Graphics and Applications, 24(3):52–63.\nVaillant, R. and Faugeras, O. D. (1992). Using extremal boundaries for 3-D object model-\ning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2):157–173.\nVaish, V., Szeliski, R., Zitnick, C. L., Kang, S. B., and Levoy, M. (2006). Reconstructing\noccluded surfaces using synthetic apertures: Shape from focus vs. shape from stereo.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2006), pp. 2331–2338, New York, NY.\nvan de Weijer, J. and Schmid, C.\n(2006).\nColoring local feature extraction.\nIn Ninth\nEuropean Conference on Computer Vision (ECCV 2006), pp. 334–348.\nvan den Hengel, A., Dick, A., Thormhlen, T., Ward, B., and Torr, P. H. S. (2007). Video-\ntrace: Rapid interactive scene modeling from video. ACM Transactions on Graphics,\n26(3).\nVan Huffel, S. and Lemmerling, P. (eds).\n(2002).\nTotal Least Squares and Errors-in-\nVariables Modeling, Springer.\nVan Huffel, S. and Vandewalle, J. (1991). The Total Least Squares Problem: Computa-\ntional Aspects and Analysis. Society for Industrial and Applied Mathematics, Philade-\nphia.",
  "938": "916\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nvan Ouwerkerk, J. D. (2006). Image super-resolution survey. Image and Vision Computing,\n24(10):1039–1052.\nVarma, M. and Ray, D. (2007). Learning the discriminative power-invariance trade-off. In\nEleventh International Conference on Computer Vision (ICCV 2007), Rio de Janeiro,\nBrazil.\nVasconcelos, N. (2007). From pixels to semantic spaces: Advances in content-based image\nretrieval. Computer, 40(7):20–26.\nVasilescu, M. A. O. and Terzopoulos, D. (2007). Multilinear (tensor) image synthesis,\nanalysis, and recognition. IEEE Signal Processing Magazine, 24(6):118–123.\nVedaldi, A. and Fulkerson, B. (2008). VLFeat: An open and portable library of computer\nvision algorithms. http://www.vlfeat.org/.\nVedaldi, A., Gulshan, V., Varma, M., and Zisserman, A. (2009). Multiple kernels for\nobject detection. In Twelfth International Conference on Computer Vision (ICCV 2009),\nKyoto, Japan.\nVedula, S., Baker, S., and Kanade, T. (2005). Image-based spatio-temporal modeling and\nview interpolation of dynamic events. ACM Transactions on Graphics, 24(2):240–261.\nVedula, S., Baker, S., Rander, P., Collins, R., and Kanade, T.\n(2005).\nThree-\ndimensional scene ﬂow. IEEE Transactions on Pattern Analysis and Machine Intel-\nligence, 27(3):475–480.\nVeeraraghavan, A., Raskar, R., Agrawal, A., Mohan, A., and Tumblin, J. (2007). Dappled\nphotography: Mask enhanced cameras for heterodyned light ﬁelds and coded aperture\nrefocusing. ACM Transactions on Graphics, 26(3).\nVeksler, O. (1999). Efﬁcient Graph-based Energy Minimization Methods in Computer\nVision. Ph.D. thesis, Cornell University.\nVeksler, O. (2001). Stereo matching by compact windows via minimum ratio cycle. In\nEighth International Conference on Computer Vision (ICCV 2001), pp. 540–547, Van-\ncouver, Canada.\nVeksler, O. (2003). Fast variable window for stereo correspondence using integral images.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2003), pp. 556–561, Madison, WI.\nVeksler, O. (2007). Graph cut based optimization for MRFs with truncated convex priors.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.",
  "939": "References\n917\nVerbeek, J. and Triggs, B. (2007). Region classiﬁcation with Markov ﬁeld aspect models.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.\nVergauwen, M. and Van Gool, L. (2006). Web-based 3D reconstruction service. Machine\nVision and Applications, 17(2):321–329.\nVetter, T. and Poggio, T. (1997). Linear object classes and image synthesis from a sin-\ngle example image. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n19(7):733–742.\nVezhnevets, V., Sazonov, V., and Andreeva, A. (2003). A survey on pixel-based skin color\ndetection techniques. In GRAPHICON03, pp. 85–92.\nVicente, S., Kolmogorov, V., and Rother, C. (2008). Graph cut based image segmentation\nwith connectivity priors. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2008), Anchorage, AK.\nVidal, R., Ma, Y., and Sastry, S. S. (2010). Generalized Principal Component Analysis.\nSpringer.\nVi´eville, T. and Faugeras, O. D. (1990). Feedforward recovery of motion and structure\nfrom a sequence of 2D-lines matches. In Third International Conference on Computer\nVision (ICCV’90), pp. 517–520, Osaka, Japan.\nVincent, L. and Soille, P. (1991). Watersheds in digital spaces: An efﬁcient algorithm\nbased on immersion simulations. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 13(6):583–596.\nVineet, V. and Narayanan, P. J. (2008). CUDA cuts: Fast graph cuts on the GPU. In CVPR\n2008 Workshop on Visual Computer Vision on GPUs (CVGPU), Anchorage, AK.\nViola, P. and Wells III, W. (1997). Alignment by maximization of mutual information.\nInternational Journal of Computer Vision, 24(2):137–154.\nViola, P., Jones, M. J., and Snow, D. (2003). Detecting pedestrians using patterns of motion\nand appearance. In Ninth International Conference on Computer Vision (ICCV 2003),\npp. 734–741, Nice, France.\nViola, P. A. and Jones, M. J. (2004). Robust real-time face detection. International Journal\nof Computer Vision, 57(2):137–154.\nVlasic, D., Baran, I., Matusik, W., and Popovi´c, J. (2008). Articulated mesh animation\nfrom multi-view silhouettes. ACM Transactions on Graphics, 27(3).\nVlasic, D., Brand, M., Pﬁster, H., and Popovi´c, J. (2005). Face transfer with multilinear\nmodels. ACM Transactions on Graphics (Proc. SIGGRAPH 2005), 24(3):426–433.",
  "940": "918\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nVogiatzis, G., Torr, P., and Cipolla, R. (2005). Multi-view stereo via volumetric graph-cuts.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2005), pp. 391–398, San Diego, CA.\nVogiatzis, G., Hernandez, C., Torr, P., and Cipolla, R. (2007). Multi-view stereo via volu-\nmetric graph-cuts and occlusion robust photo-consistency. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 29(12):2241–2246.\nvon Ahn, L. and Dabbish, L. (2004). Labeling images with a computer game. In CHI’04:\nSIGCHI Conference on Human Factors in Computing Systems, pp. 319–326, Vienna,\nAustria.\nvon Ahn, L., Liu, R., and Blum, M. (2006). Peekaboom: A game for locating objects\nin images. In CHI’06: SIGCHI Conference on Human Factors in Computing Systems,\npp. 55–64, Montr´eal, Qu´ebec, Canada.\nWainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and\nvariational inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.\nWainwright, M. J., Jaakkola, T. S., and Willsky, A. S. (2005). MAP estimation via agree-\nment on trees: message-passing and linear programming. IEEE Transactions on Infor-\nmation Theory, 51(11):3697–3717.\nWaithe, P. and Ferrie, F. (1991). From uncertainty to visual exploration. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 13(10):1038–1049.\nWalker, E. L. and Herman, M. (1988). Geometric reasoning for constructing 3D scene\ndescriptions from images. Artiﬁcial Intelligence, 37:275–290.\nWallace, G. K. (1991). The JPEG still picture compression standard. Communications of\nthe ACM, 34(4):30–44.\nWallace, J. R., Cohen, M. F., and Greenberg, D. P. (1987). A two-pass solution to the ren-\ndering equation: A synthesis of ray tracing and radiosity methods. Computer Graphics\n(SIGGRAPH ’87), 21(4):311–320.\nWaltz, D. L. (1975). Understanding line drawings of scenes with shadows. In Winston,\nP. H. (ed.), The Psychology of Computer Vision, McGraw-Hill, New York.\nWang, H. and Oliensis, J. (2010). Shape matching by segmentation averaging. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 32(4):619–635.\nWang, J. and Cohen, M. F. (2005). An iterative optimization approach for uniﬁed im-\nage segmentation and matting. In Tenth International Conference on Computer Vision\n(ICCV 2005), Beijing, China.\nWang, J. and Cohen, M. F. (2007a). Image and video matting: A survey. Foundations and\nTrends in Computer Graphics and Computer Vision, 3(2).",
  "941": "References\n919\nWang, J. and Cohen, M. F. (2007b). Optimized color sampling for robust matting. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR\n2007), Minneapolis, MN.\nWang, J. and Cohen, M. F. (2007c). Simultaneous matting and compositing. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007),\nMinneapolis, MN.\nWang, J., Agrawala, M., and Cohen, M. F. (2007). Soft scissors: An interactive tool for\nrealtime high quality matting. ACM Transactions on Graphics, 26(3).\nWang, J., Thiesson, B., Xu, Y., and Cohen, M. (2004). Image and video segmentation\nby anisotropic kernel mean shift. In Eighth European Conference on Computer Vision\n(ECCV 2004), pp. 238–249, Prague.\nWang, J., Bhat, P., Colburn, R. A., Agrawala, M., and Cohen, M. F. (2005). Video cutout.\nACM Transactions on Graphics (Proc. SIGGRAPH 2005), 24(3):585–594.\nWang, J. Y. A. and Adelson, E. H. (1994). Representing moving images with layers. IEEE\nTransactions on Image Processing, 3(5):625–638.\nWang, L., Kang, S. B., Szeliski, R., and Shum, H.-Y. (2001). Optimal texture map re-\nconstruction from multiple views. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’2001), pp. 347–354, Kauai, Hawaii.\nWang, Y. and Zhu, S.-C. (2003). Modeling textured motion: Particle, wave and sketch. In\nNinth International Conference on Computer Vision (ICCV 2003), pp. 213–220, Nice,\nFrance.\nWang, Z., Bovik, A. C., and Simoncelli, E. P. (2005). Structural approaches to image\nquality assessment. In Bovik, A. C. (ed.), Handbook of Image and Video Processing,\npp. 961–974, Elsevier Academic Press.\nWang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. (2004). Image quality as-\nsessment: From error visibility to structural similarity. IEEE Transactions on Image\nProcessing, 13(4):600–612.\nWang, Z.-F. and Zheng, Z.-G. (2008). A region based stereo matching algorithm using\ncooperative optimization. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR 2008), Anchorage, AK.\nWard, G. (1992). Measuring and modeling anisotropic reﬂection. Computer Graphics\n(SIGGRAPH ’92), 26(4):265–272.\nWard, G. (1994). The radiance lighting simulation and rendering system. In ACM SIG-\nGRAPH 1994 Conference Proceedings, pp. 459–472.",
  "942": "920\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWard, G.\n(2003).\nFast, robust image registration for compositing high dynamic range\nphotographs from hand-held exposures. Journal of Graphics Tools, 8(2):17–30.\nWard, G. (2004). High dynamic range image encodings. http://www.anyhere.com/gward/\nhdrenc/hdr encodings.html.\nWare, C., Arthur, K., and Booth, K. S. (1993). Fish tank virtual reality. In INTERCHI’03,\npp. 37–42, Amsterdam.\nWarren, J. and Weimer, H. (2001). Subdivision Methods for Geometric Design: A Con-\nstructive Approach. Morgan Kaufmann.\nWatanabe, M. and Nayar, S. K. (1998). Rational ﬁlters for passive depth from defocus.\nInternational Journal of Computer Vision, 27(3):203–225.\nWatt, A. (1995). 3D Computer Graphics. Addison-Wesley, Harlow, England, third edition.\nWeber, J. and Malik, J. (1995). Robust computation of optical ﬂow in a multi-scale differ-\nential framework. International Journal of Computer Vision, 14(1):67–81.\nWeber, M., Welling, M., and Perona, P.\n(2000).\nUnsupervised learning of models for\nrecognition. In Sixth European Conference on Computer Vision (ECCV 2000), pp. 18–\n32, Dublin, Ireland.\nWedel, A., Cremers, D., Pock, T., and Bischof, H. (2009). Structure- and motion-adaptive\nregularization for high accuracy optic ﬂow.\nIn Twelfth International Conference on\nComputer Vision (ICCV 2009), Kyoto, Japan.\nWedel, A., Rabe, C., Vaudrey, T., Brox, T., Franke, U., and Cremers, D. (2008). Efﬁcient\ndense scene ﬂow from sparse or dense stereo data. In Tenth European Conference on\nComputer Vision (ECCV 2008), pp. 739–751, Marseilles.\nWei, C. Y. and Quan, L. (2004). Region-based progressive stereo matching. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR’2005),\npp. 106–113, Washington, D. C.\nWei, L.-Y. and Levoy, M. (2000). Fast texture synthesis using tree-structured vector quan-\ntization. In ACM SIGGRAPH 2000 Conference Proceedings, pp. 479–488.\nWeickert, J. (1998). Anisotropic Diffusion in Image Processing. Tuebner, Stuttgart.\nWeickert, J., ter Haar Romeny, B. M., and Viergever, M. A. (1998). Efﬁcient and reli-\nable schemes for nonlinear diffusion ﬁltering. IEEE Transactions on Image Processing,\n7(3):398–410.\nWeinland, D., Ronfard, R., and Boyer, E. (2006). Free viewpoint action recognition using\nmotion history volumes. Computer Vision and Image Understanding, 104(2-3):249–\n257.",
  "943": "References\n921\nWeiss, Y. (1997). Smoothness in layers: Motion segmentation using nonparametric mixture\nestimation. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’97), pp. 520–526, San Juan, Puerto Rico.\nWeiss, Y. (1999). Segmentation using eigenvectors: A unifying view. In Seventh Interna-\ntional Conference on Computer Vision (ICCV’99), pp. 975–982, Kerkyra, Greece.\nWeiss, Y. (2001). Deriving intrinsic images from image sequences. In Eighth International\nConference on Computer Vision (ICCV 2001), pp. 7–14, Vancouver, Canada.\nWeiss, Y. and Adelson, E. H. (1996). A uniﬁed mixture framework for motion segmen-\ntation: Incorporating spatial coherence and estimating the number of models. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’96),\npp. 321–326, San Francisco.\nWeiss, Y. and Freeman, B. (2007). What makes a good model of natural images? In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR\n2007), Minneapolis, MN.\nWeiss, Y. and Freeman, W. T. (2001a). Correctness of belief propagation in Gaussian\ngraphical models of arbitrary topology. Neural Computation, 13(10):2173–2200.\nWeiss, Y. and Freeman, W. T. (2001b). On the optimality of solutions of the max-product\nbelief propagation algorithm in arbitrary graphs. IEEE Transactions on Information\nTheory, 47(2):736–744.\nWeiss, Y., Torralba, A., and Fergus, R. (2008). Spectral hashing. In Advances in Neural\nInformation Processing Systems.\nWeiss, Y., Yanover, C., and Meltzer, T. (2010). Linear programming and variants of belief\npropagation. In Blake, A., Kohli, P., and Rother, C. (eds), Advances in Markov Random\nFields, MIT Press.\nWells, III, W. M. (1986). Efﬁcient synthesis of Gaussian ﬁlters by cascaded uniform ﬁlters.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 8(2):234–239.\nWeng, J., Ahuja, N., and Huang, T. S. (1993). Optimal motion and structure estimation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 15(9):864–884.\nWenger, A., Gardner, A., Tchou, C., Unger, J., Hawkins, T., and Debevec, P. (2005). Per-\nformance relighting and reﬂectance transformation with time-multiplexed illumination.\nACM Transactions on Graphics (Proc. SIGGRAPH 2005), 24(3):756–764.\nWerlberger, M., Trobin, W., Pock, T., Bischof, H., Wedel, A., and Cremers, D. (2009).\nAnisotropic Huber-L1 optical ﬂow.\nIn British Machine Vision Conference (BMVC\n2009), London.",
  "944": "922\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWerner, T. (2007). A linear programming approach to max-sum problem: A review. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 29(7):1165–1179.\nWerner, T. and Zisserman, A. (2002). New techniques for automated architectural re-\nconstruction from photographs. In Seventh European Conference on Computer Vision\n(ECCV 2002), pp. 541–555, Copenhagen.\nWestin, S. H., Arvo, J. R., and Torrance, K. E. (1992). Predicting reﬂectance functions\nfrom complex surfaces. Computer Graphics (SIGGRAPH ’92), 26(4):255–264.\nWestover, L. (1989). Interactive volume rendering. In Workshop on Volume Visualization,\npp. 9–16, Chapel Hill.\nWexler, Y., Fitzgibbon, A., and Zisserman, A. (2002). Bayesian estimation of layers from\nmultiple images. In Seventh European Conference on Computer Vision (ECCV 2002),\npp. 487–501, Copenhagen.\nWexler, Y., Shechtman, E., and Irani, M. (2007). Space-time completion of video. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 29(3):463–476.\nWeyrich, T., Lawrence, J., Lensch, H. P. A., Rusinkiewicz, S., and Zickler, T. (2008).\nPrinciples of appearance acquisition and representation. Foundations and Trends in\nComputer Graphics and Computer Vision, 4(2):75–191.\nWeyrich, T., Matusik, W., Pﬁster, H., Bickel, B., Donner, C. et al. (2006). Analysis of\nhuman faces using a measurement-based skin reﬂectance model. ACM Transactions on\nGraphics, 25(3):1013–1024.\nWheeler, M. D., Sato, Y., and Ikeuchi, K. (1998). Consensus surfaces for modeling 3D\nobjects from multiple range images. In Sixth International Conference on Computer\nVision (ICCV’98), pp. 917–924, Bombay.\nWhite, R. and Forsyth, D. (2006). Combining cues: Shape from shading and texture.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2006), pp. 1809–1816, New York City, NY.\nWhite, R., Crane, K., and Forsyth, D. A. (2007). Capturing and animating occluded cloth.\nACM Transactions on Graphics, 26(3).\nWiejak, J. S., Buxton, H., and Buxton, B. F. (1985). Convolution with separable masks for\nearly image processing. Computer Vision, Graphics, and Image Processing, 32(3):279–\n290.\nWilburn, B., Joshi, N., Vaish, V., Talvala, E.-V., Antunez, E. et al. (2005). High per-\nformance imaging using large camera arrays. ACM Transactions on Graphics (Proc.\nSIGGRAPH 2005), 24(3):765–776.",
  "945": "References\n923\nWilczkowiak, M., Brostow, G. J., Tordoff, B., and Cipolla, R. (2005). Hole ﬁlling through\nphotomontage. In British Machine Vision Conference (BMVC 2005), pp. 492–501, Ox-\nford Brookes.\nWilliams, D. and Burns, P. D. (2001). Diagnostics for digital capture using MTF. In IS&T\nPICS Conference, pp. 227–232.\nWilliams, D. J. and Shah, M. (1992). A fast algorithm for active contours and curvature\nestimation. Computer Vision, Graphics, and Image Processing, 55(1):14–26.\nWilliams, L.\n(1983).\nPyramidal parametrics.\nComputer Graphics (SIGGRAPH ’83),\n17(3):1–11.\nWilliams, L.\n(1990).\nPerformace driven facial animation.\nComputer Graphics (SIG-\nGRAPH ’90), 24(4):235–242.\nWilliams, O., Blake, A., and Cipolla, R. (2003). A sparse probabilistic learning algorithm\nfor real-time tracking. In Ninth International Conference on Computer Vision (ICCV\n2003), pp. 353–360, Nice, France.\nWilliams, T. L. (1999). The Optical Transfer Function of Imaging Systems. Institute of\nPhysics Publishing, London.\nWinder, S. and Brown, M. (2007). Learning local image descriptors. In IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR 2007), Min-\nneapolis, MN.\nWinkenbach, G. and Salesin, D. H. (1994). Computer-generated pen-and-ink illustration.\nIn ACM SIGGRAPH 1994 Conference Proceedings, pp. 91–100, Orlando, Florida.\nWinn, J. and Shotton, J. (2006). The layout consistent random ﬁeld for recognizing and\nsegmenting partially occluded objects. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR’2006), pp. 37–44, New York City, NY.\nWinnem¨oller, H., Olsen, S. C., and Gooch, B. (2006). Real-time video abstraction. ACM\nTransactions on Graphics, 25(3):1221–1226.\nWinston, P. H. (ed.). (1975). The Psychology of Computer Vision, McGraw-Hill, New\nYork.\nWiskott, L., Fellous, J.-M., Kr¨uger, N., and von der Malsburg, C. (1997). Face recognition\nby elastic bunch graph matching. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 19(7):775–779.\nWitkin, A. (1981). Recovering surface shape and orientation from texture. Artiﬁcial Intel-\nligence, 17(1-3):17–45.",
  "946": "924\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nWitkin, A. (1983). Scale-space ﬁltering. In Eighth International Joint Conference on\nArtiﬁcial Intelligence (IJCAI-83), pp. 1019–1022.\nWitkin, A., Terzopoulos, D., and Kass, M. (1986). Signal matching through scale space. In\nFifth National Conference on Artiﬁcial Intelligence (AAAI-86), pp. 714–719, Philadel-\nphia.\nWitkin, A., Terzopoulos, D., and Kass, M. (1987). Signal matching through scale space.\nInternational Journal of Computer Vision, 1:133–144.\nWolberg, G. (1990). Digital Image Warping. IEEE Computer Society Press, Los Alamitos.\nWolberg, G. and Pavlidis, T. (1985). Restoration of binary images using stochastic relax-\nation with annealing. Pattern Recognition Letters, 3:375–388.\nWolff, L. B., Shafer, S. A., and Healey, G. E. (eds). (1992a). Radiometry. Physics-Based\nVision: Principles and Practice, Jones & Bartlett, Cambridge, MA.\nWolff, L. B., Shafer, S. A., and Healey, G. E. (eds). (1992b). Shape Recovery. Physics-\nBased Vision: Principles and Practice, Jones & Bartlett, Cambridge, MA.\nWood, D. N., Finkelstein, A., Hughes, J. F., Thayer, C. E., and Salesin, D. H. (1997).\nMultiperspective panoramas for cel animation. In ACM SIGGRAPH 1997 Conference\nProceedings, pp. 243–250, Los Angeles.\nWood, D. N., Azuma, D. I., Aldinger, K., Curless, B., Duchamp, T., Salesin, D. H., and\nStuetzle, W. (2000). Surface light ﬁelds for 3D photography. In ACM SIGGRAPH\n2000 Conference Proceedings, pp. 287–296.\nWoodford, O., Reid, I., Torr, P. H., and Fitzgibbon, A. (2008). Global stereo reconstruc-\ntion under second order smoothness priors. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nWoodham, R. J. (1981). Analysing images of curved surfaces. Artiﬁcial Intelligence,\n17:117–140.\nWoodham, R. J. (1994). Gradient and curvature from photometric stereo including local\nconﬁdence estimation. Journal of the Optical Society of America, A, 11:3050–3068.\nWren, C. R., Azarbayejani, A., Darrell, T., and Pentland, A. P. (1997). Pﬁnder: Real-\ntime tracking of the human body. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 19(7):780–785.\nWright, S. (2006). Digital Compositing for Film and Video. Focal Press, 2nd edition.\nWu, C.\n(2010).\nSiftGPU: A GPU implementation of scale invariant feature transform\n(SIFT). http://www.cs.unc.edu/∼ccwu/siftgpu/.",
  "947": "References\n925\nWyszecki, G. and Stiles, W. S. (2000). Color Science: Concepts and Methods, Quantitative\nData and Formulae. John Wiley & Sons, New York, 2nd edition.\nXiao, J. and Shah, M. (2003). Two-frame wide baseline matching. In Ninth International\nConference on Computer Vision (ICCV 2003), pp. 603–609, Nice, France.\nXiao, J. and Shah, M.\n(2005).\nMotion layer extraction in the presence of occlusion\nusing graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n27(10):1644–1659.\nXiong, Y. and Turkowski, K. (1997). Creating image-based VR using a self-calibrating\nﬁsheye lens. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’97), pp. 237–243, San Juan, Puerto Rico.\nXiong, Y. and Turkowski, K. (1998). Registration, calibration and blending in creating high\nquality panoramas. In IEEE Workshop on Applications of Computer Vision (WACV’98),\npp. 69–74, Princeton.\nXu, L., Chen, J., and Jia, J. (2008). A segmentation based variational model for accurate\noptical ﬂow estimation. In Tenth European Conference on Computer Vision (ECCV\n2008), pp. 671–684, Marseilles.\nYang, D., El Gamal, A., Fowler, B., and Tian, H. (1999). A 640x512 CMOS image sensor\nwith ultra-wide dynamic range ﬂoating-point pixel level ADC. IEEE Journal of Solid\nState Circuits, 34(12):1821–1834.\nYang, L. and Albregtsen, F. (1996). Fast and exact computation of Cartesian geometric\nmoments using discrete Green’s theorem. Pattern Recognition, 29(7):1061–1073.\nYang, L., Meer, P., and Foran, D. (2007). Multiple class segmentation using a uniﬁed\nframework over mean-shift patches. In IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2007), Minneapolis, MN.\nYang, L., Jin, R., Sukthankar, R., and Jurie, F. (2008). Unifying discriminative visual code-\nbook generation with classiﬁer training for object category recognition. In IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2008),\nAnchorage, AK.\nYang, M.-H., Ahuja, N., and Tabb, M. (2002). Extraction of 2D motion trajectories and\nits application to hand gesture recognition. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 24(8):1061–1074.\nYang, M.-H., Kriegman, D. J., and Ahuja, N. (2002). Detecting faces in images: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 24(1):34–58.\nYang, Q., Wang, L., Yang, R., Stew´enius, H., and Nist´er, D. (2009). Stereo matching\nwith color-weighted correlation, hierarchical belief propagation and occlusion handling.",
  "948": "926\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 31(3):492–504.\nYang, Y., Yuille, A., and Lu, J. (1993). Local, global, and multilevel stereo matching.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’93), pp. 274–279, New York.\nYanover, C., Meltzer, T., and Weiss, Y. (2006). Linear programming relaxations and belief\npropagation — an empirical study. Journal of Machine Learning Research, 7:1887–\n1907.\nYao, B. Z., Yang, X., Lin, L., Lee, M. W., and Zhu, S.-C. (2010). I2T: Image parsing to\ntext description. Proceedings of the IEEE, 98(8):1485–1508.\nYaou, M.-H. and Chang, W.-T. (1994). Fast surface interpolation using multiresolution\nwavelets. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(7):673–\n689.\nYatziv, L. and Sapiro, G. (2006). Fast image and video colorization using chrominance\nblending. IEEE Transactions on Image Processing, 15(5):1120–1129.\nYedidia, J. S., Freeman, W. T., and Weiss, Y. (2001). Understanding belief propagation and\nits generalization. In International Joint Conference on Artiﬁcial Intelligence (IJCAI\n2001).\nYezzi, Jr., A. J., Kichenassamy, S., Kumar, A., Olver, P., and Tannenbaum, A. (1997). A\ngeometric snake model for segmentation of medical imagery. IEEE Transactions on\nMedical Imaging, 16(2):199–209.\nYilmaz, A. and Shah, M. (2006). Matching actions in presence of camera motion. Com-\nputer Vision and Image Understanding, 104(2-3):221–231.\nYilmaz, A., Javed, O., and Shah, M. (2006). Object tracking: A survey. ACM Computing\nSurveys, 38(4).\nYin, P., Criminisi, A., Winn, J., and Essa, I. (2007). Tree-based classiﬁers for bilayer video\nsegmentation. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2007), Minneapolis, MN.\nYoon, K.-J. and Kweon, I.-S.\n(2006).\nAdaptive support-weight approach for corre-\nspondence search. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n28(4):650–656.\nYserentant, H. (1986). On the multi-level splitting of ﬁnite element spaces. Numerische\nMathematik, 49:379–412.\nYu, S. X. and Shi, J. (2003). Multiclass spectral clustering. In Ninth International Confer-\nence on Computer Vision (ICCV 2003), pp. 313–319, Nice, France.",
  "949": "References\n927\nYu, Y. and Malik, J. (1998). Recovering photometric properties of architectural scenes\nfrom photographs. In ACM SIGGRAPH 1996 Conference Proceedings, pp. 207–218,\nOrlando.\nYu, Y., Debevec, P., Malik, J., and Hawkins, T. (1999). Inverse global illumination: Recov-\nering reﬂectance models of real scenes from photographs. In ACM SIGGRAPH 1999\nConference Proceedings, pp. 215–224.\nYuan, L., Sun, J., Quan, L., and Shum, H.-Y. (2007). Image deblurring with blurred/noisy\nimage pairs. ACM Transactions on Graphics, 26(3).\nYuan, L., Sun, J., Quan, L., and Shum, H.-Y. (2008). Progressive inter-scale and intra-scale\nnon-blind image deconvolution. ACM Transactions on Graphics, 27(3).\nYuan, L., Wen, F., Liu, C., and Shum, H.-Y. (2004). Synthesizing dynamic texture with\nclosed-loop linear dynamic system. In Eighth European Conference on Computer Vi-\nsion (ECCV 2004), pp. 603–616, Prague.\nYuille, A. (1991). Deformable templates for face recognition. Journal of Cognitive Neuro-\nscience, 3(1):59–70.\nYuille, A. (2002). CCCP algorithms to minimize the Bethe and Kikuchi free energies:\nConvergent alternatives to belief propagation. Neural Computation, 14(7):1691–1722.\nYuille, A. (2010). Loopy belief propagation, mean-ﬁeld and Bethe approximations. In\nBlake, A., Kohli, P., and Rother, C. (eds), Advances in Markov Random Fields, MIT\nPress.\nYuille, A. and Poggio, T. (1984). A Generalized Ordering Constraint for Stereo Corre-\nspondence. A. I. Memo 777, Artiﬁcial Intelligence Laboratory, Massachusetts Institute\nof Technology.\nYuille, A., Vincent, L., and Geiger, D. (1992). Statistical morphology and Bayesian recon-\nstruction. Journal of Mathematical Imaging and Vision, 1(3):223–238.\nZabih, R. and Woodﬁll, J. (1994). Non-parametric local transforms for computing vi-\nsual correspondence. In Third European Conference on Computer Vision (ECCV’94),\npp. 151–158, Stockholm, Sweden.\nZach, C. (2008). Fast and high quality fusion of depth maps. In Fourth International Sym-\nposium on 3D Data Processing, Visualization and Transmission (3DPVT’08), Atlanta.\nZach, C., Gallup, D., and Frahm, J.-M. (2008). Fast gain-adaptive KLT tracking on the\nGPU. In CVPR 2008 Workshop on Visual Computer Vision on GPUs (CVGPU), An-\nchorage, AK.",
  "950": "928\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nZach, C., Klopschitz, M., and Pollefeys, M. (2010). Disambiguating visual relations us-\ning loop constraints. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2010), San Francisco, CA.\nZach, C., Pock, T., and Bischof, H. (2007a). A duality based approach for realtime TV-L1\noptical ﬂow. In Pattern Recognition (DAGM 2007).\nZach, C., Pock, T., and Bischof, H. (2007b). A globally optimal algorithm for robust TV-\nL1 range image integration. In Eleventh International Conference on Computer Vision\n(ICCV 2007), Rio de Janeiro, Brazil.\nZanella, V. and Fuentes, O. (2004). An approach to automatic morphing of face images\nin frontal view. In Mexican International Conference on Artiﬁcial Intelligence (MICAI\n2004), pp. 679–687, Mexico City.\nZebedin, L., Bauer, J., Karner, K., and Bischof, H. (2008). Fusion of feature- and area-\nbased information for urban buildings modeling from aerial imagery. In Tenth European\nConference on Computer Vision (ECCV 2008), pp. 873–886, Marseilles.\nZelnik-Manor, L. and Perona, P. (2007). Automating joiners. In Symposium on Non Pho-\ntorealistic Animation and Rendering, Annecy.\nZhang, G., Jia, J., Wong, T.-T., and Bao, H. (2008). Recovering consistent video depth\nmaps via bundle optimization. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nZhang, J., McMillan, L., and Yu, J. (2006). Robust tracking and stereo matching under\nvariable illumination. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’2006), pp. 871–878, New York City, NY.\nZhang, J., Marszalek, M., Lazebnik, S., and Schmid, C. (2007). Local features and kernels\nfor classiﬁcation of texture and object categories: a comprehensive study. International\nJournal of Computer Vision, 73(2):213–238.\nZhang, L., Curless, B., and Seitz, S. (2003). Spacetime stereo: Shape recovery for dy-\nnamic scenes. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’2003), pp. 367–374, Madison, WI.\nZhang, L., Dugas-Phocion, G., Samson, J.-S., and Seitz, S. M. (2002). Single view model-\ning of free-form scenes. Journal of Visualization and Computer Animation, 13(4):225–\n235.\nZhang, L., Snavely, N., Curless, B., and Seitz, S. M. (2004). Spacetime faces: High resolu-\ntion capture for modeling and animation. ACM Transactions on Graphics, 23(3):548–\n558.",
  "951": "References\n929\nZhang, R., Tsai, P.-S., Cryer, J. E., and Shah, M. (1999). Shape from shading: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 21(8):690–706.\nZhang, Y. and Kambhamettu, C. (2003). On 3D scene ﬂow and structure recovery from\nmultiview image sequences. IEEE Transactions on Systems, Man, and Cybernetics,\n33(4):592–606.\nZhang, Z. (1994). Iterative point matching for registration of free-form curves and surfaces.\nInternational Journal of Computer Vision, 13(2):119–152.\nZhang, Z. (1998a). Determining the epipolar geometry and its uncertainty: A review.\nInternational Journal of Computer Vision, 27(2):161–195.\nZhang, Z. (1998b). On the optimization criteria used in two-view motion analysis. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 20(7):717–729.\nZhang, Z. (2000). A ﬂexible new technique for camera calibration. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 22(11):1330–1334.\nZhang, Z. and He, L.-W. (2007). Whiteboard scanning and image enhancement. Digital\nSignal Processing, 17(2):414–432.\nZhang, Z. and Shan, Y. (2000). A progressive scheme for stereo matching. In Second\nEuropean Workshop on 3D Structure from Multiple Images of Large-Scale Environments\n(SMILE 2000), pp. 68–85, Dublin, Ireland.\nZhang, Z., Deriche, R., Faugeras, O., and Luong, Q. (1995). A robust technique for match-\ning two uncalibrated images through the recovery of the unknown epipolar geometry.\nArtiﬁcial Intelligence, 78:87–119.\nZhao, G. and Pietik¨ainen, M. (2007). Dynamic texture recognition using local binary pat-\nterns with an application to facial expressions. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 29(6):915–928.\nZhao, W., Chellappa, R., Phillips, P. J., and Rosenfeld, A. (2003). Face recognition: A\nliterature survey. ACM Computing Surveys, 35(4):399–358.\nZheng, J. Y. (1994). Acquiring 3-D models from sequences of contours. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 16(2):163–178.\nZheng, K. C., Kang, S. B., Cohen, M., and Szeliski, R. (2007). Layered depth panoramas.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2007), Minneapolis, MN.\nZheng, Y., Lin, S., and Kang, S. B.\n(2006).\nSingle-image vignetting correction.\nIn\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’2006), pp. 461–468, New York City, NY.",
  "952": "930\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nZheng, Y., Yu, J., Kang, S.-B., Lin, S., and Kambhamettu, C. (2008). Single-image vi-\ngnetting correction using radial gradient symmetry. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR 2008), Anchorage, AK.\nZheng, Y., Zhou, X. S., Georgescu, B., Zhou, S. K., and Comaniciu, D. (2006). Example\nbased non-rigid shape detection. In Ninth European Conference on Computer Vision\n(ECCV 2006), pp. 423–436.\nZheng, Y.-T., Zhao, M., Song, Y., Adam, H., Buddemeier, U., Bissacco, A., Brucher, F.,\nChua, T.-S., and Neven, H. (2009). Tour the world: building a web-scale landmark\nrecognition engine. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2009), Miami Beach, FL.\nZhong, J. and Sclaroff, S. (2003). Segmenting foreground objects from a dynamic, textured\nbackground via a robust Kalman ﬁlter. In Ninth International Conference on Computer\nVision (ICCV 2003), pp. 44–50, Nice, France.\nZhou, C., Lin, S., and Nayar, S. (2009). Coded aperture pairs for depth from defocus. In\nTwelfth International Conference on Computer Vision (ICCV 2009), Kyoto, Japan.\nZhou, Y., Gu, L., and Zhang, H.-J. (2003). Bayesian tangent shape model: Estimating\nshape and pose parameters via Bayesian inference. In IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (CVPR’2003), pp. 109–116, Madi-\nson, WI.\nZhu, L., Chen, Y., Lin, Y., Lin, C., and Yuille, A. (2008). Recursive segmentation and\nrecognition templates for 2D parsing. In Advances in Neural Information Processing\nSystems.\nZhu, S.-C. and Mumford, D. (2006). A stochastic grammar of images. Foundations and\nTrends in Computer Graphics and Computer Vision, 2(4).\nZhu, S. C. and Yuille, A. L. (1996). Region competition: Unifying snakes, region growing,\nand Bayes/MDL for multiband image segmentation.\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 18(9):884–900.\nZhu, Z. and Kanade, T. (2008). Modeling and representations of large-scale 3D scenes.\nInternational Journal of Computer Vision, 78(2-3):119–120.\nZisserman, A., Giblin, P. J., and Blake, A. (1989). The information available to a moving\nobserver from specularities. Image and Vision Computing, 7(1):38–42.\nZitnick, C. L. and Kanade, T. (2000). A cooperative algorithm for stereo matching and\nocclusion detection. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n22(7):675–684.",
  "953": "References\n931\nZitnick, C. L. and Kang, S. B. (2007). Stereo for image-based rendering using image\nover-segmentation. International Journal of Computer Vision, 75(1):49–65.\nZitnick, C. L., Jojic, N., and Kang, S. B. (2005). Consistent segmentation for optical\nﬂow estimation. In Tenth International Conference on Computer Vision (ICCV 2005),\npp. 1308–1315, Beijing, China.\nZitnick, C. L., Kang, S. B., Uyttendaele, M., Winder, S., and Szeliski, R. (2004). High-\nquality video view interpolation using a layered representation. ACM Transactions on\nGraphics (Proc. SIGGRAPH 2004), 23(3):600–608.\nZitov’aa, B. and Flusser, J. (2003). Image registration methods: A survey. Image and\nVision Computing, 21:997–1000.\nZoghlami, I., Faugeras, O., and Deriche, R. (1997). Using geometric corners to build a\n2D mosaic from a set of images. In IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’97), pp. 420–425, San Juan, Puerto Rico.\nZongker, D. E., Werner, D. M., Curless, B., and Salesin, D. H. (1999). Environment matting\nand compositing. In ACM SIGGRAPH 1999 Conference Proceedings, pp. 205–214.\nZorin, D., Schr¨oder, P., and Sweldens, W. (1996). Interpolating subdivision for meshes\nwith arbitrary topology. In ACM SIGGRAPH 1997 Conference Proceedings, pp. 189–\n192, New Orleans.",
  "954": "932\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)",
  "955": "Index\n3D Rotations, see Rotations\n3D alignment, 320\nabsolute orientation, 320, 588\northogonal Procrustes, 320\n3D photography, 613\n3D video, 643\nAbsolute orientation, 320, 588\nActive appearance model (AAM), 680\nActive contours, 270\nActive illumination, 585\nActive rangeﬁnding, 585\nActive shape model (ASM), 276, 680\nActivity recognition, 610\nAdaptive smoothing, 127\nAfﬁne transforms, 37, 40\nAfﬁnities (segmentation), 296\nnormalizing, 297\nAlgebraic multigrid, 288\nAlgorithms\ntesting, viii\nAliasing, 77, 476\nAlignment, see Image alignment\nAlpha\nopacity, 106\npre-multiplied, 106\nAlpha matte, 105\nAmbient illumination, 65\nAnalog to digital conversion (ADC), 77\nAnisotropic diffusion, 127\nAnisotropic ﬁltering, 168\nAnti-aliasing ﬁlter, 78, 476\nAperture, 69\nAperture problem, 394\nApplications, 5\n3D model reconstruction, 362, 371\n3D photography, 613\naugmented reality, 326, 368\nautomotive safety, 5\nbackground replacement, 558\nbiometrics, 668\ncolorization, 504\nde-interlacing, 415\ndigital heritage, 590\ndocument scanning, 432\nedge editing, 249\nfacial animation, 603\nﬂash photography, 494\nframe interpolation, 418\ngaze correction, 552\nhead tracking, 551\nhole ﬁlling, 521\nimage restoration, 192\nimage search, 717",
  "956": "934\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nindustrial, 7\nintelligent photo editing, 709\nInternet photos, 371\nlocation recognition, 693\nmachine inspection, 5\nmatch move, 368\nmedical imaging, 5, 304, 408\nmorphing, 173\nmosaic-based video compression, 436\nnon-photorealistic rendering, 522\nOptical character recognition (OCR), 5\npanography, 314\nperformance-driven animation, 237\nphoto pop-up, 710\nPhoto Tourism, 624\nPhotomontage, 459\nplanar pattern tracking, 326\nrotoscoping, 282\nscene completion, 709\nscratch removal, 521\nsingle view reconstruction, 331\ntonal adjustment, 111\nvideo denoising, 414\nvideo stabilization, 401\nvideo summarization, 436\nvideo-based walkthroughs, 645\nVideoMouse, 326\nview morphing, 357\nvisual effects, 5\nwhiteboard scanning, 432\nz-keying, 558\nArc length parameterization of a curve, 246\nArchitectural reconstruction, 598\nArea statistics, 132\nmean (centroid), 132\nperimeter, 132\nsecond moment (inertia), 132\nAspect ratio, 52, 53\nAugmented reality, 326, 338, 368\nAuto-calibration, 355\nAutomatic gain control (AGC), 76\nAxis/angle representation of rotations, 41\nB-snake, 273\nB-spline, 171, 172, 250, 273, 279, 408\ncubic, 146\nmultilevel, 592\noctree, 597\nBackground plate, 518\nBackground subtraction (maintenance), 606\nBag of words (keypoints), 697, 727\ndistance metrics, 698\nBand-pass ﬁlter, 118\nBartlett ﬁlter, see Bilinear kernel\nBayer pattern (RGB sensor mosaic), 85\ndemosaicing, 86, 502\nBayes’ rule, 141, 180, 762\nMAP (maximum a posteriori) estimate,\n763\nposterior distribution, 762\nBayesian modeling, 180, 762\nMAP estimate, 180, 763\nmatting, 510\nposterior distribution, 180, 762\nprior distribution, 180, 762\nuncertainty, 180\nBelief propagation (BP), 185, 768\nupdate rule, 769\nBias, 104, 386\nBidirectional Reﬂectance Distribution Func-\ntion, see BRDF\nBilateral ﬁlter, 125\njoint, 496\nrange kernel, 125\ntone mapping, 489\nBilinear blending, 110",
  "957": "Index\n935\nBilinear kernel, 117\nBiometrics, 668\nBipartite problem, 364\nBlind image deconvolution, 498\nBlock-based motion estimation\n(block matching), 387\nBlocks world, 11\nBlue screen matting, 106, 195, 507\nBlur kernel, 69\nestimation, 476, 528\nBlur removal, 144, 197\nBody color, 63\nBoltzmann distribution, 181, 763\nBoosting, 663\nAdaBoost algorithm, 665\ndecision stump, 663\nweak learner, 663\nBorder (boundary) effects, 114, 196\nBoundary detection, 244\nBox ﬁlter, 117\nBoxlet, 121\nBRDF, 62\nanisotropic, 62\nisotropic, 62\nrecovery, 612\nspatially varying (SVBRDF), 612\nBrightness, 104\nBrightness constancy, 3, 384\nBrightness constancy constraint, 384, 393,\n410\nBundle adjustment, 363\nCalibration, see Camera calibration\nCalibration matrix, 51\nCamera calibration, 50, 97\naccuracy, 340\naliasing, 476\nextrinsic, 51, 321\nintrinsic, 50, 327\noptical blur, 476, 528\npatterns, 327\nphotometric, 470\nplumb-line method, 335, 341\npoint spread function, 476, 528\nradial distortion, 334\nradiometric, 470, 481, 526\nrotational motion, 332, 339\nslant edge, 476\nvanishing points, 329\nvignetting, 474\nCamera matrix, 51, 54\nCatadioptric optics, 71\nCategory-level recognition, 696\nbag of words, 697, 727\ndata sets, 718\npart-based, 701\nsegmentation, 704\nsurveys, 723\nCCD, 74\nblooming, 74\nCentral difference, 118\nChained transformations, 325, 364\nChamfer matching, 129\nCharacteristic function, 131, 281, 590, 596\nCharacteristic polynomial, 740\nChirality, 347, 351\nCholesky factorization, 741\nalgorithm, 741\nincomplete, 752\nsparse, 749\nChromatic aberration, 71, 342\nChromaticity coordinates, 83\nCIE L*a*b*, see Color\nCIE L*u*v*, see Color\nCIE XYZ, see Color\nCircle of confusion, 69",
  "958": "936\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nCLAHE, see Histogram equalization\nClustering\nagglomerative, 286\ncluster analysis, 269, 305\ndivisive, 286\nCMOS, 74\nCo-vector, 37\nCoefﬁcient matrix, 177\nCollineation, 40\nColor, 80\nbalance, 86, 97, 194\ncamera, 84\ndemosaicing, 86, 502\nfringing, 504\nhue, saturation, value (HSV), 90\nL*a*b*, 83\nL*u*v*, 84, 289\nprimaries, 81\nproﬁle, 473\nratios, 90\nRGB, 81\ntransform, 104\ntwist, 86, 105\nXYZ, 81\nYIQ, 88\nYUV, 88\nColor ﬁlter array (CFA), 85, 502\nColor line model, 513\nColorChecker chart, 473\nColorization, 504\nCompositing, 105, 192, 195\nimage stitching, 450\nopacity, 106\nover operator, 106\nsurface, 451\ntransparency, 106\nCompression, 90\nComputational photography, 467\nactive illumination, 496\nﬂash and non-ﬂash, 494\nhigh dynamic range, 479\nreferences, 469, 524\ntone mapping, 487\nConcentric mosaic, 437, 634\nCONDENSATION, 279\nCondition number, 750\nConditional random ﬁeld (CRF), 188, 553,\n708\nConfusion matrix (table), 226\nConic section, 33\nConjugate gradient descent (CG), 749\nalgorithm, 751\nnon-linear, 750\npreconditioned, 751\nConnected components, 131, 198\nConstellation model, 704\nContent based image retrieval (CBIR), 717\nContinuation method, 179\nContour\narc length parameterization, 246\nchain code, 246\nmatching, 248, 263\nsmoothing, 248\nContrast, 104\nControlled-continuity spline, 175\nConvolution, 112\nkernel, 111\nmask, 111\nsuperposition, 112\nCoring, 153, 201\nCorrelation, 111, 386\nwindowed, 390\nCorrespondence map, 398\nCramer–Rao lower bound, 320, 397, 775\nCube map\nHough transform, 253",
  "959": "Index\n937\nimage stitching, 451\nCurve\narc length parameterization, 246\nevolution, 248\nmatching, 248\nsmoothing, 248\nCylindrical coordinates, 438\nData energy (term), 181, 763\nData sets and test databases, 778\nrecognition, 718\nDe-interlacing, 415\nDecimation, 148\nDecimation kernels\nbicubic, 150\nbinomial, 148, 150\nQMF, 150\nwindowed sinc, 148\nDemosaicing (Bayer), 86, 502\nDepth from defocus, 584\nDepth map, see Disparity map\nDepth of ﬁeld, 69, 95\nDi-chromatic reﬂection model, 67\nDifference matting (keying), 106, 195, 508,\n606\nDifference of Gaussians (DoG), 152\nDifference of low-pass (DOLP), 152\nDiffuse reﬂection, 63\nDiffusion\nanisotropic, 127\nDigital camera, 73\ncolor, 84\ncolor ﬁlter array (CFA), 85\ncompression, 90\nDirect current (DC), 92\nDirect linear transform (DLT), 322\nDirect sparse matrix techniques, 747\nDirectional derivative, 119\nselectivity, 120\nDiscrete cosine transform (DCT), 91, 142\nDiscrete Fourier transform (DFT), 134\nDiscriminative random ﬁeld (DRF), 190\nDisparity, 49, 539\nDisparity map, 540, 562\nmultiple, 561\nDisparity space image (DSI), 540\ngeneralized, 542\nDisplaced frame difference (DFD), 384\nDisplacement ﬁeld, 170\nDistance from face space (DFFS), 672\nDistance in face space (DIFS), 672\nDistance map, see Distance transform\nDistance transform, 129, 198\nEuclidean, 129\nimage stitching, 455\nManhattan (city block), 129\nsigned, 130\nDomain (of a function), 103\nDomain scaling law, 167\nDownsampling, see Decimation\nDynamic programming (DP), 554, 766\nmonotonicity, 556\nordering constraint, 556\nscanline optimization, 556\nDynamic snake, 276\nDynamic texture, 642\nEarth mover’s distance (EMD), 698\nEdge detection, 238, 261\nboundary detection, 244\nCanny, 239\nchain code, 246\ncolor, 243\nDifference of Gaussian, 240\nedgel (edge element), 241\nhysteresis, 246",
  "960": "938\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLaplacian of Gaussian, 240\nlinking, 244, 262\nmarching cubes, 241\nscale selection, 242\nsteerable ﬁlter, 241\nzero crossing, 241\nEigenface, 671\nEigenvalue decomposition, 275, 671, 737\nEigenvector, 737\nElastic deformations, 408\nimage registration, 408\nElastic nets, 272\nElliptical weighted average (EWA), 168\nEnvironment map, 61, 633\nEnvironment matte, 634\nEpanechnikov kernel, 294\nEpipolar constraint, 348\nEpipolar geometry, 348, 537\npure rotation, 353\npure translation, 352\nEpipolar line, 537\nEpipolar plane, 537, 544\nimage (EPI), 559, 629\nEpipolar volume, 629\nEpipole, 348, 537\nError rates\naccuracy (ACC), 229\nfalse negative (FN), 226\nfalse positive (FP), 226\npositive predictive value (PPV), 229\nprecision, 229\nrecall, 229\nROC curve, 229\ntrue negative (TN), 226\ntrue positive (TP), 226\nErrors-in-variable model, 442, 744\nheteroscedastic, 746\nEssential matrix, 348\n5-point algorithm, 352\neight-point algorithm, 349\nre-normalization, 350\nseven-point algorithm, 350\ntwisted pair, 351\nEstimation theory, 757\nEuclidean transformation, 36, 40\nEuler angles, 41\nExpectation maximization (EM), 291\nExponential twist, 43\nExposure bracketing, 480\nExposure value (EV), 70, 470\nF-number (stop), 69, 95\nFace detection, 658\nboosting, 663\ncascade of classiﬁers, 664\nclustering and PCA, 660\ndata sets, 718\nneural networks, 661\nsupport vector machines, 662\nFace modeling, 601\nFace recognition, 668\nactive appearance model, 680\ndata sets, 718\neigenface, 671\nelastic bunch graph matching, 679\nlocal binary patterns (LBP), 722\nlocal feature analysis, 679\nFace transfer, 639\nFacial motion capture, 603, 605, 639\nFactor graph, 181, 764, 768\nFactorization, 15, 357\nmissing data, 360\nprojective, 360\nFast Fourier transform (FFT), 134\nFast marching method (FMM), 282\nFeature descriptor, 222, 260",
  "961": "Index\n939\nbias and gain normalization, 222\nGLOH, 223\npatch, 222\nPCA-SIFT, 223\nperformance (evaluation), 224\nquantization, 234, 691, 698\nSIFT, 223\nsteerable ﬁlter, 224\nFeature detection, 207, 209, 259\nAdaptive non-maximal suppression, 215\nafﬁne invariance, 219\nauto-correlation, 210\nF¨orstner, 212\nHarris, 212\nLaplacian of Gaussian, 217\nMSER, 220\nregion, 221\nrepeatability, 215\nrotation invariance, 218\nscale invariance, 216\nFeature matching, 207, 225, 261\ndensiﬁcation, 234\nefﬁciency, 232\nerror rates, 226\nhashing, 232\nindexing structure, 232\nk-d trees, 233\nlocality sensitive hashing, 233\nnearest neighbor, 229\nstrategy, 226\nveriﬁcation, 234\nFeature tracking, 235, 261\nafﬁne, 235\nlearning, 236\nFeature tracks, 357, 371\nFeature-based alignment, 311\n2D, 311\n3D, 320\niterative, 315\nJacobian, 312\nleast squares, 312\nmatch veriﬁcation, 686\nRANSAC, 318\nrobust, 318\nField of Experts (FoE), 186\nFill factor, 75\nFill-in, 366, 748\nFilter\nadaptive, 127\nband-pass, 118\nbilateral, 125\ndirectional derivative, 119\nedge-preserving, 124, 127\nLaplacian of Gaussian, 119\nmedian, 124\nmoving average, 117\nnon-linear, 122\nseparable, 115, 197\nsteerable, 119, 198\nFilter coefﬁcients, 111\nFilter kernel, see Kernel\nFinding faces, see Face detection\nFinite element analysis, 176\nstiffness matrix, 177\nFinite impulse response (FIR) ﬁlter, 111, 122\nFisher information matrix, 313, 320, 758,\n775\nFisher’s linear discriminant (FLD), 676\nFisheye lens, 59\nFlash and non-ﬂash merging, 494\nFlash matting, 517\nFlip-book animation, 336\nFlying spot scanner, 587\nFocal length, 52, 53, 69\nFocus, 69\nshape-from, 584, 616",
  "962": "940\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nFocus of expansion (FOE), 352\nForm factor, 68\nForward mapping, see Forward warping\nForward warping, 164, 202\nFourier transform, 132, 198\ndiscrete, 134\nexamples, 136\nmagnitude (gain), 133\npairs, 136\nParseval’s Theorem, 136\nphase (shift), 133\npower spectrum, 140\nproperties, 134\ntwo-dimensional, 140\nFourier-based motion estimation, 388\nrotations and scale, 391\nFrame interpolation, 418\nFree-viewpoint video, 644\nFundamental matrix, 353\nestimation, see Essential matrix\nFundamental radiometric relation, 73\nGain, 104, 386\nGamma, 104\nGamma correction, 87, 96\nGap closing (image stitching), 435\nGarbage matte, 518\nGaussian kernel, 117\nGaussian Markov random ﬁeld (GMRF),\n184, 191, 499\nGaussian mixtures, see Mixture of Gaussians\nGaussian pyramid, 150\nGaussian scale mixtures (GSM), 186\nGaze correction, 552\nGeman–McClure function, 385\nGeneralized cylinders, 12, 588, 593\nGeodesic active contour, 282\nGeodesic distance (segmentation), 304\nGeometric image formation, 31\nGeometric lens aberrations, 70\nGeometric primitives, 32\nhomogeneous coordinates, 32\nlines, 32, 34\nnormal vector, 32\nnormal vectors, 34\nplanes, 33\npoints, 32, 33\nGeometric transformations\n2D, 35, 163\n3D, 39\n3D perspective, 40\n3D rotations, 41\nafﬁne, 37, 40\nbilinear, 39\ncalibration matrix, 51\ncollineation, 40\nEuclidean, 36, 40\nforward warping, 164, 202\nhierarchy, 37\nhomography, 37, 40, 56, 431\ninverse warping, 165\nperspective, 37\nprojections, 46\nprojective, 37\nrigid body, 36, 40\nscaled rotation, 36, 40\nsimilarity, 36, 40\ntranslation, 36, 39\nGeometry image, 594\nGesture recognition, 605\nGibbs distribution, 181, 763\nGibbs sampler, 765\nGimbal lock, 41\nGist (of a scene), 709, 714\nGlobal illumination, 67\nGlobal optimization, 174",
  "963": "Index\n941\nGPU algorithms, 789\nGradient\nlocation-orientation\nhistogram\n(GLOH), 223\nGraduated non-convexity (GNC), 179\nGraph cuts\nMRF inference, 183, 770\nnormalized cuts, 296\nGraph-based segmentation, 286\nGrassﬁre transform, 130, 248, 455\nGround control points, 350, 429\nHammersley–Clifford theorem, 181, 763\nHann window, 138\nHarris corner detector, see Feature detection\nHead tracking, 551\nactive appearance model (AAM), 680\nHelmholtz reciprocity, 62\nHessian, 177, 213, 313, 315, 320, 394, 399,\n743\neigenvalues, 397\nimage, 394, 411\ninverse, 320, 397, 401\nlocal, 410\npatch-based, 400\nrank-deﬁcient, 370\nreduced motion, 366\nsparse, 366, 379, 747\nHeteroscedastic, 313, 746\nHidden Markov model (HMM), 642\nHierarchical motion estimation, 387\nHigh dynamic range (HDR) imaging, 479\nformats, 486\ntone mapping, 487\nHighest conﬁdence ﬁrst, 765\nHighest conﬁdence ﬁrst (HCF), 182\nHilbert transform pair, 120\nHistogram equalization, 107, 196\nlocally adaptive, 109, 196\nHistogram intersection, 698\nHistogram of oriented gradients (HOG), 666\nHistory of computer vision, 10\nHole ﬁlling, 521\nHomogeneous coordinates, 32, 347\nHomography, 37, 56, 431\nHough transform, 251, 264\ncascaded, 253\ncube map, 253\ngeneralized, 251\nHuman body shape modeling, 609\nHuman motion tracking, 605\nactivity recognition, 610\nadaptive shape modeling, 609\nbackground subtraction, 606\nﬂow-based, 607\ninitialization, 607\nkinematic models, 607\nparticle ﬁltering, 608\nprobabilistic models, 608\nHyper-Laplacian, 179, 184, 186\nIdeal points, 32\nIll-posed (ill-conditioned) problems, 175\nIllusions, 3\nImage alignment\nfeature-based, 311, 543\nintensity-based, 384\nintensity-based vs. feature-based, 450\nImage analogies, 522\nImage blending\nfeathering, 455\nGIST, 461\ngradient domain, 459\nimage stitching, 453\nPoisson, 460\npyramid, 160, 459\nImage compositing, see Compositing",
  "964": "942\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nImage compression, 90\nImage decimation, 148\nImage deconvolution, see Blur removal\nImage ﬁltering, 111\nImage formation\ngeometric, 31\nphotometric, 60\nImage gradient, 119, 127, 392\nconstraint, 177\nImage interpolation, 145\nImage matting, 505, 529\nImage processing, 101\ntextbooks, 101, 192\nImage pyramid, 144, 200\nImage resampling, 163, 199\ntest images, 200\nImage restoration, 144, 192\nblur removal, 144, 197, 199\ndeblocking, 204\ninpainting, 192\nnoise removal, 144, 197, 203\nusing MRFs, 192\nImage search, 717\nImage segmentation, see Segmentation\nImage sensing, see Sensing\nImage statistics, 132\nImage stitching, 427\nautomatic, 446\nbundle adjustment, 441\ncompositing, 450\ncoordinate transformations, 452\ncube map, 451\ncylindrical, 438, 463\nde-ghosting, 446, 456, 464\ndirect vs. feature-based, 450\nexposure compensation, 462\nfeathering, 455\ngap closing, 435\nglobal alignment, 441\nhomography, 431\nmotion models, 430\npanography, 314\nparallax removal, 445\nphotogrammetry, 429\npixel selection, 453\nplanar perspective motion, 431\nrecognizing panoramas, 446\nrotational motion, 433\nseam selection, 456\nspherical, 439\nup vector selection, 444\nImage warping, 163, 201, 388\nImage-based modeling, 623\nImage-based rendering, 619\nconcentric mosaic, 634\nenvironment matte, 634\nimpostors, 626\nlayered depth image, 626\nlayers, 626\nlight ﬁeld, 628\nLumigraph, 628\nmodeling vs. rendering continuum, 637\nsprites, 626\nsurface light ﬁeld, 632\nunstructured Lumigraph, 632\nview interpolation, 621\nview-dependent texture maps, 623\nImage-based visual hull, 569\nImageNet, 716\nImplicit surface, 596\nImpostors, see Sprites\nImpulse response, 112\nIncremental reﬁnement\nmotion estimation, 388, 392\nIncremental rotation, 45\nIndexing structure, 232",
  "965": "Index\n943\nIndicator function, 596\nIndustrial applications, 7\nInﬁnite impulse response (IIR) ﬁlter, 122\nInﬂuence function, 179, 318, 761\nInformation matrix, 313, 320, 370, 758, 775\nInpainting, 521\nInstance recognition, 685\nalgorithm, 690\ndata sets, 718\ngeometric alignment, 686\ninverted index, 687\nlarge scale, 687\nmatch veriﬁcation, 686\nquery expansion, 692\nstop list, 689\nvisual words, 688\nvocabulary tree, 691\nIntegrability constraint, 581\nIntegral image, 120\nIntegrating sphere, 472\nIntelligent scissors, 280\nInteraction potential, 181, 763, 768\nInteractive computer vision, 614\nInternational Color Consortium (ICC), 473\nInternet photos, 371\nInterpolation, 145\nInterpolation kernels\nbicubic, 146\nbilinear, 145\nbinomial, 145\nsinc, 148\nspline, 148\nIntrinsic camera calibration, 327\nIntrinsic images, 12\nInverse kinematics (IK), 607\nInverse mapping, see Inverse warping\nInverse problems, 3, 175\nInverse warping, 165\nISO setting, 76\nIterated closest point (ICP), 272, 321, 588\nIterated conditional modes (ICM), 182, 765\nIterative back projection (IBP), 499\nIterative feature-based alignment, 315\nIterative sparse matrix techniques, 748\nconjugate gradient, 749\nIteratively reweighted least squares\n(IRLS), 318, 324, 398, 761\nJacobian, 312, 325, 364, 392, 746\nimage, 394\nmotion, 399\nsparse, 366, 379, 747\nJoint bilateral ﬁlter, 496\nJoint domain (feature space), 294\nK-d trees, 233\nK-means, 289\nKalman snakes, 276\nKanade–Lucas–Tomasi (KLT) tracker, 235\nKarhunen–Lo`eve transform, 143, 671\nKernel, 117\nbilinear, 117\nGaussian, 117\nlow-pass, 117\nSobel operator, 118\nunsharp mask, 117\nKernel basis function, 176\nKernel density estimation, 292\nKeypoint detection, see Feature detection\nKinematic model (chain), 607\nKruppa equations, 356\nL*a*b*, see Color\nL*u*v*, see Color\nL1 norm, 179, 385, 411, 597\nL∞norm, 367\nLambertian reﬂection, 63",
  "966": "944\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nLaplacian matting, 515\nLaplacian of Gaussian (LoG) ﬁlter, 119\nLaplacian pyramid, 151\nblending, 160, 200, 459\nperfect reconstruction, 151\nLatent Dirichlet process (LDP), 713\nLayered depth image (LDI), 626\nLayered depth panorama, 634\nLayered motion estimation, 415\ntransparent, 419\nLayers\nimage-based rendering, 626\nLayout consistent random ﬁeld, 708\nLearning in computer vision, 714\nLeast median of squares (LMS), 318\nLeast squares\niterative solvers, 324, 748\nlinear, 94, 312, 320, 384, 738, 742, 756,\n760, 786\nnon-linear, 315, 324, 347, 746, 760, 787\nrobust, see Robust least squares\nsparse, 364, 748, 787\ntotal, 744\nweighted, 313, 494, 498, 505\nLens\ncompound, 71\nnodal point, 71\nthin, 69\nLens distortions, 58\ncalibration, 334\ndecentering, 59\nradial, 58\nspline-based, 59\ntangential, 59\nLens law, 69\nLevel of detail (LOD), 594\nLevel sets, 281, 282\nfast marching method, 282\ngeodesic active contour, 282\nLevenberg–Marquardt, 316, 371, 379, 747,\n783\nLifting, see Wavelets\nLight ﬁeld\nhigher dimensional, 636\nlight slab, 629\nray space, 631\nrendering, 628\nsurface, 632\nLightness, 84\nLine at inﬁnity, 32\nLine detection, 250\nHough transform, 251, 264\nRANSAC, 254\nsimpliﬁcation, 250, 264\nsuccessive approximation, 251, 264\nLine equation, 32, 34\nLine ﬁtting, 94, 264\nuncertainty, 265\nLine hull, see Visual hull\nLine labeling, 11\nLine process, 194, 553, 764\nLine spread function (LSF), 476\nLine-based structure from motion, 374\nLinear algebra, 735\nleast squares, 742\nmatrix decompositions, 736\nreferences, 736\nLinear blend, 104\nLinear discriminant analysis (LDA), 676\nLinear ﬁltering, 111\nLinear operator, 104\nsuperposition, 104\nLinear shift invariant (LSI) ﬁlter, 112\nLive-wire, 280\nLocal distance functions, 679\nLocal operator, 111",
  "967": "Index\n945\nLocality sensitive hashing (LSH), 233\nLocally adaptive histogram equalization, 109\nLocation recognition, 693\nLoopy belief propagation (LBP), 185, 769\nLow-pass ﬁlter, 117\nsinc, 117\nLumigraph, 628\nunstructured, 632\nLuminance, 82\nLumisphere, 633\nM-estimator, 318, 384, 761\nMahalanobis distance, 291, 673, 677, 758\nManifold mosaic, 455, 649\nMarkov chain Monte Carlo (MCMC), 760,\n765\nMarkov random ﬁeld, 180, 763\ncliques, 181, 764\ndirected edges, 302\ndynamic, 772\nﬂux, 302\ninference, see MRF inference\nlayout consistent, 708\nlearning parameters, 180\nline process, 194, 553, 764\nneighborhood, 181, 763\norder, 181, 765\nrandom walker, 303\nstereo matching, 553\nMarr’s framework, 13\ncomputational theory, 13\nhardware implementation, 13\nrepresentations and algorithms, 13\nMatch move, 368\nMatrix decompositions, 736\nCholesky, 741\neigenvalue (ED), 737\nQR, 740\nsingular value (SVD), 736\nsquare root, 741\nMatte reﬂection, 63\nMatting, 105, 106, 505, 529\nalpha matte, 105\nBayesian, 510\nblue screen, 106, 195, 507\ndifference, 106, 195, 508, 606\nﬂash, 517\nGrabCut, 513\nLaplacian, 514\nnatural, 509\noptimization-based, 513\nPoisson, 513\nshadow, 517\nsmoke, 516\ntriangulation, 507, 518\ntrimap, 509\ntwo screen, 507\nvideo, 518\nMaximally stable extremal region (MSER),\n220\nMaximum a posteriori (MAP) estimate, 180,\n763\nMean absolute difference (MAD), 547\nMean average precision, 229\nMean shift, 289, 292\nbandwidth selection, 295\nMean square error (MSE), 92, 547\nMeasurement equation (model), 346, 757\nMeasurement matrix, 359\nMeasurement model, see Bayesian model\nMedial axis transform (MAT), 130\nMedian absolute deviation (MAD), 385\nMedian ﬁlter, 124\nweighted, 124\nMedical image registration, 408\nMedical image segmentation, 304",
  "968": "946\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nMembrane, 175\nMesh-based warping, 170, 201\nMetamer, 82\nMetric learning, 679\nMetric tree, 234\nMIP-mapping, 167\ntrilinear, 168\nMixture of Gaussians, 272, 279, 289\ncolor model, 509\nexpectation maximization (EM), 291\nmixing coefﬁcient, 291\nsoft assignment, 291\nModel selection, 430, 763\nModel-based reconstruction, 598\narchitecture, 598\nheads and faces, 601\nhuman body, 605\nModel-based stereo, 599, 624\nModels\nBayesian, 180, 762\nforward, 3\nphysically based, 14\nphysics-based, 3\nprobabilistic, 3\nModular eigenspace, 678\nModulation transfer function (MTF), 79, 476\nMorphable model\nbody, 609\nface, 603, 639\nmultidimensional, 639\nMorphing, 173, 202, 622, 623\n3D body, 609\n3D face, 603\nautomated, 424\nfacial feature, 639\nfeature-based, 173, 202\nﬂow-based, 424\nvideo textures, 642\nview morphing, 623, 650\nMorphological operator, 127\nclosing, 128\ndilation, 128\nerosion, 128\nopening, 128\nMorphology, 127\nMosaic, see Image stitching\nMosaics\nmotion models, 430\nvideo compression, 436\nwhiteboard and document scanning, 432\nMotion compensated video compression,\n387, 421\nMotion compensation, 92\nMotion estimation, 383\nafﬁne, 398\naperture problem, 394\ncompositional, 400\nFourier-based, 388\nframe interpolation, 418\nhierarchical, 387\nincremental reﬁnement, 392\nlayered, 415\nlearning, 403, 411\nlinear appearance variation, 397\noptical ﬂow, 409\nparametric, 398\npatch-based, 384, 399\nphase correlation, 390\nquadtree spline-based, 407\nreﬂections, 419\nspline-based, 404\ntranslational, 384\ntransparent, 419\nuncertainty modeling, 395\nMotion ﬁeld, 398\nMotion models",
  "969": "Index\n947\nlearned, 403\nMotion segmentation, 425\nMotion stereo, 561\nMotion-based user interaction, 425\nMoving least squares (MLS), 596\nMRF inference, 182, 765\nalpha expansion, 185, 772\nbelief propagation, 185, 768\ndynamic programming, 766\nexpansion move, 185, 772\ngradient descent, 765\ngraph cuts, 183, 770\nhighest conﬁdence ﬁrst, 182\nhighest conﬁdence ﬁrst (HCF), 765\niterated conditional modes, 182, 765\nlinear programming (LP), 773\nloopy belief propagation, 185, 769\nMarkov chain Monte Carlo, 765\nsimulated annealing, 182, 766\nstochastic gradient descent, 182, 765\nswap move (alpha-beta), 185, 772\nSwendsen–Wang, 766\nMulti-frame motion estimation, 413\nMulti-pass transforms, 169\nMulti-perspective panoramas, 437\nMulti-perspective plane sweep (MPPS), 445\nMulti-view stereo, 558\nepipolar plane image, 559\nevaluation, 567\ninitialization requirements, 566\nreconstruction algorithm, 565\nscene representation, 563\nshape priors, 565\nsilhouettes, 567\nspace carving, 566\nspatio-temporally shiftable window, 560\ntaxonomy, 563\nvisibility, 565\nvolumetric, 562\nvoxel coloring, 566\nMultigrid, 753\nalgebraic (AMG), 288, 753\nMultiple hypothesis tracking, 279\nMultiple-center-of-projection images, 437,\n649\nMultiresolution representation, 150\nMutual information, 386, 408\nNatural image matting, 509\nNearest neighbor\ndistance ratio (NNDR), 230\nmatching, see Feature matching\nNegative posterior log likelihood, 180, 758,\n762\nNeighborhood operator, 111, 122\nNeural networks, 661\nNintendo Wii, 326\nNodal point, 71\nNoise\nsensor, 76, 473\nNoise level function (NLF), 76, 96, 473, 527\nNoise removal, 144, 197, 203\nNon-linear ﬁlter, 122, 193\nNon-linear least squares\nseeLeast squares, 315\nNon-maximal suppression, see Feature detec-\ntion\nNon-parametric density modeling, 292\nNon-photorealistic rendering (NPR), 522\nNon-rigid motion, 377\nNormal equations, 313, 393, 743, 746\nNormal map (geometry image), 594\nNormal vector, 34\nNormalized cross-correlation (NCC), 386,\n422, 547\nNormalized cuts, 296",
  "970": "948\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nintervening contour, 298\nNormalized device coordinates (NDC), 49,\n54\nNormalized sum of squared differences\n(NSSD), 387\nNorms\nL1, 179, 385, 411, 597\nL∞, 367\nNyquist rate / frequency, 78\nObject detection, 658\ncar, 666, 722\nface, 658\npart-based, 667\npedestrian, 666, 684\nObject-centered projection, 57\nOccluding contours, 543\nOctree reconstruction, 569\nOctree spline, 409\nOmnidirectional vision systems, 646\nOpacity, 106\nOperator\nlinearity, 104\nOptic ﬂow, see Optical ﬂow\nOptical center, 52\nOptical ﬂow, 409\nanisotropic smoothness, 411\nevaluation, 413\nfusion move, 413\nglobal and local, 410\nMarkov random ﬁeld, 411\nmulti-frame, 413\nnormal ﬂow, 394\npatch-based, 409\nregion-based, 417\nregularization, 410\nrobust regularization, 411\nsmoothness, 410\ntotal variation, 411\nOptical ﬂow constraint equation, 393\nOptical illusions, 3\nOptical transfer function (OTF), 79, 476\nOptical triangulation, 586\nOptics, 68\nchromatic aberration, 71\nSeidel aberrations, 70\nvignetting, 72, 527\nOptimal motion estimation, 363\nOriented particles (points), 595\nOrthogonal Procrustes, 320\nOrthographic projection, 46\nOsculating circle, 544\nOver operator, 106\nOverview, 19\nPadding, 114, 196\nPanography, 314, 337\nPanorama, see Image stitching\nPanorama with depth, 438, 542, 634\nPara-perspective projection, 48\nParallel tracking and mapping (PTAM), 369\nParameter sensitive hashing, 233\nParametric motion estimation, 398\nParametric surface, 593\nParametric transformation, 163, 201\nParseval’s Theorem, see Fourier transform\nPart-based recognition, 701\nconstellation model, 704\nParticle ﬁltering, 279, 608, 760\nParzen window, 292\nPASCAL Visual Object Classes Challenge\n(VOC), 718\nPatch-based motion estimation, 384\nPeak signal-to-noise Ratio (PSNR), 92, 144\nPedestrian detection, 666\nPenumbra, 60",
  "971": "Index\n949\nPerformance-driven animation, 237, 605, 639\nPerspective n-point problem (PnP), 322\nPerspective projection, 48\nPerspective transform (2D), 37\nPhase correlation, 390, 422\nPhong shading, 65\nPhoto pop-up, 710\nPhoto Tourism, 624\nPhoto-mosaic, 429\nPhotoconsistency, 540, 565\nPhotometric image formation, 60\ncalibration, 470\nglobal illumination, 67\nlighting, 60\noptics, 68\nradiosity, 67\nreﬂectance, 62\nshading, 65\nPhotometric stereo, 582\nPhotometry, 60\nPhotomontage, 459\nPhysically based models, 14\nPhysics-based vision, 16\nPictorial structures, 12, 19, 701\nPixel transform, 103\nPl¨ucker coordinates, 35\nPlanar pattern tracking, 326\nPlane at inﬁnity, 34\nPlane equation, 33\nPlane plus parallax, 55, 405, 417, 540, 626\nPlane sweep, 540, 572\nPlane-based structure from motion, 376\nPlenoptic function, 628\nPlenoptic modeling, 623\nPlumb-line calibration method, 335, 341\nPoint distribution model, 275\nPoint operator, 101\nPoint process, 101\nPoint spread function (PSF), 78\nestimation, 476, 528\nPoint-based representations, 595\nPoints at inﬁnity, 32\nPoisson\nblending, 460\nequations, 597\nmatting, 513\nnoise, 76\nsurface reconstruction, 597\nPolar coordinates, 33\nPolar projection, 59, 440\nPolyphase ﬁlter, 145\nPop-out effect, 4\nPose estimation, 321\niterative, 324\nPower spectrum, 140\nPrecision, see Error rates\nmean average, 229\nPreconditioning, 751\nPrincipal component analysis (PCA), 275,\n660, 671, 738, 758\nface modeling, 601\ngeneralized, 740\nmissing data, 360, 740\nPrior energy (term), 181, 763\nPrior model, see Bayesian model\nProﬁle curves, 543\nProgressive mesh (PM), 594\nProjections\nobject-centered, 57\northographic, 46\npara-perspective, 48\nperspective, 48\nProjective (uncalibrated) reconstruction, 353\nProjective depth, 55, 540\nProjective disparity, 55, 540\nProjective space, 32",
  "972": "950\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nPROSAC (PROgressive SAmple Consensus),\n319\nPSNR, see Peak signal-to-noise ratio\nPyramid, 144, 200\nblending, 160, 200\nGaussian, 150\nhalf-octave, 152\nLaplacian, 151\nmotion estimation, 387\noctave, 150\nradial frequency implementation, 159\nsteerable, 159\nPyramid match kernel, 698\nQR factorization, 740\nQuadratic form, 177\nQuadrature mirror ﬁlter (QMF), 150\nQuadric equation, 33, 35\nQuadtree spline\nmotion estimation, 407\nrestricted, 407\nQuaternions, 43\nantipodal, 43\nmultiplication, 44\nQuery by image content (QBIC), 717\nQuery expansion, 692\nQuincunx sampling, 152\nRadial basis function, 171, 176, 592\nRadial distortion, 58\nbarrel, 58\ncalibration, 334\nparameters, 58\npincushion, 58\nRadiance map, 483\nRadiometric image formation, 60\nRadiometric response function, 470\nRadiometry, 60\nRadiosity, 68\nRandom walker, 303, 771\nRange (of a function), 103\nRange data, see Range scan\nRange image, see Range scan\nRange scan\nalignment, 588, 617\nlarge scenes, 590\nmerging, 589\nregistration, 588, 617\nsegmentation, 588\nvolumetric, 590\nRange sensing (rangeﬁnding), 585\ncoded pattern, 587\nlight stripe, 586\nshadow stripe, 586, 616\nspacetime stereo, 587\nstereo, 587\ntexture pattern (checkerboard), 587\ntime of ﬂight, 587\nRANSAC\n(RAndom SAmple Consensus), 318\ninliers, 319\npreemptive, 319\nprogressive (PROSAC), 319\nRAW image format, 77\nRay space (light ﬁeld), 631\nRay tracing, 68\nRayleigh quotient, 297\nRecall, see Error rates\nReceiver Operating Characteristic\narea under the curve (AUC), 229\nmean average precision, 229\nROC curve, 229, 260\nRecognition, 655\n3D models, 725\ncategory (class), 696\ncolor similarity, 717\ncontext, 712",
  "973": "Index\n951\ncontour-based, 724\ndata sets, 718\nface, 668\ninstance, 685\nlarge scale, 715\nlearning, 714\npart-based, 701\nscene understanding, 712\nsegmentation, 704\nshape context, 724\nRectangle detection, 257\nRectiﬁcation, 538, 571\nstandard rectiﬁed geometry, 539\nRecursive ﬁlter, 122\nReference plane, 55\nReﬂectance, 62\nReﬂectance map, 580\nReﬂectance modeling, 611\nReﬂection\ndi-chromatic, 67\ndiffuse, 63\nspecular, 64\nRegion\nmerging, 286\nsplitting, 286\nRegion segmentation, see Segmentation\nRegistration, see Image Alignment\nfeature-based, 311\nintensity-based, 384\nmedical image, 408\nRegularization, 174, 407\nrobust, 178\nRegularization parameter, 176\nResidual error, 312, 318, 346, 363, 384, 393,\n399, 410, 411, 742, 750\nRGB (red green blue), see Color\nRigid body transformation, 36, 40\nRobust error metric, see Robust penalty func-\ntion\nRobust least squares, 255, 256, 318, 384, 761\niteratively reweighted, 318, 324, 398,\n761\nRobust penalty function, 178, 384, 397, 499,\n542, 547, 548, 553, 761\nRobust regularization, 178\nRobust statistics, 385, 760\ninliers, 319\nM-estimator, 318, 384, 761\nRodriguez’s formula, 42\nRoot mean square error (RMS), 92, 385\nRotations, 41\nEuler angles, 41\naxis/angle, 41\nexponential twist, 43\nincremental, 45\ninterpolation, 45\nquaternions, 43\nRodriguez’s formula, 42\nSampling, 77\nScale invariant feature transform (SIFT), 223\nScale-space, 13, 119, 152, 282\nScatter matrix, 671\nbetween-class, 675\nwithin-class, 674\nScattered data interpolation, 171, 592\nScene completion, 709\nScene ﬂow, 562, 644\nScene understanding, 712\ngist, 709, 714\nscene alignment, 715\nSchur complement, 366, 748\nScratch removal, 521\nSeam selection\nimage stitching, 456",
  "974": "952\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nSecond-order cone programming (SOCP),\n367\nSeed and grow\nstereo, 543\nstructure from motion, 371\nSegmentation, 267\nactive contours, 270\nafﬁnities, 296\nbinary MRF, 182, 300\nCONDENSATION, 279\nconnected components, 131, 198\nenergy-based, 300\nfor recognition, 704\ngeodesic active contour, 282\ngeodesic distance, 304\nGrabCut, 301, 513\ngraph cuts, 300\ngraph-based, 286\nhierarchical, 285, 288\nintelligent scissors, 280\njoint feature space, 294\nk-means, 289\nlevel sets, 281\nmean shift, 289, 292\nmedical image, 304\nmerging, 286\nminimum description length (MDL),\n300\nmixture of Gaussians, 289\nMumford–Shah, 300\nnon-parametric, 292\nnormalized cuts, 296\nprobabilistic aggregation, 288\nrandom walker, 303\nsnakes, 270\nsplitting, 286\nstereo matching, 556\nthresholding, 127\ntobogganing, 281, 285\nwatershed, 284\nweighted aggregation (SWA), 300\nSeidel aberrations, 70\nSelf-calibration, 355\nbundle adjustment, 357\nKruppa equations, 356\nSensing, 73\naliasing, 77, 476\ncolor, 80\ncolor balance, 86\ngamma, 87\npipeline, 74, 471\nsampling, 77\nsampling pitch, 75\nSensor noise, 76, 473\nampliﬁer, 76\ndark current, 76\nﬁxed pattern, 76\nshot noise, 76\nSeparable ﬁltering, 115, 197\nShading, 65\nequation, 64\nshape-from, 580\nShadow matting, 517\nShape context, 249, 724\nShape from\nfocus, 584, 616\nphotometric stereo, 582\nproﬁles, 543\nshading, 580\nsilhouettes, 567\nspecularities, 584\nstereo, 533\ntexture, 583\nShape parameters, 275, 681\nShape-from-X, 14\nfocus, 14",
  "975": "Index\n953\nphotometric stereo, 14\nshading, 14\ntexture, 14\nShift invariance, 112\nShiftable multi-scale transform, 159\nShutter speed, 75\nSigned distance function, 281, 589, 595, 597\nSilhouette-based reconstruction, 567\noctree, 569\nvisual hull, 567\nSimilarity transform, 36, 40\nSimulated annealing, 182, 766\nSimultaneous\nlocalization\nand\nmapping\n(SLAM), 368\nSinc ﬁlter\ninterpolation, 148\nlow-pass, 117\nwindowed, 148\nSingle view metrology, 331, 340\nSingular value decomposition (SVD), 736\nSkeletal set, 367, 372\nSkeleton, 130, 248\nSkew, 50, 52\nSkin color detection, 96\nSlant edge calibration, 476\nSlippery spring, 273\nSmoke matting, 516\nSmoothness constraint, 176\nSmoothness penalty, 176\nSnakes, 270\nballooning, 271\ndynamic, 276\ninternal energy, 270\nKalman, 276\nshape priors, 274\nslippery spring, 273\nSoft assignment, 291\nSoftware, 780\nSpace carving\nmulti-view stereo, 566\nSpacetime stereo, 587\nSparse ﬂexible model, 703\nSparse matrices, 747, 787\ncompressed sparse row (CSR), 747\nskyline storage, 747\nSparse methods\ndirect, 747, 787\niterative, 748, 787\nSpatial pyramid matching, 699\nSpectral response function, 85\nSpectral sensitivity, 85\nSpecular ﬂow, 584\nSpecular reﬂection, 64\nSpherical coordinates, 34, 253, 256, 439\nSpherical linear interpolation, 45\nSpin image, 589\nSplatting, see Forward warping\nvolumetric, 595\nSpline\ncontrolled continuity, 175\noctree, 409\nquadtree, 407\nthin plate, 175\nSpline-based motion estimation, 404\nSplining images,\nsee Laplacian pyramid\nblending\nSprites\nimage-based rendering, 626\nmotion estimation, 415\nvideo, 642\nvideo compression, 436\nwith depth, 627\nStatistical decision theory, 757, 760\nSteerable ﬁlter, 119, 198\nSteerable pyramid, 159\nSteerable random ﬁeld, 184",
  "976": "954\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nStereo, 533\naggregation methods, 549, 573\ncoarse-to-ﬁne, 554\ncooperative algorithms, 554\ncorrespondence, 535\ncurve-based, 543\ndense correspondence, 545\ndepth map, 535\ndynamic programming, 554\nedge-based, 543\nepipolar geometry, 537\nfeature-based, 543\nglobal optimization, 552, 573\ngraph cut, 553\nlayers, 558\nlocal methods, 548\nmodel-based, 599, 624\nmulti-view, 558\nnon-parametric similarity measures, 547\nphotoconsistency, 540\nplane sweep, 540, 572\nrectiﬁcation, 538, 571\nregion-based, 548\nscanline optimization, 556\nseed and grow, 543\nsegmentation-based, 548, 556\nsemi-global optimization, 556\nshiftable window, 560\nsimilarity measure, 546\nspacetime, 587\nsparse correspondence, 543\nsub-pixel reﬁnement, 550\nsupport region, 548\ntaxonomy, 535, 545\nuncertainty, 551\nwindow-based, 548, 573\nwinner-take-all (WTA), 550\nStereo-based head tracking, 551\nStiffness matrix, 177\nStitching, see Image stitching\nStochastic gradient descent, 182\nStructural Similarity (SSIM) index, 144\nStructure from motion, 345\nafﬁne, 359\nbas-relief ambiguity, 370\nbundle adjustment, 363\nconstrained, 374\nfactorization, 357\nfeature tracks, 371\niterative factorization, 360\nline-based, 374\nmulti-frame, 357\nnon-rigid, 377\northographic, 357\nplane-based, 362, 376\nprojective factorization, 360\nseed and grow, 371\nself-calibration, 355\nskeletal set, 367, 372\ntwo-frame, 347\nuncertainty, 370\nSubdivision surface, 593\nsubdivision connectivity, 594\nSubspace learning, 679\nSum of absolute differences (SAD), 384, 422,\n547\nSum of squared differences (SSD), 384, 422,\n547\nbias and gain, 386\nFourier-based computation, 389\nnormalized, 387\nsurface, 210, 396\nweighted, 385\nwindowed, 385\nSum of sum of squared differences (SSSD),\n559",
  "977": "Index\n955\nSummed area table, 120\nSuper-resolution, 497, 529\nexample-based, 499\nfaces, 501\nhallucination, 499\nprior, 499\nSuperposition principle, 104\nSuperquadric, 597\nSupport vector machine (SVM), 662, 667\nSurface element (surfel), 595\nSurface interpolation, 592\nSurface light ﬁeld, 632\nSurface representations, 591\nnon-parametric, 593\nparametric, 593\npoint-based, 595\nsimpliﬁcation, 594\nsplines, 593\nsubdivision surface, 593\nsymmetry-seeking, 593\ntriangle mesh, 593\nSurface simpliﬁcation, 594\nSwendsen–Wang algorithm, 766\nTelecentric lens, 48, 585\nTemporal derivative, 393, 410\nTemporal texture, 642\nTerm frequency-inverse document frequency\n(TF-IDF), 689\nTesting algorithms, viii\nTextonBoost, 706\nTexture\nshape-from, 583\nTexture addressing mode, 115\nTexture map\nrecovery, 610\nview-dependent, 611, 623\nTexture mapping\nanisotropic ﬁltering, 168\nMIP-mapping, 167\nmulti-pass, 169\ntrilinear interpolation, 168\nTexture synthesis, 518, 531\nby numbers, 523\nhole ﬁlling, 521\nimage quilting, 519\nnon-parametric, 519\ntransfer, 522\nThin lens, 69\nThin-plate spline, 175\nThresholding, 127\nThrough-the-lens camera control, 326, 368\nTobogganing, 281, 285\nTonal adjustment, 111, 196\nTone mapping, 487\nadaptive, 488\nbilateral ﬁlter, 489\nglobal, 487\ngradient domain, 489\nhalos, 489\ninteractive, 493\nlocal, 488\nscale selection, 492\nTotal least squares (TLS), 265, 398, 744\nTotal variation, 179, 411, 597\nTracking\nfeature, 235\nhead, 551\nhuman motion, 605\nmultiple hypothesis, 279\nplanar pattern, 326\nPTAM, 369\nTranslational motion estimation, 384\nbias and gain, 386\nTransparency, 106\nTravelling salesman problem (TSP), 272",
  "978": "956\nComputer Vision: Algorithms and Applications (September 3, 2010 draft)\nTri-chromatic sensing, 81\nTri-stimulus values, 81, 85\nTriangulation, 345\nTrilinear interpolation, see MIP-mapping\nTrimap (matting), 509\nTrust region method, 747\nTwo-dimensional Fourier transform, 140\nUncanny valley, 3\nUncertainty\ncorrespondence, 313\nmodeling, 319, 775\nweighting, 313\nUnsharp mask, 117\nUpsampling, see Interpolation\nVanishing point\ndetection, 254, 266\nHough, 255\nleast squares, 256\nmodeling, 599\nuncertainty, 266\nVariable reordering, 748\nminimum degree, 748\nmulti-frontal, 748\nnested dissection, 748\nVariable state dimension ﬁlter (VSDF), 367\nVariational method, 175\nVideo compression\nmotion compensated, 387\nVideo compression (coding), 421\nVideo denoising, 414\nVideo matting, 518\nVideo objects (coding), 415\nVideo sprites, 642\nVideo stabilization, 401, 423\nVideo texture, 640\nVideo-based animation, 639\nVideo-based rendering, 638\n3D video, 643\nanimating pictures, 643\nsprites, 642\nvideo texture, 640\nvirtual viewpoint video, 644\nwalkthroughs, 645\nVideoMouse, 326\nView correlation, 368\nView interpolation, 357, 621, 650\nView morphing, 357, 623, 642\nView-based eigenspace, 678\nView-dependent texture maps, 623\nVignetting, 72, 386, 474, 527\nmechanical, 73\nnatural, 72\nVirtual viewpoint video, 644\nVisual hull, 567\nimage-based, 569\nVisual illusions, 3\nVisual odometry, 368\nVisual words, 234, 688, 697\nVocabulary tree, 234, 691\nVolumetric 3D reconstruction, 562\nVolumetric range image processing (VRIP),\n589\nVolumetric representations, 596\nVoronoi diagram, 455\nVoxel coloring\nmulti-view stereo, 566\nWatershed, 284, 292\nbasins, 284, 292\noriented, 285\nWavelets, 154, 201\ncompression, 201\nlifting, 156\novercomplete, 155, 159\nsecond generation, 158",
  "979": "Index\n957\nself-inverting, 159\ntight frame, 155\nweighted, 158\nWeaving wall, 544\nWeighted least squares (WLS), 493, 505\nWeighted prediction (bias and gain), 386\nWhite balance, 86, 97\nWhitening transform, 673\nWiener ﬁlter, 140, 142, 198\nWire removal, 521\nWrapping mode, 115\nXYZ, see Color\nZippering, 589"
}