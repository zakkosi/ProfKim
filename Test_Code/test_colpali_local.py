import os
import glob
from pdf2image import convert_from_path
from PIL import Image
import torch
from transformers.utils.import_utils import is_flash_attn_2_available
from colpali_engine.models import ColQwen2, ColQwen2Processor
from byaldi import RAGMultiModalModel

# ------------------------
# 1. ëª¨ë¸ ë¡œë”©
# ------------------------
print("ëª¨ë¸ ë¡œë”© ì¤‘...")

# ì§ˆì˜ì‘ë‹µìš© ColQwen2
llm_model = ColQwen2.from_pretrained(
    "vidore/colqwen2-v1.0",
    torch_dtype=torch.bfloat16,
    device_map="cuda:0",
    attn_implementation="flash_attention_2" if is_flash_attn_2_available() else None,
).eval()

processor = ColQwen2Processor.from_pretrained("vidore/colqwen2-v1.0")

# ê²€ìƒ‰ìš© RAG ì¸ë±ì„œ
rag_model = RAGMultiModalModel.from_pretrained("vidore/colqwen2-v1.0")

# ------------------------
# 2. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
# ------------------------
pdf_path = "test_materials/example.pdf"
image_dir = "converted_pages"
os.makedirs(image_dir, exist_ok=True)

image_paths = sorted(glob.glob(f"{image_dir}/page_*.jpg"))
if not image_paths:
    print("PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...")
    images = convert_from_path(pdf_path)
    for idx, img in enumerate(images):
        save_path = f"{image_dir}/page_{idx}.jpg"
        img.save(save_path, "JPEG")
        image_paths.append(save_path)
    print(f"{len(image_paths)}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ")
else:
    print("ê¸°ì¡´ ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•¨. ë³€í™˜ ê±´ë„ˆëœ€.")

# ------------------------
# 3. RAG ì¸ë±ì‹±
# ------------------------
print("ğŸ” ì´ë¯¸ì§€ ì¸ë±ì‹± ì¤‘...")
rag_model.index("converted_pages", index_name="default", overwrite=True)

# ------------------------
# 4. ì§ˆì˜ ë° ê²€ìƒ‰
# ------------------------
question = "ì´ ë¬¸ì„œì—ì„œ ìº¥ê±°ë£¨ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?"
print(f"\nì§ˆì˜: {question}")

top_docs = rag_model.search(question) 
print(top_docs)
retrieved_images = [
    Image.open(f"converted_pages/page_{doc['doc_id']}.jpg").convert("RGB")
    for doc in top_docs
]
image_batch = processor.process_images(retrieved_images).to(llm_model.device)
query_batch = processor.process_queries([question]).to(llm_model.device)

# ------------------------
# 5. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
# ------------------------
with torch.no_grad():
    image_embeds = llm_model(**image_batch)
    query_embeds = llm_model(**query_batch)

answer = processor.score_multi_vector(query_embeds, image_embeds)

print("\nâœ… ì‘ë‹µ ê²°ê³¼:")
print(answer)

scores = answer[0].tolist()  # í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

print("\n ìœ ì‚¬í•œ í˜ì´ì§€ ë° ì ìˆ˜:")
for doc, score in zip(top_docs, scores):
    page = doc['doc_id']  # doc_idëŠ” ì´ë¯¸ì§€ ë²ˆí˜¸(page_32.jpg ê°™ì€)
    print(f" page_{page}.jpg â†’ ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
